
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Dynamo Deep-Dive &#8212; PyTorch 2.10 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=047068a3" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=8998eb7a"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=940804e7"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'user_guide/torch_compiler/torch.compiler_dynamo_deepdive';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://docs.pytorch.org/docs/pytorch-versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '2.10';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <script src="../../_static/js/runllm-widget.js?v=54a6b3cb"></script>
    <link rel="canonical" href="https://docs.pytorch.org/docs/stable/user_guide/torch_compiler/torch.compiler_dynamo_deepdive.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Writing Graph Transformations on ATen IR" href="torch.compiler_transformations.html" />
    <link rel="prev" title="Advanced" href="advanced.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->


<link rel="stylesheet" type="text/css" href="../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="docs">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', '2.10');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/pytorch/pytorch" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/ray/">
                  <span class="dropdown-title">RAY</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/wp-content/uploads/2025/09/pytorch_brand_guide_091925a.pdf">
                  <span class="dropdown-title">Brand Guidelines</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started/locally">Get Started</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/get-started/locally/">
    Install PyTorch
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../index.html">
    User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../pytorch-api.html">
    Reference API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../notes.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://docs.pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/get-started/locally/">
    Install PyTorch
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../index.html">
    User Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../pytorch-api.html">
    Reference API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../notes.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://docs.pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://docs.pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">Pytorch Overview</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/get-started/locally/">Get Started</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Core Concepts</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../pytorch_main_components.html">PyTorch Main Components</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Torch Compile</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="torch.compiler.html">Torch.compile</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_get_started.html">Getting Started</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="core_concepts.html">Core Concepts</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="compile/programming_model.html">torch.compile Programming Model</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="compile/programming_model.dynamo_core_concepts.html">Dynamo Core Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="compile/programming_model.graph_breaks_index.html">Working with Graph Breaks</a></li>
<li class="toctree-l4"><a class="reference internal" href="compile/programming_model.non_strict_tracing_model.html">Non-strict Tracing Programming Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="compile/programming_model.recompilation.html">Dealing with Recompilations</a></li>
<li class="toctree-l4"><a class="reference internal" href="compile/programming_model.observability.html">tlparse / TORCH_TRACE</a></li>
<li class="toctree-l4"><a class="reference internal" href="compile/programming_model.reporting_issues.html">Reporting Issues</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_dynamo_overview.html">Dynamo Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_nn_module.html">PyTorch 2.0 NNModule Support</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_backward.html"><code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> has different autograd semantics</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="performance.html">Performance</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_performance_dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>

<li class="toctree-l3"><a class="reference internal" href="torch.compiler_cudagraph_trees.html">CUDAGraph Trees</a></li>
</ul>
</details></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="advanced.html">Advanced</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Dynamo Deep-Dive</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_transformations.html">Writing Graph Transformations on ATen IR</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_fake_tensor.html">Fake tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_custom_backends.html">Custom Backends</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="torch.compiler_dynamic_shapes.html">Dynamic Shapes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="compile/dynamic_shapes_core_concepts.html">Dynamic Shapes Core Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="compile/dynamic_shapes_troubleshooting.html">Troubleshooting Dynamic Shapes</a></li>
<li class="toctree-l4"><a class="reference internal" href="compile/dynamic_shapes_advanced_control_options.html">Advanced Options to Control Dynamic Behavior</a></li>
<li class="toctree-l4"><a class="reference internal" href="compile/dynamic_shapes_beyond_the_basics.html">Beyond the Basics</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="troubleshooting_faqs.html">Troubleshooting FAQs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="compile/programming_model.observability.html">tlparse / TORCH_TRACE</a></li>
<li class="toctree-l3"><a class="reference internal" href="compile/programming_model.reporting_issues.html">Reporting Issues</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_troubleshooting.html">torch.compile Troubleshooting</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_faq.html">Frequently Asked Questions</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="api_reference.html">Reference/API</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../torch.compiler_api.html">torch.compiler API reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.compile.html">torch.compiler.compile</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.reset.html">torch.compiler.reset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.allow_in_graph.html">torch.compiler.allow_in_graph</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.substitute_in_graph.html">torch.compiler.substitute_in_graph</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.assume_constant_result.html">torch.compiler.assume_constant_result</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.list_backends.html">torch.compiler.list_backends</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.disable.html">torch.compiler.disable</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.set_stance.html">torch.compiler.set_stance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.set_enable_guard_collectives.html">torch.compiler.set_enable_guard_collectives</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.cudagraph_mark_step_begin.html">torch.compiler.cudagraph_mark_step_begin</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.is_compiling.html">torch.compiler.is_compiling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.is_dynamo_compiling.html">torch.compiler.is_dynamo_compiling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.is_exporting.html">torch.compiler.is_exporting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.skip_guard_on_inbuilt_nn_modules_unsafe.html">torch.compiler.skip_guard_on_inbuilt_nn_modules_unsafe</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.skip_guard_on_all_nn_modules_unsafe.html">torch.compiler.skip_guard_on_all_nn_modules_unsafe</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.keep_tensor_guards_unsafe.html">torch.compiler.keep_tensor_guards_unsafe</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.skip_guard_on_globals_unsafe.html">torch.compiler.skip_guard_on_globals_unsafe</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.skip_all_guards_unsafe.html">torch.compiler.skip_all_guards_unsafe</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch.compiler.nested_compile_region.html">torch.compiler.nested_compile_region</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler.config.html">torch.compiler.config</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_fine_grain_apis.html">TorchDynamo APIs for fine-grained tracing</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_inductor_provenance.html">TorchInductor and AOTInductor Provenance Tracking</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="export.html">Torch.export</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="export/api_reference.html">torch.export API Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="export/programming_model.html">torch.export Programming Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="export/ir_spec.html">torch.export IR Specification</a></li>
<li class="toctree-l2"><a class="reference internal" href="export/pt2_archive.html">PT2 Archive Spec</a></li>
<li class="toctree-l2"><a class="reference internal" href="export/draft_export.html">Draft Export</a></li>
<li class="toctree-l2"><a class="reference internal" href="export/joint_with_descriptors.html">Joint with descriptors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cond.html">Control Flow - Cond</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../generated/exportdb/index.html">ExportDB</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../generated/exportdb/torch.escape-hatch.html">torch.escape-hatch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../generated/exportdb/torch.cond.html">torch.cond</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../generated/exportdb/torch.dynamic-shape.html">torch.dynamic-shape</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../generated/exportdb/python.closure.html">python.closure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../generated/exportdb/torch.dynamic-value.html">torch.dynamic-value</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../generated/exportdb/python.data-structure.html">python.data-structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../generated/exportdb/python.assert.html">python.assert</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../generated/exportdb/python.control-flow.html">python.control-flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../generated/exportdb/torch.map.html">torch.map</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../generated/exportdb/python.builtin.html">python.builtin</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../generated/exportdb/python.object-model.html">python.object-model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../generated/exportdb/python.context-manager.html">python.context-manager</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../generated/exportdb/torch.operator.html">torch.operator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../generated/exportdb/torch.mutation.html">torch.mutation</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="torch.compiler_aot_inductor.html">AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../logging.html">torch._logging</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../generated/torch._logging.set_logs.html">torch._logging.set_logs</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_aot_inductor_minifier.html">AOTInductor Minifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.compiler_aot_inductor_debugging_guide.html">AOTInductor Debugging Guide</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_ir.html">IRs</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="torch.compiler_dynamic_shapes.html">Dynamic Shapes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="compile/dynamic_shapes_core_concepts.html">Dynamic Shapes Core Concepts</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="compile/dynamic_shapes_troubleshooting.html">Troubleshooting Dynamic Shapes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="compile/dynamic_shapes_debugging_tlparse_torch_logs.html">Debugging with <code class="docutils literal notranslate"><span class="pre">tlparse</span></code> and <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=dynamic</span></code></a></li>

<li class="toctree-l4"><a class="reference internal" href="compile/dynamic_shapes_troubleshooting_guardon_errors.html">Troubleshooting GuardOnDataDependentSymNode Errors</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="compile/dynamic_shapes_advanced_control_options.html">Advanced Options to Control Dynamic Behavior</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="compile/dynamic_shapes_beyond_the_basics.html">Beyond the Basics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="compile/dynamic_shapes_zero_one_specialization.html">The Zero-One Specialization Problem</a></li>
<li class="toctree-l4"><a class="reference internal" href="compile/dynamic_shapes_backed_unbacked.html">Backed vs Unbacked Symints</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_fake_tensor.html">Fake tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.compiler_transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../notes.html">Developer Notes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/custom_operators.html">PyTorch Custom Operators Landing Page</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/get_start_xpu.html">Getting Started on Intel GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/libtorch_stable_abi.html">LibTorch Stable ABI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/mkldnn.html">MKLDNN backend</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../notes/modules.html">Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/out.html">Out Notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notes/windows.html">Windows FAQ</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Accelerator Integration</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../accelerator/index.html">Accelerator Integration</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../accelerator/device.html">Device Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../accelerator/hooks.html">Accelerator Hooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../accelerator/guard.html">Guard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../accelerator/autoload.html">Autoload Mechanism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../accelerator/operators.html">Operator Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../accelerator/amp.html">Automatic Mixed Precision</a></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">





<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">User Guide</a></li>
    
    
    <li class="breadcrumb-item"><i class="fa-solid fa-ellipsis"></i></li>
    
    
    <li class="breadcrumb-item"><a href="advanced.html" class="nav-link">Advanced</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Dynamo Deep-Dive</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5"></span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../index.html">
        <meta itemprop="name" content="User Guide">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="torch.compiler.html">
        <meta itemprop="name" content="torch.compiler">
        <meta itemprop="position" content="2">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="advanced.html">
        <meta itemprop="name" content="Advanced">
        <meta itemprop="position" content="3">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Dynamo Deep-Dive">
        <meta itemprop="position" content="4">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="dynamo-deep-dive">
<span id="torch-compiler-dynamo-deepdive"></span><h1>Dynamo Deep-Dive<a class="headerlink" href="#dynamo-deep-dive" title="Link to this heading">#</a></h1>
<p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Apr 02, 2024 | Last Updated On: Dec 03, 2025</p>
<p>TorchDynamo (or simply Dynamo) is the tracer within <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>,
and it is, more often than not, the one to blame for those insane
backtraces. However, we cannot blindly blame Dynamo for these errors. In
order to provide the user with the flexibility it does, Dynamo is given
the arduous task of understanding any Python program. In particular,
Dynamo has to implement a good part of the Python programming language
internally!</p>
<p>In this post, we will go over the internal design of Dynamo from the
ground up. We will discuss the functionality it provides, and how it is
implemented. By the end of this post, you will have a better
understanding of what went wrong when you <code class="docutils literal notranslate"><span class="pre">torch.compiled</span></code> a PyTorch
program and the compilation errored out, or succeeded but the speed-up
was not what you expected.</p>
<section id="a-gentle-introduction-to-dynamo">
<h2>A Gentle Introduction to Dynamo<a class="headerlink" href="#a-gentle-introduction-to-dynamo" title="Link to this heading">#</a></h2>
<p>Before getting our hands dirty with all the implementation details,
lets start by discussing what it is that Dynamo does.</p>
<p>Dynamo is a tracer. This means, given and function and inputs to it, it
executes the function and records a linear sequence of instructions
(without control flow) into a graph. For example, consider the following
program:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">z</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">mse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>If we save this program into the file <code class="docutils literal notranslate"><span class="pre">example.py</span></code> and we run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">TORCH_LOGS</span><span class="o">=</span>graph_code<span class="w"> </span>python<span class="w"> </span>example.py
</pre></div>
</div>
<p>we see the output that Dynamo traced</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">l_x_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">l_y_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="c1"># File: example.py:5, code: z = (x - y) ** 2</span>
    <span class="n">sub</span> <span class="o">=</span> <span class="n">l_x_</span> <span class="o">-</span> <span class="n">l_y_</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">sub</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="c1"># File: example.py:6, code: return z.sum()</span>
    <span class="n">sum_1</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">sum_1</span><span class="p">,)</span>
</pre></div>
</div>
<p>We call this a <strong>graph (or trace) of the function for the given
inputs</strong>. This is represented via an <a class="reference external" href="https://pytorch.org/docs/main/fx.html">FX
graph</a>. We will simply think
of an FX graph as a container that stores a list of function calls.</p>
<p>The first thing we should notice is that the graph is a linear sequence
of PyTorch operations. <a class="footnote-reference brackets" href="#id8" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> Dynamo records all the PyTorch operations
and stores them sequentially. For example, it split <code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">(x</span> <span class="pre">-</span> <span class="pre">y)</span> <span class="pre">**</span> <span class="pre">2</span></code>
into its two constituting operations, <code class="docutils literal notranslate"><span class="pre">sub</span> <span class="pre">=</span> <span class="pre">l_x_</span> <span class="pre">-</span> <span class="pre">l_y_</span></code> and
<code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">sub</span> <span class="pre">**</span> <span class="pre">2</span></code>.</p>
<p>When we say that the trace is linear, we mean that there is no branching
or any control flow. To see this, consider</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">y</span> <span class="o">/</span> <span class="n">n</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>which, when executed with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=graph_code</span></code>, returns</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">l_x_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="c1"># File: example.py:5, code: y = x ** 2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">l_x_</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="c1"># File: example.py:7, code: return (n + 1) * y</span>
    <span class="n">mul</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">y</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">mul</span><span class="p">,)</span>
</pre></div>
</div>
<p>We see that Dynamo completely removed the <code class="docutils literal notranslate"><span class="pre">if</span></code> statement from the
trace and just recorded the operations that were executed with the
inputs.</p>
<p>As such, it should be clear that <strong>the trace of a function depends on
the inputs</strong>. In particular, this means that the trace is not generated
when we write <code class="docutils literal notranslate"><span class="pre">&#64;torch.compile</span></code>, but when we execute the function
<code class="docutils literal notranslate"><span class="pre">fn(x,</span> <span class="pre">2)</span></code> with the actual arguments.</p>
<p>The other interesting thing to note here is that Dynamo removed the
second argument to the function. Instead, it treated it as a constant
and recorded the result of the operation <code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">+</span> <span class="pre">1</span></code> in the graph. This is
another feature of Dynamo: Dynamo will treat as constant any non-tensor
value other than ints. Lets see now how are ints special.</p>
<p>The last defining property of Dynamo is that it knows how to handle
dynamic shapes. Symbolic shapes refer to Dynamos ability of tracing
shapes, and more generally, integers, rather than leaving them as
constants. This allows for avoiding recompilations and deploying generic
models that work for any size in production. The main examples of places
where dynamic shapes appear are the batch size, where we might train a
model with a fixed batch size but then perform inference for an
arbitrary batch size, or the variable sequence length that one
encounters when processing text or audio.</p>
<p>We can see this by executing a few more times the example above</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">y</span> <span class="o">/</span> <span class="n">n</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>In this case, <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=graph_code</span></code> generates two more graphs</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Graph for n==2 omitted</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l_x_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">l_n_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">):</span>
    <span class="c1"># File: a.py:5, code: y = x ** 2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">l_x_</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="c1"># File: a.py:7, code: return (n + 1) * y</span>
    <span class="n">add</span> <span class="o">=</span> <span class="n">l_n_</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">mul</span> <span class="o">=</span> <span class="n">add</span> <span class="o">*</span> <span class="n">y</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">mul</span><span class="p">,)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l_x_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">l_n_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">):</span>
    <span class="c1"># File: a.py:5, code: y = x ** 2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">l_x_</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="c1"># File: a.py:9, code: return y / n</span>
    <span class="n">truediv</span> <span class="o">=</span> <span class="n">y</span> <span class="o">/</span> <span class="n">l_n_</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">truediv</span><span class="p">,)</span>
</pre></div>
</div>
<p>Dynamo detected that one integer changed its value after the first call
and started tracing it. We see that these graphs are generic, and trace
the variable <code class="docutils literal notranslate"><span class="pre">n</span></code> symbolically via an object of type <code class="docutils literal notranslate"><span class="pre">SymInt</span></code>.</p>
<p>If after these calls we call <code class="docutils literal notranslate"><span class="pre">fn(x,</span> <span class="pre">4)</span></code>, Dynamo would not recompile,
but rather reuse the graph that was already traced.</p>
<p>To summarize: 1. Dynamo is a Python tracer 2. Given some inputs, it
returns an FX graph with the PyTorch functions that were executed 3. It
can also trace integers if it detects that they changed between calls 4.
It specializes any other value that is not a tensor or a scalar</p>
<p>Of course, Dynamo does many more things, like figuring out when it needs
to retrace, rewriting the bytecode of the function, implementing graph
breaks To keep the introduction short, we will incrementally discuss
all these in the sequel.</p>
</section>
<section id="pep-523-adding-a-frame-evaluation-api-to-cpython">
<h2>PEP 523: Adding a frame evaluation API to CPython<a class="headerlink" href="#pep-523-adding-a-frame-evaluation-api-to-cpython" title="Link to this heading">#</a></h2>
<p>Imagine now that we are given the task to implement Dynamo. Where would
we even start? Rather conveniently for us, <a class="reference external" href="https://peps.python.org/pep-0523/">PEP
523</a> was released with Python 3.6.
This PEP <a class="reference external" href="https://peps.python.org/pep-0523/#a-jit-for-cpython">was
designed</a> to
allow third parties to create JIT compilers for Python. Lets see how.</p>
<p><strong>A note on CPython</strong>: CPython is internally implemented as a <a class="reference external" href="https://en.wikipedia.org/wiki/Stack_machine">stack
machine</a>. A Python
program is compiled into
<a class="reference external" href="https://en.wikipedia.org/wiki/Bytecode">bytecodes</a> that then are
executed by this interpreter. To learn more about these bytecodes, see
the <a class="reference external" href="https://docs.python.org/3/library/dis.html">dis module</a> from the
standard library. See also <a class="reference external" href="https://devguide.python.org/internals/interpreter/">the developer
docs</a> for an
introduction to CPythons interpreter. We will assume that the reader is
familiar with the notion of a stack machine.</p>
<p>PEP 523 exposes an API where a user can add a custom per-function
interpreter. Then, CPython will use this interpreter rather than its own
to execute the function. In order to be able to execute the function, on
entry, CPython provides the custom interpreter with things like - The
bytecode of the function - The value of the arguments of the function
(i.e., the local variables) and their names - The value of the global
variables and their names - The builtin functions like <code class="docutils literal notranslate"><span class="pre">abs</span></code> or
<code class="docutils literal notranslate"><span class="pre">print</span></code></p>
<p>You can see all the fields
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/csrc/dynamo/eval_frame.c#L50-L59">here</a>. <a class="footnote-reference brackets" href="#id9" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a></p>
<p>In summary, CPython provides the users interpreter with all the
information necessary to execute the function. <a class="footnote-reference brackets" href="#id10" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a></p>
<p>With this API, we can implement a tracer by implementing an interpreter
that runs the code and records in a graph all the PyTorch operations
that occur during this execution. This is exactly what Dynamo does.</p>
<p>Dynamo uses this CPython API to parse all these objects and packs them
into <a class="reference external" href="https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/csrc/dynamo/eval_frame.c#L93-L108">a Python
structure</a>.
After it has done so it goes back from C to python. Other than for this
piece of code that communicates with CPython, Dynamo is fully
implemented in Python.</p>
<p>It should be clear that it is the decorator <code class="docutils literal notranslate"><span class="pre">&#64;torch.compile</span></code>s job
to install the necessary scaffolding that will pass the bytecode, the
args, global variables and so on to Dynamo when the function is called.
Again, <code class="docutils literal notranslate"><span class="pre">&#64;torch.compile</span></code> does not actually compile anything.</p>
</section>
<section id="implementing-cpython-in-python">
<h2>Implementing CPython in Python<a class="headerlink" href="#implementing-cpython-in-python" title="Link to this heading">#</a></h2>
<p>So, we are back in the Python world. We have the bytecode of a function,
and all the context necessary to execute it. In particular, we have
landed at
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/b6df8414601e1e086e830ca9e919e7fdc8874e71/torch/_dynamo/convert_frame.py#L272-L274">_convert_frame_assert</a>.
This is the function that the decorator <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> returns! We
get to this function from
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/b6df8414601e1e086e830ca9e919e7fdc8874e71/torch/_dynamo/eval_frame.py#L715-L727">_dynamo.optimize</a>.
The decorator <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> is just a nice API around
<code class="docutils literal notranslate"><span class="pre">_dynamo.optimize</span></code>.</p>
<p>Before getting into implementing a Python interpreter, we want to define
an <a class="reference external" href="https://en.wikipedia.org/wiki/Intermediate_representation">IR</a>.
In particular, we want to wrap all the local and global variables in our
own internal classes. This allows us to better track these objects and
group together objects that can be treated in the same way to the eyes
of Dynamo.</p>
<p>The parent class of the internal class structure is <code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code>
and represents the different objects that Dynamo understands. For
example, <code class="docutils literal notranslate"><span class="pre">ListVariable</span></code>, represents a <code class="docutils literal notranslate"><span class="pre">list</span></code> object, and keeps
internally a <a class="reference external" href="https://github.com/pytorch/pytorch/blob/e38a3a6079a3861b4bc9f256120ec661f34e726d/torch/_dynamo/variables/lists.py#L48-L56">list of VariableTrackers</a>.
Another example of <code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code> is
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables/constant.py#L30">ConstantVariable</a>.
ConstantVariable wraps all the <a class="reference external" href="https://github.com/pytorch/pytorch/blob/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables/constant.py#L98-L107">objects considered constant by
Dynamo</a>.
We also have special subclasses for objects that require special
attention, like
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables/tensor.py#L68-L69">TensorVariable</a>.
All these internal classes are defined in the
<a class="reference external" href="https://github.com/pytorch/pytorch/tree/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables">torch/_dynamo/variables</a>
folder.</p>
<p>Python objects are wrapped into their corresponding <code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code>
class in
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables/builder.py#L365">VariableBuilder._wrap</a>.
This function is just a very long chain of <code class="docutils literal notranslate"><span class="pre">elif</span></code>s that tries to
recursively pattern-match the Python inputs into the appropriate type of
<code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code>.</p>
<p><strong>Debugging tip</strong>. When we get unexpected results from dynamo, it is
sometimes caused by the builder. If the logic of the builder is wrong,
sometimes Dynamo may wrap a variable in the incorrect
<code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code> type, and this may cause issues later on. It is
rather useful to have a look at the <code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code> types that
appear in the errors, and the <code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code> method that throws the
exception when you encounter a Dynamo error. In particular, sometimes we
find that an object is tracked as a <code class="docutils literal notranslate"><span class="pre">UserDefinedObjectVariable</span></code> (this
is Dynamos catch-all class), when it should have been tracked as
something more specific. In these cases, the <code class="docutils literal notranslate"><span class="pre">VariableBuilder</span></code>
logic is often to blame.</p>
<p><strong>Debugging tip</strong>. When running a program with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=dynamo</span></code>,
one of the artifacts that are printed out is lines of the form</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TRACE</span> <span class="n">LOAD_GLOBAL</span> <span class="n">y</span> <span class="p">[</span><span class="n">TorchInGraphFunctionVariable</span><span class="p">(</span><span class="o">&lt;</span><span class="n">built</span><span class="o">-</span><span class="ow">in</span> <span class="n">method</span> <span class="nb">any</span><span class="o">&gt;</span><span class="p">),</span> <span class="n">TensorVariable</span><span class="p">()]</span>
</pre></div>
</div>
<p>This is the bytecode for the original program and the state of the stack
at that point. This is very useful to find where an object was not
traced into the right <code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code>.</p>
<p>Ok, so we have an IR for our tracer, now we <em>just</em> need to reimplement
CPythons stack machine. This is implemented by
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/symbolic_convert.py#L576-L594">InstructorTranslatorBase</a>
in
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/symbolic_convert.py">symbolic_convert.py</a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">InstructionTranslatorBase</span></code> has about 200 methods, implementing almost
all of Python bytecodes. As an example, we can see the implementation of
<code class="docutils literal notranslate"><span class="pre">BUILD_LIST</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">BUILD_LIST</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
    <span class="n">items</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">popn</span><span class="p">(</span><span class="n">inst</span><span class="o">.</span><span class="n">argval</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">ListVariable</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">mutation_type</span><span class="o">=</span><span class="n">ValueMutationNew</span><span class="p">()))</span>
</pre></div>
</div>
<p>This is the bytecode generated by constructions like <code class="docutils literal notranslate"><span class="pre">l</span> <span class="pre">=</span> <span class="pre">[2,</span> <span class="pre">3,</span> <span class="pre">4]</span></code>.
In this case, since there are three elements, the generated bytecode is
<code class="docutils literal notranslate"><span class="pre">BUILD_LIST</span> <span class="pre">3</span></code>. This means that we pop the top <code class="docutils literal notranslate"><span class="pre">3</span></code> elements of the
stack and push a new list object to the top of the stack formed by these
three elements.</p>
</section>
<section id="generating-the-output-graph">
<h2>Generating the Output Graph<a class="headerlink" href="#generating-the-output-graph" title="Link to this heading">#</a></h2>
<p>With a way to symbolically execute Python code, we are set to extract
the PyTorch operations that happen during the symbolic execution of a
program given some inputs. This is implemented in Dynamo via the
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/output_graph.py#L221-L230">OutputGraph</a>
object. The <code class="docutils literal notranslate"><span class="pre">OutputGraph</span></code> object is <a class="reference external" href="https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/symbolic_convert.py#L2060-L2071">bound to an
`InstructionTranslator object</a>
and it tracks all the data necessary to create the FX graph which will
be returned by Dynamo.</p>
<p>All the inputs and intermediary elements of the FX graph are
<code class="docutils literal notranslate"><span class="pre">fx.Node</span></code>s. In Dynamo, <code class="docutils literal notranslate"><span class="pre">fx.Node</span></code>s are wrapped in
<code class="docutils literal notranslate"><span class="pre">fx.Proxy</span></code>s. <code class="docutils literal notranslate"><span class="pre">fx.Proxy</span></code>s are used to build the FX graph.
In particular, they record every PyTorch operation performed on them
into the graph. You can create a new operation to be added to
the graph by calling <a class="reference external" href="https://github.com/pytorch/pytorch/blob/fb80f05ee2e1cba17892980701bfd5dbce58349f/torch/_dynamo/output_graph.py#L430-L431">create_proxy</a>.
Then, we can add it to the graph through the function
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/fb80f05ee2e1cba17892980701bfd5dbce58349f/torch/_dynamo/variables/builder.py#L1311">wrap_fx_proxy</a>.</p>
<p>A graph stores operations on tensors and operations on symbolic
integers. We will discuss symbolic integers later on, but first we will
discuss how Dynamo addresses a rather important correctness issue.</p>
</section>
<section id="making-dynamo-sound-guards">
<span id="id4"></span><h2>Making Dynamo Sound: Guards<a class="headerlink" href="#making-dynamo-sound-guards" title="Link to this heading">#</a></h2>
<p>At this point, we have a way to trace programs completely disregarding control flow.
And for that, we have reimplemented all of CPython If this sounds like a bit of an
overkill, that is because it is.
<a class="reference external" href="https://pytorch.org/docs/main/generated/torch.jit.trace.html">torch.jit.trace</a>
already implements this without all this machinery, so what gives?</p>
<p>The issue with <code class="docutils literal notranslate"><span class="pre">torch.jit.trace</span></code>, as it is warned in its docs, is that
it just works if the traced program is not data dependent. In other
words, it will just work if the program itself is linear. This means
writing our program without using if-elses, for-while loops, exceptions.
Even more, none of the libraries that we use can use any control flow!
All in all, not using control flow in a language as dynamic as Python
is, in fact, a huge constraint.</p>
<p>JAX solves this problem by always retracing and caching the graph after
retracing. Dynamo, on the other hand, uses guards to avoid retracing the
whole program every time.</p>
<p>A <strong>guard</strong> is an assumption (a boolean expression on an input) made in
order to specialize a frame for one set of example inputs. Reusing the
graph is only valid if these assumptions hold on the new inputs.</p>
<p>For example, any constant input to a function, like a string, installs a
guard stating that that input should be of type <code class="docutils literal notranslate"><span class="pre">str</span></code> and equal to the
string we passed. Running</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="s2">&quot;Hello&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=guards</span></code> prints (among other guards)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">___check_type_id</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="mi">94334122025024</span><span class="p">)</span>
<span class="n">L</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;Hello&#39;</span>
</pre></div>
</div>
<p>This reads as the local variable <code class="docutils literal notranslate"><span class="pre">b</span></code> should have a specific type
(<code class="docutils literal notranslate"><span class="pre">str</span></code> in this case, represented by the constant <code class="docutils literal notranslate"><span class="pre">9433...</span></code>) and
its value should be <code class="docutils literal notranslate"><span class="pre">'Hello'</span></code>. If we then execute the function
again passing a different argument</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="s2">&quot;Hello&quot;</span><span class="p">)</span>
<span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="s2">&quot;Hi&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>we can see the guard that failed by running <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=recompiles</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Recompiling</span> <span class="n">function</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">script</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">3</span>
<span class="n">triggered</span> <span class="n">by</span> <span class="n">the</span> <span class="n">following</span> <span class="n">guard</span> <span class="n">failure</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
     <span class="o">-</span> <span class="n">L</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;Hello&#39;</span>
</pre></div>
</div>
<p>Guards are accumulated while <a class="reference external" href="https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/variables/builder.py#L808-L810">the inputs to the function are wrapped in
the
builder</a>
and <a class="reference external" href="https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/variables/dicts.py#L763-L769">during the execution of the
program</a>.
We will show many more examples of guards in the next section, but first
let us discuss sources.</p>
<p>A <strong>source</strong> tracks how to reconstruct a variable from the original
local or global variables present when entering the current frame. In
particular, it tracks the original local and global objects and any of
the objects they contain. In</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> have
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/40dc0580a69565b06ec5263efe5d87cecc8200f7/torch/_dynamo/source.py#L80-L92">LocalSource</a>
as their source, and <code class="docutils literal notranslate"><span class="pre">y[0]</span></code> has
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/40dc0580a69565b06ec5263efe5d87cecc8200f7/torch/_dynamo/source.py#L302">GetItemSource</a>,
which stores a <code class="docutils literal notranslate"><span class="pre">LocalSource</span></code> inside. On the other hand, <code class="docutils literal notranslate"><span class="pre">a</span></code> will not
have a source as it is an intermediate variable that only exists within
the fx graph.</p>
<p>All these are defined in
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/source.py">torch/_dynamo/source.py</a>.
We can see the guard generated by <code class="docutils literal notranslate"><span class="pre">GetItemSource</span></code> in the following
example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">),</span> <span class="p">[</span><span class="s2">&quot;Hi&quot;</span><span class="p">,</span> <span class="s2">&quot;Hello&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>generates the following guards</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">___check_type_id</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;l&#39;</span><span class="p">],</span> <span class="mi">94439025877664</span><span class="p">)</span>
<span class="nb">len</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;l&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">2</span>
<span class="n">___check_type_id</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;l&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="mi">94439025840192</span><span class="p">)</span>
<span class="n">L</span><span class="p">[</span><span class="s1">&#39;l&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;Hi&#39;</span>
<span class="n">___check_type_id</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;l&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="mi">94439025840192</span><span class="p">)</span>
<span class="n">L</span><span class="p">[</span><span class="s1">&#39;l&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;Hello&#39;</span>
</pre></div>
</div>
<p>Here, we see the code generated by <code class="docutils literal notranslate"><span class="pre">GetItemSource</span></code> (<code class="docutils literal notranslate"><span class="pre">[0]</span></code> and
<code class="docutils literal notranslate"><span class="pre">[1]</span></code>) wrapping a <code class="docutils literal notranslate"><span class="pre">LocalSource</span></code> (<code class="docutils literal notranslate"><span class="pre">L['l']</span></code>).</p>
<p>At this point, with sources and guards, we are able to implement a
caching system to avoid recompilation without having to retrace every
time. We will discuss a bit more in detail this caching system in the
sequel.</p>
<p>The attentive reader will have noticed that this does not explain yet
why we need to have such fine control over the Python interpreter as to
having to reimplement it. The examples of guards that we have shown
depend on the input objects, so we could still compute these before
executing the function. In other words, we could implement this guard
system on top of <code class="docutils literal notranslate"><span class="pre">torch.jit.trace</span></code> and get the same functionality with
much less effort Enter symbolic shapes.</p>
</section>
<section id="symbolic-shapes">
<h2>Symbolic Shapes<a class="headerlink" href="#symbolic-shapes" title="Link to this heading">#</a></h2>
<p>Another point we discussed in the introduction is that Dynamo knows how
to trace integers. In order to implement this, we use a symbolic class
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/fb80f05ee2e1cba17892980701bfd5dbce58349f/torch/__init__.py#L244-L249">torch.SymInt</a>
that acts like an <code class="docutils literal notranslate"><span class="pre">int</span></code> but it records all the operations performed on
it in the output FX graph. <a class="footnote-reference brackets" href="#id11" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> We already saw this class in the introduction
when introducing symbolic integer tracing.</p>
<p>Let us now discuss the three properties that define symbolic shape
tracing in Dynamo, and how to implement them.</p>
<section id="static-by-default">
<h3>Static by default<a class="headerlink" href="#static-by-default" title="Link to this heading">#</a></h3>
<p>Dynamo assumes that every integer, let that be an input or the shape of
a tensor, is static by default. In other words, no integers will be
traced on the first execution of a function. Then, only if it detects
that an integer or a shape changed value during the execution, it will
trace it and generate a graph generic on that variable.</p>
<p>We already saw this behavior in the introduction using integers. Let us
now look at an example using shapes of tensors.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>

<span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
<p>Running this program with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=graph_code</span></code> we see that these
two calls are traced as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l_a_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">l_b_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">mul</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">l_a_</span>
    <span class="n">mul_1</span> <span class="o">=</span> <span class="n">mul</span> <span class="o">*</span> <span class="n">l_b_</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">mul_1</span><span class="p">,)</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s0</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">l_a_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">l_b_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">l_a_</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">getitem</span> <span class="o">=</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mul</span> <span class="o">=</span> <span class="n">getitem</span> <span class="o">*</span> <span class="n">l_a_</span>
    <span class="n">mul_1</span> <span class="o">=</span> <span class="n">mul</span> <span class="o">*</span> <span class="n">l_b_</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">mul_1</span><span class="p">,)</span>
</pre></div>
</div>
<p>In the first graph the shape is traced as a constant, but once it
changes, it traces it symbolically using a <code class="docutils literal notranslate"><span class="pre">SymInt</span></code>s. In general, a
simpler way to see the shapes of the intermediary values is by running
the program with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=graph_sizes</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TRACED</span> <span class="n">GRAPH</span> <span class="n">TENSOR</span> <span class="n">SIZES</span>
<span class="o">=====</span> <span class="n">__compiled_fn_1</span> <span class="o">=====</span>
<span class="n">l_a_</span><span class="p">:</span> <span class="p">(</span><span class="n">s0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">l_a_</span> <span class="p">(</span><span class="n">concrete</span><span class="p">):</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">l_b_</span><span class="p">:</span> <span class="p">(</span><span class="n">s0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">l_b_</span> <span class="p">(</span><span class="n">concrete</span><span class="p">):</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">mul</span><span class="p">:</span> <span class="p">(</span><span class="n">s0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">mul</span> <span class="p">(</span><span class="n">concrete</span><span class="p">):</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">mul_1</span><span class="p">:</span> <span class="p">(</span><span class="n">s0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">mul_1</span> <span class="p">(</span><span class="n">concrete</span><span class="p">):</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>where we can see that the first dimension of the two tensor args is
dynamic, given that it is represented by the <code class="docutils literal notranslate"><span class="pre">s0</span></code> variable.</p>
<p>We can find how Dynamo implements this by running <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=guards</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Guards first call</span>
<span class="n">check_tensor</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">check_tensor</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># Guards second call</span>
<span class="n">check_tensor</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">check_tensor</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">L</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">L</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="mi">2</span> <span class="o">&lt;=</span> <span class="n">L</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>We see that on the first call, the guards check that the tensors have
some fixed sizes and strides. These guards fail in the second execution,
so it retraces. Since it was an <code class="docutils literal notranslate"><span class="pre">int</span></code> guard that failed, in this
second iteration it traces this <code class="docutils literal notranslate"><span class="pre">int</span></code> symbolically and it installs
more general guards on this more generic kernel.</p>
<p><strong>Compilation performance tip</strong>. If you know that a dimension will vary
in size, you can mark it as dynamic by calling
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/66a76516bfc341b2b55bb2056d2faa9c2de46d69/torch/_dynamo/decorators.py#L176">torch._dynamo.mark_dynamic</a>
before calling <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>. This will avoid the first compilation
with a static shape. There are other useful utility functions like
<code class="docutils literal notranslate"><span class="pre">maybe_mark_dynamic</span></code> or <code class="docutils literal notranslate"><span class="pre">mark_static</span></code>. You can also have all
integers and shapes traced by calling <code class="docutils literal notranslate"><span class="pre">torch.compile(dynamic=True)</span></code>.
This is mostly useful for debugging purposes.</p>
</section>
<section id="are-always-specialized">
<h3>0, 1 are always specialized<a class="headerlink" href="#are-always-specialized" title="Link to this heading">#</a></h3>
<p>Regardless of whether we mark a dimension as dynamic, if we pass an input
where that dimension is 0 or 1, Dynamo will trace it as non-dynamic and it
will generate a specific graph for it. This is the reason why in the example
above we find guards of the form <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">&lt;=</span> <span class="pre">L['a'].size()[0]</span></code>.</p>
<p>There are several reasons for this choice. There are two particularly
important - A tensor is empty if and only if any of its dimensions is
zero - A tensor can only be contiguous if one of the strides is one</p>
<p>This policy decision does NOT apply to plain Python ints; if we think a Python
int should be compiled dynamically, we wont specialize them by default;
instead, whether or not it gets specialized depends on its usage.</p>
</section>
<section id="duck-shaping">
<h3>Duck shaping<a class="headerlink" href="#duck-shaping" title="Link to this heading">#</a></h3>
<p>Dynamo performs what we call duck shaping. If two dynamic integers
have the same value at trace time, we will assume that they are equal
and guard on it. Effectively, this means that rather than having two
symbols <code class="docutils literal notranslate"><span class="pre">s0</span></code>, <code class="docutils literal notranslate"><span class="pre">s1</span></code> in the example above, we just unified them to
<code class="docutils literal notranslate"><span class="pre">s0</span></code> and had the guard <code class="docutils literal notranslate"><span class="pre">L['b'].size()[0]</span> <span class="pre">==</span> <span class="pre">L['a'].size()[0]</span></code>. This
enables performing fusions within the compiler while being able to
generate kernels that are generic enough.</p>
</section>
<section id="guards-on-symbolic-ints">
<h3>Guards on symbolic ints<a class="headerlink" href="#guards-on-symbolic-ints" title="Link to this heading">#</a></h3>
<p>We now understand how symbolic shapes are implemented at a high level
and the properties they have. Now, why is that symbolic shapes forced us
through the tricky route of getting control of the CPython interpreter?
Consider the following example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">&lt;</span> <span class="mi">16</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">))</span>
</pre></div>
</div>
<p>This code has a guard of the form <code class="docutils literal notranslate"><span class="pre">2*L['a'].size()[0]</span> <span class="pre">&gt;=</span> <span class="pre">16</span></code>. This is
a non-trivial guard in terms of the inputs of the function, but it is
registered in the middle of the execution of the program. Even more so,
we cannot know this guard is needed until we see the <code class="docutils literal notranslate"><span class="pre">if</span></code> statement
conditional on a <code class="docutils literal notranslate"><span class="pre">SymNodeVariable</span></code> argument. Such conditions are
invisible to <code class="docutils literal notranslate"><span class="pre">torch.jit.trace</span></code> and require deep analysis of the python
code.</p>
<p><strong>Debugging tip</strong> Running this code with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=dynamo</span></code> tells us
where this guard was added</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">eval</span> <span class="mi">2</span><span class="o">*</span><span class="n">s0</span> <span class="o">&gt;=</span> <span class="mi">16</span> <span class="p">[</span><span class="n">guard</span> <span class="n">added</span><span class="p">]</span> <span class="n">at</span> <span class="n">script</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">5</span> <span class="ow">in</span> <span class="n">fn</span> <span class="p">(</span><span class="n">_dynamo</span><span class="o">/</span><span class="n">variables</span><span class="o">/</span><span class="n">tensor</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">812</span> <span class="ow">in</span> <span class="n">evaluate_expr</span><span class="p">)</span>
</pre></div>
</div>
<p>Placing a breakpoint there and looking at the backtrace is rather useful
to understand where a guard came from.</p>
</section>
</section>
<section id="making-dynamo-complete-graph-breaks">
<h2>Making Dynamo Complete: Graph Breaks<a class="headerlink" href="#making-dynamo-complete-graph-breaks" title="Link to this heading">#</a></h2>
<p>With all the tools we have discussed, we have a tracer that can trace
PyTorch operations on tensors and integers and has a caching system that
knows when it can reuse a previously traced graph and when it needs to
retrace. All this executing arbitrary Python code!</p>
<p>There is just one small issue with this. The statement executing
arbitrary Python code is perhaps a bit too general. Dynamo implements a
good part of Python, but does it implement the more complex parts, like
coroutines or async? Does it implement the whole Python standard
library? NumPy also has a Python API. Does <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> also
understand NumPy? and Django? <a class="footnote-reference brackets" href="#id12" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a></p>
<p>Pythons ecosystem is massive, and a good part of it is written in other
more performant languages like C++ or Rust, and it just exposes Python
bindings. There is no hope in Dynamo tracing through Python objects that
are implemented in C++. What can a tracer do when it finds an operation
that it does not understand?</p>
<p>The usual way machine learning tracers handle this issue is by informing
the user that the operation they choked on and giving up tracing
altogether. This would pose a real usability issue in the case of
PyTorch, where its users are used to the flexibility it gives them. As a
real-world example the <code class="docutils literal notranslate"><span class="pre">doctr_det_predictor</span></code> model uses NumPy and the
<code class="docutils literal notranslate"><span class="pre">cv2</span></code> library to <a class="reference external" href="https://github.com/mindee/doctr/blob/f2114758d529ed8d3d0030581638f0520b6b98d8/doctr/models/detection/core.py#L86">postprocess the models
result</a>.</p>
<p>Here is another place where having access to CPython is interesting.
Rather than erroring out, Dynamo can let CPython run that problematic
code! To do this, Dynamo generates at trace time one graph with all the
operations before the problematic code, and one with all the operations
after. <a class="footnote-reference brackets" href="#id13" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a> Then, at runtime, it will delegate to CPython to execute the
first graph, then the problematic code, and then the second graph. This
process of stopping the tracing and generating multiple graphs is called
a <strong>graph break</strong>.</p>
<p>A small confession: I lied all throughout the introduction and the first
sections. Dynamo does not generate one graph, but <strong>multiple graphs</strong>!
For all practical purposes, starting retracing after a second graph can
be thought of as starting tracing a new function. The new graph after
the graph break will have its own guards, its new set of local
variables, and so on.</p>
<p>To discuss how to implement graph breaks, we need to first revisit how
Dynamo interacts with CPython. Using PEP 523, CPython allows a user to
use their own frame evaluation mechanism. What we had not discussed is
that CPython also exposes its own frame evaluation for others to use.
Dynamo leverages this to let the fast CPython interpreter run the
compiled code. For a function without graph breaks, the whole tracing /
execution process of a program that calls the function 2 times with the
same arguments looks like this:</p>
<ol class="arabic simple">
<li><p>In the first call to the function</p>
<ol class="arabic simple">
<li><p>Dynamo traces the function into an FX graph</p>
<ol class="arabic simple">
<li><p>The FX graph is compiled by the compiler (Inductor) into
efficient low-level code but thats a story for another day</p></li>
</ol>
</li>
<li><p>It rewrites the bytecode of the function so that it simply calls
the compiled function</p></li>
<li><p>It gives CPython this new bytecode and asks it to run it
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/csrc/dynamo/eval_frame.c#L1006">here</a></p></li>
</ol>
</li>
<li><p>In the second call to the function</p>
<ol class="arabic simple">
<li><p>It checks the guards from the first call against the new arguments
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/csrc/dynamo/eval_frame.c#L658">here</a>.
Since they are the same arguments as before, they pass</p></li>
<li><p>It asks CPython to run the bytecode associated to those guards
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/csrc/dynamo/eval_frame.c#L972-L975">here</a></p></li>
</ol>
</li>
</ol>
<p>This process on its own looks overly complicated. Why generate new
bytecode and ask CPython to run it rather than simply creating a C++
binding to the compiled function and executing it? Well, this pattern
allows us to implement graph breaks! The bytecode generated by a graph
break has the following structure:</p>
<ol class="arabic simple">
<li><p>Bytecode that executes the first graph</p></li>
<li><p>Bytecode that leaves the stack as it would be if CPython would have
executed the first graph. It also replays any modifications to local
or global variables that would be visible at this point</p></li>
<li><p>The bytecode that made Dynamo graph break</p></li>
<li><p>Bytecode that executes the second graph</p></li>
</ol>
<p>Let us see this in a simple example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="mi">2</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hi&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">b</span> <span class="o">+</span> <span class="n">a</span>

<span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
<p>Running this with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=bytecode</span></code> shows us the initial bytecode
and the modified bytecode</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">MODIFIED</span> <span class="n">BYTECODE</span> <span class="n">fn</span> <span class="n">script</span><span class="o">.</span><span class="n">py</span> <span class="n">line</span> <span class="mi">3</span>
 <span class="mi">0</span> <span class="n">LOAD_GLOBAL</span>              <span class="mi">1</span> <span class="p">(</span><span class="n">__compiled_fn_0</span><span class="p">)</span>
 <span class="mi">2</span> <span class="n">LOAD_FAST</span>                <span class="mi">0</span> <span class="p">(</span><span class="n">a</span><span class="p">)</span>
 <span class="mi">4</span> <span class="n">CALL_FUNCTION</span>            <span class="mi">1</span>
 <span class="mi">6</span> <span class="n">STORE_FAST</span>               <span class="mi">3</span> <span class="p">(</span><span class="n">graph_out_0</span><span class="p">)</span>
 <span class="mi">8</span> <span class="n">LOAD_GLOBAL</span>              <span class="mi">0</span> <span class="p">(</span><span class="nb">print</span><span class="p">)</span>
<span class="mi">10</span> <span class="n">LOAD_CONST</span>               <span class="mi">2</span> <span class="p">(</span><span class="s1">&#39;Hi&#39;</span><span class="p">)</span>
<span class="mi">12</span> <span class="n">LOAD_FAST</span>                <span class="mi">3</span> <span class="p">(</span><span class="n">graph_out_0</span><span class="p">)</span>
<span class="mi">14</span> <span class="n">LOAD_CONST</span>               <span class="mi">3</span> <span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="mi">16</span> <span class="n">BINARY_SUBSCR</span>
<span class="mi">18</span> <span class="n">STORE_FAST</span>               <span class="mi">1</span> <span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="mi">20</span> <span class="n">CALL_FUNCTION</span>            <span class="mi">1</span>
<span class="mi">22</span> <span class="n">LOAD_GLOBAL</span>              <span class="mi">2</span> <span class="p">(</span><span class="n">__resume_at_14_1</span><span class="p">)</span>
<span class="mi">24</span> <span class="n">ROT_TWO</span>
<span class="mi">26</span> <span class="n">LOAD_FAST</span>                <span class="mi">0</span> <span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="mi">28</span> <span class="n">LOAD_FAST</span>                <span class="mi">1</span> <span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="mi">30</span> <span class="n">CALL_FUNCTION</span>            <span class="mi">3</span>
<span class="mi">32</span> <span class="n">RETURN_VALUE</span>

<span class="n">MODIFIED</span> <span class="n">BYTECODE</span> <span class="n">resume_in_fn</span> <span class="n">script</span><span class="o">.</span><span class="n">py</span> <span class="n">line</span> <span class="mi">6</span>
 <span class="mi">0</span> <span class="n">LOAD_GLOBAL</span>              <span class="mi">1</span> <span class="p">(</span><span class="n">__compiled_fn_2</span><span class="p">)</span>
 <span class="mi">2</span> <span class="n">LOAD_FAST</span>                <span class="mi">2</span> <span class="p">(</span><span class="n">b</span><span class="p">)</span>
 <span class="mi">4</span> <span class="n">LOAD_FAST</span>                <span class="mi">1</span> <span class="p">(</span><span class="n">a</span><span class="p">)</span>
 <span class="mi">6</span> <span class="n">CALL_FUNCTION</span>            <span class="mi">2</span>
 <span class="mi">8</span> <span class="n">UNPACK_SEQUENCE</span>          <span class="mi">1</span>
<span class="mi">10</span> <span class="n">RETURN_VALUE</span>
</pre></div>
</div>
<p>We can see that the modified bytecode is split into two functions,
<code class="docutils literal notranslate"><span class="pre">fn</span></code>, the original function, and a function called <code class="docutils literal notranslate"><span class="pre">resume_in_fn</span></code>.
This second function is a function created by Dynamo to implement the
execution of the program starting at the graph break. This is often
called a <a class="reference external" href="https://en.wikipedia.org/wiki/Continuation">continuation
function</a>. This
continuation function simply calls the second compiled function with the
right arguments. The code for the initial function is rewritten
implementing the strategy that we described before</p>
<ul class="simple">
<li><p>L0-4. Call the compiled function (<code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">+</span> <span class="pre">2</span></code>).</p></li>
<li><p>L6. Store its result in a local variable called <code class="docutils literal notranslate"><span class="pre">graph_out_0</span></code>.
<code class="docutils literal notranslate"><span class="pre">graph_out_0</span></code> is a tuple</p></li>
<li><p>L8-18. Leave the stack as it would be at the point of the graph break</p></li>
<li><p>L20. Execute the code that caused the graph break</p></li>
<li><p>L22-32. Call the compiled continuation function (<code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">+</span> <span class="pre">b</span></code>)</p></li>
</ul>
<p>The code generation of the stack in Dynamo is delegated to
<code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code> subclasses. Every <code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code> object in
Dynamo has a <a class="reference external" href="https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/_dynamo/variables/lists.py#L307-L309">reconstruct</a>
method that generates the necessary bytecode to create the python object
it represents on the stack.</p>
<p><strong>Debugging tip</strong>. Graph breaks hamper performance, and as such, it is
best to avoid them. Running a program with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=graph_breaks</span></code>
is a great way to find how many graph breaks did our program hit. The
information it returns is in terms of <code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code> objects, so
the debugging tips above are sometimes also helpful to figure out what
caused that graph break.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>Dynamo is a complex piece of software. Once you sign up to implement a
CPython interpreter you know you are in for a ride. That being said, we
hope that this post helps demystify it a bit.</p>
<p>Dynamo is (mostly) implemented in Python. We left plenty of links to the
pieces of the code that we discussed. We hope that reading those pieces
of code and grepping for the places that call them, or putting
breakpoints on them and looking at the call stack helps understanding
the rest of the code base.</p>
<p>Of course, the best way to learn how a piece of software works is by
extending it. In this case, the best way is to have a look at the <a class="reference external" href="https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+label%3A%22module%3A+dynamo%22+">open
dynamo issues on
github</a>.
Many of them require very minor changes in the code, once you find where
you need to make those changes.</p>
</section>
<section id="footnotes">
<h2>Footnotes<a class="headerlink" href="#footnotes" title="Link to this heading">#</a></h2>
<p>Below are additional details and references for concepts mentioned in this document.</p>
</section>
</section>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id8" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>In the literature, this is called a Directed Acyclical Graph (DAG).</p>
</aside>
<aside class="footnote brackets" id="id9" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>All this binding code lives in <code class="docutils literal notranslate"><span class="pre">torch/csrc/dynamo/eval_frame.c</span></code>.</p>
</aside>
<aside class="footnote brackets" id="id10" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>In CPython lingo, the set of all these objects are called <a class="reference external" href="https://github.com/python/cpython/blob/f26bfe4b25f7e5a4f68fcac26207b7175abad208/Include/internal/pycore_frame.h#L57-L71">a
frame</a>.</p>
</aside>
<aside class="footnote brackets" id="id11" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">4</a><span class="fn-bracket">]</span></span>
<p>There are also <code class="docutils literal notranslate"><span class="pre">SymBool</span></code> and <code class="docutils literal notranslate"><span class="pre">SymFloat</span></code> classes. The latter one
is not used all that much at the time of this writing.</p>
</aside>
<aside class="footnote brackets" id="id12" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">5</a><span class="fn-bracket">]</span></span>
<p>Interestingly enough, it does understand NumPy code! Have a look at
<a class="reference external" href="https://pytorch.org/blog/compiling-numpy-code/">this blogpost</a>
and <a class="reference external" href="https://pytorch.org/docs/main/torch.compiler_faq.html#does-numpy-work-with-torch-compile">the docs</a>.
Now, this is just possible because we reimplemented NumPy using
PyTorch. Good luck implementing Django in PyTorch though</p>
</aside>
<aside class="footnote brackets" id="id13" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">6</a><span class="fn-bracket">]</span></span>
<p>Assuming there is just one piece of problematic code. If there are
more, Dynamo can split the code into as many graphs as it needs.</p>
</aside>
</aside>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5"></span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="advanced.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Advanced</p>
      </div>
    </a>
    <a class="right-next"
       href="torch.compiler_transformations.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Writing Graph Transformations on ATen IR</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="advanced.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Advanced</p>
      </div>
    </a>
    <a class="right-next"
       href="torch.compiler_transformations.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Writing Graph Transformations on ATen IR</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-gentle-introduction-to-dynamo">A Gentle Introduction to Dynamo</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pep-523-adding-a-frame-evaluation-api-to-cpython">PEP 523: Adding a frame evaluation API to CPython</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-cpython-in-python">Implementing CPython in Python</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-the-output-graph">Generating the Output Graph</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-dynamo-sound-guards">Making Dynamo Sound: Guards</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#symbolic-shapes">Symbolic Shapes</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#static-by-default">Static by default</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#are-always-specialized">0, 1 are always specialized</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#duck-shaping">Duck shaping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#guards-on-symbolic-ints">Guards on symbolic ints</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-dynamo-complete-graph-breaks">Making Dynamo Complete: Graph Breaks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#footnotes">Footnotes</a></li>
</ul>
  </nav></div>
    
       <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/pytorch/edit/main/docs/source/user_guide/torch_compiler/torch.compiler_dynamo_deepdive.md">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../../_sources/user_guide/torch_compiler/torch.compiler_dynamo_deepdive.md.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright  The Linux Foundation. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
       Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Dynamo Deep-Dive",
       "headline": "Dynamo Deep-Dive",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/user_guide/torch_compiler/torch.compiler_dynamo_deepdive.html",
       "articleBody": "Dynamo Deep-Dive# Created On: Apr 02, 2024 | Last Updated On: Dec 03, 2025 TorchDynamo (or simply Dynamo) is the tracer within torch.compile, and it is, more often than not, the one to blame for those insane backtraces. However, we cannot blindly blame Dynamo for these errors. In order to provide the user with the flexibility it does, Dynamo is given the arduous task of understanding any Python program. In particular, Dynamo has to implement a good part of the Python programming language internally! In this post, we will go over the internal design of Dynamo from the ground up. We will discuss the functionality it provides, and how it is implemented. By the end of this post, you will have a better understanding of what went wrong when you torch.compiled a PyTorch program and the compilation errored out, or succeeded but the speed-up was not what you expected. A Gentle Introduction to Dynamo# Before getting our hands dirty with all the implementation details, let\u2019s start by discussing what it is that Dynamo does. Dynamo is a tracer. This means, given and function and inputs to it, it executes the function and records a linear sequence of instructions (without control flow) into a graph. For example, consider the following program: import torch @torch.compile def mse(x, y): z = (x - y) ** 2 return z.sum() x = torch.randn(200) y = torch.randn(200) mse(x, y) If we save this program into the file example.py and we run TORCH_LOGS=graph_code python example.py we see the output that Dynamo traced def forward(l_x_: torch.Tensor, l_y_: torch.Tensor): # File: example.py:5, code: z = (x - y) ** 2 sub = l_x_ - l_y_ z = sub ** 2 # File: example.py:6, code: return z.sum() sum_1 = z.sum() return (sum_1,) We call this a graph (or trace) of the function for the given inputs. This is represented via an FX graph. We will simply think of an FX graph as a container that stores a list of function calls. The first thing we should notice is that the graph is a linear sequence of PyTorch operations. [1] Dynamo records all the PyTorch operations and stores them sequentially. For example, it split z = (x - y) ** 2 into its two constituting operations, sub = l_x_ - l_y_ and z = sub ** 2. When we say that the trace is linear, we mean that there is no branching or any control flow. To see this, consider import torch @torch.compile def fn(x, n): y = x ** 2 if n \u003e= 0: return (n + 1) * y else: return y / n x = torch.randn(200) fn(x, 2) which, when executed with TORCH_LOGS=graph_code, returns def forward(l_x_: torch.Tensor): # File: example.py:5, code: y = x ** 2 y = l_x_ ** 2 # File: example.py:7, code: return (n + 1) * y mul = 3 * y return (mul,) We see that Dynamo completely removed the if statement from the trace and just recorded the operations that were executed with the inputs. As such, it should be clear that the trace of a function depends on the inputs. In particular, this means that the trace is not generated when we write @torch.compile, but when we execute the function fn(x, 2) with the actual arguments. The other interesting thing to note here is that Dynamo removed the second argument to the function. Instead, it treated it as a constant and recorded the result of the operation n + 1 in the graph. This is another feature of Dynamo: Dynamo will treat as constant any non-tensor value\u2026 other than ints. Let\u2019s see now how are ints special. The last defining property of Dynamo is that it knows how to handle dynamic shapes. Symbolic shapes refer to Dynamo\u2019s ability of tracing shapes, and more generally, integers, rather than leaving them as constants. This allows for avoiding recompilations and deploying generic models that work for any size in production. The main examples of places where dynamic shapes appear are the batch size, where we might train a model with a fixed batch size but then perform inference for an arbitrary batch size, or the variable sequence length that one encounters when processing text or audio. We can see this by executing a few more times the example above import torch @torch.compile def fn(x, n): y = x ** 2 if n \u003e= 0: return (n + 1) * y else: return y / n x = torch.randn(200) fn(x, 2) fn(x, 3) fn(x, -2) In this case, TORCH_LOGS=graph_code generates two more graphs # Graph for n==2 omitted def forward(self, l_x_: torch.Tensor, l_n_: torch.SymInt): # File: a.py:5, code: y = x ** 2 y = l_x_ ** 2 # File: a.py:7, code: return (n + 1) * y add = l_n_ + 1 mul = add * y return (mul,) def forward(self, l_x_: torch.Tensor, l_n_: torch.SymInt): # File: a.py:5, code: y = x ** 2 y = l_x_ ** 2 # File: a.py:9, code: return y / n truediv = y / l_n_ return (truediv,) Dynamo detected that one integer changed its value after the first call and started tracing it. We see that these graphs are generic, and trace the variable n symbolically via an object of type SymInt. If after these calls we call fn(x, 4), Dynamo would not recompile, but rather reuse the graph that was already traced. To summarize: 1. Dynamo is a Python tracer 2. Given some inputs, it returns an FX graph with the PyTorch functions that were executed 3. It can also trace integers if it detects that they changed between calls 4. It specializes any other value that is not a tensor or a scalar Of course, Dynamo does many more things, like figuring out when it needs to retrace, rewriting the bytecode of the function, implementing graph breaks\u2026 To keep the introduction short, we will incrementally discuss all these in the sequel. PEP 523: Adding a frame evaluation API to CPython# Imagine now that we are given the task to implement Dynamo. Where would we even start? Rather conveniently for us, PEP 523 was released with Python 3.6. This PEP was designed to allow third parties to create JIT compilers for Python. Let\u2019s see how. A note on CPython: CPython is internally implemented as a stack machine. A Python program is compiled into bytecodes that then are executed by this interpreter. To learn more about these bytecodes, see the dis module from the standard library. See also the developer docs for an introduction to CPython\u2019s interpreter. We will assume that the reader is familiar with the notion of a stack machine. PEP 523 exposes an API where a user can add a custom per-function interpreter. Then, CPython will use this interpreter rather than its own to execute the function. In order to be able to execute the function, on entry, CPython provides the custom interpreter with things like - The bytecode of the function - The value of the arguments of the function (i.e., the local variables) and their names - The value of the global variables and their names - The builtin functions like abs or print You can see all the fields here. [2] In summary, CPython provides the user\u2019s interpreter with all the information necessary to execute the function. [3] With this API, we can implement a tracer by implementing an interpreter that runs the code and records in a graph all the PyTorch operations that occur during this execution. This is exactly what Dynamo does. Dynamo uses this CPython API to parse all these objects and packs them into a Python structure. After it has done so\u2026 it goes back from C to python. Other than for this piece of code that communicates with CPython, Dynamo is fully implemented in Python. It should be clear that it is the decorator @torch.compile\u2019s job to install the necessary scaffolding that will pass the bytecode, the args, global variables and so on to Dynamo when the function is called. Again, @torch.compile does not actually compile anything. Implementing CPython in Python# So, we are back in the Python world. We have the bytecode of a function, and all the context necessary to execute it. In particular, we have landed at _convert_frame_assert. This is the function that the decorator torch.compile returns! We get to this function from _dynamo.optimize. The decorator torch.compile is just a nice API around _dynamo.optimize. Before getting into implementing a Python interpreter, we want to define an IR. In particular, we want to wrap all the local and global variables in our own internal classes. This allows us to better track these objects and group together objects that can be treated in the same way to the eyes of Dynamo. The parent class of the internal class structure is VariableTracker and represents the different objects that Dynamo understands. For example, ListVariable, represents a list object, and keeps internally a list of VariableTrackers. Another example of VariableTracker is ConstantVariable. ConstantVariable wraps all the objects considered constant by Dynamo. We also have special subclasses for objects that require special attention, like TensorVariable. All these internal classes are defined in the torch/_dynamo/variables folder. Python objects are wrapped into their corresponding VariableTracker class in VariableBuilder._wrap. This function is just a very long chain of elifs that tries to recursively pattern-match the Python inputs into the appropriate type of VariableTracker. Debugging tip. When we get unexpected results from dynamo, it is sometimes caused by the builder. If the logic of the builder is wrong, sometimes Dynamo may wrap a variable in the incorrect VariableTracker type, and this may cause issues later on. It is rather useful to have a look at the VariableTracker types that appear in the errors, and the VariableTracker method that throws the exception when you encounter a Dynamo error. In particular, sometimes we find that an object is tracked as a UserDefinedObjectVariable (this is Dynamo\u2019s catch-all class), when it should have been tracked as something more specific. In these cases, the VariableBuilder logic is often to blame. Debugging tip. When running a program with TORCH_LOGS=dynamo, one of the artifacts that are printed out is lines of the form TRACE LOAD_GLOBAL y [TorchInGraphFunctionVariable(\u003cbuilt-in method any\u003e), TensorVariable()] This is the bytecode for the original program and the state of the stack at that point. This is very useful to find where an object was not traced into the right VariableTracker. Ok, so we have an IR for our tracer, now we just need to reimplement CPython\u2019s stack machine. This is implemented by InstructorTranslatorBase in symbolic_convert.py. InstructionTranslatorBase has about 200 methods, implementing almost all of Python bytecodes. As an example, we can see the implementation of BUILD_LIST def BUILD_LIST(self, inst): items = self.popn(inst.argval) self.push(ListVariable(items, mutation_type=ValueMutationNew())) This is the bytecode generated by constructions like l = [2, 3, 4]. In this case, since there are three elements, the generated bytecode is BUILD_LIST 3. This means that we pop the top 3 elements of the stack and push a new list object to the top of the stack formed by these three elements. Generating the Output Graph# With a way to symbolically execute Python code, we are set to extract the PyTorch operations that happen during the symbolic execution of a program given some inputs. This is implemented in Dynamo via the OutputGraph object. The OutputGraph object is bound to an `InstructionTranslator object and it tracks all the data necessary to create the FX graph which will be returned by Dynamo. All the inputs and intermediary elements of the FX graph are fx.Nodes. In Dynamo, fx.Nodes are wrapped in fx.Proxys. fx.Proxys are used to build the FX graph. In particular, they record every PyTorch operation performed on them into the graph. You can create a new operation to be added to the graph by calling create_proxy. Then, we can add it to the graph through the function wrap_fx_proxy. A graph stores operations on tensors\u2026 and operations on symbolic integers. We will discuss symbolic integers later on, but first we will discuss how Dynamo addresses a rather important correctness issue. Making Dynamo Sound: Guards# At this point, we have a way to trace programs completely disregarding control flow. And for that, we have reimplemented all of CPython\u2026 If this sounds like a bit of an overkill, that is because it is. torch.jit.trace already implements this without all this machinery, so what gives? The issue with torch.jit.trace, as it is warned in its docs, is that it just works if the traced program is not data dependent. In other words, it will just work if the program itself is linear. This means writing our program without using if-elses, for-while loops, exceptions. Even more, none of the libraries that we use can use any control flow! All in all, not using control flow in a language as dynamic as Python is, in fact, a huge constraint. JAX solves this problem by always retracing and caching the graph after retracing. Dynamo, on the other hand, uses guards to avoid retracing the whole program every time. A guard is an assumption (a boolean expression on an input) made in order to specialize a frame for one set of example inputs. Reusing the graph is only valid if these assumptions hold on the new inputs. For example, any constant input to a function, like a string, installs a guard stating that that input should be of type str and equal to the string we passed. Running import torch @torch.compile def fn(a, b): return a * len(b) fn(torch.arange(10), \"Hello\") with TORCH_LOGS=guards prints (among other guards) ___check_type_id(L[\u0027b\u0027], 94334122025024) L[\u0027b\u0027] == \u0027Hello\u0027 This reads as \u201cthe local variable b should have a specific type (str in this case, represented by the constant 9433...) and its value should be \u0027Hello\u0027\u201d. If we then execute the function again passing a different argument import torch @torch.compile def fn(a, b): return a * len(b) fn(torch.arange(10), \"Hello\") fn(torch.arange(10), \"Hi\") we can see the guard that failed by running TORCH_LOGS=recompiles Recompiling function fn in script.py:3 triggered by the following guard failure(s): - L[\u0027b\u0027] == \u0027Hello\u0027 Guards are accumulated while the inputs to the function are wrapped in the builder and during the execution of the program. We will show many more examples of guards in the next section, but first let us discuss sources. A source tracks how to reconstruct a variable from the original local or global variables present when entering the current frame. In particular, it tracks the original local and global objects and any of the objects they contain. In def foo(x: Tensor, y: List[Tensor]): a = x * y[0] return a * x x and y have LocalSource as their source, and y[0] has GetItemSource, which stores a LocalSource inside. On the other hand, a will not have a source as it is an intermediate variable that only exists within the fx graph. All these are defined in torch/_dynamo/source.py. We can see the guard generated by GetItemSource in the following example: import torch @torch.compile def fn(x, l): return x * len(l[0]) fn(torch.randn(8), [\"Hi\", \"Hello\"]) generates the following guards ___check_type_id(L[\u0027l\u0027], 94439025877664) len(L[\u0027l\u0027]) == 2 ___check_type_id(L[\u0027l\u0027][0], 94439025840192) L[\u0027l\u0027][0] == \u0027Hi\u0027 ___check_type_id(L[\u0027l\u0027][1], 94439025840192) L[\u0027l\u0027][1] == \u0027Hello\u0027 Here, we see the code generated by GetItemSource ([0] and [1]) wrapping a LocalSource (L[\u0027l\u0027]). At this point, with sources and guards, we are able to implement a caching system to avoid recompilation without having to retrace every time. We will discuss a bit more in detail this caching system in the sequel. The attentive reader will have noticed that this does not explain yet why we need to have such fine control over the Python interpreter as to having to reimplement it. The examples of guards that we have shown depend on the input objects, so we could still compute these before executing the function. In other words, we could implement this guard system on top of torch.jit.trace and get the same functionality with much less effort\u2026 Enter symbolic shapes. Symbolic Shapes# Another point we discussed in the introduction is that Dynamo knows how to trace integers. In order to implement this, we use a symbolic class torch.SymInt that acts like an int but it records all the operations performed on it in the output FX graph. [4] We already saw this class in the introduction when introducing symbolic integer tracing. Let us now discuss the three properties that define symbolic shape tracing in Dynamo, and how to implement them. Static by default# Dynamo assumes that every integer, let that be an input or the shape of a tensor, is static by default. In other words, no integers will be traced on the first execution of a function. Then, only if it detects that an integer or a shape changed value during the execution, it will trace it and generate a graph generic on that variable. We already saw this behavior in the introduction using integers. Let us now look at an example using shapes of tensors. import torch @torch.compile def fn(a, b): return a.shape[0] * a * b fn(torch.randn(4, 3), torch.randn(4, 3)) fn(torch.randn(8, 3), torch.randn(8, 3)) Running this program with TORCH_LOGS=graph_code we see that these two calls are traced as def forward(self, l_a_: torch.Tensor, l_b_: torch.Tensor): mul = 4 * l_a_ mul_1 = mul * l_b_ return (mul_1,) def forward(self, s0: torch.SymInt, l_a_: torch.Tensor, l_b_: torch.Tensor): size = l_a_.size() getitem = size[0] mul = getitem * l_a_ mul_1 = mul * l_b_ return (mul_1,) In the first graph the shape is traced as a constant, but once it changes, it traces it symbolically using a SymInts. In general, a simpler way to see the shapes of the intermediary values is by running the program with TORCH_LOGS=graph_sizes TRACED GRAPH TENSOR SIZES ===== __compiled_fn_1 ===== l_a_: (s0, 3) l_a_ (concrete): (8, 3) l_b_: (s0, 3) l_b_ (concrete): (8, 3) mul: (s0, 3) mul (concrete): (8, 3) mul_1: (s0, 3) mul_1 (concrete): (8, 3) where we can see that the first dimension of the two tensor args is dynamic, given that it is represented by the s0 variable. We can find how Dynamo implements this by running TORCH_LOGS=guards # Guards first call check_tensor(L[\u0027a\u0027], torch.float32, device=None, requires_grad=False, size=[4, 3], stride=[3, 1]) check_tensor(L[\u0027b\u0027], torch.float32, device=None, requires_grad=False, size=[4, 3], stride=[3, 1]) # Guards second call check_tensor(L[\u0027a\u0027], torch.float32, device=None, requires_grad=False, size=[None, 3], stride=[3, 1]) check_tensor(L[\u0027b\u0027], torch.float32, device=None, requires_grad=False, size=[None, 3], stride=[3, 1]) L[\u0027b\u0027].size()[0] == L[\u0027a\u0027].size()[0] 2 \u003c= L[\u0027a\u0027].size()[0] We see that on the first call, the guards check that the tensors have some fixed sizes and strides. These guards fail in the second execution, so it retraces. Since it was an int guard that failed, in this second iteration it traces this int symbolically and it installs more general guards on this more generic kernel. Compilation performance tip. If you know that a dimension will vary in size, you can mark it as dynamic by calling torch._dynamo.mark_dynamic before calling torch.compile. This will avoid the first compilation with a static shape. There are other useful utility functions like maybe_mark_dynamic or mark_static. You can also have all integers and shapes traced by calling torch.compile(dynamic=True). This is mostly useful for debugging purposes. 0, 1 are always specialized# Regardless of whether we mark a dimension as dynamic, if we pass an input where that dimension is 0 or 1, Dynamo will trace it as non-dynamic and it will generate a specific graph for it. This is the reason why in the example above we find guards of the form 2 \u003c= L[\u0027a\u0027].size()[0]. There are several reasons for this choice. There are two particularly important - A tensor is empty if and only if any of its dimensions is zero - A tensor can only be contiguous if one of the strides is one This policy decision does NOT apply to plain Python ints; if we think a Python int should be compiled dynamically, we won\u2019t specialize them by default; instead, whether or not it gets specialized depends on its usage. Duck shaping# Dynamo performs what we call \u201cduck shaping\u201d. If two dynamic integers have the same value at trace time, we will assume that they are equal and guard on it. Effectively, this means that rather than having two symbols s0, s1 in the example above, we just unified them to s0 and had the guard L[\u0027b\u0027].size()[0] == L[\u0027a\u0027].size()[0]. This enables performing fusions within the compiler while being able to generate kernels that are generic enough. Guards on symbolic ints# We now understand how symbolic shapes are implemented at a high level and the properties they have. Now, why is that symbolic shapes forced us through the tricky route of getting control of the CPython interpreter? Consider the following example: import torch @torch.compile(dynamic=True) def fn(a): if a.shape[0] * 2 \u003c 16: return a else: return a + 1 fn(torch.randn(8)) This code has a guard of the form 2*L[\u0027a\u0027].size()[0] \u003e= 16. This is a non-trivial guard in terms of the inputs of the function, but it is registered in the middle of the execution of the program. Even more so, we cannot know this guard is needed until we see the if statement conditional on a SymNodeVariable argument. Such conditions are invisible to torch.jit.trace and require deep analysis of the python code. Debugging tip Running this code with TORCH_LOGS=dynamo tells us where this guard was added eval 2*s0 \u003e= 16 [guard added] at script.py:5 in fn (_dynamo/variables/tensor.py:812 in evaluate_expr) Placing a breakpoint there and looking at the backtrace is rather useful to understand where a guard came from. Making Dynamo Complete: Graph Breaks# With all the tools we have discussed, we have a tracer that can trace PyTorch operations on tensors and integers and has a caching system that knows when it can reuse a previously traced graph and when it needs to retrace. All this executing arbitrary Python code! There is just one small issue with this. The statement \u201cexecuting arbitrary Python code\u201d is perhaps a bit too general. Dynamo implements a good part of Python, but does it implement the more complex parts, like coroutines or async? Does it implement the whole Python standard library? NumPy also has a Python API. Does torch.compile also understand NumPy? and Django? [5] Python\u2019s ecosystem is massive, and a good part of it is written in other more performant languages like C++ or Rust, and it just exposes Python bindings. There is no hope in Dynamo tracing through Python objects that are implemented in C++. What can a tracer do when it finds an operation that it does not understand? The usual way machine learning tracers handle this issue is by informing the user that the operation they choked on and giving up tracing altogether. This would pose a real usability issue in the case of PyTorch, where its users are used to the flexibility it gives them. As a real-world example the doctr_det_predictor model uses NumPy and the cv2 library to postprocess the model\u2019s result. Here is another place where having access to CPython is interesting. Rather than erroring out, Dynamo can let CPython run that problematic code! To do this, Dynamo generates at trace time one graph with all the operations before the problematic code, and one with all the operations after. [6] Then, at runtime, it will delegate to CPython to execute the first graph, then the problematic code, and then the second graph. This process of stopping the tracing and generating multiple graphs is called a graph break. A small confession: I lied all throughout the introduction and the first sections. Dynamo does not generate one graph, but multiple graphs! For all practical purposes, starting retracing after a second graph can be thought of as starting tracing a new function. The new graph after the graph break will have its own guards, its new set of local variables, and so on. To discuss how to implement graph breaks, we need to first revisit how Dynamo interacts with CPython. Using PEP 523, CPython allows a user to use their own frame evaluation mechanism. What we had not discussed is that CPython also exposes its own frame evaluation for others to use. Dynamo leverages this to let the fast CPython interpreter run the compiled code. For a function without graph breaks, the whole tracing / execution process of a program that calls the function 2 times with the same arguments looks like this: In the first call to the function Dynamo traces the function into an FX graph The FX graph is compiled by the compiler (Inductor) into efficient low-level code\u2026 but that\u2019s a story for another day It rewrites the bytecode of the function so that it simply calls the compiled function It gives CPython this new bytecode and asks it to run it here In the second call to the function It checks the guards from the first call against the new arguments here. Since they are the same arguments as before, they pass It asks CPython to run the bytecode associated to those guards here This process on its own looks overly complicated. Why generate new bytecode and ask CPython to run it rather than simply creating a C++ binding to the compiled function and executing it? Well, this pattern allows us to implement graph breaks! The bytecode generated by a graph break has the following structure: Bytecode that executes the first graph Bytecode that leaves the stack as it would be if CPython would have executed the first graph. It also replays any modifications to local or global variables that would be visible at this point The bytecode that made Dynamo graph break Bytecode that executes the second graph Let us see this in a simple example import torch @torch.compile def fn(a): b = a + 2 print(\"Hi\") return b + a fn(torch.randn(4)) Running this with TORCH_LOGS=bytecode shows us the initial bytecode and the modified bytecode MODIFIED BYTECODE fn script.py line 3 0 LOAD_GLOBAL 1 (__compiled_fn_0) 2 LOAD_FAST 0 (a) 4 CALL_FUNCTION 1 6 STORE_FAST 3 (graph_out_0) 8 LOAD_GLOBAL 0 (print) 10 LOAD_CONST 2 (\u0027Hi\u0027) 12 LOAD_FAST 3 (graph_out_0) 14 LOAD_CONST 3 (0) 16 BINARY_SUBSCR 18 STORE_FAST 1 (b) 20 CALL_FUNCTION 1 22 LOAD_GLOBAL 2 (__resume_at_14_1) 24 ROT_TWO 26 LOAD_FAST 0 (a) 28 LOAD_FAST 1 (b) 30 CALL_FUNCTION 3 32 RETURN_VALUE MODIFIED BYTECODE resume_in_fn script.py line 6 0 LOAD_GLOBAL 1 (__compiled_fn_2) 2 LOAD_FAST 2 (b) 4 LOAD_FAST 1 (a) 6 CALL_FUNCTION 2 8 UNPACK_SEQUENCE 1 10 RETURN_VALUE We can see that the modified bytecode is split into two functions, fn, the original function, and a function called resume_in_fn. This second function is a function created by Dynamo to implement the execution of the program starting at the graph break. This is often called a continuation function. This continuation function simply calls the second compiled function with the right arguments. The code for the initial function is rewritten implementing the strategy that we described before L0-4. Call the compiled function (a + 2). L6. Store its result in a local variable called graph_out_0. graph_out_0 is a tuple L8-18. Leave the stack as it would be at the point of the graph break L20. Execute the code that caused the graph break L22-32. Call the compiled continuation function (a + b) The code generation of the stack in Dynamo is delegated to VariableTracker subclasses. Every VariableTracker object in Dynamo has a reconstruct method that generates the necessary bytecode to create the python object it represents on the stack. Debugging tip. Graph breaks hamper performance, and as such, it is best to avoid them. Running a program with TORCH_LOGS=graph_breaks is a great way to find how many graph breaks did our program hit. The information it returns is in terms of VariableTracker objects, so the debugging tips above are sometimes also helpful to figure out what caused that graph break. Conclusion# Dynamo is a complex piece of software. Once you sign up to implement a CPython interpreter you know you are in for a ride. That being said, we hope that this post helps demystify it a bit. Dynamo is (mostly) implemented in Python. We left plenty of links to the pieces of the code that we discussed. We hope that reading those pieces of code and grepping for the places that call them, or putting breakpoints on them and looking at the call stack helps understanding the rest of the code base. Of course, the best way to learn how a piece of software works is by extending it. In this case, the best way is to have a look at the open dynamo issues on github. Many of them require very minor changes in the code, once you find where you need to make those changes. Footnotes# Below are additional details and references for concepts mentioned in this document. [1] In the literature, this is called a Directed Acyclical Graph (DAG). [2] All this binding code lives in torch/csrc/dynamo/eval_frame.c. [3] In CPython lingo, the set of all these objects are called a frame. [4] There are also SymBool and SymFloat classes. The latter one is not used all that much at the time of this writing. [5] Interestingly enough, it does understand NumPy code! Have a look at this blogpost and the docs. Now, this is just possible because we reimplemented NumPy using PyTorch. Good luck implementing Django in PyTorch though\u2026 [6] Assuming there is just one piece of problematic code. If there are more, Dynamo can split the code into as many graphs as it needs.",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/user_guide/torch_compiler/torch.compiler_dynamo_deepdive.html"
       },
       "datePublished": "Apr 02, 2024T00:00:00Z",
       "dateModified": "Dec 03, 2025T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>