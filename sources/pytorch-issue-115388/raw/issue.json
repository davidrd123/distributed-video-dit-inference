{"url":"https://api.github.com/repos/pytorch/pytorch/issues/115388","repository_url":"https://api.github.com/repos/pytorch/pytorch","labels_url":"https://api.github.com/repos/pytorch/pytorch/issues/115388/labels{/name}","comments_url":"https://api.github.com/repos/pytorch/pytorch/issues/115388/comments","events_url":"https://api.github.com/repos/pytorch/pytorch/issues/115388/events","html_url":"https://github.com/pytorch/pytorch/issues/115388","id":2031658849,"node_id":"I_kwDOA-j9z855GKdh","number":115388,"title":"`torch.distributed.destroy_process_group()` hangs after CUDA graph capture of NCCL operations","user":{"login":"cbcase","id":238403,"node_id":"MDQ6VXNlcjIzODQwMw==","avatar_url":"https://avatars.githubusercontent.com/u/238403?v=4","gravatar_id":"","url":"https://api.github.com/users/cbcase","html_url":"https://github.com/cbcase","followers_url":"https://api.github.com/users/cbcase/followers","following_url":"https://api.github.com/users/cbcase/following{/other_user}","gists_url":"https://api.github.com/users/cbcase/gists{/gist_id}","starred_url":"https://api.github.com/users/cbcase/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cbcase/subscriptions","organizations_url":"https://api.github.com/users/cbcase/orgs","repos_url":"https://api.github.com/users/cbcase/repos","events_url":"https://api.github.com/users/cbcase/events{/privacy}","received_events_url":"https://api.github.com/users/cbcase/received_events","type":"User","user_view_type":"public","site_admin":false},"labels":[{"id":466131885,"node_id":"MDU6TGFiZWw0NjYxMzE4ODU=","url":"https://api.github.com/repos/pytorch/pytorch/labels/triage%20review","name":"triage review","color":"cc317c","default":false,"description":""},{"id":679953883,"node_id":"MDU6TGFiZWw2Nzk5NTM4ODM=","url":"https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed","name":"oncall: distributed","color":"f7e101","default":false,"description":"Add this issue/PR to distributed oncall triage queue"},{"id":2391776253,"node_id":"MDU6TGFiZWwyMzkxNzc2MjUz","url":"https://api.github.com/repos/pytorch/pytorch/labels/module:%20c10d","name":"module: c10d","color":"f7e101","default":false,"description":"Issues/PRs related to collective communications and process groups"},{"id":2588636288,"node_id":"MDU6TGFiZWwyNTg4NjM2Mjg4","url":"https://api.github.com/repos/pytorch/pytorch/labels/module:%20cuda%20graphs","name":"module: cuda graphs","color":"adb0ea","default":false,"description":"Ability to capture and then replay streams of CUDA kernels"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2023-12-07T23:01:16Z","updated_at":"2026-02-19T19:17:35Z","closed_at":null,"author_association":"NONE","type":null,"active_lock_reason":null,"sub_issues_summary":{"total":0,"completed":0,"percent_completed":0},"issue_dependencies_summary":{"blocked_by":0,"total_blocked_by":0,"blocking":0,"total_blocking":0},"body":"### üêõ Describe the bug\n\nWhen I use the CUDA graphs API to capture execution of a model that performs NCCL collectives as part of that execution, it puts the NCCL process group into a state where it cannot cleanly exit -- a call to `destroy_process_group()` hangs. Squinting at a profile, it looks like it is a hang in `ncclCommDestroy`. Here is a fairly small reproducer:\r\n\r\n```\r\nimport os\r\n\r\nimport torch\r\nimport torch.distributed as dist\r\n\r\nclass Model(torch.nn.Module):\r\n    def __init__(self, dim, nlayers):\r\n        super().__init__()\r\n        self.layers = torch.nn.ModuleList()\r\n        for _ in range(nlayers):\r\n            self.layers.append(torch.nn.Linear(dim, dim, bias=False))\r\n\r\n    @torch.inference_mode()\r\n    def forward(self, x):\r\n        for layer in self.layers:\r\n            x = layer(x)\r\n            dist.all_reduce(x)\r\n        return x\r\n\r\ndef main():\r\n    os.environ[\"NCCL_ASYNC_ERROR_HANDLING\"] = \"0\"\r\n    dist.init_process_group(backend=\"nccl\")\r\n    rank = dist.get_rank()\r\n    my_device = f\"cuda:{rank}\"\r\n    torch.cuda.set_device(my_device)\r\n\r\n    model = Model(2048, 12)\r\n    model.cuda()\r\n\r\n    g = torch.cuda.CUDAGraph()\r\n    static_input = torch.empty((1024, 2048), dtype=torch.float32, device=my_device)\r\n    s = torch.cuda.Stream()\r\n    s.wait_stream(torch.cuda.current_stream())\r\n    # Warmup\r\n    with torch.cuda.stream(s):\r\n        for _ in range(11):\r\n            static_output = model(static_input)\r\n    torch.cuda.current_stream().wait_stream(s)\r\n    with torch.cuda.graph(g):\r\n        static_output = model(static_input)\r\n\r\n    torch.cuda.synchronize()\r\n    print(\"Calling destroy_process_group()...\")\r\n    dist.destroy_process_group()\r\n    print(\"Done.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\nAssuming this file is `test.py`, I ran it with:\r\n```\r\ntorchrun --standalone --nproc-per-node 2 test.py\r\n```\r\nand you see both processes hang with `Calling destroy_process_group()...`.\r\n\r\nA few more notes:\r\n- It doesn't appear to matter whether you every replay the graph (I don't in the example above)\r\n- But if you remove the graph capture itself, it exits cleanly\r\n- And if you remove the `dist.all_reduce()` call inside the model definition, it also exits cleanly\r\n\r\nFor long-running training jobs, this isn't much of an issue, but for various testing and inference workloads, it is useful to have short-lived process groups (usually spawned with `torch.multiprocessing`), and it's important to clean those up without needing to tear down the parent process as well.\n\n### Versions\n\nCollecting environment information...\r\nPyTorch version: 2.1.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.4\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-75-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA RTX A5000\r\nGPU 1: NVIDIA RTX A5000\r\nGPU 2: NVIDIA RTX A5000\r\nGPU 3: NVIDIA RTX A5000\r\n\r\nNvidia driver version: 535.54.03\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   48 bits physical, 48 bits virtual\r\nCPU(s):                          64\r\nOn-line CPU(s) list:             0-63\r\nThread(s) per core:              1\r\nCore(s) per socket:              64\r\nSocket(s):                       1\r\nNUMA node(s):                    1\r\nVendor ID:                       AuthenticAMD\r\nCPU family:                      25\r\nModel:                           1\r\nModel name:                      AMD EPYC 7513 32-Core Processor\r\nStepping:                        1\r\nCPU MHz:                         2599.998\r\nBogoMIPS:                        5199.99\r\nVirtualization:                  AMD-V\r\nL1d cache:                       4 MiB\r\nL1i cache:                       4 MiB\r\nL2 cache:                        32 MiB\r\nL3 cache:                        16 MiB\r\nNUMA node0 CPU(s):               0-63\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr wbnoinvd arat npt lbrv nrip_save tsc_scale vmcb_clean pausefilter pfthreshold v_vmsave_vmload vgif umip pku ospke vaes vpclmulqdq rdpid arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.0\r\n[pip3] torch==2.1.0+cu121\r\n[pip3] torch-tb-profiler==0.4.1\r\n[pip3] torchaudio==2.1.0+cu121\r\n[pip3] torchvision==0.16.0+cu121\r\n[pip3] triton==2.1.0\r\n[conda] numpy                     1.26.0                   pypi_0    pypi\r\n[conda] torch                     2.1.0+cu121              pypi_0    pypi\r\n[conda] torch-tb-profiler         0.4.1                    pypi_0    pypi\r\n[conda] torchaudio                2.1.0+cu121              pypi_0    pypi\r\n[conda] torchvision               0.16.0+cu121             pypi_0    pypi\r\n[conda] triton                    2.1.0                    pypi_0    pypi\r\n\n\ncc @ezyang @gchanan @zou3519 @kadeng @msaroufim @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @mcarilli @eellison @penguinwu @BoyuanFeng @chauhang @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @kwen2501 @XilunWu @tianyu-l @yf225","closed_by":null,"reactions":{"url":"https://api.github.com/repos/pytorch/pytorch/issues/115388/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"timeline_url":"https://api.github.com/repos/pytorch/pytorch/issues/115388/timeline","performed_via_github_app":null,"state_reason":null,"pinned_comment":null}