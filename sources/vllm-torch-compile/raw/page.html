<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Introduction to torch.compile and How It Works with vLLM | vLLM Blog</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Introduction to torch.compile and How It Works with vLLM" />
<meta name="author" content="Luka Govedič (Red Hat), Richard Zou (Meta), Addie Stevens (Red Hat), Kaichao You (Tsinghua University), Michael Goin (Red Hat), Saša Zelenović (Red Hat)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="[!NOTE] This blog originated from our biweekly vLLM office hours, a community forum hosted by Red Hat with vLLM project committers and the UC Berkeley team. Each session covers recent updates, a deep dive with a guest speaker, and open Q&amp;A. Join us every other Thursday at 2:00 PM ET / 11:00 AM PT on Google Meet, and get the recording and slides afterward on our YouTube playlist." />
<meta property="og:description" content="[!NOTE] This blog originated from our biweekly vLLM office hours, a community forum hosted by Red Hat with vLLM project committers and the UC Berkeley team. Each session covers recent updates, a deep dive with a guest speaker, and open Q&amp;A. Join us every other Thursday at 2:00 PM ET / 11:00 AM PT on Google Meet, and get the recording and slides afterward on our YouTube playlist." />
<link rel="canonical" href="https://blog.vllm.ai/2025/08/20/torch-compile.html" />
<meta property="og:url" content="https://blog.vllm.ai/2025/08/20/torch-compile.html" />
<meta property="og:site_name" content="vLLM Blog" />
<meta property="og:image" content="https://blog.vllm.ai/assets/logos/vllm-logo-text-light.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-20T00:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://blog.vllm.ai/assets/logos/vllm-logo-text-light.png" />
<meta property="twitter:title" content="Introduction to torch.compile and How It Works with vLLM" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Luka Govedič (Red Hat), Richard Zou (Meta), Addie Stevens (Red Hat), Kaichao You (Tsinghua University), Michael Goin (Red Hat), Saša Zelenović (Red Hat)"},"dateModified":"2025-08-20T00:00:00+00:00","datePublished":"2025-08-20T00:00:00+00:00","description":"[!NOTE] This blog originated from our biweekly vLLM office hours, a community forum hosted by Red Hat with vLLM project committers and the UC Berkeley team. Each session covers recent updates, a deep dive with a guest speaker, and open Q&amp;A. Join us every other Thursday at 2:00 PM ET / 11:00 AM PT on Google Meet, and get the recording and slides afterward on our YouTube playlist.","headline":"Introduction to torch.compile and How It Works with vLLM","image":"https://blog.vllm.ai/assets/logos/vllm-logo-text-light.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.vllm.ai/2025/08/20/torch-compile.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://blog.vllm.ai/assets/logos/vllm-logo-only-light.png"},"name":"Luka Govedič (Red Hat), Richard Zou (Meta), Addie Stevens (Red Hat), Kaichao You (Tsinghua University), Michael Goin (Red Hat), Saša Zelenović (Red Hat)"},"url":"https://blog.vllm.ai/2025/08/20/torch-compile.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://blog.vllm.ai/feed.xml" title="vLLM Blog" /><script async src="https://www.googletagmanager.com/gtag/js?id=G-9C5R3JR3QS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){window.dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9C5R3JR3QS');
</script>
<style>.markdown-alert{padding:.5rem 1rem;margin-bottom:1rem;color:inherit;border-left:.25em solid #30363d}.markdown-alert>:first-child{margin-top:0}.markdown-alert>:last-child{margin-bottom:0}.markdown-alert .markdown-alert-title{display:flex;font-weight:500;align-items:center;line-height:1}.markdown-alert svg{margin-right:.5rem!important}.markdown-alert svg path{fill:currentColor}.markdown-alert.markdown-alert-note{border-left-color:#4493f8}.markdown-alert.markdown-alert-note .markdown-alert-title{color:#4493f8}.markdown-alert.markdown-alert-important{border-left-color:#ab7df8}.markdown-alert.markdown-alert-important .markdown-alert-title{color:#ab7df8}.markdown-alert.markdown-alert-warning{border-left-color:#9e6a03}.markdown-alert.markdown-alert-warning .markdown-alert-title{color:#d29922}.markdown-alert.markdown-alert-tip{border-left-color:#238636}.markdown-alert.markdown-alert-tip .markdown-alert-title{color:#3fb950}.markdown-alert.markdown-alert-caution{border-left-color:#da3633}.markdown-alert.markdown-alert-caution .markdown-alert-title{color:#f85149}</style></head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">vLLM Blog</a></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Introduction to torch.compile and How It Works with vLLM</h1>
    <p class="post-meta"><time class="dt-published" datetime="2025-08-20T00:00:00+00:00" itemprop="datePublished">
        Aug 20, 2025
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Luka Govedič (Red Hat), Richard Zou (Meta), Addie Stevens (Red Hat), Kaichao You (Tsinghua University), Michael Goin (Red Hat), Saša Zelenović (Red Hat)</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg> Note</p><p>This blog originated from our biweekly vLLM office hours, a community forum hosted by Red Hat with vLLM project committers and the UC Berkeley team. Each session covers recent updates, a deep dive with a guest speaker, and open Q&amp;A. <a href="https://red.ht/office-hours">Join us every other Thursday</a> at 2:00 PM ET / 11:00 AM PT on Google Meet, and get the recording and slides afterward on our <a href="https://www.youtube.com/playlist?list=PLbMP1JcGBmSHxp4-lubU5WYmJ9YgAQcf3">YouTube playlist</a>.</p>
</div>

<h2 id="introduction">Introduction</h2>

<p>Fast large language model (LLM) inference today requires executing models as efficiently as possible across diverse hardware, workloads, and scale. Efficient execution requires heavily optimized kernels that often require hand-tuning for different models and platforms. <strong>torch.compile</strong>, PyTorch’s just-in-time (JIT) compiler generates optimized kernels automatically, which makes PyTorch code run significantly faster without requiring developers to manually optimize kernels across all supported hardware platforms.</p>

<p>For vLLM, the de-facto open-source inference engine for portable and efficient LLM inference, torch.compile isn’t just a performance enhancer. It’s a core component that shifts the responsibility of optimization from model developers to the compiler. Instead of requiring changes to model definitions, optimizations are applied during compilation, enabling cleaner separation of concerns and achieving maximal performance. In this post, we’ll walk through how torch.compile works, how it’s integrated into vLLM, and how vLLM uses custom compiler passes to maximize performance. We will also discuss ongoing and future work on the torch.compile integration in vLLM to further improve its usability and performance.</p>

<h2 id="what-is-torchcompile">What Is torch.compile?</h2>

<p>torch.compile lets you optimize PyTorch code with minimal effort: using torch.compile is as simple as applying a decorator to a function or torch.nn.Module. torch.compile automatically captures tensor operations into a computation graph that it then generates optimized code for.</p>

<p>In the following example, torch.compile produces a single fused kernel for all pointwise operations in function <code class="language-plaintext highlighter-rouge">fn</code>. It captures and compiles the function just-in-time, potentially recompiling if any of the capture conditions (e.g. input shapes) change.</p>

<p align="center">
<picture>
<img src="/assets/figures/2025-torch-compile/figure1.png" width="100%" />
</picture><br />
<b>Figure 1</b>: torch.compile is a JIT compiler for PyTorch code. You can wrap functions, nn.Modules, and other callables in torch.compile.
</p>

<p>There are multiple ways to use torch.compile. You can use it as a kernel generator (like in Figure 1), where we compile a function. But you can also apply torch.compile to your full nn.Module model or submodules of it. Depending on the structure of the model and your requirements (e.g. compile times), <a href="https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html#setting-expectations">we recommend applying torch.compile in different places</a>.</p>

<h2 id="why-use-torchcompile">Why Use torch.compile?</h2>

<p>One way of optimizing models is to write custom CPU/CUDA operations that perform the same operations as in the model but faster. Writing custom kernels for every model is time-consuming and requires a deep understanding of performance and hardware. torch.compile gets you a decent amount of the way to peak performance with almost no additional engineering effort. For example, PyTorch’s <a href="https://hud.pytorch.org/benchmark/compilers">open source TorchBench benchmark suite</a> shows 1.8-2x geomean speedups on 80+ models.</p>

<p align="center">
<picture>
<img src="/assets/figures/2025-torch-compile/figure2.png" width="100%" />
</picture><br />
<b>Figure 2</b>: torch.compile gives you fast baseline performance to save YOU development time from tuning model performance.
</p>

<h2 id="how-torchcompile-works">How torch.compile Works</h2>

<p>The torch.compile pipeline consists of two major stages: the frontend (TorchDynamo) and backend (TorchInductor). We’ll give a brief overview, but for more details, please see the <a href="https://docs.pytorch.org/assets/pytorch2-2.pdf">official PyTorch 2 paper</a>.</p>

<h3 id="1-frontend-torchdynamo-graph-capture">1. Frontend (TorchDynamo): Graph Capture</h3>

<p>torch.compile’s frontend is a custom bytecode interpreter. It traces arbitrary Python functions and extracts straight-line <a href="https://docs.pytorch.org/docs/stable/fx.html">torch.fx</a> graphs that consist only of Tensor operations. One of torch.compile’s key features that gives it good coverage over all Python code is <strong>graph breaks</strong>. Whenever torch.compile sees an operation it cannot support, it doesn’t error. Instead, it ends the current graph being traced, runs the operation, and then begins to trace out a new graph. torch.compile sends each graph that gets traced to the backend for optimization.</p>

<p>In the following code example, torch.save is an unsupported operation: torch.compile doesn’t know how to perform disk I/O. Applying torch.compile to the function <code class="language-plaintext highlighter-rouge">f</code> is equivalent to applying torch.compile to the region of compute before the call to torch.save and the region after torch.save.</p>

<p align="center">
<picture>
<img src="/assets/figures/2025-torch-compile/figure3.png" width="100%" />
</picture><br />
<b>Figure 3</b>: torch.compile captures straight-line graphs of Tensor operations and works around unsupported operations like torch.save.
</p>

<h3 id="2-backend-torchinductor-optimization-and-kernel-generation">2. Backend (TorchInductor): Optimization and Kernel Generation</h3>

<p>torch.compile’s backend receives graphs from the frontend and optimizes them via graph passes and lowering to optimized C++, triton, or other kernels. It is able to:</p>

<ul>
  <li>Fuse pointwise and reduction operations</li>
  <li>Auto-tune kernel configurations like block sizes</li>
  <li>Choose between different backends for matmul (cuBLAS, Triton, CUTLASS) and perform prologue and epilogue fusion.</li>
  <li>Use CUDA Graphs to cache and replay kernel launches efficiently</li>
</ul>

<p>CUDA Graphs is one example where having a compiler is helpful. CUDA Graphs reduce launch overhead but require certain assumptions on your code (e.g. it must only use CUDA operations, input Tensors must have static memory addresses). torch.compile is able to automatically split graphs at unsupported operations to create smaller graphs that are safe to CUDA Graph as well as automatically manage static input buffers.</p>

<h2 id="vllm-integration">vLLM Integration</h2>

<p>vLLM V1 integrates torch.compile by default for both online and offline inference. You can disable it using <code class="language-plaintext highlighter-rouge">-O0</code> or <code class="language-plaintext highlighter-rouge">--enforce-eager</code>, but for most use cases, leaving it on provides performance benefits. <a href="https://docs.vllm.ai/en/latest/design/v1/torch_compile.html">See the docs for more details</a>.</p>

<h3 id="compilation-cache">Compilation Cache</h3>

<p>vLLM compiles models during cold start and saves the artifacts (FX graphs, Triton kernels) in a cache directory (by default, <code class="language-plaintext highlighter-rouge">~/.cache/vllm/torch_compile_cache</code>). On warm start, the artifacts are retrieved from the cache. You can disable the cache via <code class="language-plaintext highlighter-rouge">VLLM_DISABLE_COMPILE_CACHE=1</code> or by deleting the cache directory.</p>

<p>The compiled artifacts and the cache can be reused across machines with the same environment. If you have an autoscaling use case, make sure to generate the cache directory once and share it among instances.</p>

<p align="center">
<picture>
<img src="/assets/figures/2025-torch-compile/figure4.png" width="100%" />
</picture><br />  
<b>Figure 4</b>: Compiled artifacts are cached after cold start and can be reused across machines to ensure fast, consistent startup when set up correctly.
</p>

<h3 id="dynamic-batch-sizes-and-specialization">Dynamic Batch Sizes and Specialization</h3>

<p>By default, vLLM compiles a single graph with a dynamic batch size that supports all possible batch sizes. This means one artifact can serve variable input sizes. However, specializing for known batch sizes—like 1, 2, or 4—can yield performance improvements.</p>

<p>Use <code class="language-plaintext highlighter-rouge">compile_sizes: [1, 2, 4]</code> in your config to trigger this specialization. Under the hood, this tells torch.compile to compile for these static sizes and possibly perform more autotuning to select the best kernels.</p>

<p align="center">
<picture>
<img src="/assets/figures/2025-torch-compile/figure5_a.png" width="100%" />
<img src="/assets/figures/2025-torch-compile/figure5_b.png" width="100%" />
</picture><br />  
<b>Figure 5</b>: How to specify specializing compilation on specific batch sizes.
</p>

<h3 id="piecewise-cuda-graphs">Piecewise CUDA Graphs</h3>

<p>Not all operations are compatible with CUDA Graphs; for example, <a href="https://docs.vllm.ai/en/latest/design/v1/torch_compile.html#full-cudagraph-capture">cascade attention is not</a>. vLLM works around this by breaking the captured graph into CUDA Graph -safe and -unsafe parts and executing them separately. This gives us the performance benefits of CUDA Graphs without losing correctness.</p>

<p align="center">
<picture>
<img src="/assets/figures/2025-torch-compile/figure6.png" width="100%" />
</picture><br />  
<b>Figure 6</b>: Piecewise CUDA Graphs in vLLM capture and replay supported GPU kernel sequences for low-overhead execution, while skipping unsupported operations like cascade attention.
</p>

<h2 id="custom-compiler-passes-in-vllm">Custom Compiler Passes in vLLM</h2>

<p>While torch.compile includes many built-in optimizations, vLLM adds custom compiler passes that apply additional optimizations to further improve performance. .</p>

<h3 id="why-custom-passes">Why Custom Passes?</h3>

<p>Model authors write declarative, modular code that focuses on correctness and uses clean abstractions, separating higher-level operations into separate submodules and grouping them by layer. However, achieving peak performance often requires breaking those abstractions, like fusing operations across submodules and layers. Rather than rewriting the models, vLLM custom passes rewrite the torch.fx graph.</p>

<p>These passes:</p>

<ul>
  <li>Fuse memory-bound custom ops like activation functions and quantization</li>
  <li>Add optimizations not present in Inductor (like removing additional no-ops)</li>
</ul>

<h3 id="example-silu--quantize-fusion">Example: SiLU + Quantize Fusion</h3>

<p>A common pattern in quantized MLPs is SiLU activation followed by a quantized down-projection linear layer. The quantized linear layer consists of a quantization operation on the input, followed by a quantized matrix multiplication. Individually, SiLU and quantization operations are slow and memory-bound. Using the Inductor pattern matcher utility, the <code class="language-plaintext highlighter-rouge">ActivationFusionPass</code> custom pass in vLLM replaces them with a single fused kernel, improving throughput by up to 8 percent.</p>

<p align="center">
<picture>
<img src="/assets/figures/2025-torch-compile/figure7.png" width="100%" />
</picture><br />
<b>Figure 7</b>: On Llama 3.1 405B quantized to FP8, tested on 8x AMD MI300s, fused kernels (<code>fusion</code>, in yellow) outperformed both <code>default</code> (using torch ops for RMSNorm and SiLU and custom FP8 quant kernel) and <code>custom</code> (unfused custom kernels). 
</p>

<p align="center">
<picture>
<img src="/assets/figures/2025-torch-compile/figure8.png" width="100%" />
</picture><br />
<b>Figure 8</b>: Detailed throughput speedup comparing <code>fusion</code> and <code>default</code> regimes above. If all quantization overhead (8%) was removed via fusion, the theoretical maximum improvement to throughput would be 8%, and we can see that improvement reached in some cases.
</p>

<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg> Note</p><p>Since the office hours, we have added an implementation of quantization using torch operations, which (when compiled by Inductor) is faster than the custom CUDA/ROCm kernel. Because Inductor can fuse those torch ops with the SiLU torch ops automatically, the SiLU+quant and RMSNorm+quant passes are now obsolete in some cases. However, any fusion involving custom ops (attention, collectives, sub-byte quantization) continues to require custom passes. We present the SiLU+Quant example for consistency with the office hours slides and recording, but other fusion passes work in a very similar way.</p>
</div>

<h3 id="example-sequence-parallelism--async-tp">Example: Sequence Parallelism + Async TP</h3>

<p>When using Tensor Parallelism (TP), the linear layer shards the weights and computes incomplete matrix multiplication results, which need to be synchronized across GPUs. When using separate kernels for the compute and communication pieces, we incur communication overhead as the GPUs sit idle while waiting for the network latency of communication results.</p>

<p>Instead, we can overlap computation and communication by using fused GEMM+collective kernels. One example of such kernels are the GEMM+reduce_scatter and all_gather+GEMM kernels. To utilize these kernels, we need to decompose the all_reduce collective operation into a reduce_scatter and an all_gather while also postponing the all_gather until after layernorm to allow it to fuse with the following GEMM.</p>

<p>If we were to implement this kind of optimization in model definitions, we would have to touch every model vLLM supports (there are hundreds of them!). It would be intrusive, break abstractions, increase developer friction, and be unlikely to be accepted into vLLM in the first place. Instead, by implementing the optimization in torch.compile, it is contained to just 2 custom passes and can be turned on using CLI flags, providing better performance for all models supported by vLLM.</p>

<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg> Note</p><p>This optimization was implemented in full by a community member <a href="https://github.com/cascade812">@cascade812</a> who we thank for the incredible contribution. More information on Async TP can be found on the <a href="https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487">PyTorch blog</a>.</p>
</div>

<h3 id="current-and-upcoming-passes">Current and Upcoming Passes</h3>

<p><strong>Available Today:</strong></p>
<ul>
  <li>Fusion passes:
    <ul>
      <li>RMSNorm + Quant (FP8) fusion</li>
      <li>SiLU-Mul + Quant (FP8) fusion</li>
      <li>Attention + Quant (FP8) fusion (up to 7% improvement)</li>
      <li>AllReduce + RMSNorm fusion (up to 15% improvement)</li>
      <li>AllReduce + RMSNorm + Quant (FP8) fusion (up to 8% improvement)</li>
      <li>AllReduce + RMSNorm + Quant (FP4) fusion (up to 10% improvement)</li>
      <li>Sequence Parallelism &amp; Async TP (up to 10% improvement)</li>
    </ul>
  </li>
  <li>Other passes:
    <ul>
      <li>No-op Elimination: eliminates or simplifies redundant reshape operations</li>
      <li>Fix Functionalization: manually reinplaces auto_functionalized operations to avoid redundant copies and memory use</li>
    </ul>
  </li>
</ul>

<p><strong>Coming Soon:</strong></p>
<ul>
  <li>Attention + Quant (FP4) fusion: <a href="https://github.com/vllm-project/vllm/pull/22703">#22703</a></li>
  <li>SiLU-Mul + Quant (FP4) fusion: <a href="https://github.com/vllm-project/vllm/pull/22448">#22448</a></li>
</ul>

<p>Passes can be added via the <code class="language-plaintext highlighter-rouge">PostGradPassManager</code>, CLI (<code class="language-plaintext highlighter-rouge">--compilation-config</code>), or by specifying a config object in offline mode. This allows users of vLLM to perform custom graph transformations (kernel substitution or something else) required by their use case without modifying vLLM source code.</p>

<h2 id="future-work">Future Work</h2>

<p>We’ve come very far on the vLLM-torch.compile integration. Here are some areas that we’re focusing on in the next six months.</p>

<p><strong>Improving stability</strong><br />
The vLLM-torch.compile integration uses many private (begin with an underscore) torch.compile APIs and relies on unstable implementation details. We did this because using the public torch.compile API wasn’t sufficient to fulfill our requirements - vLLM wants fast serving performance and no recompilations during model serving. This has led to issues like weird caching issues, or needing to disable vLLM’s torch.compile cache for certain models. The PyTorch compiler team is working on upstreaming vLLM (and general inference) related features from vLLM to torch.compile and migrating vLLM to using more stable APIs. A lot of these features are already present in torch 2.8, which is coming to vLLM <a href="https://github.com/vllm-project/vllm/pull/20358">soon</a>!</p>

<p><strong>Improving start-up time</strong><br />
We’ve heard that start-up time is a huge pain point with vLLM torch.compile and CUDAGraphs, especially in the autoscaling setting where one dynamically spins up new machines according to demand. We plan to significantly reduce both cold (first time) and warm (second time and on) start up for vLLM, especially as related to Dynamo and Inductor compilation. Please follow the <a href="https://github.com/vllm-project/vllm/issues?q=is%3Aissue%20state%3Aopen%20label%3Astartup-ux">startup-ux label</a> on GitHub or join the <a href="https://vllm-dev.slack.com/archives/C0911AKUZQX">#feat-startup-ux</a> channel on <a href="http://slack.vllm.ai">vLLM Slack</a> to stay updated on the progress!</p>

<p>An important UX improvement is the <a href="https://github.com/vllm-project/vllm/issues/20283">planned revamp of the <code class="language-plaintext highlighter-rouge">-O</code> command-line flag</a>. By specifying <code class="language-plaintext highlighter-rouge">-O&lt;n&gt;</code> on the vLLM CLI (where <code class="language-plaintext highlighter-rouge">n</code> is an integer between 0-3), users will get easier direct control over trading off startup time for performance. While <code class="language-plaintext highlighter-rouge">-O0</code> will perform almost no optimizations and spin up as quickly as possible, <code class="language-plaintext highlighter-rouge">-O3</code> will take much longer but provide the best possible performance.</p>

<p><strong>Custom pass improvements</strong><br />
We are planning on making a few broad improvements to the custom pass mechanism to increase their flexibility and make them easier to write, as well as improve the final performance of applied optimizations:</p>

<ul>
  <li>Compile multiple dynamic shape <code class="language-plaintext highlighter-rouge">torch.fx</code> graphs. This would let us specialize the forward pass graph depending on the size of the batch without compiling for each static size separately. More information in the <a href="https://github.com/vllm-project/vllm/issues/23113">RFC</a>.</li>
  <li>Enable matching torch implementations of custom ops. Currently, custom ops (rms_norm, quant, etc.) need to be enabled to allow pattern matching and fusing them, but there might be custom ops that don’t end up getting fused (especially for quant which happens 4x per layer). Those ops are slower than their torch equivalents, which reduces the benefits of fusion. We have a working prototype that pattern-matches torch implementations of custom ops, promising further performance improvements.</li>
</ul>

<p><strong>Experimental torch.compile backend integration</strong><br />
We are also exploring an experimental MPK/Mirage compiler integration. MPK is a precision-scheduling megakernel compiler, meaning it produces a single kernel for the whole model forward pass, which can further reduce CPU overheads and eliminate kernel launch overhead as compared to CUDA Graphs. More information on the proposed integration in the <a href="https://github.com/vllm-project/vllm/issues/22201">RFC</a>.</p>

<p><strong>Other performance improvements</strong><br />
The goal of vLLM’s torch.compile integration is provide good baseline performance to avoid needing to write and maintain a significant amount of custom kernels. We will continue to maintain and improve performance. Some highlights of work-in-progress work includes:</p>

<ul>
  <li>Improved <a href="https://github.com/vllm-project/vllm/issues/19765">FlexAttention</a> support. FlexAttention is an API that allows the use  of different attention variants without needing to write a custom attention kernel for each. Under the hood, it uses torch.compile to produce a custom triton template.</li>
  <li><a href="https://github.com/vllm-project/vllm/pull/20059">Full CUDA Graphs</a> support for Flash Attention v2 and FlashInfer. Full CUDAGraphs have less overhead than piecewise CUDA Graphs and should improve performance in those high-overhead settings.</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>torch.compile provides a powerful and accessible way to accelerate PyTorch models. In vLLM, it’s a core part of the inference pipeline. Combined with caching, dynamic shape support, CUDA Graphs, and custom passes, it enables efficient, scalable LLM serving across any environment.</p>

<p>As the compiler stack matures and support for new hardware expands, torch.compile and vLLM will continue to push the boundaries of inference performance—while keeping model development clean and modular.
Read more about torch.compile in the <a href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html">PyTorch documentation</a> and the <a href="https://docs.vllm.ai/en/latest/design/v1/torch_compile.html">vLLM documentation</a>, and join the <a href="https://vllm-dev.slack.com/archives/C08K1FAHFPH">#sig-torch-compile channel</a> on <a href="http://slack.vllm.ai">vLLM Slack</a> to ask questions, share feedback, and contribute your own custom passes!</p>

  </div><a class="u-url" href="/2025/08/20/torch-compile.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="https://blog.vllm.ai/feed.xml">
            <svg class="svg-icon orange">
              <path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194
                11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017 0
                13.806c0-1.21.983-2.195 2.194-2.195zM10.606
                16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"
              />
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">© 2026. vLLM Team. All rights reserved.</li>
          
        </ul>
      </div>
      <div class="footer-col">
        <p>vLLM is a fast and easy-to-use library for LLM inference and serving.
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
    <a rel="me" href="https://github.com/vllm-project/vllm" target="_blank" title="vLLM repository on GitHub">
      <span class="grey fa-brands fa-github fa-lg"></span>
    </a>
  </li><li>
    <a rel="me" href="https://x.com/vllm_project" target="_blank" title="vLLM on X (formerly Twitter)">
      <span class="grey fa-brands fa-x-twitter fa-lg"></span>
    </a>
  </li><li>
    <a rel="me" href="https://www.linkedin.com/company/vllm-project" target="_blank" title="vLLM on LinkedIn">
      <span class="grey fa-brands fa-linkedin fa-lg"></span>
    </a>
  </li></ul>
</div>

  </div>

</footer>
</body>

</html>
