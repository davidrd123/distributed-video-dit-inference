{"url":"https://api.github.com/repos/pytorch/pytorch/issues/138773","repository_url":"https://api.github.com/repos/pytorch/pytorch","labels_url":"https://api.github.com/repos/pytorch/pytorch/issues/138773/labels{/name}","comments_url":"https://api.github.com/repos/pytorch/pytorch/issues/138773/comments","events_url":"https://api.github.com/repos/pytorch/pytorch/issues/138773/events","html_url":"https://github.com/pytorch/pytorch/issues/138773","id":2610080927,"node_id":"I_kwDOA-j9z86bkqyf","number":138773,"title":"[funcol] functional collectives are 67% slower than torch.distributed collectives","user":{"login":"jc-bytedance","id":101832757,"node_id":"U_kgDOBhHYNQ","avatar_url":"https://avatars.githubusercontent.com/u/101832757?v=4","gravatar_id":"","url":"https://api.github.com/users/jc-bytedance","html_url":"https://github.com/jc-bytedance","followers_url":"https://api.github.com/users/jc-bytedance/followers","following_url":"https://api.github.com/users/jc-bytedance/following{/other_user}","gists_url":"https://api.github.com/users/jc-bytedance/gists{/gist_id}","starred_url":"https://api.github.com/users/jc-bytedance/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jc-bytedance/subscriptions","organizations_url":"https://api.github.com/users/jc-bytedance/orgs","repos_url":"https://api.github.com/users/jc-bytedance/repos","events_url":"https://api.github.com/users/jc-bytedance/events{/privacy}","received_events_url":"https://api.github.com/users/jc-bytedance/received_events","type":"User","user_view_type":"public","site_admin":false},"labels":[{"id":679952992,"node_id":"MDU6TGFiZWw2Nzk5NTI5OTI=","url":"https://api.github.com/repos/pytorch/pytorch/labels/module:%20performance","name":"module: performance","color":"f7e101","default":false,"description":"Issues related to performance, either of kernel code or framework glue"},{"id":679953883,"node_id":"MDU6TGFiZWw2Nzk5NTM4ODM=","url":"https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed","name":"oncall: distributed","color":"f7e101","default":false,"description":"Add this issue/PR to distributed oncall triage queue"},{"id":1301347167,"node_id":"MDU6TGFiZWwxMzAxMzQ3MTY3","url":"https://api.github.com/repos/pytorch/pytorch/labels/triaged","name":"triaged","color":"006b75","default":false,"description":"This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2024-10-24T00:06:26Z","updated_at":"2024-11-03T02:06:37Z","closed_at":null,"author_association":"NONE","type":null,"active_lock_reason":null,"sub_issues_summary":{"total":0,"completed":0,"percent_completed":0},"issue_dependencies_summary":{"blocked_by":0,"total_blocked_by":0,"blocking":0,"total_blocking":0},"body":"### üêõ Describe the bug\n\nHi torch distributed team!\r\n\r\nAs we discussed in PTC, we found functional collectives are 34%~67% slower than c10d collectives due to the heavy CPU overhead.\r\n\r\nTo be specific, we benchmarked functional collectives (the big three: all_gather, reduce_scatter, all_reduce) in torch 2.4 and compared them with the c10d collectives. Here is the summary:\r\n\r\n| collective    | c10d time | funcol time  |\r\n|---------------|-----------|--------------|\r\n| AllGather     | 131us     | 205us (156%) |\r\n| ReduceScatter | 122us     | 164us (134%) |\r\n| AllReduce     | 89us      | 149us (167%) |\r\n\r\nWe believe the overhead of funcol comes from extra copy, extra wrapping, and extra aten ops due to the tracing requirement for torch.compile.\r\n\r\nHere are the profiles:\r\n\r\n- AllGather\r\n\r\n    #### torch.distributed.all_gather (totally 131us with creating empty tensor)\r\n    ![image](https://github.com/user-attachments/assets/87df7aef-1dc7-4345-82e6-e9d89bb8d409)\r\n\r\n    #### funcol.all_gather (205us)\r\n    ![image](https://github.com/user-attachments/assets/7c4fcfba-685f-4f31-8958-81d845e8717a)\r\n\r\n- ReduceScatter\r\n    \r\n    #### torch.distributed.reduce_scatter\r\n    ![image](https://github.com/user-attachments/assets/c56e3d6b-3a6e-4a2a-b0af-1263b1aff48b)\r\n    #### funcol.reduce_scatter\r\n    ![image](https://github.com/user-attachments/assets/a5916e58-682c-4f0c-b1b5-39c73f321850)\r\n\r\n- AllReduce \r\n\r\n    #### torch.distributed.all_reduce\r\n    ![image](https://github.com/user-attachments/assets/3e01f159-97e8-4885-be88-61057641108d)\r\n    #### funcol.all_reduce\r\n    ![image](https://github.com/user-attachments/assets/af3e3f8f-0e6a-4da6-b64d-2ccfd3551a89)\r\n\r\nWe can reproduce the trace with the following code. Thanks : )\r\n\r\n```\r\nimport os\r\nimport torch\r\nimport torch.distributed as dist\r\nfrom torch.distributed.device_mesh import init_device_mesh\r\nimport torch.distributed._functional_collectives as funcol\r\n\r\nworld_size = int(os.environ[\"WORLD_SIZE\"])\r\n\r\n\r\nclass TorchCollectiveProfile:\r\n    @property\r\n    def world_size(self):\r\n        return world_size\r\n\r\n    @property\r\n    def device_type(self):\r\n        return \"cuda\"\r\n\r\n    def profile_funcol_all_gather(self):\r\n        device_mesh = init_device_mesh(self.device_type, (self.world_size,))\r\n        tensor = torch.randn(128, 128).cuda()\r\n        torch.cuda.synchronize()\r\n        dist.barrier()\r\n        if device_mesh.get_rank() == 0:\r\n            with torch.profiler.profile(\r\n                schedule=torch.profiler.schedule(wait=5, warmup=10, active=5),\r\n                on_trace_ready=lambda p: p.export_chrome_trace(\r\n                    \"test_profile_funcol_all_gather_\" + str(p.step_num) + \".json\"\r\n                ),\r\n                with_stack=True,\r\n            ) as p:\r\n                for _ in range(20):\r\n                    _ = funcol.all_gather_tensor(tensor, 1, device_mesh)\r\n                    p.step()\r\n        else:\r\n            for _ in range(20):\r\n                _ = funcol.all_gather_tensor(tensor, 1, device_mesh)\r\n\r\n        dist.barrier()\r\n        torch.cuda.synchronize()\r\n\r\n    def profile_c10d_all_gather(self):\r\n        device_mesh = init_device_mesh(self.device_type, (self.world_size,))\r\n        tensor = torch.randn(128, 128).cuda()\r\n\r\n        def c10d_call():\r\n            output_tensor = torch.empty(128, 128 * self.world_size, dtype=tensor.dtype, device=tensor.device)\r\n            _ = dist.all_gather_into_tensor(output_tensor, tensor, device_mesh.get_group())\r\n\r\n        torch.cuda.synchronize()\r\n        dist.barrier()\r\n        if device_mesh.get_rank() == 0:\r\n            with torch.profiler.profile(\r\n                schedule=torch.profiler.schedule(wait=5, warmup=10, active=5),\r\n                on_trace_ready=lambda p: p.export_chrome_trace(\r\n                    \"test_profile_c10d_all_gather_\" + str(p.step_num) + \".json\"\r\n                ),\r\n                with_stack=True,\r\n            ) as p:\r\n                for _ in range(20):\r\n                    c10d_call()\r\n                    p.step()\r\n        else:\r\n            for _ in range(20):\r\n                c10d_call()\r\n\r\n        dist.barrier()\r\n        torch.cuda.synchronize()\r\n```\r\n\n\n### Versions\n\nPyTorch version: 2.4.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\r\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.2 (default, Feb 28 2021, 17:03:44) [GCC 10.2.1 20210110] (64-bit runtime)\r\nPython platform: Linux-5.15.120.bsk.2-amd64-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A800-SXM4-40GB\r\nGPU 1: NVIDIA A800-SXM4-40GB\r\nGPU 2: NVIDIA A800-SXM4-40GB\r\nGPU 3: NVIDIA A800-SXM4-40GB\r\nGPU 4: NVIDIA A800-SXM4-40GB\r\nGPU 5: NVIDIA A800-SXM4-40GB\r\nGPU 6: NVIDIA A800-SXM4-40GB\r\nGPU 7: NVIDIA A800-SXM4-40GB\r\n\r\nNvidia driver version: 535.161.08\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture: x86_64\r\nCPU op-mode(s): 32-bit, 64-bit\r\nByte Order: Little Endian\r\nAddress sizes: 52 bits physical, 57 bits virtual\r\nCPU(s): 120\r\nOn-line CPU(s) list: 0-119\r\nThread(s) per core: 2\r\nCore(s) per socket: 30\r\nSocket(s): 2\r\nNUMA node(s): 2\r\nVendor ID: GenuineIntel\r\nCPU family: 6\r\nModel: 106\r\nModel name: Intel(R) Xeon(R) Platinum 8336C CPU @ 2.30GHz\r\nStepping: 6\r\nCPU MHz: 2294.635\r\nBogoMIPS: 4589.27\r\nHypervisor vendor: KVM\r\nVirtualization type: full\r\nL1d cache: 2.8 MiB\r\nL1i cache: 1.9 MiB\r\nL2 cache: 75 MiB\r\nL3 cache: 108 MiB\r\nNUMA node0 CPU(s): 0-59\r\nNUMA node1 CPU(s): 60-119\r\nVulnerability Itlb multihit: Not affected\r\nVulnerability L1tf: Not affected\r\nVulnerability Mds: Not affected\r\nVulnerability Meltdown: Not affected\r\nVulnerability Mmio stale data: Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed: Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2: Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds: Not affected\r\nVulnerability Tsx async abort: Mitigation; TSX disabled\r\nFlags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd arat avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm md_clear arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==2.0.2\r\n[pip3] torch==2.4.1\r\n[pip3] torchaudio==2.4.1\r\n[pip3] torchvision==0.19.1\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\n\ncc @msaroufim @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @c-p-i-o","closed_by":null,"reactions":{"url":"https://api.github.com/repos/pytorch/pytorch/issues/138773/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/pytorch/pytorch/issues/138773/timeline","performed_via_github_app":null,"state_reason":null,"pinned_comment":null}