<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>PyTorch : ezyang's blog</title>
<link rel=icon href=/favicon.ico><link rel=stylesheet href=/css/style.css type=text/css media=screen><link href="https://fonts.googleapis.com/css?family=Gentium+Book+Basic" rel=stylesheet type=text/css><link href="https://fonts.googleapis.com/css?family=Ubuntu+Mono" rel=stylesheet type=text/css><link rel=alternate type=application/rss+xml title="ezyang's blog RSS Feed" href=https://blog.ezyang.com/feed.xml><style>pre{margin-left:0;padding:1em;background:var(--code-bg);overflow-x:auto;font-family:ubuntu mono,monospace;font-size:14px;line-height:1.4;color:var(--code-text)}code{font-family:ubuntu mono,monospace}p code,li code{background:var(--code-bg);padding:.2em .4em;border-radius:3px;font-size:.9em}@media screen and (min-width:640px){pre{margin-left:3em}}</style><meta name=generator content="Hugo 0.145.0"></head><body><header><h1 class="vcard author"><a href=/ title=Home>ezyang's blog</a></h1><p>the arc of software bends towards understanding<ul class=pages><li><a href=/archives/>archives</a></li><li><a href=/feed.xml>subscribe</a></li></ul></p></header><div class="content wrap"><section class=posts><h2 class=archiveTitle>PyTorch</h2><article class=post><h2><a class=post-title href=/2026/02/dtensor-erasure/>DTensor erasure</a> <abbr class="small postDate" title=2026-02-01>February 1, 2026</abbr></h2><div class=summary><p>DTensor has famously terrible eager mode performance; for example, <a href=https://arxiv.org/abs/2509.07003v1>this paper</a> measured a 35-60% slowdown in end-to-end training performance with and without DTensor (with DTensor operations taking at least 7x longer than actually running the computation for real). While it is possible to alleviate some of this slowdown via optimizations (in the paper, veScale shows fast bypass of sharding propagation, improved cache lookups and C++ code can take dispatch overhead to 30us), this is still too high for some settings.</p><p><a href=/2026/02/dtensor-erasure/>Read more...</a></p></div></article><article class=post><h2><a class=post-title href=/2026/01/global-vs-local-spmd/>Global vs Local SPMD</a> <abbr class="small postDate" title=2026-01-27>January 27, 2026</abbr></h2><div class=summary><p><strong>Global SPMD</strong> (also known as the &ldquo;global view&rdquo;, exposed by code using <a href=https://docs.pytorch.org/docs/stable/distributed.tensor.html>DTensor</a> or <a href=https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html>jax.Array</a>) refers to writing multi-device code as if it was on a single device, with an orthogonal mechanism for expressing how these full tensors are distributed over multiple devices (this mechanism can be implicit or explicit, e.g., as seen in <a href=https://docs.jax.dev/en/latest/notebooks/explicit-sharding.html#using-a-mixture-of-sharding-modes>this table</a>).</p><p><strong>Local SPMD</strong> (also known as &ldquo;per-device view&rdquo;, and exposed by <a href=https://docs.pytorch.org/docs/stable/distributed.tensor.html#torch.distributed.tensor.experimental.local_map>local_map</a> and <a href=https://docs.jax.dev/en/latest/notebooks/shard_map.html>shard_map</a>, and also traditional PyTorch distributed code operating on plain Tensors, e.g., Megatron-style) refers to writing code from the &ldquo;local&rdquo; view on a single device, with explicit collectives when communicating across devices.</p><p><a href=/2026/01/global-vs-local-spmd/>Read more...</a></p></div></article><article class=post><h2><a class=post-title href=/2026/01/megatron-via-shard-map/>Megatron via shard_map</a> <abbr class="small postDate" title=2026-01-26>January 26, 2026</abbr></h2><div class=summary><p>In <a href=/2026/01/computing-sharding-with-einsum/>Computing sharding with einsum</a>, we worked an example of Megatron style tensor parallelism where we discover that the ordinary backwards formula for linear results in a pending reduction on <code>grad_input</code>, even though the <code>input</code> was replicated and no communications happened in forwards. In Megatron, which is implemented with plain Tensors and manual collectives, you just have to <em>know</em> that this reduction is necessary and manually insert it with a custom autograd function.</p><p><a href=/2026/01/megatron-via-shard-map/>Read more...</a></p></div></article><article class=post><h2><a class=post-title href=/2026/01/computing-sharding-with-einsum/>Computing sharding with einsum</a> <abbr class="small postDate" title=2026-01-25>January 25, 2026</abbr></h2><div class=summary><p>Mental arithmetic in grade school (e.g., memorizing your times tables) is typically justified on the grounds that facility in basic calculations makes it easier to focus on higher-level problems that require being able to do these manipulations. When working on DTensor, I have also found it important to be able to quickly calculate what shardings you get when you do matrix multiplies on sharded tensors. Without being able to do this quickly and accurately, working through examples becomes a slog. I&rsquo;ve also found that while diagrammatic approaches (e.g., drawing a matrix and slicing it into shards) are intuitive, they are slow and unwieldy to do calculations with.</p><p><a href=/2026/01/computing-sharding-with-einsum/>Read more...</a></p></div></article><article class=post><h2><a class=post-title href=/2025/12/learning-to-love-mesh-oriented-sharding/>Learning to love mesh-oriented sharding</a> <abbr class="small postDate" title=2025-12-08>December 8, 2025</abbr></h2><div class=summary><p>Famously, PyTorch and JAX don&rsquo;t agree on how shardings should be represented: PyTorch takes a mesh-dim oriented view, where for each dimension in your device mesh, you specify what sharding should be applied; JAX takes a tensor-dim oriented view, where for each dimension on your tensor, you say which mesh dimensions (potentially multiple!) shard it. Among my Twitter followers, it is generally agreed that the <a href=https://x.com/ezyang/status/1960188772554846236>JAX formulation is more intuitive</a> from a user perspective. OK, fine; if you prefer one representation over another, it&rsquo;s easy enough to translate between the two representations (in easy situations, at least!) In this post, I want to talk more about the framework implementation side: what is the better <em>internal</em> representation of sharding? I don&rsquo;t claim to have all the answers, but my motivation for writing this post is to help explain where I currently stand and how I evaluate proposals for evolving DTensor and sharding in PyTorch.</p><p><a href=/2025/12/learning-to-love-mesh-oriented-sharding/>Read more...</a></p></div></article><article class=post><h2><a class=post-title href=/2025/10/draw-high-dimensional-tensors-as-a-matrix-of-matrices/>Draw high dimensional tensors as a matrix of matrices</a> <abbr class="small postDate" title=2025-10-25>October 25, 2025</abbr></h2><div class=summary><p>I have recently needed to draw the contents of high-dimensional (e.g., 4D and up) tensors where it is important to ensure that is clear how to identify each of the dimensions in the representation. Common strategies I&rsquo;ve seen people do in this situation include printing a giant list 2D slices (what the default PyTorch printer will do) or flattening the Tensor in some way back down to a 2D tensor. However, if you have a lot of horizontal space, there is a strategy that I like that makes it easy to identify all the axes of the higher dimensional tensor: draw it as a matrix of matrices.</p><p><a href=/2025/10/draw-high-dimensional-tensors-as-a-matrix-of-matrices/>Read more...</a></p></div></article><article class=post><h2><a class=post-title href=/2025/09/so-you-want-to-control-flow-in-pt2/>So you want to control flow in PT2</a> <abbr class="small postDate" title=2025-09-05>September 5, 2025</abbr></h2><div class=summary><p><em>With contributions from Richard Zou.</em></p><p>PT2â€™s dominant internal representation, FX graphs, do not directly support control flow (if statements, while loops): they only represent straight-line basic blocks. Most of our graph capture mechanisms are tracing based (<code>fx.symbolic_trace</code>, <code>make_fx</code>, Dynamo), which means that we expect to be able to linearize all conditionals we encounter into a straight line program. Sometimes, you want to work with code that has control flow while working the compiler stack. There is no silver bullet, instead there are a lot of different options with different tradeoffs.</p><p><a href=/2025/09/so-you-want-to-control-flow-in-pt2/>Read more...</a></p></div></article><article class=post><h2><a class=post-title href=/2025/08/the-parallelism-mesh-zoo/>The Parallelism Mesh Zoo</a> <abbr class="small postDate" title=2025-08-30>August 30, 2025</abbr></h2><div class=summary><p>When training large scale LLMs, there is a large assortment of parallelization strategies which you can employ to scale your training runs to work on more GPUs. There are already a number of good resources for understanding how to parallelize your models: I particularly recommend <a href=https://jax-ml.github.io/scaling-book/>How To Scale Your Model</a> and <a href=https://huggingface.co/spaces/nanotron/ultrascale-playbook>The Ultra-Scale Playbook</a>. The purpose of this blog post is to discuss parallelization strategies in a more <em>schematic</em> way by focusing only on how they affect your <strong>device mesh</strong>. The device mesh is an abstraction used by both PyTorch and JAX that takes your GPUs (however many of them you&rsquo;ve got in your cluster!) and organizes them into a N-D tensor that expresses how the devices communicate with each other. When we parallelize computation, we shard a tensor along one dimension of the mesh, and then do collectives along that dimension when there are nontrivial dependencies between shards. Being able to explain why a device mesh is set up the way it is for a collection of parallelization strategies is a good check for seeing if you understand how the parallelization strategies work in the first place! (Credit: This post was influenced by <a href=https://main-horse.github.io/posts/visualizing-6d/>Visualizing 6D Mesh Parallelism</a>.)</p><p><a href=/2025/08/the-parallelism-mesh-zoo/>Read more...</a></p></div></article><article class=post><h2><a class=post-title href=/2025/08/you-could-have-invented-cute-hierarchical-layout-but-maybe-not-the-rest-of-it/>You could have invented CuTe hierarchical layout (but maybe not the rest of it?)</a> <abbr class="small postDate" title=2025-08-22>August 22, 2025</abbr></h2><div class=summary><p>CuTe is a C++ library that aims to make dealing with complicated indexing easier. A key part of how it does this is by defining a <a href=https://docs.nvidia.com/cutlass/media/docs/cpp/cute/01_layout.html>Layout</a> type, which specifies how to map from logical coordinates to physical locations (CuTe likes to say layouts are &ldquo;functions from integers to integers.&rdquo;) In fact, CuTe layouts are a generalization of PyTorch strides, which say you always do this mapping by multiplying each coordinate with its respective stride and summing them together, e.g., <code>i0 * s0 + i1 * s1 + ...</code>. Although NVIDIA&rsquo;s docs don&rsquo;t spell it out, the CuTe&rsquo;s generalization here is actually very natural, and in this blog post I&rsquo;d like to explain how you could have invented it (on a good day).</p><p><a href=/2025/08/you-could-have-invented-cute-hierarchical-layout-but-maybe-not-the-rest-of-it/>Read more...</a></p></div></article><article class=post><h2><a class=post-title href=/2025/08/state-of-torch-compile-august-2025/>State of torch.compile for training (August 2025)</a> <abbr class="small postDate" title=2025-08-13>August 13, 2025</abbr></h2><div class=summary><p>The purpose of this post is to sum up, in one place, the state of torch.compile for training as of August 2025. Nothing in here isn&rsquo;t something you might not already know about from elsewhere on the Internet, but we rarely put everything together in one place. The target audience for this document are teams who are evaluating the use of torch.compile for large scale training runs.</p><p>First, the basics. torch.compile (also known as PT2) is a compiler for PyTorch eager programs for both inference and training workloads. Speedups from 1.5-2x compared to eager code are typical, and torch.compile also makes it possible to do global optimizations for memory (e.g., automatic activation checkpointing) and distributed communications (e.g., async tensor parallelism).</p><p><a href=/2025/08/state-of-torch-compile-august-2025/>Read more...</a></p></div></article><div class="pagination p"><span class=next><a href=/categories/pytorch/page/2/>Older Posts &#187;</a></span></div></section></div><footer class="bottom small wrap"><p class=small>&copy; ezyang's blog. Powered by <a href=https://gohugo.io/>Hugo</a>, theme based off of <a href=http://jxnblk.com/ashley/>Ashley</a>.</p></footer></body></html>