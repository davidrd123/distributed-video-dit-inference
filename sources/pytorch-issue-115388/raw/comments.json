[{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/1852630001","html_url":"https://github.com/pytorch/pytorch/issues/115388#issuecomment-1852630001","issue_url":"https://api.github.com/repos/pytorch/pytorch/issues/115388","id":1852630001,"node_id":"IC_kwDOA-j9z85ubOPx","user":{"login":"wconstab","id":4984825,"node_id":"MDQ6VXNlcjQ5ODQ4MjU=","avatar_url":"https://avatars.githubusercontent.com/u/4984825?v=4","gravatar_id":"","url":"https://api.github.com/users/wconstab","html_url":"https://github.com/wconstab","followers_url":"https://api.github.com/users/wconstab/followers","following_url":"https://api.github.com/users/wconstab/following{/other_user}","gists_url":"https://api.github.com/users/wconstab/gists{/gist_id}","starred_url":"https://api.github.com/users/wconstab/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wconstab/subscriptions","organizations_url":"https://api.github.com/users/wconstab/orgs","repos_url":"https://api.github.com/users/wconstab/repos","events_url":"https://api.github.com/users/wconstab/events{/privacy}","received_events_url":"https://api.github.com/users/wconstab/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-12T18:56:43Z","updated_at":"2023-12-12T19:15:01Z","body":"Repro's for me. taking a look\r\n\r\nI've added these 3 envs to aid in debugging.  I also want to point out that we're in the process of flipping MONITORING on by default, which will convert this hang into a crash (slightly less annoying, but obviously not a fix for your issue) \r\n`TORCH_CPP_LOG_LEVEL=INFO TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC=5 TORCH_NCCL_ENABLE_MONITORING=1`\r\n\r\nThe info logs point to something interesting:\r\n```\r\n[rank0]:[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\r\n[rank1]:[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\r\nCalling destroy_process_group()...\r\nCalling destroy_process_group()...\r\n```\r\n\r\ncc @eellison @fduwjj \r\n\r\nActually, i want to refine the issue at this point.  What do you expect to happen here- \r\n\r\n(1) it should not hang.  We agree, and that's actually why we introduced the heartbeat monitor.  (Sometimes, nccl/CUDA APIs themselves hang when we call them, and we work offline with NV on whether those are bugs or whether we need to change how we use the APIs.  But in the mean time, we claim that after enabling heartbeat monitoring we will never hang in the watchdog/PG destructor.  So that particular issue should be solved now, since the PR to enable heartbeat monitoring has already landed (https://github.com/pytorch/pytorch/pull/115577) and I've verified it kills the program as expected in your case.\r\n\r\n(2) are we using cudagraphs correctly here? if so, why is it 'waiting for nccl ops to finish' and what do we need to do to fix that? (cc @eellison)","author_association":"CONTRIBUTOR","pin":null,"reactions":{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/1852630001/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/1971755526","html_url":"https://github.com/pytorch/pytorch/issues/115388#issuecomment-1971755526","issue_url":"https://api.github.com/repos/pytorch/pytorch/issues/115388","id":1971755526,"node_id":"IC_kwDOA-j9z851hpoG","user":{"login":"eqy","id":2239616,"node_id":"MDQ6VXNlcjIyMzk2MTY=","avatar_url":"https://avatars.githubusercontent.com/u/2239616?v=4","gravatar_id":"","url":"https://api.github.com/users/eqy","html_url":"https://github.com/eqy","followers_url":"https://api.github.com/users/eqy/followers","following_url":"https://api.github.com/users/eqy/following{/other_user}","gists_url":"https://api.github.com/users/eqy/gists{/gist_id}","starred_url":"https://api.github.com/users/eqy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/eqy/subscriptions","organizations_url":"https://api.github.com/users/eqy/orgs","repos_url":"https://api.github.com/users/eqy/repos","events_url":"https://api.github.com/users/eqy/events{/privacy}","received_events_url":"https://api.github.com/users/eqy/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2024-02-29T18:50:03Z","updated_at":"2024-02-29T18:52:33Z","body":"@wconstab \r\nThat check was added [here](https://github.com/pytorch/pytorch/pull/110665) because eager-mode NCCL will issue event queries which will crash during graph capture. It should not deadlock unless the number of event queries is not being decremented somewhere...\r\n\r\nA sleep between the warmup iteration and the capture should make the \"waiting for nccl ops to finish\" message go away.","author_association":"COLLABORATOR","pin":null,"reactions":{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/1971755526/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/2154156835","html_url":"https://github.com/pytorch/pytorch/issues/115388#issuecomment-2154156835","issue_url":"https://api.github.com/repos/pytorch/pytorch/issues/115388","id":2154156835,"node_id":"IC_kwDOA-j9z86AZdMj","user":{"login":"GuWei007","id":95560489,"node_id":"U_kgDOBbIjKQ","avatar_url":"https://avatars.githubusercontent.com/u/95560489?v=4","gravatar_id":"","url":"https://api.github.com/users/GuWei007","html_url":"https://github.com/GuWei007","followers_url":"https://api.github.com/users/GuWei007/followers","following_url":"https://api.github.com/users/GuWei007/following{/other_user}","gists_url":"https://api.github.com/users/GuWei007/gists{/gist_id}","starred_url":"https://api.github.com/users/GuWei007/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/GuWei007/subscriptions","organizations_url":"https://api.github.com/users/GuWei007/orgs","repos_url":"https://api.github.com/users/GuWei007/repos","events_url":"https://api.github.com/users/GuWei007/events{/privacy}","received_events_url":"https://api.github.com/users/GuWei007/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2024-06-07T06:16:57Z","updated_at":"2024-06-07T06:16:57Z","body":"> Repro's for me. taking a look\r\n> \r\n> I've added these 3 envs to aid in debugging. I also want to point out that we're in the process of flipping MONITORING on by default, which will convert this hang into a crash (slightly less annoying, but obviously not a fix for your issue) `TORCH_CPP_LOG_LEVEL=INFO TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC=5 TORCH_NCCL_ENABLE_MONITORING=1`\r\n> \r\n> The info logs point to something interesting:\r\n> \r\n> ```\r\n> [rank0]:[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\r\n> [rank1]:[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\r\n> Calling destroy_process_group()...\r\n> Calling destroy_process_group()...\r\n> ```\r\n> \r\n> cc @eellison @fduwjj\r\n> \r\n> Actually, i want to refine the issue at this point. What do you expect to happen here-\r\n> \r\n> (1) it should not hang. We agree, and that's actually why we introduced the heartbeat monitor. (Sometimes, nccl/CUDA APIs themselves hang when we call them, and we work offline with NV on whether those are bugs or whether we need to change how we use the APIs. But in the mean time, we claim that after enabling heartbeat monitoring we will never hang in the watchdog/PG destructor. So that particular issue should be solved now, since the PR to enable heartbeat monitoring has already landed (#115577) and I've verified it kills the program as expected in your case.\r\n> \r\n> (2) are we using cudagraphs correctly here? if so, why is it 'waiting for nccl ops to finish' and what do we need to do to fix that? (cc @eellison)\r\n\r\nWhere can I find the documentation and application scenario description of TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC? This environment variable is very valuable for me to analyze hang","author_association":"NONE","pin":null,"reactions":{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/2154156835/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/2154872059","html_url":"https://github.com/pytorch/pytorch/issues/115388#issuecomment-2154872059","issue_url":"https://api.github.com/repos/pytorch/pytorch/issues/115388","id":2154872059,"node_id":"IC_kwDOA-j9z86AcLz7","user":{"login":"wconstab","id":4984825,"node_id":"MDQ6VXNlcjQ5ODQ4MjU=","avatar_url":"https://avatars.githubusercontent.com/u/4984825?v=4","gravatar_id":"","url":"https://api.github.com/users/wconstab","html_url":"https://github.com/wconstab","followers_url":"https://api.github.com/users/wconstab/followers","following_url":"https://api.github.com/users/wconstab/following{/other_user}","gists_url":"https://api.github.com/users/wconstab/gists{/gist_id}","starred_url":"https://api.github.com/users/wconstab/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wconstab/subscriptions","organizations_url":"https://api.github.com/users/wconstab/orgs","repos_url":"https://api.github.com/users/wconstab/repos","events_url":"https://api.github.com/users/wconstab/events{/privacy}","received_events_url":"https://api.github.com/users/wconstab/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2024-06-07T13:43:28Z","updated_at":"2024-06-07T13:43:28Z","body":"There isn't a whole lot of documentation for this yet, but there is something here\r\nhttps://github.com/pytorch/pytorch/blob/main/torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp#L77\r\n\r\nYou can see the implementation of the monitor thread here:\r\nhttps://github.com/pytorch/pytorch/blob/main/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp#L1234\r\n\r\nThe monitor thread actually runs all the time, and the env `TORCH_NCCL_ENABLE_MONITORING` governs whether it just prints a warning when it detects a hang (0) or actually terminates the process (1).  The timeout value governs how long it waits for watchdog activity before it declares a hang.","author_association":"CONTRIBUTOR","pin":null,"reactions":{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/2154872059/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/2252268013","html_url":"https://github.com/pytorch/pytorch/issues/115388#issuecomment-2252268013","issue_url":"https://api.github.com/repos/pytorch/pytorch/issues/115388","id":2252268013,"node_id":"IC_kwDOA-j9z86GPuHt","user":{"login":"grimoire","id":1239736,"node_id":"MDQ6VXNlcjEyMzk3MzY=","avatar_url":"https://avatars.githubusercontent.com/u/1239736?v=4","gravatar_id":"","url":"https://api.github.com/users/grimoire","html_url":"https://github.com/grimoire","followers_url":"https://api.github.com/users/grimoire/followers","following_url":"https://api.github.com/users/grimoire/following{/other_user}","gists_url":"https://api.github.com/users/grimoire/gists{/gist_id}","starred_url":"https://api.github.com/users/grimoire/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/grimoire/subscriptions","organizations_url":"https://api.github.com/users/grimoire/orgs","repos_url":"https://api.github.com/users/grimoire/repos","events_url":"https://api.github.com/users/grimoire/events{/privacy}","received_events_url":"https://api.github.com/users/grimoire/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2024-07-26T08:47:37Z","updated_at":"2024-07-26T08:47:37Z","body":"I have met a similar problem.\r\n\r\n`graph.reset()` before destroying pg would trigger use-count assert:\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  uc >= 0 INTERNAL ASSERT FAILED at \"../c10/cuda/CUDACachingAllocator.cpp\":2138, please report a bug to PyTorch.\r\n```\r\n\r\nAnd `del graph` at the end of each process works on me.\r\n","author_association":"NONE","pin":null,"reactions":{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/2252268013/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/2252978419","html_url":"https://github.com/pytorch/pytorch/issues/115388#issuecomment-2252978419","issue_url":"https://api.github.com/repos/pytorch/pytorch/issues/115388","id":2252978419,"node_id":"IC_kwDOA-j9z86GSbjz","user":{"login":"wconstab","id":4984825,"node_id":"MDQ6VXNlcjQ5ODQ4MjU=","avatar_url":"https://avatars.githubusercontent.com/u/4984825?v=4","gravatar_id":"","url":"https://api.github.com/users/wconstab","html_url":"https://github.com/wconstab","followers_url":"https://api.github.com/users/wconstab/followers","following_url":"https://api.github.com/users/wconstab/following{/other_user}","gists_url":"https://api.github.com/users/wconstab/gists{/gist_id}","starred_url":"https://api.github.com/users/wconstab/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wconstab/subscriptions","organizations_url":"https://api.github.com/users/wconstab/orgs","repos_url":"https://api.github.com/users/wconstab/repos","events_url":"https://api.github.com/users/wconstab/events{/privacy}","received_events_url":"https://api.github.com/users/wconstab/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2024-07-26T15:15:38Z","updated_at":"2024-07-26T15:15:38Z","body":"@grimoire are you saying that if you put `del graph` into the repro script at the top of the issue, it stops hanging? Can you be more specific about which line you insert it after?\r\n\r\n@eellison can you take a look at whether we need to do something 'atexit' from the context manager to decrement the cudagraph counter in C10d?","author_association":"CONTRIBUTOR","pin":null,"reactions":{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/2252978419/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/2254850998","html_url":"https://github.com/pytorch/pytorch/issues/115388#issuecomment-2254850998","issue_url":"https://api.github.com/repos/pytorch/pytorch/issues/115388","id":2254850998,"node_id":"IC_kwDOA-j9z86GZku2","user":{"login":"grimoire","id":1239736,"node_id":"MDQ6VXNlcjEyMzk3MzY=","avatar_url":"https://avatars.githubusercontent.com/u/1239736?v=4","gravatar_id":"","url":"https://api.github.com/users/grimoire","html_url":"https://github.com/grimoire","followers_url":"https://api.github.com/users/grimoire/followers","following_url":"https://api.github.com/users/grimoire/following{/other_user}","gists_url":"https://api.github.com/users/grimoire/gists{/gist_id}","starred_url":"https://api.github.com/users/grimoire/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/grimoire/subscriptions","organizations_url":"https://api.github.com/users/grimoire/orgs","repos_url":"https://api.github.com/users/grimoire/repos","events_url":"https://api.github.com/users/grimoire/events{/privacy}","received_events_url":"https://api.github.com/users/grimoire/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2024-07-29T03:01:04Z","updated_at":"2024-07-29T03:01:04Z","body":"@wconstab \r\n\r\nSure. Add either `del g` or `g.reset()` in the script provided by @cbcase works on `torch==2.2.2 A100`. `g.reset()` failed on `torch==2.1.0 V100`\r\n\r\n```python\r\n    torch.cuda.synchronize()\r\n    print(\"Calling destroy_process_group()...\")\r\n    del g\r\n    # g.reset()\r\n    dist.destroy_process_group()\r\n    print(\"Done.\")\r\n```\r\n\r\n```\r\nCalling destroy_process_group()...\r\nCalling destroy_process_group()...\r\nDone.\r\nDone.\r\n```\r\n","author_association":"NONE","pin":null,"reactions":{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/2254850998/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/2272565983","html_url":"https://github.com/pytorch/pytorch/issues/115388#issuecomment-2272565983","issue_url":"https://api.github.com/repos/pytorch/pytorch/issues/115388","id":2272565983,"node_id":"IC_kwDOA-j9z86HdJrf","user":{"login":"victbr","id":22766181,"node_id":"MDQ6VXNlcjIyNzY2MTgx","avatar_url":"https://avatars.githubusercontent.com/u/22766181?v=4","gravatar_id":"","url":"https://api.github.com/users/victbr","html_url":"https://github.com/victbr","followers_url":"https://api.github.com/users/victbr/followers","following_url":"https://api.github.com/users/victbr/following{/other_user}","gists_url":"https://api.github.com/users/victbr/gists{/gist_id}","starred_url":"https://api.github.com/users/victbr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/victbr/subscriptions","organizations_url":"https://api.github.com/users/victbr/orgs","repos_url":"https://api.github.com/users/victbr/repos","events_url":"https://api.github.com/users/victbr/events{/privacy}","received_events_url":"https://api.github.com/users/victbr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2024-08-07T03:56:17Z","updated_at":"2024-08-07T03:56:17Z","body":"Probably the same issue. Unlike using torchrun, I use multiprocess to set up the environment. \r\n\r\nIn my case, `dist.destroy_process_group()` did not cause a hang; however, the process failed to terminate properly. Inserting del graph either before or after `dist.destroy_process_group()` resolved the issue. JUst omitting `dist.destroy_process_group()` also work for me. \r\n\r\nThere might be a problem in the cleanup phase.\r\n\r\n```python\r\nimport torch\r\nfrom torch import distributed as dist\r\nfrom multiprocessing import Process\r\nimport os\r\n\r\ndef main(rank, world_size):\r\n    os.environ[\"NCCL_ASYNC_ERROR_HANDLING\"] = \"0\"\r\n    dist.init_process_group(backend=\"nccl\",\r\n                            world_size=world_size,\r\n                            rank=rank\r\n                            )\r\n    torch.cuda.set_device(rank)\r\n    group = dist.group.WORLD\r\n    bsz = 1\r\n    N = 1\r\n    data = torch.randn((bsz, N)).cuda()\r\n    W = torch.randn((N,N)).cuda()\r\n    static_output = torch.empty(bsz,N).cuda()\r\n\r\n    with torch.inference_mode():\r\n        s = torch.cuda.Stream()\r\n        s.wait_stream(torch.cuda.current_stream())\r\n        for _ in range(11):\r\n            with torch.cuda.stream(s):\r\n                data *= rank\r\n                dist.all_reduce(data, group=group)\r\n                torch.matmul(data, W, out=data)\r\n        torch.cuda.current_stream().wait_stream(s)\r\n        g = torch.cuda.CUDAGraph()\r\n        mem_pool = None\r\n        with torch.cuda.graph(g, pool=mem_pool):\r\n            data *= rank\r\n            dist.all_reduce(data, group=group)\r\n            temp = data @ W\r\n            static_output.copy_(temp)\r\n\r\n        data2 = torch.randn((bsz, N)).cuda()\r\n        W2 = torch.randn((N,N)).cuda()\r\n        data.copy_(data2)\r\n        W.copy_(W2)\r\n        g.replay()\r\n        data2 *= rank\r\n        dist.all_reduce(data2, group=group)\r\n        data3 = data2 @ W2\r\n        assert torch.allclose(static_output, data3), f\"{static_output=}, {data3=}\"\r\n        dist.destroy_process_group()\r\n        del g\r\ndef allreduce_test():\r\n    world_size = 4\r\n    procs = [Process(target=main, args=(rank, world_size)) for rank in range(world_size)]\r\n    for p in procs:\r\n        p.start()\r\n    for p in procs:\r\n        p.join()\r\n    print(\"finish\")\r\nif __name__ == \"__main__\":\r\n    allreduce_test()\r\n    pass\r\n``` \r\nI ran with `python <filename>`. on `torch==2.1.0 A100`","author_association":"NONE","pin":null,"reactions":{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/2272565983/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/3009880966","html_url":"https://github.com/pytorch/pytorch/issues/115388#issuecomment-3009880966","issue_url":"https://api.github.com/repos/pytorch/pytorch/issues/115388","id":3009880966,"node_id":"IC_kwDOA-j9z86zZyOG","user":{"login":"kwen2501","id":6676466,"node_id":"MDQ6VXNlcjY2NzY0NjY=","avatar_url":"https://avatars.githubusercontent.com/u/6676466?v=4","gravatar_id":"","url":"https://api.github.com/users/kwen2501","html_url":"https://github.com/kwen2501","followers_url":"https://api.github.com/users/kwen2501/followers","following_url":"https://api.github.com/users/kwen2501/following{/other_user}","gists_url":"https://api.github.com/users/kwen2501/gists{/gist_id}","starred_url":"https://api.github.com/users/kwen2501/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kwen2501/subscriptions","organizations_url":"https://api.github.com/users/kwen2501/orgs","repos_url":"https://api.github.com/users/kwen2501/repos","events_url":"https://api.github.com/users/kwen2501/events{/privacy}","received_events_url":"https://api.github.com/users/kwen2501/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2025-06-26T20:25:34Z","updated_at":"2025-06-26T20:26:42Z","body":"Reason:\nWhen CUDA Graph is on with NCCL operations, NCCL would internally allocate some resources for Graph replay in future. Such allocation calls for free at some point. Today NCCL frees them when the communicator is being destroyed, i.e. when user calls `destroy_process_group`. But during the communicator destruction, NCCL wants to make sure there is no more use of those resources, thus it checks whether the graph object has been destroyed, and wait if not. \n\nThat causes a deadlock if user code does not have a `del graph` before `destroy_process_group`, because the graph object is not garbage-collected until the end of the scope. ","author_association":"COLLABORATOR","pin":null,"reactions":{"url":"https://api.github.com/repos/pytorch/pytorch/issues/comments/3009880966/reactions","total_count":3,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"performed_via_github_app":null}]