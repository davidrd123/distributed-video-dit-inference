---
title: "NCCL User Guide"
source_url: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html
fetch_date: 2026-02-22
source_type: docs
author: NVIDIA
conversion_notes: |
  Assembled from 7 key sub-pages of the NCCL User Guide v2.29 multi-page Sphinx site.
  Pages included: overview, usage/collectives, usage/streams, usage/groups, usage/p2p,
  usage/cudagraph, env. Pages NOT included: setup, usage/communicators, usage/data,
  usage/threadsafety, usage/inplace, usage/bufferreg, usage/deviceapi, api/* (C API reference),
  nccl1 migration, examples, mpi, troubleshooting.
  Site chrome (nav, sidebar, footer, breadcrumbs, prev/next) stripped from each page.
---

# Overview of NCCL

The NVIDIA Collective Communications Library (NCCL, pronounced "Nickel") is a library providing inter-GPU communication primitives that are topology-aware and can be easily integrated into applications.

NCCL implements both collective communication and point-to-point send/receive primitives. It is not a full-blown parallel programming framework; rather, it is a library focused on accelerating inter-GPU communication.

NCCL provides the following collective communication primitives :

-   AllReduce

-   Broadcast

-   Reduce

-   AllGather

-   ReduceScatter

-   AlltoAll

-   Gather

-   Scatter

Additionally, it allows for point-to-point send/receive communication which allows for scatter, gather, or all-to-all operations.

Tight synchronization between communicating processors is a key aspect of collective communication. CUDA based collectives would traditionally be realized through a combination of CUDA memory copy operations and CUDA kernels for local reductions. NCCL, on the other hand, implements each collective in a single kernel handling both communication and computation operations. This allows for fast synchronization and minimizes the resources needed to reach peak bandwidth.

NCCL conveniently removes the need for developers to optimize their applications for specific machines. NCCL provides fast collectives over multiple GPUs both within and across nodes. It supports a variety of interconnect technologies including PCIe, NVLINK, InfiniBand Verbs, and IP sockets.

Next to performance, ease of programming was the primary consideration in the design of NCCL. NCCL uses a simple C API, which can be easily accessed from a variety of programming languages. NCCL closely follows the popular collectives API defined by MPI (Message Passing Interface). Anyone familiar with MPI will thus find NCCL's API very natural to use. In a minor departure from MPI, NCCL collectives take a "stream" argument which provides direct integration with the CUDA programming model. Finally, NCCL is compatible with virtually any multi-GPU parallelization model, for example:

-   single-threaded control of all GPUs

-   multi-threaded, for example, using one thread per GPU

-   multi-process, for example, MPI

NCCL has found great application in Deep Learning Frameworks, where the AllReduce collective is heavily used for neural network training. Efficient scaling of neural network training is possible with the multi-GPU and multi node communication provided by NCCL.

---

# Collective Operations

Collective operations have to be called for each rank (hence CUDA device), using the same count and the same datatype, to form a complete collective operation. Failure to do so will result in undefined behavior, including hangs, crashes, or data corruption.


## AllReduce

The AllReduce operation performs reductions on data (for example, sum, min, max) across devices and stores the result in the receive buffer of every rank.

In a *sum* allreduce operation between *k* ranks, each rank will provide an array in of N values, and receive identical results in array out of N values, where out\[i\] = in0\[i\]+in1\[i\]+...+in(k-1)\[i\].

[Figure: All-Reduce operation: each rank receives the reduction of input values across ranks.]

Related links: `ncclAllReduce()`.

## Broadcast

The Broadcast operation copies an N-element buffer from the root rank to all the ranks.

[Figure: Broadcast operation: all ranks receive data from a "root" rank.]

Important note: The root argument is one of the ranks, not a device number, and is therefore impacted by a different rank to device mapping.

Related links: `ncclBroadcast()`.

## Reduce

The Reduce operation performs the same operation as AllReduce, but stores the result only in the receive buffer of a specified root rank.

[Figure: Reduce operation: one rank receives the reduction of input values across ranks.]

Important note: The root argument is one of the ranks (not a device number), and is therefore impacted by a different rank to device mapping.

Note: A Reduce, followed by a Broadcast, is equivalent to the AllReduce operation.

Related links: `ncclReduce()`.

## AllGather

The AllGather operation gathers N values from k ranks into an output buffer of size k\*N, and distributes that result to all ranks.

The output is ordered by the rank index. The AllGather operation is therefore impacted by a different rank to device mapping.

[Figure: AllGather operation: each rank receives the aggregation of data from all ranks in the order of the ranks.]

Note: Executing ReduceScatter, followed by AllGather, is equivalent to the AllReduce operation.

Related links: `ncclAllGather()`.

## ReduceScatter

The ReduceScatter operation performs the same operation as Reduce, except that the result is scattered in equal-sized blocks between ranks, each rank getting a chunk of data based on its rank index.

The ReduceScatter operation is impacted by a different rank to device mapping since the ranks determine the data layout.

[Figure: Reduce-Scatter operation: input values are reduced across ranks, with each rank receiving a subpart of the result.]

Related links: `ncclReduceScatter()`

## AlltoAll

In an AlltoAll operation between k ranks, each rank provides an input buffer of size k\*N values, where the j-th chunk of N values is sent to destination rank j. Each rank receives an output buffer of size k\*N values, where the i-th chunk of N values comes from source rank i.

[Figure: AlltoAll operation: exchanges data between all ranks, where each rank sends different data to every other rank and receives different data from every other rank.]

Related links: `ncclAlltoAll()`.

## Gather

The Gather operation gathers N values from k ranks into an output buffer on the root rank of size k\*N.

[Figure: Gather operation: root rank receives data from all ranks.]

Important note: The root argument is one of the ranks, not a device number, and is therefore impacted by a different rank to device mapping.

Related links: `ncclGather()`.

## Scatter

The Scatter operation distributes a total of N\*k values from the root rank to k ranks, each rank receiving N values.

[Figure: Scatter operation: root rank distributes data to all ranks.]

Important note: The root argument is one of the ranks, not a device number, and is therefore impacted by a different rank to device mapping.

Related links: `ncclScatter()`.

---

# CUDA Stream Semantics

NCCL calls are associated to a stream which is passed as the last argument of the collective communication function. The NCCL call returns when the operation has been effectively enqueued to the given stream, or returns an error. The collective operation is then executed asynchronously on the CUDA device. The operation status can be queried using standard CUDA semantics, for example, calling cudaStreamSynchronize or using CUDA events.


## Mixing Multiple Streams within the same ncclGroupStart/End() group

NCCL allows for using multiple streams within a group call. This will enforce a stream dependency of all streams before the NCCL kernel starts and block all streams until the NCCL kernel completes.

It will behave as if the NCCL group operation was posted on every stream, but given it is a single operation, it will cause a global synchronization point between the streams.

---

# Group Calls

Group functions (ncclGroupStart/ncclGroupEnd) can be used to merge multiple calls into one. This is needed for three purposes: managing multiple GPUs from one thread (to avoid deadlocks), aggregating communication operations to improve performance, or merging multiple send/receive point-to-point operations (see Point-to-point communication section). All three usages can be combined together, with one exception : calls to `ncclCommInitRank()` cannot be merged with others.


## Management Of Multiple GPUs From One Thread

When a single thread is managing multiple devices, group semantics must be used. This is because every NCCL call may have to block, waiting for other threads/ranks to arrive, before effectively posting the NCCL operation on the given stream. Hence, a simple loop on multiple devices like shown below could block on the first call waiting for the other ones:

```c
for (int i=0; i<nLocalDevs; i++) {
  ncclAllReduce(..., comm[i], stream[i]);
}
```

To define that these calls are part of the same collective operation, ncclGroupStart and ncclGroupEnd should be used:

```c
ncclGroupStart();
for (int i=0; i<nLocalDevs; i++) {
  ncclAllReduce(..., comm[i], stream[i]);
}
ncclGroupEnd();
```

This will tell NCCL to treat all calls between ncclGroupStart and ncclGroupEnd as a single call to many devices.

Caution: When called inside a group, stream operations (like ncclAllReduce) can return without having enqueued the operation on the stream. Stream operations like cudaStreamSynchronize can therefore be called only after ncclGroupEnd returns.

Group calls must also be used to create a communicator when one thread manages more than one device:

```c
ncclGroupStart();
for (int i=0; i<nLocalDevs; i++) {
  cudaSetDevice(device[i]);
  ncclCommInitRank(comms+i, nranks, commId, rank[i]);
}
ncclGroupEnd();
```

Note: Contrary to NCCL 1.x, there is no need to set the CUDA device before every NCCL communication call within a group, but it is still needed when calling ncclCommInitRank within a group.

Related links:

-   `ncclGroupStart()`

-   `ncclGroupEnd()`

## Aggregated Operations (2.2 and later)

The group semantics can also be used to have multiple collective operations performed within a single NCCL launch. This is useful for reducing the launch overhead, in other words, latency, as it only occurs once for multiple operations. Init functions cannot be aggregated with other init functions, nor with communication functions.

Aggregation of collective operations can be done simply by having multiple calls to NCCL within a ncclGroupStart / ncclGroupEnd section.

In the following example, we launch one broadcast and two allReduce operations together as a single NCCL launch.

```c
ncclGroupStart();
ncclBroadcast(sendbuff1, recvbuff1, count1, datatype, root, comm, stream);
ncclAllReduce(sendbuff2, recvbuff2, count2, datatype, comm, stream);
ncclAllReduce(sendbuff3, recvbuff3, count3, datatype, comm, stream);
ncclGroupEnd();
```

It is permitted to combine aggregation with multi-GPU launch and use different communicators in a group launch as shown in the Management Of Multiple GPUs From One Thread topic. When combining multi-GPU launch and aggregation, ncclGroupStart and ncclGroupEnd can be either used once or at each level. The following example groups the allReduce operations from different layers and on multiple CUDA devices :

```c
ncclGroupStart();
for (int i=0; i<nlayers; i++) {
  ncclGroupStart();
  for (int g=0; g<ngpus; g++) {
    ncclAllReduce(sendbuffs[g]+offsets[i], recvbuffs[g]+offsets[i], counts[i], datatype[i], comms[g], streams[g]);
  }
  ncclGroupEnd();
}
ncclGroupEnd();
```

Note: The NCCL operation will only be started as a whole during the last call to ncclGroupEnd. The ncclGroupStart and ncclGroupEnd calls within the for loop are not necessary and do nothing.

Related links:

-   `ncclGroupStart()`

-   `ncclGroupEnd()`

## Group Operation Ordering Semantics

Although NCCL group allows different operations to be issued in one shot, users still need to guarantee the same issuing order of the operations among different GPUs no matter whether the operations are issued to the same or different communicators.

For example, the following code provides the correct order of the operations. In this example, *comm0* and *comm1* are duplicated independent communicators that include rank 0 and 1.

```c
RANK0/GPU0/Process0:
ncclGroupStart();
ncclBroadcast(sendbuff1, recvbuff1, count1, datatype, root, comm0, stream);
ncclAllReduce(sendbuff2, recvbuff2, count2, datatype, comm0, stream);
ncclAllReduce(sendbuff3, recvbuff3, count3, datatype, comm0, stream);
ncclAllReduce(sendbuff4, recvbuff4, count4, datatype, comm1, stream);
ncclGroupEnd();

RANK1/GPU1/Process1:
ncclGroupStart();
ncclBroadcast(sendbuff1, recvbuff1, count1, datatype, root, comm0, stream);
ncclAllReduce(sendbuff2, recvbuff2, count2, datatype, comm0, stream);
ncclAllReduce(sendbuff3, recvbuff3, count3, datatype, comm0, stream);
ncclAllReduce(sendbuff4, recvbuff4, count4, datatype, comm1, stream);
ncclGroupEnd();
```

However, changing the order of the any operations will lead to incorrect results or hang as shown in the following 2 examples:

```c
RANK0/GPU0/Process0:
ncclGroupStart();
ncclBroadcast(sendbuff1, recvbuff1, count1, datatype, root, comm0, stream);
ncclAllReduce(sendbuff3, recvbuff3, count3, datatype, comm0, stream); // WRONG: reversed order
ncclAllReduce(sendbuff2, recvbuff2, count2, datatype, comm0, stream); // WRONG: reversed order
ncclAllReduce(sendbuff4, recvbuff4, count4, datatype, comm1, stream);
ncclGroupEnd();

RANK1/GPU1/Process1:
ncclGroupStart();
ncclBroadcast(sendbuff1, recvbuff1, count1, datatype, root, comm0, stream);
ncclAllReduce(sendbuff2, recvbuff2, count2, datatype, comm0, stream); // WRONG: reversed order
ncclAllReduce(sendbuff3, recvbuff3, count3, datatype, comm0, stream); // WRONG: reversed order
ncclAllReduce(sendbuff4, recvbuff4, count4, datatype, comm1, stream);
ncclGroupEnd();
```

```c
RANK0/GPU0/Process0:
ncclGroupStart();
ncclAllReduce(sendbuff4, recvbuff4, count4, datatype, comm1, stream); // WRONG: reversed order
ncclBroadcast(sendbuff1, recvbuff1, count1, datatype, root, comm0, stream);
ncclAllReduce(sendbuff2, recvbuff2, count2, datatype, comm0, stream);
ncclAllReduce(sendbuff3, recvbuff3, count3, datatype, comm0, stream);
ncclGroupEnd();

RANK1/GPU1/Process1:
ncclGroupStart();
ncclBroadcast(sendbuff1, recvbuff1, count1, datatype, root, comm0, stream);
ncclAllReduce(sendbuff2, recvbuff2, count2, datatype, comm0, stream);
ncclAllReduce(sendbuff3, recvbuff3, count3, datatype, comm0, stream);
ncclAllReduce(sendbuff4, recvbuff4, count4, datatype, comm1, stream); // WRONG: reversed order
ncclGroupEnd();
```

## Nonblocking Group Operation

If a communicator is marked as nonblocking through ncclCommInitRankConfig, the group functions become asynchronous correspondingly. In this case, if users issue multiple NCCL operations in one group, returning from ncclGroupEnd() might not mean the NCCL communication kernels have been issued to CUDA streams. If ncclGroupEnd() returns ncclSuccess, it means NCCL kernels have been issued to streams; if it returns ncclInProgress, it means NCCL kernels are being issued to streams in the background. It is users' responsibility to make sure the state of the communicator changes into ncclSuccess before calling related CUDA calls (e.g. cudaStreamSynchronize):

```c
ncclGroupStart();
  for (int g=0; g<ngpus; g++) {
    ncclAllReduce(sendbuffs[g]+offsets[i], recvbuffs[g]+offsets[i], counts[i], datatype[i], comms[g], streams[g]);
  }
ret = ncclGroupEnd();
if (ret == ncclInProgress) {
   for (int g=0; g<ngpus; g++) {
     do {
       ncclCommGetAsyncError(comms[g], &state);
     } while (state == ncclInProgress);
   }
} else if (ret == ncclSuccess) {
   /* Successfully issued */
   printf("NCCL kernel issue succeeded\n");
} else {
   /* Errors happen */
   reportErrorAndRestart();
}

for (int g=0; g<ngpus; g++) {
  cudaStreamSynchronize(streams[g]);
}
```

Related links:

-   `ncclCommInitRankConfig()`

-   `ncclCommGetAsyncError()`

---

# Point-to-point communication


## Two-sided communication

(Since NCCL 2.7) Point-to-point communication can be used to express any communication pattern between ranks. Any point-to-point communication needs two NCCL calls : a call to `ncclSend()` on one rank and a corresponding `ncclRecv()` on the other rank, with the same count and data type.

Multiple calls to `ncclSend()` and `ncclRecv()` targeting different peers can be fused together with `ncclGroupStart()` and `ncclGroupEnd()` to form more complex communication patterns such as one-to-all (scatter), all-to-one (gather), all-to-all or communication with neighbors in an N-dimensional space.

Point-to-point calls within a group will be blocking until that group of calls completes, but calls within a group can be seen as progressing independently, hence should never block each other. It is therefore important to merge calls that need to progress concurrently to avoid deadlocks. The only exception is point-to-point calls within a group targeting the *same* peer, which are executed in order.

Below are a few examples of classic point-to-point communication patterns used by parallel applications. NCCL semantics allow for all variants with different sizes, datatypes, and buffers, per rank.


### Sendrecv

In MPI terms, a sendrecv operation is when two ranks exchange data, both sending and receiving at the same time. This can be done by merging both ncclSend and ncclRecv calls into one :

```c
ncclGroupStart();
ncclSend(sendbuff, sendcount, sendtype, peer, comm, stream);
ncclRecv(recvbuff, recvcount, recvtype, peer, comm, stream);
ncclGroupEnd();
```

### One-to-all (scatter)

A one-to-all operation from a `root` rank can be expressed by merging all send and receive operations in a group :

```c
ncclGroupStart();
if (rank == root) {
  for (int r=0; r<nranks; r++)
    ncclSend(sendbuff[r], size, type, r, comm, stream);
}
ncclRecv(recvbuff, size, type, root, comm, stream);
ncclGroupEnd();
```

### All-to-one (gather)

Similarly, an all-to-one operations to a `root` rank would be implemented this way :

```c
ncclGroupStart();
if (rank == root) {
  for (int r=0; r<nranks; r++)
    ncclRecv(recvbuff[r], size, type, r, comm, stream);
}
ncclSend(sendbuff, size, type, root, comm, stream);
ncclGroupEnd();
```

### All-to-all

An all-to-all operation would be a merged loop of send/recv operations to/from all peers :

```c
ncclGroupStart();
for (int r=0; r<nranks; r++) {
  ncclSend(sendbuff[r], sendcount, sendtype, r, comm, stream);
  ncclRecv(recvbuff[r], recvcount, recvtype, r, comm, stream);
}
ncclGroupEnd();
```

### Neighbor exchange

Finally, exchanging data with neighbors in an N-dimensions space could be done with :

```c
ncclGroupStart();
for (int d=0; d<ndims; d++) {
  ncclSend(sendbuff[d], sendcount, sendtype, next[d], comm, stream);
  ncclRecv(recvbuff[d], recvcount, recvtype, prev[d], comm, stream);
}
ncclGroupEnd();
```


## One-sided communication

(Since NCCL 2.29) One-sided communication enables a rank to write data to remote memory using `ncclPutSignal()` without requiring the target rank to issue a matching operation. The target memory must be pre-registered using `ncclCommWindowRegister()`. Point-to-point synchronization can be achieved by having the target rank call `ncclWaitSignal()` to wait for signals.

Multiple `ncclPutSignal()` calls can be grouped using `ncclGroupStart()` and `ncclGroupEnd()`. Operations to different peers or contexts within a group may execute concurrently and complete in any order. The completion of `ncclGroupEnd()` guarantees that all operations in the group have achieved completion. Operations to the same peer and context are executed in order: both data delivery and signal updates on the remote peer follow the program order.

Below are a few examples of classic one-sided communication patterns used by parallel applications.


### PutSignal and WaitSignal

A ping-pong pattern using `ncclPutSignal()` and `ncclWaitSignal()`. This example shows the full setup including memory allocation and window registration:

```c
// Allocate symmetric memory for RMA operations
void *sendbuff, *recvbuff;
NCCLCHECK(ncclMemAlloc((void**)&sendbuff, size));
NCCLCHECK(ncclMemAlloc((void**)&recvbuff, size));

// Register buffers as symmetric windows
ncclWindow_t sendWindow, recvWindow;
NCCLCHECK(ncclCommWindowRegister(comm, sendbuff, size, &sendWindow, NCCL_WIN_COLL_SYMMETRIC));
NCCLCHECK(ncclCommWindowRegister(comm, recvbuff, size, &recvWindow, NCCL_WIN_COLL_SYMMETRIC));

int peer = (rank == 0) ? 1 : 0;
ncclWaitSignalDesc_t waitDesc = {.opCnt = 1, .peer = peer, .sigIdx = 0, .ctx = ctx};

if (rank == 0) {
  // Rank 0: wait then put
  NCCLCHECK(ncclWaitSignal(1, &waitDesc, comm, stream));
  NCCLCHECK(ncclPutSignal(sendbuff, count, datatype, peer, recvWindow, 0,
                    0, 0, 0, comm, stream));
} else {
  // Rank 1: put then wait
  NCCLCHECK(ncclPutSignal(sendbuff, count, datatype, peer, recvWindow, 0,
                    0, 0, 0, comm, stream));
  NCCLCHECK(ncclWaitSignal(1, &waitDesc, comm, stream));
}

CUDACHECK(cudaStreamSynchronize(stream));

// Cleanup
NCCLCHECK(ncclCommWindowDeregister(comm, sendWindow));
NCCLCHECK(ncclCommWindowDeregister(comm, recvWindow));
NCCLCHECK(ncclMemFree(sendbuff));
NCCLCHECK(ncclMemFree(recvbuff));
```

### Barrier

A barrier pattern using `ncclSignal()` and `ncclWaitSignal()`. Each rank signals to all other ranks and waits for signals from all ranks:

```c
ncclWaitSignalDesc_t *waitDescs = malloc(nranks * sizeof(ncclWaitSignalDesc_t));
for (int r = 0; r < nranks; r++) {
  waitDescs[r].opCnt = 1;
  waitDescs[r].peer = r;
  waitDescs[r].sigIdx = 0;
  waitDescs[r].ctx = 0;
}

ncclGroupStart();
for (int r = 0; r < nranks; r++) {
  ncclSignal(r, 0, 0, 0, comm, stream);
}
ncclGroupEnd();

ncclWaitSignal(nranks, waitDescs, comm, stream);
```

### All-to-all

An all-to-all operation using `ncclPutSignal()`. Each rank sends data to all other ranks and waits for signals from all ranks. User needs to register the memory window for each peer using `ncclCommWindowRegister()` in advance. User needs to guarantee the buffers are ready before calling `ncclPutSignal()`. This could be done with the barrier shown above.

```c
size_t offset[nranks];
ncclWaitSignalDesc_t *waitDescs = malloc(nranks * sizeof(ncclWaitSignalDesc_t));
for (int r = 0; r < nranks; r++) {
  offset[r] = r * count * wordSize(datatype);
  waitDescs[r].opCnt = 1;
  waitDescs[r].peer = r;
  waitDescs[r].sigIdx = 0;
  waitDescs[r].ctx = 0;
}

ncclGroupStart();
for (int r = 0; r < nranks; r++) {
  ncclPutSignal(sendbuff[r], count, datatype, r, window, offset[r],
          0, 0, 0, comm, stream);
}
ncclGroupEnd();

ncclWaitSignal(nranks, waitDescs, comm, stream);
```

---

# Using NCCL with CUDA Graphs

Starting with NCCL 2.9, NCCL operations can be captured by CUDA Graphs.

CUDA Graphs provide a way to define workflows as graphs rather than single operations. They may reduce overhead by launching multiple GPU operations through a single CPU operation. More details about CUDA Graphs can be found in the [CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-graphs).

NCCL's collective, P2P and group operations all support CUDA Graph captures. This support requires a minimum CUDA version of 11.3.

Whether an operation launch is graph-captured is considered a collective property of that operation and therefore must be uniform over all ranks participating in the launch (for collectives this is all ranks in the communicator, for peer-to-peer this is both the sender and receiver). The launch of a graph (via cudaGraphLaunch, etc.) containing a captured NCCL operation is considered collective for the same set of ranks that were present in the capture, and each of those ranks must be using the graph derived from that collective capture.

The following sample code shows how to capture computational kernels and NCCL operations in a CUDA Graph:

```cpp
cudaGraph_t graph;
cudaStreamBeginCapture(stream);
kernel_A<<< ..., stream >>>(...);
kernel_B<<< ..., stream >>>(...);
ncclAllreduce(..., stream);
kernel_C<<< ..., stream >>>(...);
cudaStreamEndCapture(stream, &graph);

cudaGraphExec_t instance;
cudaGraphInstantiate(&instance, graph, NULL, NULL, 0);
cudaGraphLaunch(instance, stream);
cudaStreamSynchronize(stream);
```

Starting with NCCL 2.11, when NCCL communication is captured and the CollNet algorithm is used, NCCL allows for further performance improvement via user buffer registration. For details, please see the environment variable NCCL\_GRAPH\_REGISTER.

Having multiple outstanding NCCL operations that are any combination of graph-captured or non-captured is supported. There is a caveat that the mechanism NCCL uses internally to accomplish this has been seen to cause CUDA to deadlock when the graphs of multiple communicators are cudaGraphLaunch()'d from the same thread. To disable this mechansim see the environment variable NCCL\_GRAPH\_MIXING\_SUPPORT.

---

# Environment Variables

NCCL has an extensive set of environment variables to tune for specific usage.

Environment variables can also be set statically in /etc/nccl.conf (for an administrator to set system-wide values) or in \${NCCL\_CONF\_FILE} (since 2.23; see below). For example, those files could contain :

```c
NCCL_DEBUG=WARN
NCCL_SOCKET_IFNAME==ens1f0
```

There are two categories of environment variables. Some are needed to make NCCL follow system-specific configuration, and can be kept in scripts and system configuration. Other parameters listed in the "Debugging" section should not be used in production nor retained in scripts, or only as workaround, and removed as soon as the issue is resolved. Keeping them set may result in sub-optimal behavior, crashes, or hangs.


## System configuration


### NCCL\_SOCKET\_IFNAME

The `NCCL_SOCKET_IFNAME` variable specifies which IP interfaces to use for communication.


#### Values accepted

Define to a list of prefixes to filter interfaces to be used by NCCL.

Multiple prefixes can be provided, separated by the `,` symbol.

Using the `^` symbol, NCCL will exclude interfaces starting with any prefix in that list.

To match (or not) an exact interface name, begin the prefix string with the `=` character.

Examples:

`eth` : Use all interfaces starting with `eth`, e.g. `eth0`, `eth1`, ...

`=eth0` : Use only interface `eth0`

`=eth0,eth1` : Use only interfaces `eth0` and `eth1`

`^docker` : Do not use any interface starting with `docker`

`^=docker0` : Do not use interface `docker0`.

Note: By default, the loopback interface (`lo`) and docker interfaces (`docker*`) would not be selected unless there are no other interfaces available. If you prefer to use `lo` or `docker*` over other interfaces, you would need to explicitly select them using `NCCL_SOCKET_IFNAME`. The default algorithm will also favor interfaces starting with `ib` over others. Setting `NCCL_SOCKET_IFNAME` will bypass the automatic interface selection algorithm and may use all interfaces matching the manual selection.


### NCCL\_SOCKET\_FAMILY

The `NCCL_SOCKET_FAMILY` variable allows users to force NCCL to use only IPv4 or IPv6 interface.


#### Values accepted

Set to `AF_INET` to force the use of IPv4, or `AF_INET6` to force IPv6 usage.


### NCCL\_SOCKET\_RETRY\_CNT

(since 2.24)

The `NCCL_SOCKET_RETRY_CNT` variable specifies the number of times NCCL retries to establish a socket connection after an `ETIMEDOUT`, `ECONNREFUSED`, or `EHOSTUNREACH` error.


#### Values accepted

The default value is 34, any positive value is valid.


### NCCL\_SOCKET\_RETRY\_SLEEP\_MSEC

(since 2.24)

The `NCCL_SOCKET_RETRY_SLEEP_MSEC` variable specifies the number of milliseconds NCCL waits before retrying to establish a socket connection after the first `ETIMEDOUT`, `ECONNREFUSED`, or `EHOSTUNREACH` error. For subsequent errors, the waiting time scales linearly with the error count. The total time will therefore be (N+1) \* N/2 \* `NCCL_SOCKET_RETRY_SLEEP_MSEC`, where N is given by `NCCL_SOCKET_RETRY_CNT`. With the default values of `NCCL_SOCKET_RETRY_CNT` and `NCCL_SOCKET_RETRY_SLEEP_MSEC`, the total retry time will be approx. 60 seconds.


#### Values accepted

The default value is 100 milliseconds, any positive value is valid.


### NCCL\_SOCKET\_POLL\_TIMEOUT\_MSEC

(since 2.28)

The `NCCL_SOCKET_POLL_TIMEOUT_MSEC` variable specifies a timeout in milliseconds for a poll which can reduce the CPU usage during bootstrap. Normally NCCL will retry the operation until it completes. Polling in between attempts should reduce load on the CPU so that it can engage in activities that might make the operation able to complete sooner.


#### Values accepted

Non-negative integer. The old behavior corredponds to 0 (the default). If 0, it will not poll, but keep trying to progress the socket operation without pause. If non-zero, it will poll for up that amount of time before trying to progress the operation again.


### NCCL\_SOCKET\_NTHREADS

(since 2.4.8)

The `NCCL_SOCKET_NTHREADS` variable specifies the number of CPU helper threads used per network connection for socket transport. Increasing this value may increase the socket transport performance, at the cost of a higher CPU usage.


#### Values accepted

1 to 16. On AWS, the default value is 2; on Google Cloud instances with the gVNIC network interface, the default value is 4 (since 2.5.6); in other cases, the default value is 1.

For generic 100G networks, this value can be manually set to 4. However, the product of `NCCL_SOCKET_NTHREADS` and `NCCL_NSOCKS_PERTHREAD` cannot exceed 64. See also `NCCL_NSOCKS_PERTHREAD`.


### NCCL\_NSOCKS\_PERTHREAD

(since 2.4.8)

The `NCCL_NSOCKS_PERTHREAD` variable specifies the number of sockets opened by each helper thread of the socket transport. In environments where per-socket speed is limited, setting this variable larger than 1 may improve the network performance.


#### Values accepted

On AWS, the default value is 8; in other cases, the default value is 1.

For generic 100G networks, this value can be manually set to 4. However, the product of `NCCL_SOCKET_NTHREADS` and `NCCL_NSOCKS_PERTHREAD` cannot exceed 64. See also `NCCL_SOCKET_NTHREADS`.


### NCCL\_CROSS\_NIC

The `NCCL_CROSS_NIC` variable controls whether NCCL should allow rings/trees to use different NICs, causing inter-node communication to use different NICs on different nodes.

To maximize inter-node communication performance when using multiple NICs, NCCL tries to use the same NICs when communicating between nodes, to allow for a network design where each NIC on a node connects to a different network switch (network rail), and avoid any risk of traffic flow interference. The `NCCL_CROSS_NIC` setting is therefore dependent on the network topology, and in particular on whether the network fabric is rail-optimized or not.

This has no effect on systems with only one NIC.


#### Values accepted

0: Always use the same NIC for the same ring/tree, to avoid crossing network rails. Suited for networks with per NIC switches (rails), with a slow inter-rail connection. Note that if the communicator does not contain the same GPUs on each node, NCCL may still need to communicate across NICs.

1: Allow the use of different NICs for the same ring/tree. This is suited for networks where all NICs from a node are connected to the same switch, hence trying to communicate across the same NICs does not help avoiding flow collisions.

2: (Default) Try to use the same NIC for the same ring/tree, but still allow for the use of different NICs if it would result in a better performance.


### NCCL\_IB\_HCA

The `NCCL_IB_HCA` variable specifies which Host Channel Adapter (RDMA) interfaces to use for communication.


#### Values accepted

Define to filter IB Verbs interfaces to be used by NCCL. The list is comma-separated; port numbers can be specified using the `:` symbol. An optional prefix `^` indicates the list is an exclude list. A second optional prefix `=` indicates that the tokens are exact names, otherwise by default NCCL would treat each token as a prefix.

Examples:

`mlx5` : Use all ports of all cards starting with `mlx5`

`=mlx5_0:1,mlx5_1:1` : Use ports 1 of cards `mlx5_0` and `mlx5_1`.

`^=mlx5_1,mlx5_4` : Do not use cards `mlx5_1` and `mlx5_4`.

Note: using `mlx5_1` without a preceding `=` will select `mlx5_1` as well as `mlx5_10` to `mlx5_19`, if they exist. It is therefore always recommended to add the `=` prefix to ensure an exact match.

Note: There is a fixed upper limit of 32 Host Channel Adapter (HCA) devices supported in NCCL.


### NCCL\_IB\_TIMEOUT

The `NCCL_IB_TIMEOUT` variable controls the InfiniBand Verbs Timeout.

The timeout is computed as 4.096 Âµs \* 2 \^ *timeout*, and the correct value is dependent on the size of the network. Increasing that value can help on very large networks, for example, if NCCL is failing on a call to *ibv\_poll\_cq* with error 12.

For more information, see section 12.7.34 of the InfiniBand specification Volume 1 (Local Ack Timeout).


#### Values accepted

The default value used by NCCL is 20 (since 2.23; it was 18 since 2.14, and 14 before that).

Values can be 1-31.

Note: Setting a value of 0 or \>= 32 will result in an infinite timeout value.


### NCCL\_IB\_RETRY\_CNT

(since 2.1.15)

The `NCCL_IB_RETRY_CNT` variable controls the InfiniBand retry count.

For more information, see section 12.7.38 of the InfiniBand specification Volume 1.


#### Values accepted

The default value is 7.


### NCCL\_IB\_GID\_INDEX

(since 2.1.4)

The `NCCL_IB_GID_INDEX` variable defines the Global ID index used in RoCE mode. See the InfiniBand *show\_gids* command in order to set this value.

For more information, see the InfiniBand specification Volume 1 or vendor documentation.


#### Values accepted

The default value is -1.


### NCCL\_IB\_ADDR\_FAMILY

(since 2.21)

The `NCCL_IB_ADDR_FAMILY` variable defines the IP address family associated to the infiniband GID dynamically selected by NCCL when `NCCL_IB_GID_INDEX` is left unset.


#### Values accepted

The default value is "AF\_INET".


### NCCL\_IB\_ADDR\_RANGE

(since 2.21)

The `NCCL_IB_ADDR_RANGE` variable defines the range of valid GIDs dynamically selected by NCCL when `NCCL_IB_GID_INDEX` is left unset.


#### Values accepted

By default, ignored if unset.

GID ranges can be defined using the Classless Inter-Domain Routing (CIDR) format for IPv4 and IPv6 families.


### NCCL\_IB\_ROCE\_VERSION\_NUM

(since 2.21)

The `NCCL_IB_ROCE_VERSION_NUM` variable defines the RoCE version associated to the infiniband GID dynamically selected by NCCL when `NCCL_IB_GID_INDEX` is left unset.


#### Values accepted

The default value is 2.


### NCCL\_IB\_SL

(since 2.1.4)

Defines the InfiniBand Service Level.

For more information, see the InfiniBand specification Volume 1 or vendor documentation.


#### Values accepted

The default value is 0.


### NCCL\_IB\_TC

(since 2.1.15)

Defines the InfiniBand traffic class field.

For more information, see the InfiniBand specification Volume 1 or vendor documentation.


#### Values accepted

The default value is 0.


### NCCL\_IB\_FIFO\_TC

(since 2.22.3)

Defines the InfiniBand traffic class for control messages. Control messages are short RDMA write operations which control credit return, contrary to other RDMA operations transmitting large segments of data. This setting allows to have those messages use a high priority, low-latency traffic class and avoid being delayed by the rest of the traffic.


#### Values accepted

The default value is the traffic class set by NCCL\_IB\_TC, which defaults to 0 if not set.


### NCCL\_IB\_RETURN\_ASYNC\_EVENTS

(since 2.23)

IB events are reported to the user as warnings. If enabled, NCCL will also stop IB communications upon fatal IB asynchronous events.


#### Values accepted

The default value is 1, set to 0 to disable


### NCCL\_OOB\_NET\_ENABLE

(since 2.23) The variable `NCCL_OOB_NET_ENABLE` enables the use of NCCL net for out-of-band communications. Enabling the usage of NCCL net will change the implementation of the allgather performed during the communicator initialization.


#### Values accepted

Set the variable to 0 to disable, and to 1 to enable.


### NCCL\_OOB\_NET\_IFNAME

(since 2.23) If NCCL net is enabled for out-of-band communication (see `NCCL_OOB_NET_ENABLE`), the `NCCL_OOB_NET_IFNAME` variable specifies which network interfaces to use.


#### Values accepted

Define to filter interfaces to be used by NCCL for out-of-band communications. The list of accepted interface depends on the network used by NCCL. The list is comma-separated; port numbers can be specified using the `:` symbol. An optional prefix `^` indicates the list is an exclude list. A second optional prefix `=` indicates that the tokens are exact names, otherwise by default NCCL would treat each token as a prefix. If multiple devices are specified, NCCL will select the first matching device in the list.

Example:

`NCCL_NET="IB" NCCL_OOB_NET_ENABLE=1 NCCL_OOB_NET_IFNAME="=mlx5_1"` will use the Infiniband NET, with the interface `mlx5_1`

`NCCL_NET="IB" NCCL_OOB_NET_ENABLE=1 NCCL_OOB_NET_IFNAME="mlx5_1"` will use the Infiniband NET, with the first interface found in the list of `mlx5_1`, `mlx5_10`, `mlx5_11`, etc.

`NCCL_NET="Socket" NCCL_OOB_NET_ENABLE=1 NCCL_OOB_NET_IFNAME="ens1"` will use the socket NET, with the first interface found in the list of `ens1f0`, `ens1f1`, etc.


### NCCL\_UID\_STAGGER\_THRESHOLD

(since 2.23) The `NCCL_UID_STAGGER_THRESHOLD` variable is used to trigger staggering of communications between NCCL ranks and the ncclUniqueId in order to avoid overflowing the ncclUniqueId. If the number of NCCL ranks communicating exceeds the specified threshold, the communications are staggered using the rank value (see NCCL\_UID\_STAGGER\_RATE below). If the number of NCCL ranks per ncclUniqueId is smaller or equal to the threshold, no staggering is performed.

For example, if we have 128 NCCL ranks, 1 ncclUniqueId, and a threshold at 64, staggering is performed. However, if 2 ncclUniqueIds are used with 128 NCCL ranks and a threshold at 64, no staggering is done.


#### Values accepted

The value of `NCCL_UID_STAGGER_THRESHOLD` must be a strictly positive integer. If unspecified, the default value is 256.


### NCCL\_UID\_STAGGER\_RATE

(since 2.23)

The `NCCL_UID_STAGGER_RATE` variable is used to define the message rate targeted when staggering the communications between NCCL ranks and the ncclUniqueId. If staggering is used (see NCCL\_UID\_STAGGER\_THRESHOLD above), the message rate is used to compute the time a given NCCL rank has to wait.


#### Values accepted

The value of `NCCL_UID_STAGGER_RATE` must be a strictly positive integer, expressed in messages/second. If unspecified, the default value is 7000.


### NCCL\_NET

(since 2.10)

Forces NCCL to use a specific network, for example to make sure NCCL uses an external plugin and doesn't automatically fall back on the internal IB or Socket implementation. Setting this environment variable will override the `netName` configuration in all communicators (see ncclConfig\_t); if not set (undefined), the network module will be determined by the configuration; if not passing configuration, NCCL will automatically choose the best network module.


#### Values accepted

The value of NCCL\_NET has to match exactly the name of the NCCL network used (case-insensitive). Internal network names are "IB" (generic IB verbs) and "Socket" (TCP/IP sockets). External network plugins define their own names. Default value is undefined.


### NCCL\_NET\_PLUGIN

(since 2.11)

Set it to either a suffix string or to a library name to choose among multiple NCCL net plugins. This setting will cause NCCL to look for the net plugin library using the following strategy:

-   If NCCL\_NET\_PLUGIN is set, attempt loading the library with name specified by NCCL\_NET\_PLUGIN;

-   If NCCL\_NET\_PLUGIN is set and previous failed, attempt loading libnccl-net-\<NCCL\_NET\_PLUGIN\>.so;

-   If NCCL\_NET\_PLUGIN is not set, attempt loading libnccl-net.so;

-   If no plugin was found (neither user defined nor default), use internal network plugin.

For example, setting `NCCL_NET_PLUGIN=foo` will cause NCCL to try load `foo` and, if `foo` cannot be found, `libnccl-net-foo.so` (provided that it exists on the system).


#### Values accepted

Plugin suffix, plugin file name, or "none".


### NCCL\_TUNER\_PLUGIN

Set it to either a suffix string or to a library name to choose among multiple NCCL tuner plugins. This setting will cause NCCL to look for the tuner plugin library using the following strategy:

-   If NCCL\_TUNER\_PLUGIN is set, attempt loading the library with name specified by NCCL\_TUNER\_PLUGIN;

-   If NCCL\_TUNER\_PLUGIN is set and previous failed, attempt loading libnccl-net-\<NCCL\_TUNER\_PLUGIN\>.so;

-   If NCCL\_TUNER\_PLUGIN is not set, attempt loading libnccl-tuner.so;

-   If no plugin was found look for the tuner symbols in the net plugin (refer to `NCCL_NET_PLUGIN`);

-   If no plugin was found (neither through NCCL\_TUNER\_PLUGIN nor NCCL\_NET\_PLUGIN), use internal tuner plugin.

For example, setting `NCCL_TUNER_PLUGIN=foo` will cause NCCL to try load `foo` and, if `foo` cannot be found, `libnccl-tuner-foo.so` (provided that it exists on the system).


#### Values accepted

Plugin suffix, plugin file name, or "none".


### NCCL\_PROFILER\_PLUGIN

Set it to either a suffix string or to a library name to choose among multiple NCCL profiler plugins. This setting will cause NCCL to look for the profiler plugin library using the following strategy:

-   If NCCL\_PROFILER\_PLUGIN is set, attempt loading the library with name specified by NCCL\_PROFILER\_PLUGIN;

-   If NCCL\_PROFILER\_PLUGIN is set and previous failed, attempt loading libnccl-profiler-\<NCCL\_PROFILER\_PLUGIN\>.so;

-   If NCCL\_PROFILER\_PLUGIN is not set, attempt loading libnccl-profiler.so;

-   If no plugin was found (neither user defined nor default), do not enable profiling.

-   If NCCL\_PROFILER\_PLUGIN is set to `STATIC_PLUGIN`, the plugin symbols are searched in the program binary.

For example, setting `NCCL_PROFILER_PLUGIN=foo` will cause NCCL to try load `foo` and, if `foo` cannot be found, `libnccl-profiler-foo.so` (provided that it exists on the system).


#### Values accepted

Plugin suffix, plugin file name, or "none".


### NCCL\_ENV\_PLUGIN

(since 2.28)

The `NCCL_ENV_PLUGIN` variable can be used to let NCCL load an external environment plugin. Set it to either a library name or a suffix string to choose among multiple NCCL environment plugins. This setting will cause NCCL to look for the environment plugin library using the following strategy:

-   If `NCCL_ENV_PLUGIN` is set to a library name, attempt loading that library (e.g. `NCCL_ENV_PLUGIN=/path/to/library/libfoo.so` will cause NCCL to try load `/path/to/library/libfoo.so`);

-   If `NCCL_ENV_PLUGIN` is set to a suffix string, attempt loading `libnccl-env-<NCCL_ENV_PLUGIN>.so` (e.g. `NCCL_ENV_PLUGIN=foo` will cause NCCL to try load `libnccl-env-foo.so` from the system library path);

-   If `NCCL_ENV_PLUGIN` is not set, attempt loading the default `libnccl-env.so` library from the system library path;

-   If `NCCL_ENV_PLUGIN` is set to "none", explicitly disable the external plugin and use the internal one;

-   If no plugin was found (neither user defined nor default) or the variable is set to "none", use the internal environment plugin.


#### Values accepted

Plugin library name (e.g., `/path/to/library/libfoo.so`), suffix (e.g., `foo`), or "none".


### NCCL\_IGNORE\_CPU\_AFFINITY

(since 2.4.6)

The `NCCL_IGNORE_CPU_AFFINITY` variable can be used to cause NCCL to ignore the job's supplied CPU affinity and instead use the GPU affinity only.


#### Values accepted

The default is 0, set to 1 to cause NCCL to ignore the job's supplied CPU affinity.


### NCCL\_CONF\_FILE

(since 2.23)

The `NCCL_CONF_FILE` variable allows the user to specify a file with the static configuration. This does not accept the `~` character as part of the path; please convert to a relative or absolute path first.


#### Values accepted

If unset or if the version is prior to 2.23, NCCL uses .nccl.conf in the home directory if available.


### NCCL\_DEBUG

The `NCCL_DEBUG` variable controls the debug information that is displayed from NCCL. This variable is commonly used for debugging.


#### Values accepted

VERSION - Prints the NCCL version at the start of the program.

WARN - Prints an explicit error message whenever any NCCL call errors out.

INFO - Prints debug information

TRACE - Prints replayable trace information on every call.


### NCCL\_DEBUG\_FILE

(since 2.2.12)

The `NCCL_DEBUG_FILE` variable directs the NCCL debug logging output to a file. The filename format can be set to *filename.%h.%p* where *%h* is replaced with the hostname and *%p* is replaced with the process PID. This does not accept the `~` character as part of the path, please convert to a relative or absolute path first.


#### Values accepted

The default output file is *stdout* unless this environment variable is set. The filename can also be set to `/dev/stdout` or `/dev/stderr` to direct NCCL debug logging output to those predefined I/O streams. This also has the effect of making the output line buffered.

Setting `NCCL_DEBUG_FILE` will cause NCCL to create and overwrite any previous files of that name.

Note: If the filename is not unique across all the job processes, then the output may be lost or corrupted.


### NCCL\_DEBUG\_SUBSYS

(since 2.3.4)

The `NCCL_DEBUG_SUBSYS` variable allows the user to filter the `NCCL_DEBUG=INFO` output based on subsystems. The value should be a comma separated list of the subsystems to include in the NCCL debug log traces.

Prefixing the subsystem name with '\^' will disable the logging for that subsystem.


#### Values accepted

The default value is INIT,BOOTSTRAP,ENV.

Supported subsystem names are INIT (stands for initialization), COLL (stands for collectives), P2P (stands for peer-to-peer), SHM (stands for shared memory), NET (stands for network), GRAPH (stands for topology detection and graph search), TUNING (stands for algorithm/protocol tuning), ENV (stands for environment settings), ALLOC (stands for memory allocations), CALL (standard for function calls), PROXY (stands for the proxy thread operations), NVLS (standard for NVLink SHARP), BOOTSTRAP (stands for early initialization), REG (stands for memory registration), PROFILE (stands for coarse-grained profiling of initialization), RAS (stands for reliability, availability, and serviceability subsystem) and ALL (includes every subsystem).


### NCCL\_DEBUG\_TIMESTAMP\_FORMAT

(since 2.26)

The `NCCL_DEBUG_TIMESTAMP_FORMAT` variable allows the user to change the format used when printing debug log messages.

The time is printed as a local time. This can be changed by setting the `TZ` environment variable. UTC is available by setting `TZ=UTC`. Valid values for TZ look like: `US/Pacific`, `America/Los_Angeles`, etc.

Note that the non-call `TRACE` level of logs continues to print the microseconds since the NCCL debug subsystem was initialized. The `TRACE` logs can also print the strftime formatted timestamp at the beginning if so configured (see `NCCL_DEBUG_TIMESTAMP_LEVELS`).

(since 2.26) Underscores in the format are rendered as spaces.


#### Value accepted

The value of the environment variable is passed to strftime, so any valid format will work here. The default is `[%F %T] `, which is `[YYYY-MM-DD HH:MM:SS] `. If the value is set, but empty, then no timestamp will be printed (`NCCL_DEBUG_TIMESTAMP_FORMAT=`).

In addition to conversion specifications supported by strftime, `%Xf` can be specified, where `X` is a single numerical digit from 1-9. This will print fractions of a second. The value of `X` indicates how many digits will be printed. For example, `%3f` will print milliseconds. The value is zero padded. For example: `[%F %T.%9f] `. (Note that this can only be used once in the format string.)


### NCCL\_DEBUG\_TIMESTAMP\_LEVELS

(since 2.26)

The `NCCL_DEBUG_TIMESTAMP_LEVELS` variable allows the user to set which log lines get a timestamp depending upon the level of the log.


#### Value accepted

The value should be a comma separated list of the levels which should have the timestamp. Valid levels are: `VERSION`, `WARN`, `INFO`, `ABORT`, and `TRACE`. In addition, `ALL` can be used to turn it on for all levels. Setting it to an empty value disables it for all levels. If the value is prefixed with a caret (`^`) then the listed levels will NOT log a timestamp, and the rest will. The default is to enable timestamps for `WARN`, but disable it for the rest.

For example, `NCCL_DEBUG_TIMESTAMP_LEVELS=WARN,INFO,TRACE` will turn it on for warnings, info logs, and traces. Or, `NCCL_DEBUG_TIMESTAMP_LEVELS=^TRACE` will turn them on for everything but traces, which (except call traces) have their own type of timestamp (microseconds since nccl debug initialization).


### NCCL\_COLLNET\_ENABLE

(since 2.6)

Enable the use of the CollNet plugin.


#### Value accepted

Default is 0, define and set to 1 to use the CollNet plugin.


### NCCL\_COLLNET\_NODE\_THRESHOLD

(since 2.9.9)

A threshold for the number of nodes below which CollNet will not be enabled.


#### Value accepted

Default is 2, define and set to an integer.


### NCCL\_CTA\_POLICY

(since 2.29, legacy values since 2.27)

The `NCCL_CTA_POLICY` variable allows the user to set the policy for the NCCL communicator.


#### Value accepted

Set to `DEFAULT` (or `0`, legacy) to use `NCCL_CTA_POLICY_DEFAULT` policy (default). Set to `EFFICIENCY` (or `1`, legacy) to use `NCCL_CTA_POLICY_EFFICIENCY` policy. Set to `ZERO` (or `2`, legacy) to use `NCCL_CTA_POLICY_ZERO` policy.

Set multiple non-legacy policies with the `|` operator.

For more explanation about NCCL policies, please see NCCL Communicator CTA Policy Flags.


### NCCL\_NETDEVS\_POLICY

(since 2.28)

The `NCCL_NETDEVS_POLICY` variable allows the user to set the policy for the assignment of network devices to the GPUs. For each GPU, NCCL detects automatically available network devices, taking into account their network bandwidth and the node topology.


#### Value accepted

If set to `AUTO` (default), NCCL also takes into account the other GPUs in the same communicator in order to assign network devices. In specific scenarios, this policy might lead to different GPUs from different communicators sharing the same network devices, and therefore impacts performance.

If set to `MAX:N`, NCCL uses up to N of the network devices available to each GPU. This is intended to be used when device sharing happens with `AUTO` and impacts the performance.

If set to `ALL`, NCCL will use all the available network devices for each GPU, disregarding other GPUs.


### NCCL\_TOPO\_FILE

(since 2.6)

Path to an XML file to load before detecting the topology. By default, NCCL will load `/var/run/nvidia-topologyd/virtualTopology.xml` if present.


#### Value accepted

A path to an accessible file describing part or all of the topology.


### NCCL\_TOPO\_DUMP\_FILE

(since 2.6)

Path to a file to dump the XML topology to after detection.


#### Value accepted

A path to a file which will be created or overwritten.


### NCCL\_SET\_THREAD\_NAME

(since 2.12)

Give more meaningful names to NCCL CPU threads to ease debugging and analysis.


#### Value accepted

0 or 1. Default is 0 (disabled).


## Debugging

These environment variables should be used with caution. New versions of NCCL could work differently and forcing them to a particular value will prevent NCCL from selecting the best setting automatically. They can therefore cause performance problems in the long term, or even break some functionality.

They are fine to use for experiments, or to debug a problem, but should generally not be set for production code.


### NCCL\_P2P\_DISABLE

The `NCCL_P2P_DISABLE` variable disables the peer to peer (P2P) transport, which uses CUDA direct access between GPUs, using NVLink or PCI.


#### Values accepted

Define and set to 1 to disable direct GPU-to-GPU (P2P) communication.


### NCCL\_P2P\_LEVEL

(since 2.3.4)

The `NCCL_P2P_LEVEL` variable allows the user to finely control when to use the peer to peer (P2P) transport between GPUs. The level defines the maximum distance between GPUs where NCCL will use the P2P transport. A short string representing the path type should be used to specify the topographical cutoff for using the P2P transport.

If this isn't specified, NCCL will attempt to optimally select a value based on the architecture and environment it's run in.


#### Values accepted

-   LOC : Never use P2P (always disabled)

-   NVL : Use P2P when GPUs are connected through NVLink

-   PIX : Use P2P when GPUs are on the same PCI switch.

-   PXB : Use P2P when GPUs are connected through PCI switches (potentially multiple hops).

-   PHB : Use P2P when GPUs are on the same NUMA node. Traffic will go through the CPU.

-   SYS : Use P2P between NUMA nodes, potentially crossing the SMP interconnect (e.g. QPI/UPI).

#### Integer Values (Legacy)

There is also the option to declare `NCCL_P2P_LEVEL` as an integer corresponding to the path type. These numerical values were kept for retro-compatibility, for those who used numerical values before strings were allowed.

Integer values are discouraged due to breaking changes in path types - the literal values can change over time. To avoid headaches debugging your configuration, use string identifiers.

-   LOC : 0

-   PIX : 1

-   PXB : 2

-   PHB : 3

-   SYS : 4

Values greater than 4 will be interpreted as SYS. NVL is not supported using the legacy integer values.


### NCCL\_P2P\_DIRECT\_DISABLE

The `NCCL_P2P_DIRECT_DISABLE` variable forbids NCCL to directly access user buffers through P2P between GPUs of the same process. This is useful when user buffers are allocated with APIs which do not automatically make them accessible to other GPUs managed by the same process and with P2P access.


#### Values accepted

Define and set to 1 to disable direct user buffer access across GPUs.


### NCCL\_SHM\_DISABLE

The `NCCL_SHM_DISABLE` variable disables the Shared Memory (SHM) transports. SHM is used between devices when peer-to-peer cannot happen, therefore, host memory is used. NCCL will use the network (i.e. InfiniBand or IP sockets) to communicate between the CPU sockets when SHM is disabled.


#### Values accepted

Define and set to 1 to disable communication through shared memory (SHM).


### NCCL\_BUFFSIZE

The `NCCL_BUFFSIZE` variable controls the size of the buffer used by NCCL when communicating data between pairs of GPUs.

Use this variable if you encounter memory constraint issues when using NCCL or you think that a different buffer size would improve performance.


#### Values accepted

The default is 4194304 (4 MiB).

Values are integers, in bytes. The recommendation is to use powers of 2. For example, 1024 will give a 1KiB buffer.


### NCCL\_NTHREADS

The `NCCL_NTHREADS` variable sets the number of CUDA threads per CUDA block. NCCL will launch one CUDA block per communication channel.

Use this variable if you think your GPU clocks are low and you want to increase the number of threads.

You can also use this variable to reduce the number of threads to decrease the GPU workload.


#### Values accepted

The default is 512 for recent generation GPUs, and 256 for some older generations.

The values allowed are 64, 128, 256 and 512.


### NCCL\_MAX\_NCHANNELS

(NCCL\_MAX\_NRINGS since 2.0.5, NCCL\_MAX\_NCHANNELS since 2.5.0)

The `NCCL_MAX_NCHANNELS` variable limits the number of channels NCCL can use. Reducing the number of channels also reduces the number of CUDA blocks used for communication, hence the impact on GPU computing resources.

The old `NCCL_MAX_NRINGS` variable (used until 2.4) still works as an alias in newer versions but is ignored if `NCCL_MAX_NCHANNELS` is set.

This environment variable has been superseded by `NCCL_MAX_CTAS` which can also be set programmatically using ncclCommInitRankConfig.


#### Values accepted

Any value above or equal to 1.


### NCCL\_MIN\_NCHANNELS

(NCCL\_MIN\_NRINGS since 2.2.0, NCCL\_MIN\_NCHANNELS since 2.5.0)

The `NCCL_MIN_NCHANNELS` variable controls the minimum number of channels you want NCCL to use. Increasing the number of channels also increases the number of CUDA blocks NCCL uses, which may be useful to improve performance; however, it uses more CUDA compute resources.

This is especially useful when using aggregated collectives on platforms where NCCL would usually only create one channel.

The old `NCCL_MIN_NRINGS` variable (used until 2.4) still works as an alias in newer versions, but is ignored if `NCCL_MIN_NCHANNELS` is set.

This environment variable has been superseded by `NCCL_MIN_CTAS` which can also be set programmatically using ncclCommInitRankConfig.


#### Values accepted

The default is platform dependent. Set to an integer value, up to 12 (up to 2.2), 16 (2.3 and 2.4) or 32 (2.5 and later).


### NCCL\_CHECKS\_DISABLE

(since 2.0.5, deprecated in 2.2.12)

The `NCCL_CHECKS_DISABLE` variable can be used to disable argument checks on each collective call. Checks are useful during development but can increase the latency. They can be disabled to improve performance in production.


#### Values accepted

The default is 0, set to 1 to disable checks.


### NCCL\_CHECK\_POINTERS

(since 2.2.12)

The `NCCL_CHECK_POINTERS` variable enables checking of the CUDA memory pointers on each collective call. Checks are useful during development but can increase the latency.


#### Values accepted

The default is 0, set to 1 to enable checking.

Setting to 1 restores the original behavior of NCCL prior to 2.2.12.


### NCCL\_LAUNCH\_MODE

(since 2.1.0)

The `NCCL_LAUNCH_MODE` variable controls how NCCL launches CUDA kernels.


#### Values accepted

The default value is PARALLEL.

Setting is to GROUP will use cooperative groups (CUDA 9.0 and later) for processes managing more than one GPU. This is deprecated in 2.9 and may be removed in future versions.


### NCCL\_IB\_DISABLE

The `NCCL_IB_DISABLE` variable prevents the IB/RoCE transport from being used by NCCL. Instead, NCCL will fall back to using IP sockets.


#### Values accepted

Define and set to 1 to disable the use of InfiniBand Verbs for communication (and force another method, e.g. IP sockets).


### NCCL\_IB\_AR\_THRESHOLD

(since 2.6)

Threshold above which we send InfiniBand data in a separate message which can leverage adaptive routing.


#### Values accepted

Size in bytes, the default value is 8192.

Setting it above NCCL\_BUFFSIZE will disable the use of adaptive routing completely.


### NCCL\_IB\_QPS\_PER\_CONNECTION

(since 2.10)

Number of IB queue pairs to use for each connection between two ranks. This can be useful on multi-level fabrics which need multiple queue pairs to have good routing entropy. See `NCCL_IB_SPLIT_DATA_ON_QPS` for different ways to split data on multiple QPs, as it can affect performance.


#### Values accepted

Number between 1 and 128, default is 1.


### NCCL\_IB\_SPLIT\_DATA\_ON\_QPS

(since 2.18)

This parameter controls how we use the queue pairs when we create more than one. Set to 1 (split mode), each message will be split evenly on each queue pair. This may cause a visible latency degradation if many QPs are used. Set to 0 (round-robin mode), queue pairs will be used in round-robin mode for each message we send. Operations which do not send multiple messages will not use all QPs.


#### Values accepted

0 or 1. Default is 0 (since NCCL 2.20). Setting it to 1 will enable split mode (default in 2.18 and 2.19).


### NCCL\_IB\_CUDA\_SUPPORT

(removed in 2.4.0, see NCCL\_NET\_GDR\_LEVEL)

The `NCCL_IB_CUDA_SUPPORT` variable is used to force or disable the usage of GPU Direct RDMA. By default, NCCL enables GPU Direct RDMA if the topology permits it. This variable can disable this behavior or force the usage of GPU Direct RDMA in all cases.


#### Values accepted

Define and set to 0 to disable GPU Direct RDMA.

Define and set to 1 to force the usage of GPU Direct RDMA.


### NCCL\_IB\_PCI\_RELAXED\_ORDERING

(since 2.12)

Enable the use of Relaxed Ordering for the IB Verbs transport. Relaxed Ordering can greatly help the performance of InfiniBand networks in virtualized environments.


#### Values accepted

Set to 2 to automatically use Relaxed Ordering if available. Set to 1 to force the use of Relaxed Ordering and fail if not available. Set to 0 to disable the use of Relaxed Ordering. Default is 2.


### NCCL\_IB\_ADAPTIVE\_ROUTING

(since 2.16)

Enable the use of Adaptive Routing capable data transfers for the IB Verbs transport. Adaptive routing can improve the performance of communications at scale. A system defined Adaptive Routing enabled SL has to be selected accordingly (cf. `NCCL_IB_SL`).


#### Values accepted

Enabled (1) by default on IB networks. Disabled (0) by default on RoCE networks. Set to 1 to force use of Adaptive Routing capable data transmission.


### NCCL\_IB\_ECE\_ENABLE

(since 2.23)

Enable the use of Enhanced Connection Establishment (ECE) on IB/RoCE Verbs networks. ECE can be used to enable advanced networking features such as Congestion Control, Adaptive Routing and Selective Repeat. Note: These parameters are not interpreted or controlled by NCCL and are passed through directly to the HCAs via the ECE mechanism.


#### Values accepted

Enabled (1) by default (since 2.19). Set to 0 to disable use of ECE network capabilities.

Note: Incorrect configuration of the ECE parameters on a system can adversely affect NCCL performance. Administrators should ensure ECE is correctly configured if it is enabled at the system level.


### NCCL\_MEM\_SYNC\_DOMAIN

(since 2.16)

Sets the default Memory Sync Domain for NCCL kernels (CUDA 12.0 & sm90 and later). Memory Sync Domains can help eliminate interference between the NCCL kernels and the application compute kernels, when they use different domains.


#### Values accepted

Default value is `cudaLaunchMemSyncDomainRemote` (1). Currently supported values are 0 and 1.


### NCCL\_CUMEM\_ENABLE

(since 2.18)

Use CUDA cuMem\* functions to allocate memory in NCCL.


#### Values accepted

0 or 1. Default is 0 in 2.18 (disabled); since 2.19 this feature is auto-enabled by default if the system supports it (NCCL\_CUMEM\_ENABLE can still be used to override the autodetection).


### NCCL\_CUMEM\_HOST\_ENABLE

(since 2.23)

Use CUDA cuMem\* functions to allocate host memory in NCCL. See Shared memory for more information.


#### Values accepted

0 or 1. Default is 0 in 2.23; since 2.24, default is 1 if CUDA driver \>= 12.6, CUDA runtime \>= 12.2, and cuMem host allocations are supported.


### NCCL\_NET\_GDR\_LEVEL (formerly NCCL\_IB\_GDR\_LEVEL)

(since 2.3.4. In 2.4.0, NCCL\_IB\_GDR\_LEVEL was renamed to NCCL\_NET\_GDR\_LEVEL)

The `NCCL_NET_GDR_LEVEL` variable allows the user to finely control when to use GPU Direct RDMA between a NIC and a GPU. The level defines the maximum distance between the NIC and the GPU. A string representing the path type should be used to specify the topographical cutoff for GpuDirect.

If this isn't specified, NCCL will attempt to optimally select a value based on the architecture and environment it's run in.


#### Values accepted

-   LOC : Never use GPU Direct RDMA (always disabled).

-   PIX : Use GPU Direct RDMA when GPU and NIC are on the same PCI switch.

-   PXB : Use GPU Direct RDMA when GPU and NIC are connected through PCI switches (potentially multiple hops).

-   PHB : Use GPU Direct RDMA when GPU and NIC are on the same NUMA node. Traffic will go through the CPU.

-   SYS : Use GPU Direct RDMA even across the SMP interconnect between NUMA nodes (e.g., QPI/UPI) (always enabled).

#### Integer Values (Legacy)

There is also the option to declare `NCCL_NET_GDR_LEVEL` as an integer corresponding to the path type. These numerical values were kept for retro-compatibility, for those who used numerical values before strings were allowed.

Integer values are discouraged due to breaking changes in path types - the literal values can change over time. To avoid headaches debugging your configuration, use string identifiers.

-   LOC : 0

-   PIX : 1

-   PXB : 2

-   PHB : 3

-   SYS : 4

Values greater than 4 will be interpreted as SYS.


### NCCL\_NET\_GDR\_C2C

(since 2.26)

The `NCCL_NET_GDR_C2C` variable enables GPU Direct RDMA when sending data via a NIC attached to a CPU (i.e. distance PHB) where the CPU is connected to the GPU via a C2C interconnect. This effectively overrides the `NCCL_NET_GDR_LEVEL` setting for this particular NIC.


#### Values accepted

0 or 1. Define and set to 1 to use GPU Direct RDMA to send data to the NIC directly via C2C connected CPUs.

The default value was 0 in 2.26. The default value is 1 since 2.27.


### NCCL\_NET\_GDR\_READ

The `NCCL_NET_GDR_READ` variable enables GPU Direct RDMA when sending data as long as the GPU-NIC distance is within the distance specified by `NCCL_NET_GDR_LEVEL`. Before 2.4.2, GDR read is disabled by default, i.e. when sending data, the data is first stored in CPU memory, then goes to the InfiniBand card. Since 2.4.2, GDR read is enabled by default for NVLink-based platforms.

Note: Reading directly from GPU memory when sending data is known to be slightly slower than reading from CPU memory on some platforms, such as PCI-E.


#### Values accepted

0 or 1. Define and set to 1 to use GPU Direct RDMA to send data to the NIC directly (bypassing CPU).

Before 2.4.2, the default value is 0 for all platforms. Since 2.4.2, the default value is 1 for NVLink-based platforms and 0 otherwise.


### NCCL\_NET\_SHARED\_BUFFERS

(since 2.8)

Allows the usage of shared buffers for inter-node point-to-point communication. This will use a single large pool for all remote peers, having a constant memory usage instead of increasing linearly with the number of remote peers.


#### Value accepted

Default is 1 (enabled). Set to 0 to disable.


### NCCL\_NET\_SHARED\_COMMS

(since 2.12)

Reuse the same connections in the context of PXN. This allows for message aggregation but can also decrease the entropy of network packets.


#### Value accepted

Default is 1 (enabled). Set to 0 to disable.


### NCCL\_SINGLE\_RING\_THRESHOLD

(since 2.1, removed in 2.3)

The `NCCL_SINGLE_RING_THRESHOLD` variable sets the limit under which NCCL will only use one ring. This will limit bandwidth but improve latency.


#### Values accepted

The default value is 262144 (256kB) on GPUs with compute capability 7 and above. Otherwise, the default value is 131072 (128kB).

Values are integers, in bytes.


### NCCL\_LL\_THRESHOLD

(since 2.1, removed in 2.5)

The `NCCL_LL_THRESHOLD` variable sets the size limit under which NCCL uses low-latency algorithms.


#### Values accepted

The default is 16384 (up to 2.2) or is dependent on the number of ranks (2.3 and later).

Values are integers, in bytes.


### NCCL\_TREE\_THRESHOLD

(since 2.4, removed in 2.5)

The `NCCL_TREE_THRESHOLD` variable sets the size limit under which NCCL uses tree algorithms instead of rings.


#### Values accepted

The default is dependent on the number of ranks.

Values are integers, in bytes.


### NCCL\_ALGO

(since 2.5)

The `NCCL_ALGO` variable defines which algorithms NCCL will use.


#### Values accepted

(since 2.5)

Comma-separated list of algorithms (not case sensitive) among:

  Version       Algorithm
  ------------- ---------------
  2.5+          Ring
  2.5+          Tree
  2.5 to 2.13   Collnet
  2.14+         CollnetChain
  2.14+         CollnetDirect
  2.17+         NVLS
  2.18+         NVLSTree
  2.23+         PAT

NVLS and NVLSTree enable NVLink SHARP offload.

To specify algorithms to exclude (instead of include), start the list with `^`.

(since 2.24)

The accepted values are expanded to allow more flexibility, and parsing will issue a warning and fail if an unexpected token is found. Also, if `ring` is not specified as a valid algorithm then it will not implicitly fall back to `ring` if there is no other valid algorithm for the function. Instead, it will fail.

The format is now a semicolon-separated list of pairs of function name and list of algorithms, where the function name is optional for the first entry. If not present, then it applies to all functions not later listed. A colon separates the function (when present) and the comma-separated list of algorithms. Also, if the first character of the comma-separated list of algorithms is a caret (`^`), then all the selections are inverted.

For example, `NCCL_ALGO="ring,collnetdirect;allreduce:tree,collnetdirect;broadcast:ring"` Will enable ring and collnetdirect for all functions, then enable tree and collnetdirect for allreduce and ring for broadcast.

And, `NCCL_ALGO=allreduce:^tree` will allow the default (all algorithms available) for all the functions except allreduce, which will have all algorithms available except tree.

The default is unset, which causes NCCL to automatically choose the available algorithms based on the node topology and architecture.


### NCCL\_PROTO

(since 2.5)

The `NCCL_PROTO` variable defines which protocol(s) NCCL will be allowed to use.

Users are discouraged from setting this variable, with the exception of disabling a specific protocol in case a bug in NCCL is suspected. In particular, enabling LL128 on platforms that don't support it can lead to data corruption.


#### Values accepted

(since 2.5) Comma-separated list of protocols (not case sensitive) among: `LL`, `LL128`, and `Simple`. To specify protocols to exclude (instead of to include), start the list with `^`.

The default behavior enables all supported algorithms: equivalent to `LL,LL128,Simple` on platforms which support LL128, and `LL,Simple` otherwise.

(since 2.24) The accepted values are expanded to allow more flexibility, just as decribed for `NCCL_ALGO` above, allowing the user to specify protocols for each function.


### NCCL\_NVB\_DISABLE

(since 2.11)

Disable intra-node communication through NVLink via an intermediate GPU.


#### Value accepted

Default is 0, set to 1 to disable this mechanism.


### NCCL\_PXN\_DISABLE

(since 2.12)

Disable inter-node communication using a non-local NIC, using NVLink and an intermediate GPU.


#### Value accepted

Default is 0, set to 1 to disable this mechanism.


### NCCL\_P2P\_PXN\_LEVEL

(since 2.12)

Control in which cases PXN is used for send/receive operations.


#### Value accepted

A value of 0 will disable the use of PXN for send/receive. A value of 1 will enable the use of PXN when the NIC preferred by the destination is not accessible through PCI switches. A value of 2 (default) will cause PXN to always be used, even if the NIC is connected through PCI switches, storing data from all GPUs within the node on an intermediate GPU to maximize aggregation.


### NCCL\_PXN\_C2C

(since 2.27)

Allow NCCL to use the PXN mechanism if the peer GPU is connected through C2C + PCIe to the targeted NIC.


#### Value accepted

Default is 1 (since NCCL 2.28; it was 0 in NCCL 2.27). Set to 1 to enable and to 0 to disable.


### NCCL\_RUNTIME\_CONNECT

(since 2.22)

Dynamically connect peers during runtime (e.g., calling ncclAllreduce()) instead of init stage.


#### Value accepted

Default is 1, set to 0 to connect peers at init stage.


### NCCL\_GRAPH\_REGISTER

(since 2.11)

Enable user buffer registration when NCCL calls are captured by CUDA Graphs.

Effective only when: (i) the CollNet algorithm is being used; (ii) all GPUs within a node have P2P access to each other; (iii) there is at most one GPU per process.

User buffer registration may reduce the number of data copies between user buffers and the internal buffers of NCCL. The user buffers will be automatically de-registered when the CUDA Graphs are destroyed.


#### Value accepted

0 or 1. Default value is 1 (enabled).


### NCCL\_LOCAL\_REGISTER

(since 2.19)

Enable user local buffer registration when users explicitly call *ncclCommRegister*.


#### Value accepted

0 or 1. Default value is 1 (enabled).


### NCCL\_LEGACY\_CUDA\_REGISTER

(since 2.24)

Cuda buffers allocated through *cudaMalloc* (and related memory allocators) are legacy buffers. Registering legacy buffer can cause implicit synchronization, which is unsafe and can possibly cause a hang for NCCL. NCCL disables legacy buffer registration by default, and users should move to cuMem-based memory allocators for buffer registration.


#### Value accepted

0 or 1. Default value is 0 (disabled).


### NCCL\_WIN\_ENABLE

(since 2.27)

Enable window memory registration.


#### Value accepted

0 or 1. Default value is 1 (enabled).


### NCCL\_SET\_STACK\_SIZE

(since 2.9)

Set CUDA kernel stack size to the maximum stack size amongst all NCCL kernels.

It may avoid a CUDA memory reconfiguration on load. Set to 1 if you experience hang due to CUDA memory reconfiguration.


#### Value accepted

0 or 1. Default value is 0 (disabled).


### NCCL\_GRAPH\_MIXING\_SUPPORT

(since 2.13)

Enable/disable support for multiple outstanding NCCL calls from parallel CUDA graphs or a CUDA graph and non-captured NCCL calls. NCCL calls are considered outstanding starting from their host-side launch (e.g., a call to ncclAllreduce() for non-captured calls or cudaGraphLaunch() for captured calls) and ending when the device kernel execution completes. With graph mixing support disabled, the following use cases are NOT supported:

1.  Using a NCCL communicator (or split-shared communicators) from parallel graph launches, where parallel means on different streams without dependencies that would serialize their execution.

2.  Launching a non-captured NCCL collective during an outstanding graph launch that uses the same communicator (or split-shared communicators), regardless of stream ordering.

The ability to disable support is motivated by observed hangs in the CUDA launches when support is enabled and multiple ranks have work launched via cudaGraphLaunch from the same thread.


#### Value accepted

0 or 1. Default is 1 (enabled).


### NCCL\_DMABUF\_ENABLE

(since 2.13)

Enable GPU Direct RDMA buffer registration using the Linux dma-buf subsystem.

The Linux dma-buf subsystem allows GPU Direct RDMA capable NICs to read and write CUDA buffers directly without CPU involvement.


#### Value accepted

0 or 1. Default value is 1 (enabled), but the feature is automatically disabled if the Linux kernel or the CUDA/NIC driver do not support it.


### NCCL\_P2P\_NET\_CHUNKSIZE

(since 2.14)

The `NCCL_P2P_NET_CHUNKSIZE` controls the size of messages sent through the network for ncclSend/ncclRecv operations.


#### Values accepted

The default is 131072 (128 K).

Values are integers, in bytes. The recommendation is to use powers of 2, hence 262144 would be the next value.


### NCCL\_P2P\_LL\_THRESHOLD

(since 2.14)

The `NCCL_P2P_LL_THRESHOLD` is the maximum message size that NCCL will use the LL protocol for P2P operations.


#### Values accepted

Decimal number. Default is 16384.


### NCCL\_ALLOC\_P2P\_NET\_LL\_BUFFERS

(since 2.14)

`NCCL_ALLOC_P2P_NET_LL_BUFFERS` instructs communicators to allocate dedicated LL buffers for all P2P network connections. This enables all ranks to use the LL protocol for latency-bound send and receive operations below `NCCL_P2P_LL_THRESHOLD` sizes. Intranode P2P transfers always have dedicated LL buffers allocated. If running all-to-all workloads with high numbers of ranks, this will result in a high scaling memory overhead.


#### Values accepted

0 or 1. Default value is 0 (disabled).


### NCCL\_COMM\_BLOCKING

(since 2.14)

The `NCCL_COMM_BLOCKING` variable controls whether NCCL calls are allowed to block or not. This includes all calls to NCCL, including init/finalize functions, as well as communication functions which may also block due to the lazy initialization of connections for send/receive calls. Setting this environment variable will override the `blocking` configuration in all communicators (see ncclConfig\_t); if not set (undefined), communicator behavior will be determined by the configuration; if not passing configuration, communicators are blocking.


#### Values accepted

0 or 1. 1 indicates blocking communicators, and 0 indicates nonblocking communicators. The default value is undefined.


### NCCL\_CGA\_CLUSTER\_SIZE

(since 2.16)

Set CUDA Cooperative Group Array (CGA) cluster size. On sm90 and later we have an extra level of hierarchy where we can group together several blocks within the Grid, called Thread Block Clusters. Setting this to non-zero will cause NCCL to launch the communication kernels with the Cluster Dimension attribute set accordingly. Setting this environment variable will override the `cgaClusterSize` configuration in all communicators (see ncclConfig\_t); if not set (undefined), CGA cluster size will be determined by the configuration; if not passing configuration, NCCL will automatically choose the best value.


#### Values accepted

0 to 8. Default value is undefined.


### NCCL\_MAX\_CTAS

(since 2.17)

Set the maximal number of CTAs the NCCL should use. Setting this environment variable will override the `maxCTAs` configuration in all communicators (see ncclConfig\_t); if not set (undefined), maximal CTAs will be determined by the configuration; if not passing configuration, NCCL will automatically choose the best value.


#### Values accepted

Set to a positive integer value up to 64 (32 prior to 2.25). Default value is undefined.


### NCCL\_MIN\_CTAS

(since 2.17)

Set the minimal number of CTAs the NCCL should use. Setting this environment variable will override the `minCTAs` configuration in all communicators (see ncclConfig\_t); if not set (undefined), minimal CTAs will be determined by the configuration; if not passing configuration, NCCL will automatically choose the best value.


#### Values accepted

Set to a positive integer value up to 64 (32 prior to 2.25). Default value is undefined.


### NCCL\_NVLS\_ENABLE

(since 2.17)

Enable the use of NVLink SHARP (NVLS). NVLink SHARP is available in third-generation NVSwitch systems (NVLink4) with Hopper and later GPU architectures, allowing collectives such as `ncclAllReduce` to be offloaded to the NVSwitch domain. The default value is 2.


#### Values accepted

0: Disable the use of NVLink SHARP. No NVLink SHARP resources will be allocated.

1: Enable NVLink SHARP. NCCL initialization will not fail if NVLink SHARP is not supported on a given system, but it will fail if the NVLink SHARP resources cannot be allocated.

2: (Default) Same as 1: not failing if there is no support, but failing if resources could not be allocated. In versions 2.27 and 2.28, NCCL tried to silently fall back on other transports if NVLS resource allocation failed. This, however, could result in a hang if the allocation failed on just some of the ranks, so in 2.29 the behavior reverted to pre-2.27 (i.e., same as 1). Future versions may re-enable the silent fallback if it can be made to work reliably.


### NCCL\_IB\_MERGE\_NICS

(since 2.20)

Enable NCCL to combine dual-port IB NICs into a single logical network device. This allows NCCL to more easily aggregate dual-port NIC bandwidth.


#### Values accepted

Default is 1 (enabled), define and set to 0 to disable NIC merging


### NCCL\_MNNVL\_ENABLE

(since 2.21)

Enable NCCL to use Multi-Node NVLink (MNNVL) when available. If the system or driver are not Multi-Node NVLink capable then MNNVL will automatically be disabled. This feature also requires NCCL CUMEM support (`NCCL_CUMEM_ENABLE`) to be enabled. MNNVL requires a fully configured and operational IMEX domain for all the nodes that form the NVLink domain. See the CUDA documentation for more details on IMEX domains.


#### Values accepted

0: Disable MNNVL support.

1: Enable MNNVL support. NCCL initialization will fail if MNNVL is not supported or cannot be enabled.

2: Automatic detection of MNNVL support. Will *not* fail if MNNVL is unsupported or if MNNVL resources cannot be allocated.


### NCCL\_MNNVL\_UUID

(since 2.25) Can be used to set the Multi-Node NVLink (MNNVL) UUID to a user defined value. The supplied value will be assigned to both the upper and lower 64-bit words of the 128-bit UUID. Normally the MNNVL UUID is assigned by the Fabric Manager, and it should not need to be overridden.


#### Values accepted

64-bit integer value.


### NCCL\_MNNVL\_CLIQUE\_ID

(since 2.25) Can be used to set the Multi-Node NVLink (MNNVL) Clique Id to a user defined value. Normally the Clique Id is assigned by the Fabric Manager, but this environment variable can be used to "soft" partition MNNVL jobs. i.e. NCCL will only treat ranks with the same \<UUID,CLIQUE\_ID\> as being part of the same NVLink domain.


#### Values accepted

32-bit integer value.


### NCCL\_RAS\_ENABLE

(since 2.24)

Enable NCCL's reliability, availability, and serviceability (RAS) subsystem, which can be used to query the health of NCCL jobs during execution (see RAS).


#### Values accepted

Default is 1 (enabled); define and set to 0 to disable RAS.


### NCCL\_RAS\_ADDR

(since 2.24)

Specify the IP address and port number of a socket that the RAS subsystem will listen on for client connections. RAS can share this socket between multiple processes but that would not be desirable if multiple independent NCCL jobs share a single node (and if those jobs belong to different users, the OS will not allow the socket to be shared). In such cases, each job should be started with a different value (e.g., `localhost:12345`, `localhost:12346`, etc.). Since `localhost` is normally used, only those with access to the nodes where the job is running can connect to the socket. If desired, the address of an externally accessible network interface can be specified instead, which will make RAS accessible from other nodes (such as a cluster's head node), but that has security implications that should be considered.


#### Values accepted

Default is `localhost:28028`. Either a host name or an IP address can be used for the first part; an IPv6 address needs to be enclosed in square brackets (e.g., `[::1]`).


### NCCL\_RAS\_TIMEOUT\_FACTOR

(since 2.24)

Specify the multiplier factor to apply to all the timeouts of the RAS subsystem. RAS relies on multiple timeouts, ranging from 5 to 60 seconds, to determine the state of the application and to maintain its internal communication, with complex interdependecies between different timeouts. This variable can be used to scale up all these timeouts in a safe, consistent manner, should any of the defaults turn out to be too small; e.g., if the NCCL application is subject to high-overhead debugging/tracing/etc., which makes its execution less predictable. If one wants to use the `ncclras` client in such circumstances, its timeout may need to be increased as well (or disabled).


#### Values accepted

Default is 1; define and set to larger values to increase the timeouts.


### NCCL\_LAUNCH\_ORDER\_IMPLICIT

(since 2.26)

Implicitly order NCCL operations from different communicators on the same device using the host program order. This ensures the operations will not deadlock. When the CUDA runtime and driver are 12.3+, overlapped execution is permitted. On older CUDA versions the operations will be serialized.


#### Values accepted

Default is 0 (disabled); set to 1 to enable.


### NCCL\_LAUNCH\_RACE\_FATAL

(since 2.26)

Attempt to catch host threads racing to launch to the same device and if so return a fatal error. Such a race would violate the determinacy of the program order relied upon by NCCL\_LAUNCH\_ORDER\_IMPLICIT.


#### Values accepted

Default is 1 (enabled); set to 0 to disable.


### NCCL\_IPC\_USE\_ABSTRACT\_SOCKET

(since 2.29)

Use the Linux Abstract Socket mechanism when creating Unix Domain Sockets (UDS) for intra-node CUDA IPC handle exchange. This is enabled by default, but having it enabled can prevent intra-node GPU communication when using multiple containers in certain situations (e.g. different network namespaces).


#### Values accepted

Default is 1 (enabled); set to 0 to disable.