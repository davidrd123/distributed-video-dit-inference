<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>State of torch.compile for training (August 2025) : ezyang's blog</title>
<link rel=icon href=/favicon.ico><link rel=stylesheet href=/css/style.css type=text/css media=screen><link href="https://fonts.googleapis.com/css?family=Gentium+Book+Basic" rel=stylesheet type=text/css><link href="https://fonts.googleapis.com/css?family=Ubuntu+Mono" rel=stylesheet type=text/css><link rel=alternate type=application/rss+xml title="ezyang's blog RSS Feed" href=https://blog.ezyang.com/feed.xml><style>pre{margin-left:0;padding:1em;background:var(--code-bg);overflow-x:auto;font-family:ubuntu mono,monospace;font-size:14px;line-height:1.4;color:var(--code-text)}code{font-family:ubuntu mono,monospace}p code,li code{background:var(--code-bg);padding:.2em .4em;border-radius:3px;font-size:.9em}@media screen and (min-width:640px){pre{margin-left:3em}}</style><meta name=generator content="Hugo 0.145.0"></head><body class=single><header><h1 class="vcard author"><a href=/ title=Home>ezyang's blog</a></h1><p>the arc of software bends towards understanding<ul class=pages><li><a href=/archives/>archives</a></li><li><a href=/feed.xml>subscribe</a></li></ul></p></header><div class="content wrap"><section class=posts><article class=post><h2>State of torch.compile for training (August 2025)</h2><abbr class="small postDate" title=2025-08-13>August 13, 2025</abbr><p>The purpose of this post is to sum up, in one place, the state of torch.compile for training as of August 2025. Nothing in here isn&rsquo;t something you might not already know about from elsewhere on the Internet, but we rarely put everything together in one place. The target audience for this document are teams who are evaluating the use of torch.compile for large scale training runs.</p><p>First, the basics. torch.compile (also known as PT2) is a compiler for PyTorch eager programs for both inference and training workloads. Speedups from 1.5-2x compared to eager code are typical, and torch.compile also makes it possible to do global optimizations for memory (e.g., automatic activation checkpointing) and distributed communications (e.g., async tensor parallelism).</p><h3 id=what-is-torchcompiles-functionality id=what-is-torchcompiles-functionality><a class=heading-link href=/2025/08/state-of-torch-compile-august-2025/#what-is-torchcompiles-functionality>What is torch.compile&rsquo;s functionality?<span class=heading-anchor aria-label=Anchor>¶</span></a></h3><p>The headline functionality of torch.compile is a decorator you can attach to a function to compile it:</p><pre><code>@torch.compile()
def f(x, y):
    ...
</code></pre><p>Here are some non-functional properties of compile which are important to know:</p><ul><li><strong>Just-in-time compilation</strong>. We don&rsquo;t actually compile the function until it is called for the first time, and execution blocks until compilation completes. There is both local and remote caching to skip compilation cost when you rerun the model. (Ahead-of-time compilation is possible for inference with AOTInductor, and is being worked on for training.)</li><li><strong>Compositional with Eager</strong>. PyTorch&rsquo;s original success comes from the extreme hackability of eager mode, and torch.compile seeks to preserve this. The function can be as big or as small part of your training loop as you like; compiled functions compose with autograd, DDP, FSDP and other PyTorch subsystems. (This composition is sometimes imperfect, e.g., in the case of double backwards (not supported), tensor subclasses (requires specific support from the subclass), autograd (differentiating with respect to intermediates returned from a compiled region does not work).) If compilation doesn&rsquo;t work on a region, you can disable it entirely with torch.compiler.disable() and fall back to eager.</li><li><strong>Gradient updates are delayed to the end of compiled regions.</strong> This arises because PyTorch eager autograd does not support streaming gradients incrementally from a large backward node. (This can be solved by using <a href=https://docs.pytorch.org/tutorials/intermediate/compiled_autograd_tutorial.html>compiled autograd</a>, but this requires that the <em>entirety</em> of your backwards be compileable.)</li><li><strong>Graphs may be recompiled</strong>. We aggressively specialize on all non-Tensor arguments/globals used in the function to ensure we always generate straight-line computation graphs with no control flow. If those arguments/globals change we will recompile the graph. (Recompilations can be banned with <code>torch._dynamo.config.error_on_recompile = True</code>.)</li><li><strong>Static by default, recompile to dynamic shapes</strong>. We aggressively specialize all sizes to static. However, if we discover that a size varies over time, on the first recompile we will attempt to generate a single compiled region that handles dynamic shapes. We are not guaranteed to be able to compile a model with dynamic shapes. (You can use <a href=https://docs.pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html>mark_dynamic</a> to force an input shape to be dynamic, and you can use mark_unbacked to error if we specialize.)</li><li><strong>Graph breaks transparently bypass non-capturable code</strong>. By default, if the compiler encounters a line of code that it is not able to handle, it will trigger a graph break, disabling compilation for that line of code, but still attempting to compile regions before and after it. (This behavior can be banned with <a href=https://docs.pytorch.org/docs/main/compile/programming_model.fullgraph_true.html>fullgraph=True</a>.)</li><li><strong>Function calls are inlined and loops are unrolled by default</strong>. If you have many copies of a Transformer block in your model, your compile time will scale with the number of Transformer blocks. (You can reduce compile time by doing &ldquo;`regional compilation &lt;https://docs.pytorch.org/tutorials/recipes/regional_compilation.html>`_&rdquo;, where you only compile the Transformer block instead of compiling the entire model.)</li><li><strong>NOT bitwise equivalent with eager PyTorch.</strong> The biggest divergence with eager PyTorch is that when float16/bfloat16 operations are fused together, we do not insert redundant down/up-conversions. (This can be disabled torch._inductor.config.emulate_precision_casts = True; you can also rewrite eager code to perform operations in higher precision with the understanding torch.compile will optimize it. XLA has a similar config xla_allow_excess_precision which JAX enables by default.) However, we may also make decisions to swap out, e.g., matmul implementations, and there may also be slight divergence that arise from differences in reduction ordering that are unavoidable when compilation occurs. We support ablating the graph capture frontend separately from the compiler backend to help diagnose these kinds of problems.</li><li><strong>Distributed collectives and DTensor can be compiled, but are unoptimized by default</strong>. We are able to capture c10d collectives and also programs that handle DTensors, but we don&rsquo;t apply optimizations to collectives by default. (There are experimental optimizations that can be enabled, but this is active work in progress.) We generally do not expect to be able to trace through highly optimized distributed framework code.</li></ul><h3 id=state-of-advanced-parallelism id=state-of-advanced-parallelism><a class=heading-link href=/2025/08/state-of-torch-compile-august-2025/#state-of-advanced-parallelism>State of advanced parallelism<span class=heading-anchor aria-label=Anchor>¶</span></a></h3><p>For large scale training runs, torch.compile faces stiff competition from (1) PyTorch native distributed frameworks which embrace eager mode and implement all optimizations by hand (e.g., megatron), (2) custom &ldquo;compiler&rdquo; stacks which reuse our tracing mechanisms (e.g., symbolic_trace and make_fx) but implement their desired passes by hand, (3) JAX, which has always been XLA first and is years ahead in compile-driven parallelism techniques.</p><p>Here is where we currently are for advanced parallelism (with an emphasis on comparing with JAX):</p><ul><li><p><strong>DTensor, a &ldquo;global tensor&rdquo; abstraction for representing sharded tensors.</strong> DTensor is a tensor subclass which allows us to represent tensors which are sharded over an SPMD device mesh. The shape of a DTensor reflects the global shape of the original full tensor, but it only stores locally a shard of the data according to the placement. Here are some important details:</p><ul><li><strong>Shard placements.</strong> Unlike JAX placements, DTensor placements are &ldquo;device mesh&rdquo; oriented; that is to say, you conventionally specify a device mesh dim size list of placements, and Shard(i) indicates that the ith dimension of a tensor is sharded. This is opposite of JAX, which is &ldquo;tensor&rdquo; oriented. For example, given a 2-D mesh [&ldquo;dp&rdquo;, &ldquo;tp&rdquo;], a tensor with [Replicate, Shard(0)] in DTensor placement (or {&ldquo;dp&rdquo;: Replicate, &ldquo;tp&rdquo;: Shard(0)} with named device mesh axes), would correspond to a JAX placement of P(&ldquo;tp&rdquo;, None). The reason for this is that DTensor supports a Partial placement, which indicates that an axis on the device mesh has a pending reduction. Partial shows up ubiquitously from matrix multiplies, and it isn&rsquo;t associated with any particular tensor axis, making it more convenient to represent in a device-mesh oriented formulation. The tradeoff is that device-mesh oriented placements don&rsquo;t naively support specifying sharding ordering, e.g., suppose I want to shard a 1-D tensor on tp and then dp, in JAX I&rsquo;d represent this as P((&ldquo;tp&rdquo;, &ldquo;dp&rdquo;),) but this order cannot be disambiguated from [Shard(0), Shard(0)] and in fact DTensor always forces left-to-right sharding. There is currently a proposal to extend our sharding specification to support ordering to bring us to parity with JAX expressiveness, but it is not yet implemented.</li><li><strong>Autograd.</strong> DTensor is directly differentiable; we run autograd on programs that have DTensors (as opposed to desugaring a DTensor program to one with regular Tensors and differentiating it). This ensures that the sharding strategy of a primal and its corresponding tangent can diverge. This is parity with JAX.</li><li><strong>Python subclass of Tensor.</strong> Unlike JAX, DTensor is a separate subclass from Tensor. However, Tensor and DTensor interoperate fine; a Tensor can simply be thought of as a DTensor that is replicated on all dimensions. DTensor is implemented in Python, which makes it easy to modify and debug but imposes quite a bit of overhead (for example, FSDP2 does not directly accumulate gradients into DTensor, because with thousands of parameters, performing detach and add operations on DTensor is a bottleneck). Still, despite this overhead, DTensor was designed for good eager performance, and extensively caches the results of sharding propagation so that in the fastpath, it only needs to lookup what redistribute it should perform and then directly dispatches to the local eager operation. However, this caching strategy means that overhead can be quite high for workloads with dynamic shapes, as the cache requires exact matches of all input shapes.</li><li><strong>Compilation.</strong> DTensor is compilable by torch.compile, and doing so will desugar it into its underlying collectives and eliminate any eager mode DTensor overhead (even if you do not perform any other optimizations.) However, DTensor with dynamic shapes in compile is not well supported, see <a href=http://github.com/pytorch/pytorch/issues/159635>http://github.com/pytorch/pytorch/issues/159635</a> (we don&rsquo;t think this is currently critical path for any critical use cases, so a relatively junior engineer has been chipping away at it.)</li><li><strong>Greedy propagation.</strong> Because DTensor must work in eager mode, it only implements greedy shard propagation, where for every eager operation we greedily pick whatever output shard minimizes the collective costs of an operation. It is work in progress to support backward propagation of sharding with the assistance of a compiler-like framework.</li><li><strong>Operator coverage.</strong> DTensor requires sharding propagation rules to work for operations. If a sharding propagation rule is not implemented, DTensor will fail rather than trigger an inefficient allgather to run the operator under replication. We don&rsquo;t currently have full coverage of all operators, but important operators for transformer models like llama3 are all covered (<a href=https://github.com/pytorch/pytorch/tree/main/torch/distributed/tensor/_ops>sharding rules are defined here</a>). You can write custom shardings for user defined operators.</li><li><strong>Jagged sharding.</strong> We do not support a &ldquo;jagged sharding&rdquo; concept which would be necessary for expert parallelism with imbalanced routing. However, we believe that our existing sharding rules could largely be reused to support such an idea. As dynamism would only be exposed in the local tensor for the jagged shard, jagged shards don&rsquo;t suffer from the dynamic shapes problems mentioned in the compilation section.</li><li><strong>Ecosystem.</strong> We are committed to DTensor as the standard representation for sharded tensors, and DTensor is integrated with checkpointing, FSDP2, SimpleFSDP, AutoParallel, torchtitan, among others.</li></ul></li><li><p><strong>Functional collectives.</strong> If you don&rsquo;t like DTensor, we also support &ldquo;functional collectives&rdquo;, which are non-mutating versions of collective operations that can be used to manually implement SPMD operations in a compiler-friendly way without needing DTensor. (In fact, if you use traditional collective APIs and compile them, we will silently translate them into functional collectives for compiler passes.) When compiled, functional collectives don&rsquo;t necessarily force allocation of the output buffer as they can be re-inplaced. Importantly, functional collectives currently do NOT support autograd, see <a href=https://discuss.pytorch.org/t/supporting-autograd-for-collectives/219430>https://discuss.pytorch.org/t/supporting-autograd-for-collectives/219430</a></p></li><li><p><strong>Graph capture.</strong> There are two particularly popular graph capture mechanisms which people have used to perform distributed optimizations separate from model code. All graph capture mechanisms produce <strong>FX graphs</strong>, which are a simple Python basic block IR representation with no control flow, which is entirely unopinionated about what actual operator set can occur in the graph.</p><ul><li><strong>Symbolic_trace.</strong> This was the original graph capture mechanism and is quite popular, despite its limitations. It is implemented entirely with Python operator overloading and will give you exactly whatever operations are overloadable in the graph. We consider this largely a legacy pipeline as you are unable to trace code involving conditionals on shapes and you end up with a graph that has no useful metadata about the shapes/dtypes of intermediate values. For example, PiPPY, a legacy stack for performing pipeline parallelism, was built on top of symbolic_trace graph capture.</li><li><strong>make_fx/torch.export.</strong> This graph capture mechanism works by actually sending (fake) tensors through your program and recording ATen operators. There are a number of different variants: e.g., whether or not it is a Python tracing approach ala JAX jit, or whether it uses sophisticated bytecode analysis ala Dynamo; similarly, there are various levels of IR you can extract (pre-dispatch, post-dispatch; also, operators can be decomposed or kept as single units). Our compiler parallelism efforts are built on top of this capture mechanism, but there is nothing stopping you <em>per se</em> from writing your own graph pass on top of this IR. In practice, this can be difficult without PyTorch expertise, because (1) integrating a traced graph into PyTorch&rsquo;s autograd system so it can interoperate with other code is quite complicated to do in full generality, (2) the exact operator sets you get at various phases of compilation are undocumented and in practice very tied to the Inductor lowering stack, and it is poorly documented on how to prevent operators from getting decomposed before your pass gets to them.</li></ul></li><li><p><strong>Not SPMD compiler by default.</strong> torch.compile does not assume the program being compiled is SPMD by default, which means it will not do things like drop unused collectives (you can change this behavior with <a href=https://github.com/pytorch/pytorch/issues/146045>a config flag</a>). Additionally, the default mode of use for torch.compile is to compile in parallel on all nodes, which means care has to be taken to ensure that every instance of the compiler compiles identically (only one rank recompiling, or compilers making different decisions, can lead to NCCL timeout). We ultimately think that we should compile a program once and send it to all nodes, but as this is not currently implemented, the general approach people have taken to solve this problem is to either (1) eliminate all sources of divergent behavior from ranks, e.g., don&rsquo;t allow the compiler to look at the actual size for dynamic inputs when making compiler decisions, or (2) introducing extra collectives to the compiler to communicate decisions that must be made consistently across all ranks.</p></li></ul><p>Our vision for the future of advanced parallelism, spearheaded by the in-progress SimpleFSDP and AutoParallel, is that users should write single-node programs that express mathematically what they want to do. These are then transformed into efficient distributed programs in two steps: (1) first, collectives are inserted into the graph in a naive way (i.e., simply to express what the sharding of all intermediates should be), and (2) the collectives are optimized to handle scheduling concerns such as pre-fetching and bucketing. AutoParallel sets a GSPMD style goal of automatically determining a good enough sharding for a program&ndash;it should be able to rediscover data parallel, tensor parallel, even expert parallel(!)&ndash;but SimpleFSDP sets a smaller goal of just inserting collectives in the pattern that FSDP would mandate, and then writing FSDP-specific optimization passes for recovering FSDP2&rsquo;s performance. It is very common to write domain specific optimizations; for example, async tensor parallelism is also implemented as a pass that <a href=https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487>detects TP patterns and rewriting them to async TP operations</a>. Unlike JAX, which started with a very generic solver and has needed to add more manual escape hatches over time, PyTorch has started with writing all of the distributed patterns exactly by hand, and we are only recently adding more automatic mechanisms as an alternative to doing everything by hand.</p><h3 id=state-of-optimization id=state-of-optimization><a class=heading-link href=/2025/08/state-of-torch-compile-august-2025/#state-of-optimization>State of optimization<span class=heading-anchor aria-label=Anchor>¶</span></a></h3><p>torch.compile performs many optimizations, but here are some particularly important ones to know about:</p><ul><li><strong>Inductor.</strong> Inductor is our backend for torch.compile that generates Triton kernels for PyTorch programs. It has very good coverage of PyTorch&rsquo;s operator set and can do fusions of pointwise and reductions, including in the patterns that typically occur for backwards. It also is able to fuse pointwise operations into matmuls and autotune different matmul backends (including cuBlas, cutlass and Triton) to select the best one for any given size. When people talk about torch.compile speeding up their programs, they are conventionally talking about Inductor; however, you don&rsquo;t <em>have</em> to use torch.compile with Inductor; for example, you could run with AOTAutograd only and skip Inductor compilation.</li><li><strong>CUDA graphs.</strong> Inductor builds in support for CUDA graphing models. Unlike manual CUDA graphs application, we can give better soundness guarantees than manual CUDA graphs application (e.g., forgetting to copy in all input buffers, CPU compute inside the CUDA graph region). torch.compile CUDA graphs is typically used with Inductor but we also offer an eager-only cudagraphs integration (that is less well exercised).</li><li><strong>Automatic activation checkpointing</strong>. With torch.compile, we can globally optimize the memory-compute tradeoff, much better than the activation checkpointing APIs that eager PyTorch supports (and require the user to manually feed in what they want checkpointed or not). However, some folks have reported that it can be quite miserable tuning the hyperparameter for AC; we have also found bugs in it.</li><li><strong>FP8 optimizations</strong>. One big success story for traditional compilation was adding support for a custom FP8 flavor. With torch.compile, they didn&rsquo;t have to write manual kernels for their variant. This has since been upstreamed to <a href=https://github.com/pytorch/ao>torchao</a>.</li><li><strong>Flex attention.</strong> Flex attention usage continues to grow, with 632 downstream repo users in OSS (vs 125 in Jan &lsquo;25). It has been used to enable chunked attention, document masking and context parallelism in llama family models. It is a really good research tool, although sometimes people complain about slight numerical differences.</li><li><strong>Helion.</strong> <a href=https://github.com/pytorch/helion>Helion</a> is an actively developed project aiming to go beta in October this year which offers a higher level interface for programming Triton kernels that looks just like writing PyTorch eager code. It relies heavily on autotuning to explore the space of possible structural choices of kernels to find the best one. It is not production ready but it is worth knowing that it is coming soon.</li></ul><h3 id=state-of-compile-time id=state-of-compile-time><a class=heading-link href=/2025/08/state-of-torch-compile-august-2025/#state-of-compile-time>State of compile time<span class=heading-anchor aria-label=Anchor>¶</span></a></h3><p>torch.compile is a just-in-time compiler and as such, in its default configuration, compilation will occur on your GPU cluster (preventing you from using the GPUs to do other useful work!) In general, most pathological compile times arise from repeated recompilation (often due to dynamic shapes, but sometimes not). In Transformer models, compile time can also be improved by only compiling the Transformer block (which can then be compiled only once, instead of having to be compiled N times for each Transformer block in the model).</p><p>We don&rsquo;t think caching is an ideal long-term solution for large scale training runs, and we have been working on precompile to solve the gap here. Precompile simply means having compilation be an ahead-of-time process which produces a binary which you can directly run from your training script to get the compiled model. The compilation products are built on top of our ABI stable interface (developed for AOTInductor) which allows the same binaries to target multiple PyTorch versions, even though PyTorch the library does not offer ABI compatibility from version to version.</p><h3 id=how-do-i-get-started id=how-do-i-get-started><a class=heading-link href=/2025/08/state-of-torch-compile-august-2025/#how-do-i-get-started>How do I get started?<span class=heading-anchor aria-label=Anchor>¶</span></a></h3><p>The most typical pattern we see for people who want to make use of torch.compile for large-scale training is to fork <a href=https://github.com/pytorch/torchtitan>torchtitan</a> and use this codebase as the basis for your training stack. torchtitan showcases PyTorch native functionality, including torch.compile&ndash;in effect, it shows you how to use features in PyTorch together in a way that lets you do large-scale training. From there, swap out the components you are opinionated about and keep the things you don&rsquo;t care about.</p><footer><ul class=small><li><a href=/categories/pytorch/>PyTorch</a></li></ul></footer></article><section id=comments><h3>3 Comments</h3><ol class=commentlist><li class=comment id=comment-36143><div class=comment-body><div class=comment-author><strong>xgbj</strong></div><div class=comment-meta><time datetime="2025-09-03 04:52:57">2025-09-03 04:52:57</time></div><div class=comment-content>Very useful blog, thank you！</div></div></li><li class=comment id=comment-36145><div class=comment-body><div class=comment-author><strong>Aditya V</strong></div><div class=comment-meta><time datetime="2025-09-03 14:32:04">2025-09-03 14:32:04</time></div><div class=comment-content><p>Thanks for writing this, very helpful!</p><p>> We don&rsquo;t think caching is an ideal long-term solution for large scale training runs, and we have been working on precompile to solve the gap here.</p><p>Could you share some more on this? What are the perceived downsides of caching for large-scale training runs?</p></div></div></li><li class=comment id=comment-36217><div class=comment-body><div class=comment-author><strong>Aastha Varma</strong></div><div class=comment-meta><time datetime="2025-09-22 12:12:47">2025-09-22 12:12:47</time></div><div class=comment-content>Thanks! Really helpful tech writeup!</div></div></li></ol></section><div class="pagination p"><span class=previous><a href=/2025/06/vibe-coding-case-study-scubaduck/>&#171; Previous Post</a></span>
<span class=next><a href=/2025/08/you-could-have-invented-cute-hierarchical-layout-but-maybe-not-the-rest-of-it/>Next Post &#187;</a></span></div></section></div><footer class="bottom small wrap"><p class=small>&copy; ezyang's blog. Powered by <a href=https://gohugo.io/>Hugo</a>, theme based off of <a href=http://jxnblk.com/ashley/>Ashley</a>.</p></footer></body></html>