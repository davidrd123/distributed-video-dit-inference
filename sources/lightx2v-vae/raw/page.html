<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/4cf2300e9c8272f7-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/93f479601ee12b01-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/de70bee13400563f.css?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/b54d92a85d180acc.css?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-5400bf868d87f903.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2"/><script src="/_next/static/chunks/87c73c54-dd8d81ac9604067c.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/18-a3e1296cc1a2d8a7.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/main-app-57aa1716f0d0f500.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/7963-2c20578d72f6c7cc.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/9885-b57089f03806c3b8.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/659-ee9e1e775e30dcef.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/app/layout-0537c2076823e553.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/7bf36345-1ac10ec2f0e0c88f.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/c16f53c3-b390b6f98a69dcec.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/2602-735d398a11941702.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/4336-624d5ae6f4cc1cc7.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/8461-369a9f0c48ea2626.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/7198-985a49f2b6072d25.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/6230-12a4ab40267b069f.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/4429-92c77bf2d3e82e1c.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/9976-726fc148e3fe0730.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/1481-e8a771bf9a2ec0bd.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/app/%5Borg%5D/%5Brepo%5D/%5B%5B...wikiRoutes%5D%5D/page-82e5155a284526b9.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/25-9f305b682cea7558.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/7391-25ac5affe6ef6a7d.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/2512-fc2ad2e11d161d20.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/6375-6b1e677037ddc4d1.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/9437-00f165dda9ad3a42.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><script src="/_next/static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-1d48b1e2f124e49f.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" async=""></script><meta name="next-size-adjust" content=""/><title>Input Encoder Architecture | ModelTC/lightx2v | DeepWiki</title><meta name="description" content="This document describes the input encoder architecture in LightX2V, covering how text, image, and audio inputs are processed before entering the diffusion transformer. The encoders transform raw input"/><meta name="keywords" content="ModelTC/lightx2v,ModelTC,lightx2v,documentation,wiki,codebase,AI documentation,Devin,Input Encoder Architecture"/><link rel="canonical" href="https://deepwiki.com/ModelTC/lightx2v/4.5-input-encoder-architecture"/><meta property="og:title" content="Input Encoder Architecture | ModelTC/lightx2v | DeepWiki"/><meta property="og:description" content="This document describes the input encoder architecture in LightX2V, covering how text, image, and audio inputs are processed before entering the diffusion transformer. The encoders transform raw input"/><meta property="og:url" content="https://deepwiki.com/ModelTC/lightx2v/4.5-input-encoder-architecture"/><meta property="og:site_name" content="DeepWiki"/><meta property="og:image" content="https://deepwiki.com/ModelTC/lightx2v/og-image.png?page=4.5"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@cognition"/><meta name="twitter:creator" content="@cognition"/><meta name="twitter:title" content="Input Encoder Architecture | ModelTC/lightx2v | DeepWiki"/><meta name="twitter:description" content="This document describes the input encoder architecture in LightX2V, covering how text, image, and audio inputs are processed before entering the diffusion transformer. The encoders transform raw input"/><meta name="twitter:image" content="https://deepwiki.com/ModelTC/lightx2v/og-image.png?page=4.5"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="48x48"/><link rel="icon" href="/icon.png?1ee4c6a68a73a205" type="image/png" sizes="48x48"/><link rel="apple-touch-icon" href="/apple-icon.png?a4f658907db0ab87" type="image/png" sizes="180x180"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" noModule=""></script></head><body class="__variable_188709 font-geist-sans relative min-h-screen __variable_9a8899 bg-background antialiased"><div hidden=""><!--$--><!--/$--></div><section aria-label="Notifications alt+T" tabindex="-1" aria-live="polite" aria-relevant="additions text" aria-atomic="false"></section><script>((a,b,c,d,e,f,g,h)=>{let i=document.documentElement,j=["light","dark"];function k(b){var c;(Array.isArray(a)?a:[a]).forEach(a=>{let c="class"===a,d=c&&f?e.map(a=>f[a]||a):e;c?(i.classList.remove(...d),i.classList.add(f&&f[b]?f[b]:b)):i.setAttribute(a,b)}),c=b,h&&j.includes(c)&&(i.style.colorScheme=c)}if(d)k(d);else try{let a=localStorage.getItem(b)||c,d=g&&"system"===a?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":a;k(d)}catch(a){}})("class","theme","light",null,["light","dark"],null,true,true)</script><!--$?--><template id="B:0"></template><div class="flex min-h-screen w-full flex-col text-white"><div class="container-wrapper flex flex-1 items-center justify-center px-4"><div class="inline-block bg-clip-text text-[#b5b5b5a4] animate-shine text-center text-lg" style="background-image:linear-gradient(120deg, rgba(255, 255, 255, 0) 40%, rgba(255, 255, 255, 0.8) 50%, rgba(255, 255, 255, 0) 60%);background-size:200% 100%;-webkit-background-clip:text;animation-duration:1s">Loading...</div></div></div><!--/$--><script>requestAnimationFrame(function(){$RT=performance.now()});</script><script src="/_next/static/chunks/webpack-5400bf868d87f903.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2" id="_R_" async=""></script><div hidden id="S:0"><div class="flex min-h-screen w-full flex-col text-white" id="codebase-wiki-repo-page"><div class="bg-background border-b-border sticky top-0 z-30 border-b border-dashed"><div class="font-geist-mono relative flex h-8 items-center justify-center text-xs font-medium sm:hidden"><div class="powered-by-devin-gradient absolute inset-0 z-[-1] h-8 w-full"></div><button class="flex items-center gap-2"><svg class="size-3 [&amp;_path]:stroke-0 [&amp;_path]:animate-[custom-pulse_1.8s_infinite_var(--delay,0s)]" xmlns="http://www.w3.org/2000/svg" viewBox="110 110 460 500"><path style="fill:#21c19a" class="[--delay:0.6s]" d="M418.73,332.37c9.84-5.68,22.07-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.58,1.06.19.06.37.11.55.16.87.21,1.76.34,2.65.35.04,0,.08.02.13.02.1,0,.19-.03.29-.04.83-.02,1.64-.13,2.45-.32.14-.03.28-.05.42-.09.87-.24,1.7-.59,2.5-1.03.08-.04.17-.06.25-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.06,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.57-1.06-.19-.06-.37-.11-.56-.16-.88-.21-1.76-.34-2.65-.34-.13,0-.26.02-.4.02-.84.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.24.1.8.44,1.64.79,2.5,1.03.14.04.28.06.42.09.81.19,1.62.3,2.45.32.1,0,.19.04.29.04.04,0,.08-.02.13-.02.89,0,1.77-.13,2.65-.35.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.58-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-.08-.04-.16-.06-.24-.1-.8-.44-1.64-.8-2.51-1.04-.13-.04-.26-.05-.39-.09-.82-.2-1.65-.31-2.49-.33-.13,0-.25-.02-.38-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.75.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.07,5.68-31.9,0-9.84-5.68-15.95-16.27-15.95-27.63s6.11-21.95,15.95-27.63Z"></path><path style="fill:#3969ca" d="M141.09,317.65l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c.08-.04.13-.11.2-.16.78-.48,1.51-1.02,2.15-1.66.1-.1.18-.21.28-.31.57-.6,1.08-1.26,1.51-1.97.07-.12.15-.22.22-.34.44-.77.77-1.6,1.03-2.47.05-.19.1-.37.14-.56.22-.89.37-1.81.37-2.76v-29.43c0-11.36,6.11-21.95,15.96-27.63s22.06-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.57,1.06.19.06.37.11.56.16.87.21,1.76.34,2.64.35.04,0,.09.02.13.02.1,0,.19-.04.29-.04.83-.02,1.65-.13,2.45-.32.14-.03.28-.05.41-.09.87-.24,1.71-.6,2.51-1.04.08-.04.16-.06.24-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.07,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.58-1.06-.19-.06-.37-.11-.55-.16-.88-.21-1.76-.34-2.65-.35-.13,0-.26.02-.4.02-.83.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22Z"></path><path style="fill:#0294de" class="[--delay:1.2s]" d="M396.88,484.35l-50.97-29.43c-.08-.04-.17-.06-.24-.1-.8-.44-1.64-.79-2.51-1.03-.14-.04-.27-.06-.41-.09-.81-.19-1.64-.3-2.47-.32-.13,0-.26-.02-.39-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.76.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.06,5.68-31.9,0-9.84-5.68-15.96-16.27-15.96-27.63v-29.43c0-.95-.15-1.87-.37-2.76-.05-.19-.09-.37-.14-.56-.25-.86-.59-1.69-1.03-2.47-.07-.12-.15-.22-.22-.34-.43-.71-.94-1.37-1.51-1.97-.1-.1-.18-.21-.28-.31-.65-.63-1.37-1.18-2.15-1.66-.07-.04-.13-.11-.2-.16l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.25.1.8.44,1.63.79,2.5,1.03.14.04.29.06.43.09.8.19,1.61.3,2.43.32.1,0,.2.04.3.04.04,0,.09-.02.13-.02.88,0,1.77-.13,2.64-.34.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.57-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22Z"></path></svg>Index your code with Devin</button></div><div class="container-wrapper"><div class="container mx-auto flex w-full flex-row items-center gap-2 py-4 md:py-6"><a class="flex items-center gap-3" href="/"><span class="text-base font-medium leading-none md:text-lg hidden sm:block">DeepWiki</span></a><div class="flex-1"><div class="flex flex-row items-center gap-2"><a class="block text-xs font-medium leading-none text-white sm:hidden md:text-lg" href="/">DeepWiki</a><p class="text-sm font-normal leading-none md:text-lg"><a href="https://github.com/ModelTC/lightx2v" target="_blank" rel="noopener noreferrer" title="Open repository" class="text-muted-foreground hover:text-muted-foreground/80 group inline-flex items-center gap-1 transition-colors">ModelTC/lightx2v<!-- --> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256" class="opacity-0 transition-opacity group-hover:opacity-100"><path d="M224,104a8,8,0,0,1-16,0V59.32l-66.33,66.34a8,8,0,0,1-11.32-11.32L196.68,48H152a8,8,0,0,1,0-16h64a8,8,0,0,1,8,8Zm-40,24a8,8,0,0,0-8,8v72H48V80h72a8,8,0,0,0,0-16H48A16,16,0,0,0,32,80V208a16,16,0,0,0,16,16H176a16,16,0,0,0,16-16V136A8,8,0,0,0,184,128Z"></path></svg></a></p></div></div><div class="flex items-center gap-4"><button class="group hidden items-center gap-1.5 md:flex"><div class="relative"><span class="text-foreground/70 group-hover:text-foreground text-xs font-light transition-colors">Index your code with</span><div class="bg-foreground/30 absolute bottom-0 left-0 h-[1px] w-0 transition-all duration-300 group-hover:w-full"></div></div><div class="flex items-center gap-1 transition-transform duration-300 group-hover:translate-x-0.5"><svg class="size-4 transform transition-transform duration-700 group-hover:rotate-180 [&amp;_path]:stroke-0" xmlns="http://www.w3.org/2000/svg" viewBox="110 110 460 500"><path style="fill:#21c19a" class="" d="M418.73,332.37c9.84-5.68,22.07-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.58,1.06.19.06.37.11.55.16.87.21,1.76.34,2.65.35.04,0,.08.02.13.02.1,0,.19-.03.29-.04.83-.02,1.64-.13,2.45-.32.14-.03.28-.05.42-.09.87-.24,1.7-.59,2.5-1.03.08-.04.17-.06.25-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.06,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.57-1.06-.19-.06-.37-.11-.56-.16-.88-.21-1.76-.34-2.65-.34-.13,0-.26.02-.4.02-.84.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.24.1.8.44,1.64.79,2.5,1.03.14.04.28.06.42.09.81.19,1.62.3,2.45.32.1,0,.19.04.29.04.04,0,.08-.02.13-.02.89,0,1.77-.13,2.65-.35.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.58-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-.08-.04-.16-.06-.24-.1-.8-.44-1.64-.8-2.51-1.04-.13-.04-.26-.05-.39-.09-.82-.2-1.65-.31-2.49-.33-.13,0-.25-.02-.38-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.75.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.07,5.68-31.9,0-9.84-5.68-15.95-16.27-15.95-27.63s6.11-21.95,15.95-27.63Z"></path><path style="fill:#3969ca" d="M141.09,317.65l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c.08-.04.13-.11.2-.16.78-.48,1.51-1.02,2.15-1.66.1-.1.18-.21.28-.31.57-.6,1.08-1.26,1.51-1.97.07-.12.15-.22.22-.34.44-.77.77-1.6,1.03-2.47.05-.19.1-.37.14-.56.22-.89.37-1.81.37-2.76v-29.43c0-11.36,6.11-21.95,15.96-27.63s22.06-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.57,1.06.19.06.37.11.56.16.87.21,1.76.34,2.64.35.04,0,.09.02.13.02.1,0,.19-.04.29-.04.83-.02,1.65-.13,2.45-.32.14-.03.28-.05.41-.09.87-.24,1.71-.6,2.51-1.04.08-.04.16-.06.24-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.07,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.58-1.06-.19-.06-.37-.11-.55-.16-.88-.21-1.76-.34-2.65-.35-.13,0-.26.02-.4.02-.83.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22Z"></path><path style="fill:#0294de" class="" d="M396.88,484.35l-50.97-29.43c-.08-.04-.17-.06-.24-.1-.8-.44-1.64-.79-2.51-1.03-.14-.04-.27-.06-.41-.09-.81-.19-1.64-.3-2.47-.32-.13,0-.26-.02-.39-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.76.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.06,5.68-31.9,0-9.84-5.68-15.96-16.27-15.96-27.63v-29.43c0-.95-.15-1.87-.37-2.76-.05-.19-.09-.37-.14-.56-.25-.86-.59-1.69-1.03-2.47-.07-.12-.15-.22-.22-.34-.43-.71-.94-1.37-1.51-1.97-.1-.1-.18-.21-.28-.31-.65-.63-1.37-1.18-2.15-1.66-.07-.04-.13-.11-.2-.16l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.25.1.8.44,1.63.79,2.5,1.03.14.04.29.06.43.09.8.19,1.61.3,2.43.32.1,0,.2.04.3.04.04,0,.09-.02.13-.02.88,0,1.77-.13,2.64-.34.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.57-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22Z"></path></svg><span class="text-sm font-medium">Devin</span></div></button><button aria-label="Edit Wiki" class="flex items-center rounded-md cursor-pointer transition-all border border-border bg-surface hover:border-border-hover hover:bg-component disabled:cursor-default disabled:opacity-50 disabled:hover:border-border disabled:hover:bg-surface gap-2 px-3 py-1.5 text-sm"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 256 256"><path d="M227.32,73.37,182.63,28.69a16,16,0,0,0-22.63,0L36.69,152A15.86,15.86,0,0,0,32,163.31V208a16,16,0,0,0,16,16H216a8,8,0,0,0,0-16H115.32l112-112A16,16,0,0,0,227.32,73.37ZM92.69,208H48V163.31l88-88L180.69,120ZM192,108.69,147.32,64l24-24L216,84.69Z"></path></svg>Edit Wiki</button><button class="flex items-center rounded-md !text-white cursor-pointer transition-all border bg-blue-500 hover:bg-blue-600 border-blue-500 hover:border-blue-600 dark:bg-blue-900 dark:hover:bg-blue-800 dark:border-blue-900 dark:hover:border-blue-800 disabled:cursor-default disabled:opacity-50 disabled:hover:bg-blue-500 disabled:hover:border-blue-500 dark:disabled:hover:bg-blue-900 dark:disabled:hover:border-blue-900 gap-1.5 px-3 py-1.5 text-sm" aria-label="Share" data-state="closed" data-slot="tooltip-trigger"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-4 w-4"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg><span>Share</span></button><div class="h-8 w-8"></div></div></div></div></div><!--$?--><template id="B:1"></template><div class="container-wrapper flex flex-1 items-center justify-center px-4"><div class="inline-block bg-clip-text text-[#b5b5b5a4] animate-shine text-center text-lg" style="background-image:linear-gradient(120deg, rgba(255, 255, 255, 0) 40%, rgba(255, 255, 255, 0.8) 50%, rgba(255, 255, 255, 0) 60%);background-size:200% 100%;-webkit-background-clip:text;animation-duration:1s">Loading...</div></div><!--/$--></div></div><script>$RB=[];$RV=function(a){$RT=performance.now();for(var b=0;b<a.length;b+=2){var c=a[b],e=a[b+1];null!==e.parentNode&&e.parentNode.removeChild(e);var f=c.parentNode;if(f){var g=c.previousSibling,h=0;do{if(c&&8===c.nodeType){var d=c.data;if("/$"===d||"/&"===d)if(0===h)break;else h--;else"$"!==d&&"$?"!==d&&"$~"!==d&&"$!"!==d&&"&"!==d||h++}d=c.nextSibling;f.removeChild(c);c=d}while(c);for(;e.firstChild;)f.insertBefore(e.firstChild,c);g.data="$";g._reactRetry&&requestAnimationFrame(g._reactRetry)}}a.length=0};
$RC=function(a,b){if(b=document.getElementById(b))(a=document.getElementById(a))?(a.previousSibling.data="$~",$RB.push(a,b),2===$RB.length&&("number"!==typeof $RT?requestAnimationFrame($RV.bind(null,$RB)):(a=performance.now(),setTimeout($RV.bind(null,$RB),2300>a&&2E3<a?2300-a:$RT+300-a)))):b.parentNode.removeChild(b)};$RC("B:0","S:0")</script><div hidden id="S:1"><script type="application/ld+json">{"@context":"https://schema.org","@type":"TechArticle","headline":"Input Encoder Architecture","description":"This document describes the input encoder architecture in LightX2V, covering how text, image, and audio inputs are processed before entering the diffusion transformer. The encoders transform raw input","image":"https://deepwiki.com/ModelTC/lightx2v/og-image.png","datePublished":"2026-02-14T08:53:03.969892","dateModified":"2026-02-14T08:53:03.969892","author":{"@type":"Organization","name":"DeepWiki","url":"https://deepwiki.com"},"publisher":{"@type":"Organization","name":"DeepWiki","logo":{"@type":"ImageObject","url":"https://deepwiki.com/icon.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://deepwiki.com/ModelTC/lightx2v/4.5-input-encoder-architecture"}}</script><div class="w-full flex-1"><div class="container-wrapper relative mx-auto h-full px-0"><div class="container relative mx-auto flex h-full w-full flex-col gap-0 max-md:!px-0 md:flex-row md:gap-6 lg:gap-10"><div class="border-r-border hidden max-h-screen border-r border-dashed py-6 pr-4 transition-[border-radius] md:sticky md:left-0 md:top-20 md:block md:h-[calc(100vh-82px)] md:w-64 md:flex-shrink-0 md:overflow-y-auto lg:py-9 xl:w-72"><div class="flex h-full w-full max-w-full flex-shrink-0 flex-col overflow-hidden" style="scrollbar-color:var(--color-border) transparent"><div class="flex-shrink-0 px-2"><div class="text-secondary pb-1 text-xs">Last indexed: <!-- -->14 February 2026<!-- --> (<a href="https://github.com/ModelTC/lightx2v/commits/5573905f" target="_blank" rel="noopener noreferrer" class="underline-offset-2 hover:underline">557390</a>)</div></div><ul class="flex-1 flex-shrink-0 space-y-1 overflow-y-auto py-1" style="scrollbar-width:none"><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/1-overview">Overview</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/1.1-key-features-and-optimization-techniques">Key Features and Optimization Techniques</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/1.2-system-architecture-overview">System Architecture Overview</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/1.3-hardware-platform-support">Hardware Platform Support</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/1.4-model-ecosystem-and-supported-tasks">Model Ecosystem and Supported Tasks</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/2-getting-started">Getting Started</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/2.1-installation-and-environment-setup">Installation and Environment Setup</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/2.2-model-download-and-organization">Model Download and Organization</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/2.3-quick-start-tutorial">Quick Start Tutorial</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/3-user-interfaces">User Interfaces</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/3.1-gradio-web-interface">Gradio Web Interface</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/3.2-python-api-(lightx2vpipeline)">Python API (LightX2VPipeline)</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/3.3-command-line-interface">Command Line Interface</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/3.4-http-server-and-production-deployment">HTTP Server and Production Deployment</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/3.5-comfyui-integration">ComfyUI Integration</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/4-core-architecture">Core Architecture</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/4.1-runner-system-and-registry-pattern">Runner System and Registry Pattern</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/4.2-three-stage-inference-pipeline">Three-Stage Inference Pipeline</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/4.3-scheduler-system-and-diffusion-process">Scheduler System and Diffusion Process</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/4.4-weight-management-system">Weight Management System</a></li><li style="padding-left:12px"><a data-selected="true" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/4.5-input-encoder-architecture">Input Encoder Architecture</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/4.6-vae-system-and-latent-space">VAE System and Latent Space</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/5-model-runners-and-tasks">Model Runners and Tasks</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/5.1-wan-model-family-overview">WAN Model Family Overview</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/5.2-wanaudiorunner-audio-to-video-generation">WanAudioRunner - Audio-to-Video Generation</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/5.3-wanrunner-text-to-video-and-image-to-video">WanRunner - Text-to-Video and Image-to-Video</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/5.4-wandistillrunner-4-step-distilled-models">WanDistillRunner - 4-Step Distilled Models</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/5.5-wan22moerunner-mixture-of-experts">Wan22MoeRunner - Mixture-of-Experts</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/5.6-qwenimagerunner-textimage-to-image-generation">QwenImageRunner - Text/Image-to-Image Generation</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/5.7-zimagerunner-fast-image-generation">ZImageRunner - Fast Image Generation</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/5.8-wancausvidrunner-autoregressive-video">WanCausVidRunner - Autoregressive Video</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/5.9-longcatimagerunner-and-ltx2runner">LongCatImageRunner and LTX2Runner</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/5.10-shotpipeline-multi-clip-generation">ShotPipeline - Multi-Clip Generation</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/6-performance-optimization">Performance Optimization</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/6.1-quantization-system">Quantization System</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/6.2-attention-operators-and-sparse-patterns">Attention Operators and Sparse Patterns</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/6.3-memory-management-and-cpu-offloading">Memory Management and CPU Offloading</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/6.4-feature-caching-and-streaming">Feature Caching and Streaming</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/6.5-distributed-and-parallel-inference">Distributed and Parallel Inference</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/6.6-lora-dynamic-application">LoRA Dynamic Application</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/6.7-jit-compilation-and-shape-caching">JIT Compilation and Shape Caching</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/7-advanced-architecture-topics">Advanced Architecture Topics</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/7.1-transformer-block-architecture">Transformer Block Architecture</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/7.2-rotary-position-embeddings-(rope)">Rotary Position Embeddings (RoPE)</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/7.3-audio-adapter-and-cross-attention">Audio Adapter and Cross-Attention</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/7.4-triton-kernel-implementation">Triton Kernel Implementation</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/7.5-vae-tiling-and-distributed-processing">VAE Tiling and Distributed Processing</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/7.6-lazy-loading-and-async-weight-streaming">Lazy Loading and Async Weight Streaming</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/7.7-multi-person-audio-processing">Multi-Person Audio Processing</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/8-developer-tools-and-workflows">Developer Tools and Workflows</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/8.1-model-conversion-and-quantization-tools">Model Conversion and Quantization Tools</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/8.2-configuration-system">Configuration System</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/8.3-inputinfo-and-data-structures">InputInfo and Data Structures</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/8.4-custom-runner-development">Custom Runner Development</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/8.5-prompt-templates-and-text-processing">Prompt Templates and Text Processing</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/ModelTC/lightx2v/8.6-debugging-and-profiling">Debugging and Profiling</a></li></ul></div></div><div class="flex h-full flex-1 flex-col overflow-hidden"><div class="bg-background border-b-border sticky top-0 z-10 border-b border-dashed md:hidden"><div class="flex cursor-pointer items-center gap-2 p-3"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256" class="transition-transform"><path d="M184.49,136.49l-80,80a12,12,0,0,1-17-17L159,128,87.51,56.49a12,12,0,1,1,17-17l80,80A12,12,0,0,1,184.49,136.49Z"></path></svg><span class="truncate text-base font-normal">Menu</span></div></div><div class="relative flex-1 overflow-y-auto px-3 pt-3 md:rounded-md md:px-0 md:pt-0 [&amp;_::selection]:bg-purple-500/40" style="scrollbar-color:var(--color-night) transparent"><div class="pb-30 mx-auto max-w-2xl md:pb-40 md:pt-6 lg:pt-8"><div class="prose prose-invert dark:prose-invert prose-headings:text-inherit prose-p:text-inherit max-w-none"><div><div class="prose-custom prose-custom-md prose-custom-gray !max-w-none text-neutral-300 [overflow-wrap:anywhere]"><h1 id="input-encoder-architecture" class="group" data-header="true">Input Encoder Architecture<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h1>
<details>
<summary>Relevant source files</summary>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/app/utils/image_page.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>app/utils/image_page.py</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/configs/z_image/z_image_turbo_t2i_offload.json" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>configs/z_image/z_image_turbo_t2i_offload.json</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/examples/z-image-turbo/z_image_turbo.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>examples/z-image-turbo/z_image_turbo.py</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/input_encoders/hf/z_image/qwen3_model.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/input_encoders/hf/z_image/qwen3_model.py</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/networks/base_model.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/networks/base_model.py</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/networks/qwen_image/infer/pre_infer.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/networks/qwen_image/infer/pre_infer.py</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/networks/qwen_image/infer/transformer_infer.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/networks/qwen_image/infer/transformer_infer.py</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/networks/qwen_image/model.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/networks/qwen_image/model.py</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/networks/z_image/model.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/networks/z_image/model.py</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/networks/z_image/weights/post_weights.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/networks/z_image/weights/post_weights.py</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/networks/z_image/weights/transformer_weights.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/networks/z_image/weights/transformer_weights.py</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/qwen_image/qwen_image_runner.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/qwen_image/qwen_image_runner.py</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/z_image/z_image_runner.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/z_image/z_image_runner.py</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/schedulers/qwen_image/scheduler.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/schedulers/qwen_image/scheduler.py</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/video_encoders/hf/qwen_image/vae.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/video_encoders/hf/qwen_image/vae.py</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/video_encoders/hf/z_image/vae.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/video_encoders/hf/z_image/vae.py</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/scripts/z_image/z_image_turbo_t2i.sh" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>scripts/z_image/z_image_turbo_t2i.sh</span></a></li>
</ul>
</details>
<h2 id="purpose-and-scope" class="group" data-header="true">Purpose and Scope<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>This document describes the input encoder architecture in LightX2V, covering how text, image, and audio inputs are processed before entering the diffusion transformer. The encoders transform raw inputs into high-dimensional embeddings that condition the video generation process.</p>
<p>For information about the VAE encoder/decoder system, see <a href="/ModelTC/lightx2v/4.5-input-encoder-architecture" class="text-neutral-300 hover:text-neutral-200 hover:underline">VAE System and Video Encoding</a>. For the complete inference pipeline that uses these encoders, see <a href="/ModelTC/lightx2v/4.2-three-stage-inference-pipeline" class="text-neutral-300 hover:text-neutral-200 hover:underline">Three-Stage Inference Pipeline</a>. For model-specific encoding implementations, see <a href="/ModelTC/lightx2v/5-model-runners-and-tasks" class="text-neutral-300 hover:text-neutral-200 hover:underline">Model Variants and Tasks</a>.</p>
<hr/>
<h2 id="text-encoder-system" class="group" data-header="true">Text Encoder System<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<h3 id="supported-text-encoders" class="group" data-header="true">Supported Text Encoders<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>LightX2V supports multiple text encoder architectures depending on the model family:</p>








































<table><thead><tr><th>Encoder</th><th>Model Family</th><th>Output Dimension</th><th>Context Length</th><th>Purpose</th></tr></thead><tbody><tr><td><strong>T5-XXL</strong></td><td>WAN 2.1/2.2</td><td>4096</td><td>512 tokens</td><td>Primary text conditioning</td></tr><tr><td><strong>ByT5</strong></td><td>HunyuanVideo 1.5</td><td>Variable</td><td>Variable</td><td>Multilingual support</td></tr><tr><td><strong>Qwen2.5-VL</strong></td><td>HunyuanVideo 1.5</td><td>7B LLM</td><td>Variable</td><td>Vision-language understanding</td></tr><tr><td><strong>XLM-RoBERTa CLIP</strong></td><td>WAN (image tasks)</td><td>1280</td><td>77 tokens</td><td>Visual-text alignment</td></tr></tbody></table>
<p><strong>Text Encoder Loading Architecture</strong></p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_runner.py#L98-L136" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">98-136</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/input_encoders/hf/wan/t5/model.py#L44-L49" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/input_encoders/hf/wan/t5/model.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">44-49</span></a></li>
</ul>
<h3 id="t5encodermodel-implementation" class="group" data-header="true">T5EncoderModel Implementation<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>The <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">T5EncoderModel</code> class implements Google&#x27;s T5 (Text-to-Text Transfer Transformer) encoder for text conditioning.</p>
<p><strong>Key Architecture Components:</strong></p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p><strong>Encoding Process:</strong></p>
<ol>
<li><strong>Tokenization</strong>: Text is tokenized using the ByT5/UMT5 tokenizer, supporting multilingual input</li>
<li><strong>Embedding Lookup</strong>: Token IDs are converted to 4096-dimensional embeddings</li>
<li><strong>Transformer Layers</strong>: 24 encoder blocks apply self-attention and feed-forward transformations</li>
<li><strong>Normalization</strong>: Final RMSNorm produces stable output embeddings</li>
<li><strong>Padding</strong>: Sequences are padded to <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">text_len</code> (default 512) with zeros</li>
</ol>
<p><strong>Code Implementation:</strong></p>
<p>The T5 encoder&#x27;s forward pass is defined in <a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/input_encoders/hf/wan/t5/model.py#L448-L492" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/input_encoders/hf/wan/t5/model.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">448-492</span></a>:</p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><div class="rounded-sm border border-[#8F8F8F]/30 p-2 text-xs font-normal leading-[15px]" style="background-color:transparent;min-height:2em"></div></pre>
<p><strong>Memory Optimization Features:</strong></p>



































<table><thead><tr><th>Feature</th><th>Configuration</th><th>Memory Savings</th><th>Implementation</th></tr></thead><tbody><tr><td><strong>Quantization</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">t5_quantized=True</code></td><td>~4x (INT8/FP8)</td><td>QuantLinear layers</td></tr><tr><td><strong>CPU Offload</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">t5_cpu_offload=True</code></td><td>GPU memory  CPU</td><td>Async transfers</td></tr><tr><td><strong>Lazy Loading</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">t5_lazy_load=True</code></td><td>Reduced peak memory</td><td>Load on demand</td></tr><tr><td><strong>Block-level Offload</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">offload_granularity=&quot;block&quot;</code></td><td>Gradual memory usage</td><td>2-block rotation</td></tr></tbody></table>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/input_encoders/hf/wan/t5/model.py#L338-L492" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/input_encoders/hf/wan/t5/model.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">338-492</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_runner.py#L98-L136" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">98-136</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/utils/set_config.py#L14-L34" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/utils/set_config.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">14-34</span></a></li>
</ul>
<h3 id="byt5-encoder-for-multilingual-support" class="group" data-header="true">ByT5 Encoder for Multilingual Support<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>HunyuanVideo 1.5 uses ByT5 (Byte-level T5) for multilingual text conditioning with special token support.</p>
<p><strong>ByT5 Unique Features:</strong></p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p>The <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">add_special_token()</code> function extends the vocabulary with color and font tokens for fine-grained text rendering control in text-to-image generation.</p>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/input_encoders/hf/hunyuan15/byt5/model.py#L21-L61" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/input_encoders/hf/hunyuan15/byt5/model.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">21-61</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/input_encoders/hf/hunyuan15/byt5/model.py#L63-L102" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/input_encoders/hf/hunyuan15/byt5/model.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">63-102</span></a></li>
</ul>
<h3 id="text-encoder-integration-in-runners" class="group" data-header="true">Text Encoder Integration in Runners<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>The text encoder is invoked during the pre-inference stage via <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">run_text_encoder()</code>:</p>
<p><strong>WanRunner Text Encoding Flow:</strong></p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_runner.py#L216-L260" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">216-260</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/default_runner.py#L275-L295" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/default_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">275-295</span></a></li>
</ul>
<hr/>
<h2 id="image-encoder-system" class="group" data-header="true">Image Encoder System<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<h3 id="clip-visual-encoder" class="group" data-header="true">CLIP Visual Encoder<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>The image encoder uses OpenAI&#x27;s CLIP (XLM-RoBERTa CLIP ViT-H/14) to extract visual features from input images for image-to-video (i2v) and related tasks.</p>
<p><strong>CLIP Architecture:</strong></p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p><strong>Output Format:</strong></p>
<ul>
<li>Shape: <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">[257, 1280]</code> where 257 = 1 CLS token + 256 patch tokens (1616 grid)</li>
<li>Dtype: Configurable (default <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">torch.float16</code>)</li>
<li>Used for: Cross-attention conditioning in i2v/animate/s2v tasks</li>
</ul>
<p><strong>CLIP Loading and Quantization:</strong></p>
<p>The CLIP model supports the same quantization schemes as T5:</p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><div class="rounded-sm border border-[#8F8F8F]/30 p-2 text-xs font-normal leading-[15px]" style="background-color:transparent;min-height:2em"></div></pre>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/input_encoders/hf/wan/xlm_roberta/model.py#L1-L32" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/input_encoders/hf/wan/xlm_roberta/model.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-32</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_runner.py#L60-L95" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">60-95</span></a></li>
</ul>
<h3 id="image-encoder-invocation" class="group" data-header="true">Image Encoder Invocation<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p><strong>Encoding Process in WanRunner:</strong></p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p>The <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">run_image_encoder()</code> method handles lazy loading and cleanup:</p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><div class="rounded-sm border border-[#8F8F8F]/30 p-2 text-xs font-normal leading-[15px]" style="background-color:transparent;min-height:2em"></div></pre>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_runner.py#L262-L279" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">262-279</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/default_runner.py#L241-L273" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/default_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">241-273</span></a></li>
</ul>
<hr/>
<h2 id="audio-encoder-system" class="group" data-header="true">Audio Encoder System<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>The audio encoder system is specialized for speech-to-video (s2v) tasks, converting audio waveforms into conditioning features.</p>
<h3 id="audio-encoding-pipeline" class="group" data-header="true">Audio Encoding Pipeline<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p><strong>Complete Audio Processing Architecture:</strong></p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_audio_runner.py#L175-L253" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_audio_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">175-253</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_audio_runner.py#L292-L329" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_audio_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">292-329</span></a></li>
</ul>
<h3 id="sekoaudioencodermodel" class="group" data-header="true">SekoAudioEncoderModel<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>The <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">SekoAudioEncoderModel</code> wraps a pre-trained Hubert model (TencentGameMate-chinese-hubert-large) to extract audio features.</p>
<p><strong>Key Characteristics:</strong></p>






























<table><thead><tr><th>Property</th><th>Value</th><th>Description</th></tr></thead><tbody><tr><td><strong>Input</strong></td><td>Raw audio waveform @ 16kHz</td><td>Mono channel audio</td></tr><tr><td><strong>Output</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">[T, 1024]</code> features</td><td>T = temporal frames</td></tr><tr><td><strong>Base Model</strong></td><td>Hubert Large</td><td>Self-supervised speech model</td></tr><tr><td><strong>Temporal Granularity</strong></td><td>~50Hz</td><td>One feature per 20ms</td></tr></tbody></table>
<p><strong>Audio Encoding Process:</strong></p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><div class="rounded-sm border border-[#8F8F8F]/30 p-2 text-xs font-normal leading-[15px]" style="background-color:transparent;min-height:2em"></div></pre>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_audio_runner.py#L749-L753" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_audio_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">749-753</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_audio_runner.py#L569-L580" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_audio_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">569-580</span></a></li>
</ul>
<h3 id="audioadapter-architecture" class="group" data-header="true">AudioAdapter Architecture<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>The <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">AudioAdapter</code> projects Hubert features into the transformer&#x27;s conditioning space with specialized temporal and frequency modeling.</p>
<p><strong>AudioAdapter Structure:</strong></p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p><strong>Configuration Parameters:</strong></p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><div class="rounded-sm border border-[#8F8F8F]/30 p-2 text-xs font-normal leading-[15px]" style="background-color:transparent;min-height:2em"></div></pre>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_audio_runner.py#L755-L779" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_audio_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">755-779</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/networks/wan/audio_model.py#L1-L17" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/networks/wan/audio_model.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-17</span></a></li>
</ul>
<h3 id="multi-person-audio-processing" class="group" data-header="true">Multi-Person Audio Processing<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>For multi-speaker audio-to-video generation, the system handles multiple audio streams with spatial masking:</p>
<p><strong>Multi-Person Audio Flow:</strong></p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p>The mask latents are used in the diffusion model to spatially separate different speakers&#x27; lip movements.</p>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_audio_runner.py#L331-L348" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_audio_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">331-348</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_audio_runner.py#L350-L372" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_audio_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">350-372</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_audio_runner.py#L188-L204" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_audio_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">188-204</span></a></li>
</ul>
<hr/>
<h2 id="encoder-memory-management" class="group" data-header="true">Encoder Memory Management<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>All input encoders support advanced memory optimization techniques to enable inference on consumer hardware.</p>
<h3 id="quantization-support" class="group" data-header="true">Quantization Support<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p><strong>Quantization Scheme Comparison:</strong></p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p>All quantization schemes are unified through the <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">MM_WEIGHT_REGISTER</code> and <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">QuantLinear</code> abstractions.</p>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/input_encoders/hf/q_linear.py#L1-L33" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/input_encoders/hf/q_linear.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-33</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/input_encoders/hf/q_linear.py#L37-L95" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/input_encoders/hf/q_linear.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">37-95</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/input_encoders/hf/q_linear.py#L97-L223" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/input_encoders/hf/q_linear.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">97-223</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/common/ops/mm/mm_weight.py#L149-L319" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/common/ops/mm/mm_weight.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">149-319</span></a></li>
</ul>
<h3 id="cpu-offloading" class="group" data-header="true">CPU Offloading<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>Encoders can be offloaded to CPU memory between encoding stages:</p>
<p><strong>Offloading Strategy:</strong></p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p><strong>Offloading Configuration:</strong></p>








































<table><thead><tr><th>Config Key</th><th>Effect</th><th>Use Case</th></tr></thead><tbody><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">t5_cpu_offload</code></td><td>T5 lives on CPU</td><td>8GB VRAM systems</td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">clip_cpu_offload</code></td><td>CLIP lives on CPU</td><td>Low memory mode</td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">audio_encoder_cpu_offload</code></td><td>Audio encoder on CPU</td><td>Multi-person s2v</td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">audio_adapter_cpu_offload</code></td><td>Audio adapter on CPU</td><td>Further memory reduction</td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">lazy_load</code></td><td>Load encoders on-demand</td><td>Minimal peak memory</td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">unload_modules</code></td><td>Unload after each use</td><td>Sequential processing</td></tr></tbody></table>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_runner.py#L98-L136" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">98-136</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_runner.py#L216-L260" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">216-260</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_audio_runner.py#L749-L779" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_audio_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">749-779</span></a></li>
</ul>
<h3 id="lazy-loading-from-disk" class="group" data-header="true">Lazy Loading from Disk<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>The <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">lazy_load</code> mode enables loading encoder weights directly from disk to GPU on-demand:</p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><div class="rounded-sm border border-[#8F8F8F]/30 p-2 text-xs font-normal leading-[15px]" style="background-color:transparent;min-height:2em"></div></pre>
<p>This pattern is repeated across all encoder types.</p>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_runner.py#L222-L260" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">222-260</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/default_runner.py#L120-L127" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/default_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">120-127</span></a></li>
</ul>
<hr/>
<h2 id="encoder-output-integration" class="group" data-header="true">Encoder Output Integration<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<h3 id="pre-inference-stage-integration" class="group" data-header="true">Pre-Inference Stage Integration<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>All encoder outputs are collected and passed to the <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">WanPreInfer</code> module:</p>
<p><strong>Encoder Output Flow:</strong></p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/networks/wan/infer/pre_infer.py#L9-L142" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/networks/wan/infer/pre_infer.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">9-142</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/default_runner.py#L275-L295" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/default_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">275-295</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_audio_runner.py#L440-L465" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_audio_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">440-465</span></a></li>
</ul>
<h3 id="conditioning-tensor-shapes" class="group" data-header="true">Conditioning Tensor Shapes<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p><strong>Summary of Encoder Output Shapes:</strong></p>






















































<table><thead><tr><th>Encoder</th><th>Output Name</th><th>Shape</th><th>Dtype</th><th>Purpose</th></tr></thead><tbody><tr><td><strong>T5</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">context</code></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">[1, 512, 4096]</code></td><td>bf16</td><td>Text conditioning</td></tr><tr><td><strong>T5</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">context_null</code></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">[1, 512, 4096]</code></td><td>bf16</td><td>Negative prompt (CFG)</td></tr><tr><td><strong>CLIP Visual</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">clip_encoder_out</code></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">[257, 1280]</code></td><td>fp16</td><td>Image conditioning</td></tr><tr><td><strong>VAE Encoder</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">vae_encoder_out</code></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">[16, T, H, W]</code></td><td>bf16/fp16</td><td>First frame latent</td></tr><tr><td><strong>Audio Hubert</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">audio_features</code></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">[N, T, 128, 1024]</code></td><td>bf16</td><td>Audio conditioning</td></tr><tr><td><strong>Audio Mask</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">person_mask_latens</code></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">[N, 1, H/16, W/16]</code></td><td>int8</td><td>Spatial masks</td></tr></tbody></table>
<p>Where:</p>
<ul>
<li><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">T</code> = temporal dimension (latent frames)</li>
<li><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">H, W</code> = spatial latent dimensions</li>
<li><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">N</code> = number of audio streams (speakers)</li>
</ul>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_runner.py#L437-L445" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">437-445</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/runners/wan/wan_audio_runner.py#L456-L465" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/runners/wan/wan_audio_runner.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">456-465</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/networks/wan/audio_model.py#L75-L92" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/networks/wan/audio_model.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">75-92</span></a></li>
</ul>
<hr/>
<h2 id="platform-specific-optimizations" class="group" data-header="true">Platform-Specific Optimizations<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>Input encoders support platform-specific quantization kernels:</p>
<p><strong>Platform Support Matrix:</strong></p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p>The platform abstraction allows the same encoder code to run on different hardware with optimized kernels.</p>
<p><strong>Sources:</strong></p>
<ul>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/input_encoders/hf/q_linear.py#L1-L33" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/input_encoders/hf/q_linear.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-33</span></a></li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v_platform/ops/mm/cambricon_mlu/q_linear.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v_platform/ops/mm/cambricon_mlu/q_linear.py</span></a> (referenced in imports)</li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v_platform/ops/mm/ascend_npu/npu_q_linear.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v_platform/ops/mm/ascend_npu/npu_q_linear.py</span></a> (referenced in imports)</li>
<li><a href="https://github.com/ModelTC/lightx2v/blob/5573905f/lightx2v/models/input_encoders/hf/wan/t5/model.py#L18-L42" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>lightx2v/models/input_encoders/hf/wan/t5/model.py</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">18-42</span></a></li>
</ul></div></div></div></div></div></div><div class="hidden overflow-hidden transition-[border-radius] xl:sticky xl:right-0 xl:top-20 xl:block xl:h-[calc(100vh-82px)] xl:w-64 xl:flex-shrink-0 2xl:w-72" style="scrollbar-width:none"><div class="flex max-h-full w-full flex-shrink-0 flex-col py-6 pt-0 text-sm lg:pb-4 lg:pt-8 xl:w-64 2xl:w-72" style="scrollbar-color:var(--color-night) transparent"><div><div class="relative mx-4 my-4 rounded-md border border-neutral-200 bg-neutral-100 p-3 text-sm text-neutral-600 dark:border-neutral-800 dark:bg-neutral-900 dark:text-neutral-400"><button class="absolute right-2 top-2 rounded-sm p-1 opacity-70 transition-opacity hover:opacity-100 focus:outline-none focus:ring-2 focus:ring-neutral-400 focus:ring-offset-2"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M205.66,194.34a8,8,0,0,1-11.32,11.32L128,139.31,61.66,205.66a8,8,0,0,1-11.32-11.32L116.69,128,50.34,61.66A8,8,0,0,1,61.66,50.34L128,116.69l66.34-66.35a8,8,0,0,1,11.32,11.32L139.31,128Z"></path></svg><span class="sr-only">Dismiss</span></button><p class="text-sm font-medium">Refresh this wiki</p><button class="mt-2 flex items-center gap-1 rounded-md bg-neutral-200 px-2 py-1 text-sm font-medium text-neutral-700 transition-colors hover:bg-neutral-300 dark:bg-neutral-800 dark:text-neutral-300 dark:hover:bg-neutral-700">Enter email to refresh</button></div></div><h3 class="px-4 pb-5 text-lg font-medium leading-none">On this page</h3><ul style="scrollbar-width:none" class="min-h-0 flex-1 space-y-3 overflow-y-auto p-4 pt-0"><li class=""><a href="#input-encoder-architecture" class="hover:text-primary pr-1 transition-all text-primary font-medium">Input Encoder Architecture</a></li><li class="ml-3"><a href="#purpose-and-scope" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Purpose and Scope</a></li><li class="ml-3"><a href="#text-encoder-system" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Text Encoder System</a></li><li class="ml-6"><a href="#supported-text-encoders" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Supported Text Encoders</a></li><li class="ml-6"><a href="#t5encodermodel-implementation" class="hover:text-primary pr-1 font-normal transition-all text-secondary">T5EncoderModel Implementation</a></li><li class="ml-6"><a href="#byt5-encoder-for-multilingual-support" class="hover:text-primary pr-1 font-normal transition-all text-secondary">ByT5 Encoder for Multilingual Support</a></li><li class="ml-6"><a href="#text-encoder-integration-in-runners" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Text Encoder Integration in Runners</a></li><li class="ml-3"><a href="#image-encoder-system" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Image Encoder System</a></li><li class="ml-6"><a href="#clip-visual-encoder" class="hover:text-primary pr-1 font-normal transition-all text-secondary">CLIP Visual Encoder</a></li><li class="ml-6"><a href="#image-encoder-invocation" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Image Encoder Invocation</a></li><li class="ml-3"><a href="#audio-encoder-system" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Audio Encoder System</a></li><li class="ml-6"><a href="#audio-encoding-pipeline" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Audio Encoding Pipeline</a></li><li class="ml-6"><a href="#sekoaudioencodermodel" class="hover:text-primary pr-1 font-normal transition-all text-secondary">SekoAudioEncoderModel</a></li><li class="ml-6"><a href="#audioadapter-architecture" class="hover:text-primary pr-1 font-normal transition-all text-secondary">AudioAdapter Architecture</a></li><li class="ml-6"><a href="#multi-person-audio-processing" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Multi-Person Audio Processing</a></li><li class="ml-3"><a href="#encoder-memory-management" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Encoder Memory Management</a></li><li class="ml-6"><a href="#quantization-support" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Quantization Support</a></li><li class="ml-6"><a href="#cpu-offloading" class="hover:text-primary pr-1 font-normal transition-all text-secondary">CPU Offloading</a></li><li class="ml-6"><a href="#lazy-loading-from-disk" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Lazy Loading from Disk</a></li><li class="ml-3"><a href="#encoder-output-integration" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Encoder Output Integration</a></li><li class="ml-6"><a href="#pre-inference-stage-integration" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Pre-Inference Stage Integration</a></li><li class="ml-6"><a href="#conditioning-tensor-shapes" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Conditioning Tensor Shapes</a></li><li class="ml-3"><a href="#platform-specific-optimizations" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Platform-Specific Optimizations</a></li></ul></div></div><div class="pointer-events-none fixed bottom-2 left-2 right-2 mt-2 md:bottom-4 md:left-0 md:right-0"><div class="z-10 mx-auto max-w-3xl"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></div></div></div></div></div><!--$--><!--/$--></div><script>$RC("B:1","S:1")</script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n"])</script><script>self.__next_f.push([1,"2:I[49138,[\"9453\",\"static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"8970\",\"static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"1585\",\"static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"7963\",\"static/chunks/7963-2c20578d72f6c7cc.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"1265\",\"static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"9885\",\"static/chunks/9885-b57089f03806c3b8.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"659\",\"static/chunks/659-ee9e1e775e30dcef.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"7177\",\"static/chunks/app/layout-0537c2076823e553.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\"],\"RootProvider\"]\n"])</script><script>self.__next_f.push([1,"3:I[85341,[],\"\"]\n4:I[90025,[],\"\"]\n7:I[41012,[],\"ClientPageRoot\"]\n"])</script><script>self.__next_f.push([1,"8:I[57456,[\"9453\",\"static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"8970\",\"static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"1585\",\"static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"4129\",\"static/chunks/7bf36345-1ac10ec2f0e0c88f.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"2545\",\"static/chunks/c16f53c3-b390b6f98a69dcec.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"7963\",\"static/chunks/7963-2c20578d72f6c7cc.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"1265\",\"static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"2602\",\"static/chunks/2602-735d398a11941702.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"9885\",\"static/chunks/9885-b57089f03806c3b8.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"4336\",\"static/chunks/4336-624d5ae6f4cc1cc7.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"659\",\"static/chunks/659-ee9e1e775e30dcef.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"8461\",\"static/chunks/8461-369a9f0c48ea2626.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"7198\",\"static/chunks/7198-985a49f2b6072d25.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"6230\",\"static/chunks/6230-12a4ab40267b069f.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"4429\",\"static/chunks/4429-92c77bf2d3e82e1c.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"9976\",\"static/chunks/9976-726fc148e3fe0730.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"1481\",\"static/chunks/1481-e8a771bf9a2ec0bd.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"3285\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/%5B%5B...wikiRoutes%5D%5D/page-82e5155a284526b9.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"b:I[15104,[],\"OutletBoundary\"]\nd:I[94777,[],\"AsyncMetadataOutlet\"]\nf:I[15104,[],\"ViewportBoundary\"]\n11:I[15104,[],\"MetadataBoundary\"]\n12:\"$Sreact.suspense\"\n14:I[34431,[],\"\"]\n:HL[\"/_next/static/media/4cf2300e9c8272f7-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/93f479601ee12b01-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/de70bee13400563f.css?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"style\"]\n:HL[\"/_next/static/css/b54d92a85d180acc.css?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"AZ4Bl7rle-5-Wxk90moD1\",\"p\":\"\",\"c\":[\"\",\"ModelTC\",\"lightx2v\",\"4.5-vae-system\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"org\",\"ModelTC\",\"d\"],{\"children\":[[\"repo\",\"lightx2v\",\"d\"],{\"children\":[[\"wikiRoutes\",\"4.5-vae-system\",\"oc\"],{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/de70bee13400563f.css?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/b54d92a85d180acc.css?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{}],[\"$\",\"body\",null,{\"className\":\"__variable_188709 font-geist-sans relative min-h-screen __variable_9a8899 bg-background antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}]]}],{\"children\":[[\"org\",\"ModelTC\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"repo\",\"lightx2v\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,\"$L5\"]}],{\"children\":[[\"wikiRoutes\",\"4.5-vae-system\",\"oc\"],[\"$\",\"$1\",\"c\",{\"children\":[null,\"$L6\"]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L7\",null,{\"Component\":\"$8\",\"searchParams\":{},\"params\":{\"org\":\"ModelTC\",\"repo\":\"lightx2v\",\"wikiRoutes\":[\"4.5-vae-system\"]},\"promises\":[\"$@9\",\"$@a\"]}],null,[\"$\",\"$Lb\",null,{\"children\":[\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]],[\"$\",\"$L11\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$12\",null,{\"fallback\":null,\"children\":\"$L13\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$14\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"9:{}\na:\"$0:f:0:1:2:children:2:children:2:children:2:children:1:props:children:0:props:params\"\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nc:null\n"])</script><script>self.__next_f.push([1,"15:I[13550,[\"9453\",\"static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"8970\",\"static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"1585\",\"static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"7963\",\"static/chunks/7963-2c20578d72f6c7cc.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"1265\",\"static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"2602\",\"static/chunks/2602-735d398a11941702.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"4336\",\"static/chunks/4336-624d5ae6f4cc1cc7.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"25\",\"static/chunks/25-9f305b682cea7558.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"7391\",\"static/chunks/7391-25ac5affe6ef6a7d.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"2512\",\"static/chunks/2512-fc2ad2e11d161d20.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"6375\",\"static/chunks/6375-6b1e677037ddc4d1.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"9437\",\"static/chunks/9437-00f165dda9ad3a42.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"2933\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-1d48b1e2f124e49f.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\"],\"HeaderWrapperWithSuspense\"]\n"])</script><script>self.__next_f.push([1,"16:I[82188,[\"9453\",\"static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"8970\",\"static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"1585\",\"static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"7963\",\"static/chunks/7963-2c20578d72f6c7cc.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"1265\",\"static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"2602\",\"static/chunks/2602-735d398a11941702.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"4336\",\"static/chunks/4336-624d5ae6f4cc1cc7.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"25\",\"static/chunks/25-9f305b682cea7558.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"7391\",\"static/chunks/7391-25ac5affe6ef6a7d.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"2512\",\"static/chunks/2512-fc2ad2e11d161d20.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"6375\",\"static/chunks/6375-6b1e677037ddc4d1.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"9437\",\"static/chunks/9437-00f165dda9ad3a42.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\",\"2933\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-1d48b1e2f124e49f.js?dpl=dpl_4bHaQfioPxUkem84st99xaU8n7S2\"],\"WikiContextProvider\"]\n"])</script><script>self.__next_f.push([1,"17:T4bce,"])</script><script>self.__next_f.push([1,"# Overview\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [README_zh.md](README_zh.md)\n- [lightx2v_platform/README.md](lightx2v_platform/README.md)\n- [lightx2v_platform/README_zh.md](lightx2v_platform/README_zh.md)\n- [scripts/hunyuan_video_15/README.md](scripts/hunyuan_video_15/README.md)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nLightX2V is an advanced lightweight inference framework for image and video generation models. It provides a unified platform for running state-of-the-art generative models with extreme performance optimization and memory efficiency. The framework supports diverse generation tasks including text-to-video (T2V), image-to-video (I2V), speech-to-video (S2V), text-to-image (T2I), and image-to-image editing (I2I).\n\n**X2V** denotes the transformation of arbitrary input modalities (X) into vision outputs (Vision), encompassing text, images, audio, and their combinations.\n\nThis page provides a high-level overview of the LightX2V framework architecture, supported models, and key capabilities. For detailed information on specific topics:\n- Installation and setup: see [Getting Started](#2)\n- User interfaces and APIs: see [User Interfaces](#3)\n- Performance optimization techniques: see [Performance Optimization](#6)\n- Specific model runners: see [Model Runners and Tasks](#5)\n- Hardware platform support: see [Hardware Platform Support](#1.3)\n\nSources: [README.md:1-19](), [README_zh.md:1-19]()\n\n## System Architecture\n\nLightX2V is organized into five distinct architectural layers, each serving a specific purpose in the inference pipeline:\n\n```mermaid\ngraph TB\n    subgraph \"Layer 1: Entry Points\"\n        Python[\"LightX2VPipeline\u003cbr/\u003ePython API\"]\n        CLI[\"lightx2v.infer\u003cbr/\u003eCommand Line\"]\n        Gradio[\"gradio_demo.py\u003cbr/\u003eWeb Interface\"]\n        ComfyUI[\"ComfyUI Nodes\u003cbr/\u003eWorkflow Integration\"]\n        Server[\"TorchrunInferenceWorker\u003cbr/\u003eDistributed Server\"]\n    end\n    \n    subgraph \"Layer 2: Core Framework\"\n        Pipeline[\"LightX2VPipeline\u003cbr/\u003e- create_generator()\u003cbr/\u003e- enable_offload()\u003cbr/\u003e- enable_quantize()\u003cbr/\u003e- generate()\"]\n        ConfigMgmt[\"Configuration System\u003cbr/\u003e- set_config()\u003cbr/\u003e- get_auto_config_dict()\u003cbr/\u003e- InputInfo dataclasses\"]\n        Registry[\"RUNNER_REGISTER\u003cbr/\u003eRunner factory pattern\"]\n    end\n    \n    subgraph \"Layer 3: Model Runners\"\n        WanAudio[\"WanAudioRunner\u003cbr/\u003eS2V audio-to-video\"]\n        Wan22[\"Wan22AudioRunner\u003cbr/\u003eWAN 2.2 specific\"]\n        WanDistill[\"WanDistillRunner\u003cbr/\u003e4-step distilled\"]\n        QwenImage[\"QwenImageRunner\u003cbr/\u003eT2I/I2I generation\"]\n        ZImage[\"ZImageRunner\u003cbr/\u003eFast T2I/I2I\"]\n        Hunyuan[\"HunyuanVideo15Runner\u003cbr/\u003eT2V/I2V\"]\n        LTX[\"LTX2Runner\u003cbr/\u003eAudio+Video synthesis\"]\n    end\n    \n    subgraph \"Layer 4: Model Architecture Components\"\n        Transformer[\"Transformer Models\u003cbr/\u003eWanModel, QwenImageModel, etc\"]\n        Encoders[\"Input Encoders\u003cbr/\u003eT5EncoderV2, QwenVLEncoder\u003cbr/\u003eSekoAudioEncoder\"]\n        VAEs[\"VAE Systems\u003cbr/\u003eWanVAE, QwenVAE\u003cbr/\u003eencode/decode\"]\n        Schedulers[\"Schedulers\u003cbr/\u003eEulerScheduler, WanStepDistillScheduler\"]\n    end\n    \n    subgraph \"Layer 5: Optimization \u0026 Hardware\"\n        Quantization[\"Quantization\u003cbr/\u003eMMWeightQuant\u003cbr/\u003eINT8/FP8/NVFP4\"]\n        Offloading[\"CPU Offloading\u003cbr/\u003eWeightAsyncStreamManager\u003cbr/\u003eblock/phase granularity\"]\n        Caching[\"Feature Caching\u003cbr/\u003eTea/Mag/Taylor/Ada strategies\"]\n        Parallel[\"Distributed Processing\u003cbr/\u003eCFG/Tensor/Sequence parallel\"]\n        Platform[\"lightx2v_platform\u003cbr/\u003eMulti-backend support\"]\n    end\n    \n    Python --\u003e Pipeline\n    CLI --\u003e Pipeline\n    Gradio --\u003e Pipeline\n    ComfyUI --\u003e Pipeline\n    Server --\u003e Pipeline\n    \n    Pipeline --\u003e ConfigMgmt\n    Pipeline --\u003e Registry\n    \n    Registry --\u003e WanAudio\n    Registry --\u003e Wan22\n    Registry --\u003e WanDistill\n    Registry --\u003e QwenImage\n    Registry --\u003e ZImage\n    Registry --\u003e Hunyuan\n    Registry --\u003e LTX\n    \n    WanAudio --\u003e Transformer\n    WanDistill --\u003e Transformer\n    QwenImage --\u003e Transformer\n    \n    Transformer --\u003e Encoders\n    Transformer --\u003e VAEs\n    Transformer --\u003e Schedulers\n    \n    Transformer --\u003e Quantization\n    Transformer --\u003e Offloading\n    Transformer --\u003e Caching\n    Pipeline --\u003e Parallel\n    \n    Offloading --\u003e Platform\n    Parallel --\u003e Platform\n```\n\n**Layer 1: Entry Points** - Five different interfaces provide access to the framework, from direct Python API usage to production-ready distributed servers.\n\n**Layer 2: Core Framework** - The `LightX2VPipeline` class orchestrates the entire generation workflow, managing configuration, runner selection, and optimization application. The `RUNNER_REGISTER` implements a factory pattern for model-specific runner instantiation.\n\n**Layer 3: Model Runners** - Task-specific runners inherit from `BaseRunner` and `DefaultRunner` to implement model-specific inference logic. Each runner handles its model family's unique requirements while maintaining a consistent interface.\n\n**Layer 4: Model Components** - Reusable components including transformer models, input encoders (text, image, audio), VAE encoder/decoders, and diffusion schedulers. These components are shared across different runners where applicable.\n\n**Layer 5: Optimization \u0026 Hardware** - Cross-cutting optimization features (quantization, offloading, caching, parallelism) and hardware abstraction through `lightx2v_platform` enable deployment across diverse hardware backends.\n\nSources: [README.md:1-340](), High-level Diagram 1\n\n## Supported Model Families and Tasks\n\nLightX2V integrates multiple state-of-the-art model families, each optimized for specific generation tasks:\n\n| Model Family | Primary Tasks | Key Features | Runner Classes |\n|--------------|--------------|--------------|----------------|\n| **WAN 2.1/2.2** | T2V, I2V, S2V | Audio-driven video, MoE architecture, distilled variants | `WanRunner`, `WanAudioRunner`, `Wan22AudioRunner`, `WanDistillRunner` |\n| **Qwen-Image** | T2I, I2I | Vision-language models, layered generation, image editing | `QwenImageRunner` |\n| **Z-Image** | T2I, I2I | Fast turbo variants, Qwen3 text encoder | `ZImageRunner` |\n| **HunyuanVideo 1.5** | T2V, I2V | 720p support, 4-step distillation, LightTAE support | `HunyuanVideo15Runner` |\n| **LTX-2** | T2AV, I2AV | Simultaneous audio+video generation, multi-stage pipeline | `LTX2Runner` |\n| **LongCat** | T2I, I2I | High-resolution image generation | `LongCatImageRunner` |\n| **CausVid** | T2V | Autoregressive generation with KV cache | `WanCausVidRunner` |\n\n### Task Type Definitions\n\n- **T2V**: Text-to-Video - Generate video sequences from text descriptions\n- **I2V**: Image-to-Video - Animate static images into video sequences  \n- **S2V**: Speech-to-Video - Generate videos driven by audio/speech inputs\n- **T2I**: Text-to-Image - Generate images from text descriptions\n- **I2I**: Image-to-Image - Edit or transform existing images\n- **T2AV**: Text-to-Audio+Video - Generate synchronized audio and video from text\n- **I2AV**: Image-to-Audio+Video - Generate audio+video from image conditioning\n\nThe WAN family is the most extensively featured, particularly the audio-to-video (S2V) capability which is unique among open-source models. The framework also provides distilled and quantized variants across multiple model families, enabling 4-step inference instead of the standard 40-50 steps.\n\nSources: [README.md:207-234](), [README_zh.md:206-233](), High-level Diagram 2\n\n## Core Framework Components\n\n```mermaid\ngraph LR\n    subgraph \"LightX2VPipeline API\"\n        Init[\"__init__()\u003cbr/\u003emodel_path, model_cls, task\"]\n        CreateGen[\"create_generator()\u003cbr/\u003eheight, width, steps, guidance_scale\"]\n        EnableOpt[\"Optimization Methods\u003cbr/\u003eenable_offload()\u003cbr/\u003eenable_quantize()\u003cbr/\u003eenable_lightvae()\"]\n        Generate[\"generate()\u003cbr/\u003eprompt, seed, image_path, audio_path\"]\n    end\n    \n    subgraph \"Configuration Layer\"\n        SetConfig[\"set_config()\u003cbr/\u003eGlobal config override\"]\n        AutoConfig[\"get_auto_config_dict()\u003cbr/\u003eAuto-calculate optimal params\"]\n        InputInfo[\"InputInfo Classes\u003cbr/\u003eT2VInputInfo\u003cbr/\u003eI2VInputInfo\u003cbr/\u003eS2VInputInfo\"]\n    end\n    \n    subgraph \"Runner Registry\"\n        Register[\"RUNNER_REGISTER\u003cbr/\u003e@RUNNER_REGISTER.register()\"]\n        GetRunner[\"get_runner()\u003cbr/\u003eFactory method\"]\n        RunnerBase[\"BaseRunner\u003cbr/\u003e- init_modules()\u003cbr/\u003e- run_pipeline()\"]\n    end\n    \n    Init --\u003e SetConfig\n    CreateGen --\u003e AutoConfig\n    CreateGen --\u003e GetRunner\n    GetRunner --\u003e Register\n    Register --\u003e RunnerBase\n    Generate --\u003e InputInfo\n    Generate --\u003e RunnerBase\n```\n\n### LightX2VPipeline Class\n\nThe `LightX2VPipeline` serves as the primary user-facing API. It provides a fluent interface for configuring and executing inference:\n\n**Initialization Methods:**\n- `__init__(model_path, model_cls, task)` - Initialize with model location and type\n- `create_generator(config_json=None, **kwargs)` - Configure generation parameters\n\n**Optimization Methods:**\n- `enable_offload(cpu_offload, offload_granularity)` - Configure CPU/disk offloading\n- `enable_quantize(dit_quantized, text_encoder_quantized)` - Enable weight quantization\n- `enable_lightvae(use_tae, use_lightvae)` - Use lightweight VAE variants\n- `enable_lora(lora_path, lora_alpha)` - Apply LoRA adapters\n\n**Execution Methods:**\n- `generate(prompt, seed, image_path, audio_path, **kwargs)` - Execute generation\n- `switch_lora(lora_path, lora_alpha)` - Dynamically switch LoRA at runtime\n\n### Runner System\n\nThe runner system uses a registry pattern (`RUNNER_REGISTER`) to map model types to their corresponding runner implementations. Each runner implements:\n\n- `init_modules()` - Load and initialize model components\n- `run_pipeline()` - Execute the complete inference pipeline\n- `run_input_encoder()` - Process inputs (text, image, audio)\n- `run_dit()` - Execute diffusion transformer denoising\n- `run_vae_decoder()` - Decode latents to pixels\n\nSources: [README.md:137-197](), High-level Diagrams 1, 3, 4\n\n## Key Optimization Features\n\nLightX2V achieves approximately **20-25x speedup** through a comprehensive suite of optimizations that can be applied independently or combined:\n\n### Performance Benchmarks\n\n**Cross-Framework Comparison (H100, Wan2.1-I2V-14B-480P):**\n\n| Framework | Single GPU | 8 GPUs | Speedup |\n|-----------|------------|--------|---------|\n| Diffusers (baseline) | 9.77s/it | - | 1.0x |\n| xDiT | 8.93s/it | 2.70s/it | 1.1x |\n| FastVideo | 7.35s/it | 2.94s/it | 1.3x |\n| SGL-Diffusion | 6.13s/it | 1.19s/it | 1.6-2.5x |\n| **LightX2V** | **5.18s/it** | **0.75s/it** | **1.9-3.9x** |\n| **LightX2V + FP8** | - | **0.35s/it** | **Up to 28x** |\n\n**Combined with 4-step distillation**, the total speedup reaches approximately **~200x** compared to baseline 50-step inference.\n\n### Optimization Categories\n\n**1. Step Distillation**\n- Reduces 40-50 diffusion steps to 4 steps\n- Eliminates CFG requirement (Classifier-Free Guidance)\n- Available for WAN, Qwen-Image, HunyuanVideo families\n- Achieves ~10-12x speedup from step reduction alone\n\n**2. Quantization**\n- `w8a8-int8`: INT8 weights and activations\n- `w8a8-fp8`: FP8 precision (NVIDIA H100+)\n- `w4a4-nvfp4`: 4-bit NVIDIA FP4 format\n- `MXFP4/6/8`: Microscaling formats\n- Conversion tools in `tools/convert/converter.py`\n\n**3. Memory Offloading**\n- Disk  CPU  GPU three-tier architecture\n- Granularities: model-level, block-level, phase-level\n- `WeightAsyncStreamManager` with double-buffering\n- Enables 14B models on 8GB VRAM + 16GB RAM\n\n**4. Attention Operators**\n- Sage Attention (sage_attn2)\n- Flash Attention 2/3 (flash_attn2, flash_attn3)\n- Radial Attention for sparse patterns\n- Neighborhood attention for local focus\n\n**5. Feature Caching**\n- Tea Cache: cache early denoising steps\n- Mag Cache: cache magnitude features\n- Taylor Cache: Taylor expansion approximation\n- Ada Cache: adaptive caching strategies\n\n**6. Distributed Processing**\n- CFG Parallelism: separate GPUs for conditional/unconditional\n- Tensor Parallelism: split model weights across GPUs\n- Sequence Parallelism: Ulysses and Ring attention\n- Pipeline Parallelism: stage-based execution\n\nSources: [README.md:72-114](), [README.md:254-272](), High-level Diagram 5\n\n## Hardware Platform Support\n\nThe `lightx2v_platform` module provides hardware abstraction, enabling deployment across diverse compute backends:\n\n```mermaid\ngraph TB\n    subgraph \"Application Layer\"\n        App[\"LightX2V Framework\"]\n    end\n    \n    subgraph \"Abstraction Layer\"\n        Platform[\"lightx2v_platform\u003cbr/\u003eBackend-agnostic APIs\"]\n    end\n    \n    subgraph \"Hardware Backends\"\n        NVIDIA[\"NVIDIA CUDA\u003cbr/\u003eRTX 30/40/50, H100, A100\"]\n        Cambricon[\"Cambricon MLU590\u003cbr/\u003e\"]\n        MetaX[\"MetaX C500\u003cbr/\u003e\"]\n        Hygon[\"Hygon DCU\u003cbr/\u003e\"]\n        Ascend[\"Ascend 910B\u003cbr/\u003e\"]\n        AMD[\"AMD ROCm\u003cbr/\u003eMI series\"]\n        MThreads[\"MThreads MUSA\u003cbr/\u003e\"]\n        Enflame[\"Enflame S60 GCU\u003cbr/\u003e\"]\n    end\n    \n    App --\u003e Platform\n    Platform --\u003e NVIDIA\n    Platform --\u003e Cambricon\n    Platform --\u003e MetaX\n    Platform --\u003e Hygon\n    Platform --\u003e Ascend\n    Platform --\u003e AMD\n    Platform --\u003e MThreads\n    Platform --\u003e Enflame\n```\n\n### Supported Hardware Platforms\n\n| Platform | Device Types | Status | Docker Images |\n|----------|-------------|--------|---------------|\n| **NVIDIA CUDA** | RTX 30/40/50, A100, H100 | Primary target | cu124, cu128 |\n| **Cambricon** | MLU590 | Supported | Available |\n| **MetaX** | C500 | Supported | Available |\n| **Hygon** | DCU | Supported | Available |\n| **Ascend** | 910B | Supported | Available |\n| **AMD** | ROCm MI series | Supported | Available |\n| **MThreads** | MUSA | Supported | Available |\n| **Enflame** | S60 GCU | Supported | Available |\n\nThe platform abstraction layer allows model developers to write hardware-agnostic code while the `lightx2v_platform` module handles backend-specific implementations. This architecture is particularly important for Chinese domestic chip adoption where multiple vendors provide AI accelerators.\n\nDocker environments for each platform are available in `dockerfiles/platforms/`, and usage scripts are provided in `scripts/platforms/`.\n\nSources: [lightx2v_platform/README.md:1-19](), [lightx2v_platform/README_zh.md:1-20](), [README.md:43-66](), High-level Diagram 6\n\n## Deployment Interfaces\n\nLightX2V provides five distinct interfaces for different deployment scenarios:\n\n### 1. Python API (LightX2VPipeline)\nDirect Python integration for programmatic usage and custom workflows. Provides full control over all configuration parameters.\n\nExample initialization:\n```python\nfrom lightx2v import LightX2VPipeline\npipe = LightX2VPipeline(\n    model_path=\"/path/to/model\",\n    model_cls=\"wan2.2_moe\",\n    task=\"i2v\"\n)\n```\n\n### 2. Command Line Interface\nShell script execution via `lightx2v.infer` command, accepting JSON configuration files or command-line arguments for batch processing and automation.\n\n### 3. Gradio Web Interface\nInteractive web UI defined in `app/gradio_demo.py`. Provides user-friendly controls for model selection, parameter tuning, and result visualization. Configurable via `run_gradio.sh/bat` scripts.\n\n### 4. ComfyUI Integration\nNode-based workflow integration for complex multi-stage generation pipelines. Enables visual programming of generation workflows with LightX2V models as nodes.\n\n### 5. Distributed Server (TorchrunInferenceWorker)\nProduction-ready HTTP server for multi-worker distributed inference. Supports task queuing, load balancing, and dynamic LoRA serving from directories.\n\nEach interface shares the same underlying `LightX2VPipeline` infrastructure while providing different levels of abstraction and control appropriate to the use case.\n\nSources: [README.md:238-252](), High-level Diagram 1\n\n## Inference Pipeline Flow\n\nAll model runners follow a standardized three-stage inference pipeline:\n\n```mermaid\ngraph TB\n    Start[\"User Input\u003cbr/\u003eprompt, seed, config\"]\n    \n    subgraph \"Stage 1: Input Processing\"\n        LoadConfig[\"Load Configuration\u003cbr/\u003eset_config, auto_calc_config\"]\n        InitRunner[\"Initialize Runner\u003cbr/\u003eRUNNER_REGISTER.get(model_cls)\"]\n        InitModules[\"runner.init_modules()\u003cbr/\u003eLoad transformer, encoders, VAE\"]\n        EncodeInputs[\"runner.run_input_encoder()\u003cbr/\u003eEncode text/image/audio\"]\n    end\n    \n    subgraph \"Stage 2: Diffusion Loop\"\n        PrepareLatents[\"scheduler.prepare()\u003cbr/\u003eInitialize latent noise\"]\n        LoopStart{\"For step in steps\"}\n        StepPre[\"scheduler.step_pre()\u003cbr/\u003eCalculate timestep\"]\n        ModelInfer[\"model.infer()\u003cbr/\u003ePredict noise\"]\n        StepPost[\"scheduler.step_post()\u003cbr/\u003eUpdate latents\"]\n        LoopEnd{\"Done?\"}\n    end\n    \n    subgraph \"Stage 3: Decoding\"\n        VAEDecode[\"runner.run_vae_decoder()\u003cbr/\u003eDecode latents to pixels\"]\n        SaveOutput[\"Save output\u003cbr/\u003evideo/image file\"]\n    end\n    \n    Start --\u003e LoadConfig\n    LoadConfig --\u003e InitRunner\n    InitRunner --\u003e InitModules\n    InitModules --\u003e EncodeInputs\n    EncodeInputs --\u003e PrepareLatents\n    PrepareLatents --\u003e LoopStart\n    LoopStart --\u003e StepPre\n    StepPre --\u003e ModelInfer\n    ModelInfer --\u003e StepPost\n    StepPost --\u003e LoopEnd\n    LoopEnd --\u003e|No| LoopStart\n    LoopEnd --\u003e|Yes| VAEDecode\n    VAEDecode --\u003e SaveOutput\n```\n\n### Stage 1: Input Processing\n- Configuration loading and validation via `set_config()`\n- Runner instantiation from `RUNNER_REGISTER`\n- Module loading: transformer, text/image/audio encoders, VAE\n- Input encoding to latent representations\n\n### Stage 2: Diffusion Loop\nThe core denoising process managed by scheduler classes:\n- `prepare()`: Initialize latent tensors with noise\n- `step_pre()`: Calculate timestep embeddings\n- `model.infer()`: Transformer predicts noise\n- `step_post()`: Update latents using predicted noise\n\nFor distilled models, this loop executes only 4 iterations. For standard models, 40-50 iterations.\n\n### Stage 3: Decoding\n- `run_vae_decoder()`: Convert latents back to pixel space\n- Post-processing and file I/O\n- Optional frame interpolation for higher FPS\n\nOptimizations (quantization, offloading, caching, parallelism) are transparently applied throughout this pipeline without changing the fundamental flow.\n\nSources: High-level Diagram 4, [README.md:137-197]()\n\n## Model Distribution and Storage\n\nModels are distributed through multiple channels:\n\n| Distribution Channel | Region | Model Types | Purpose |\n|---------------------|---------|-------------|---------|\n| **HuggingFace Hub** | Global | All official models | Primary distribution |\n| **ModelScope** | China | All official models | China mirror |\n| **Quark Cloud** | China | Windows packages | One-click installers |\n| **lightx2v HuggingFace** | Global | Distilled, quantized, LoRA | Optimized variants |\n\nOfficial models include base transformer weights, text encoders (T5, CLIP, Qwen), image encoders, VAEs, and schedulers. The `lightx2v` HuggingFace organization hosts performance-optimized variants:\n- 4-step distilled models (Wan2.1/2.2, HunyuanVideo)\n- Quantized models (INT8, FP8, NVFP4)\n- LoRA adapters for distillation and style control\n- Lightweight VAE variants (LightTAE)\n\nModels should be stored on SSD for optimal performance when using offloading features. The framework supports both local path specification and automatic download from model hubs.\n\nSources: [README.md:207-237](), [scripts/hunyuan_video_15/README.md:16-18]()"])</script><script>self.__next_f.push([1,"18:T63eb,"])</script><script>self.__next_f.push([1,"# Key Features and Optimization Techniques\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [README_zh.md](README_zh.md)\n- [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json](configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json)\n- [docs/EN/source/getting_started/quickstart.md](docs/EN/source/getting_started/quickstart.md)\n- [docs/ZH_CN/source/getting_started/quickstart.md](docs/ZH_CN/source/getting_started/quickstart.md)\n- [lightx2v/__init__.py](lightx2v/__init__.py)\n- [lightx2v/common/ops/attn/utils/ring_comm.py](lightx2v/common/ops/attn/utils/ring_comm.py)\n- [lightx2v/common/ops/mm/triton_kernels.py](lightx2v/common/ops/mm/triton_kernels.py)\n- [lightx2v_platform/README.md](lightx2v_platform/README.md)\n- [lightx2v_platform/README_zh.md](lightx2v_platform/README_zh.md)\n- [pyproject.toml](pyproject.toml)\n- [scripts/hunyuan_video_15/README.md](scripts/hunyuan_video_15/README.md)\n- [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh](scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis page catalogs the optimization techniques implemented in LightX2V and documents their performance characteristics with concrete benchmark data. These optimizations enable efficient video/image generation inference across diverse hardware configurations, from high-end server GPUs to consumer-grade devices with limited VRAM.\n\nFor architectural details on how these optimizations integrate into the overall system, see [System Architecture Overview](#1.2). For hardware-specific platform support, see [Hardware Platform Support](#1.3). For practical deployment guidance using these optimizations, see sections 2 and 6.\n\n## Optimization Categories Overview\n\nLightX2V implements seven independent optimization categories that compose multiplicatively to achieve up to **~20x speedup** for single-GPU inference and **3.9x speedup** compared to other frameworks on 8-GPU configurations. The optimization categories are:\n\n| Optimization Category | Primary Impact | Memory Reduction | Speed Improvement | Implementation Location |\n|----------------------|----------------|------------------|-------------------|------------------------|\n| **Model Precision Quantization** | Memory footprint | 2-8x | 1.1-1.3x | `lightx2v/common/ops/mm/` |\n| **Attention Operators** | Compute efficiency | 1.5-2x (memory) | 1.5-2x | `lightx2v/common/ops/attn/` |\n| **Memory Management** | VRAM requirements | 4-100x | 0.5-1x (tradeoff) | `lightx2v/common/offload/` |\n| **Feature Caching** | Redundant computation | Minimal | 1.2-1.3x | Model-specific runners |\n| **Parallel Inference** | Multi-GPU throughput | N/A | 2-8x (scaling) | `lightx2v/common/ops/attn/utils/` |\n| **Step Distillation** | Iteration count | N/A | ~25x | Distilled model variants |\n| **Lightweight VAE** | Decode latency | 2-3x (VAE only) | 2-4x (decode) | Autoencoder models |\n\nSources: [README.md:250-268](), [README.md:68-109]()\n\n## Model Precision Quantization\n\n### Supported Quantization Formats\n\nLightX2V supports six quantization schemes with multiple backend implementations for hardware optimization:\n\n| Format | Bit Width | Backend Options | Memory Reduction | Typical Use Case |\n|--------|-----------|-----------------|------------------|------------------|\n| **FP32** | 32-bit | PyTorch native | 1x (baseline) | Development/debugging |\n| **FP16/BF16** | 16-bit | PyTorch native | 2x | Standard inference |\n| **INT8 (w8a8)** | 8-bit | Triton, VLLM, Q8F | 4x | Balanced quality/speed |\n| **FP8 (E4M3/E5M2)** | 8-bit | Triton, SGL, TorchAO | 2-4x | High-quality quantization |\n| **NVFP4 (w4a4)** | 4-bit | Q8F kernels | 8x | Extreme compression (QAT) |\n| **MxFP4/6/8** | 4-8 bit | Microsoft formats | 4-8x | Block-scaled quantization |\n\n### Quantization Backend Implementations\n\n```mermaid\ngraph TB\n    subgraph \"Quantization API Layer\"\n        QLinear[\"q_linear abstraction\"]\n        QuantConfig[\"Quantization Config\u003cbr/\u003equant_scheme parameter\"]\n    end\n    \n    subgraph \"Backend Implementations\"\n        Triton[\"Triton Kernels\u003cbr/\u003eint8_quantize_triton\u003cbr/\u003efp8_quantize_triton\"]\n        VLLM[\"VLLM Kernels\u003cbr/\u003eFP8 operations\"]\n        SGL[\"SGL Kernels\u003cbr/\u003efp8-sgl scheme\"]\n        TorchAO[\"TorchAO\u003cbr/\u003eAO quantization\"]\n        Q8F[\"Q8F Kernels\u003cbr/\u003eAda architecture\"]\n    end\n    \n    subgraph \"Quantization Operations\"\n        INT8Quant[\"INT8 Quantization\u003cbr/\u003eper-channel symmetric\"]\n        FP8Quant[\"FP8 Quantization\u003cbr/\u003eE4M3 format\u003cbr/\u003eFP8_MAX=448.0\"]\n        NVFP4Quant[\"NVFP4 QAT\u003cbr/\u003e4-bit activations\"]\n    end\n    \n    QLinear --\u003e QuantConfig\n    QuantConfig --\u003e Triton\n    QuantConfig --\u003e VLLM\n    QuantConfig --\u003e SGL\n    QuantConfig --\u003e TorchAO\n    QuantConfig --\u003e Q8F\n    \n    Triton --\u003e INT8Quant\n    Triton --\u003e FP8Quant\n    VLLM --\u003e FP8Quant\n    SGL --\u003e FP8Quant\n    Q8F --\u003e NVFP4Quant\n```\n\nSources: [lightx2v/common/ops/mm/triton_kernels.py:1-744](), [README.md:257-260]()\n\n### Triton Quantization Kernels\n\nThe system implements custom Triton kernels for efficient INT8 and FP8 quantization:\n\n**INT8 Per-Channel Symmetric Quantization:**\n- Implementation: [lightx2v/common/ops/mm/triton_kernels.py:14-35]()\n- Scale computation: `x_scale = 127.0 / max(abs(x))`\n- Rounding with bias: `x_scaled += (0.5 * sign(x_scaled))`\n\n**FP8 E4M3 Quantization:**\n- Implementation: [lightx2v/common/ops/mm/triton_kernels.py:38-65]()\n- Max value clamping: `FP8_MAX = 448.0`\n- Scale computation: `x_scale = absmax / FP8_MAX`\n- Clamping: `x_scaled = clamp(x_scaled, -FP8_MAX, FP8_MAX)`\n\n**Fused INT8/FP8 GEMM with Bias:**\n- INT8 GEMM+Bias: [lightx2v/common/ops/mm/triton_kernels.py:100-268]()\n- FP8 GEMM+Bias: [lightx2v/common/ops/mm/triton_kernels.py:458-591]()\n- Optional GELU fusion: `fuse_gelu` parameter\n- Autotuning configurations for block sizes: `BLOCK_M`, `BLOCK_N`, `BLOCK_K`\n\n### Performance Impact\n\nQuantization performance on Wan2.1-I2V-14B (40 steps, 81 frames):\n\n| Configuration | GPU | Step Time | Memory Usage | Speedup vs FP32 |\n|--------------|-----|-----------|--------------|-----------------|\n| FP32 baseline | H100 | ~1.2s/it | ~60GB | 1x |\n| FP16/BF16 | H100 | ~0.9s/it | ~30GB | 1.3x |\n| FP8 (w8a8) | H100 | 0.35s/it | ~15GB | 3.4x |\n| INT8 (w8a8) | RTX 4090 | 2.35s/it | ~12GB | - |\n\nCombined with CFG-free distillation (4 steps): **0.35s/it  4 steps = 1.4s total** vs **baseline ~50s total** = **~36x end-to-end speedup**.\n\nSources: [README.md:100-108]()\n\n## Attention Operators\n\n### Operator Comparison\n\nLightX2V integrates six attention operator variants optimized for different workloads:\n\n| Operator | Backend | Memory Complexity | Speedup | Hardware Requirements | Configuration |\n|----------|---------|-------------------|---------|----------------------|---------------|\n| **Vanilla Attention** | PyTorch | O(n) | 1x | Any | `attn_mode=\"default\"` |\n| **Flash Attention 2** | CUDA/Triton | O(n) | 2-3x | Ampere+ | `attn_mode=\"flash_attn2\"` |\n| **Flash Attention 3** | CUDA (Hopper) | O(n) | 3-4x | Hopper (H100) | `attn_mode=\"flash_attn3\"` |\n| **SageAttention 2** | Triton | O(n) with INT8 KV | 2x throughput | Ampere+ | `attn_mode=\"sage_attn2\"` |\n| **Neighborhood Attention** | Custom | O(n  local) | 1.5-2x | Any | `self_attn_1_type=\"nbhd_attn\"` |\n| **Radial Attention** | Custom | O(n  pattern) | 1.3-1.8x | Any | `attn_mode=\"radial\"` |\n\n### SageAttention Architecture\n\nSageAttention achieves 2x throughput improvement through INT8 KV cache quantization:\n\n```mermaid\ngraph LR\n    subgraph \"Standard Attention Flow\"\n        Q1[\"Query (FP16)\"]\n        K1[\"Key (FP16)\"]\n        V1[\"Value (FP16)\"]\n        Attn1[\"Attention\u003cbr/\u003eFP16 GEMM\"]\n        Q1 --\u003e Attn1\n        K1 --\u003e Attn1\n        V1 --\u003e Attn1\n    end\n    \n    subgraph \"SageAttention Flow\"\n        Q2[\"Query (FP16)\"]\n        K2[\"Key (FP16)\"]\n        V2[\"Value (FP16)\"]\n        KQuant[\"Quantize KINT8\u003cbr/\u003eper-channel\"]\n        VQuant[\"Quantize VINT8\u003cbr/\u003eper-channel\"]\n        K2INT8[\"K (INT8)\"]\n        V2INT8[\"V (INT8)\"]\n        Attn2[\"Attention\u003cbr/\u003eINT8 GEMM\u003cbr/\u003e2x faster\"]\n        Dequant[\"Dequantize output\"]\n        \n        K2 --\u003e KQuant\n        V2 --\u003e VQuant\n        KQuant --\u003e K2INT8\n        VQuant --\u003e V2INT8\n        Q2 --\u003e Attn2\n        K2INT8 --\u003e Attn2\n        V2INT8 --\u003e Attn2\n        Attn2 --\u003e Dequant\n    end\n```\n\n**Key Features:**\n- KV cache quantization: Per-channel symmetric INT8\n- Query remains FP16 for precision\n- 50% memory reduction for KV cache\n- Triton kernel implementation for efficiency\n\nSources: [README.md:255]()\n\n### Neighborhood Attention Configuration\n\nNeighborhood attention applies spatially-localized attention patterns for video generation:\n\nConfiguration example from [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:8-11]():\n```json\n{\n    \"self_attn_1_type\": \"nbhd_attn\",\n    \"nbhd_attn_setting\": {\n        \"coefficient\": [1.0, 0.25, 0.056]\n    }\n}\n```\n\n**Attention Pattern:**\n- Coefficient array defines temporal/spatial locality weights\n- `[1.0, 0.25, 0.056]` = full weight for current, 25% for adjacent, 5.6% for next-nearest\n- Reduces attention computation from `O(THW)` to `O(THW  local_window)`\n\nSources: [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:8-11]()\n\n## Memory Management and CPU Offloading\n\n### Five-Level Offloading Hierarchy\n\nLightX2V implements a granular offloading system to support hardware ranging from 8GB consumer GPUs to 640GB server configurations:\n\n| Strategy Level | GPU Resident | Transfer Granularity | Memory Reduction | Speed Impact | Use Case |\n|---------------|--------------|---------------------|------------------|--------------|----------|\n| **No Offload** | Full model | N/A | 1x | 1x (baseline) | High-VRAM servers (80GB+) |\n| **Model Offload** | Per-inference load | Entire model | 10x+ | ~2x slower | Batch processing |\n| **Block Offload** | 2 blocks | Per-block (DiT layers) | 4-8x | ~1.3x slower | Consumer GPUs (24GB) |\n| **Phase Offload** | Single phase | Per-phase group | 8-16x | ~1.5x slower | Low VRAM (16GB) |\n| **Lazy Load** | Active compute only | DiskCPUGPU streaming | 50-100x | ~3-10x slower | Minimal VRAM (8GB) |\n\n### Offload Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Storage Hierarchy\"\n        Disk[\"Disk Storage\u003cbr/\u003eModel safetensors\"]\n        RAM[\"CPU Memory\u003cbr/\u003eoffload_block_cpu_buffers\"]\n        VRAM[\"GPU Memory\u003cbr/\u003eActive weights\"]\n    end\n    \n    subgraph \"Offload Manager\"\n        WeightAsync[\"WeightAsyncStreamManager\u003cbr/\u003easync transfers\"]\n        LazyLoader[\"Lazy Loading\u003cbr/\u003eload_state_dict_from_disk\"]\n        Prefetch[\"Prefetch Thread\u003cbr/\u003eThreadPoolExecutor\"]\n    end\n    \n    subgraph \"Model Execution\"\n        Block1[\"Block 1\u003cbr/\u003eOn GPU\"]\n        Block2[\"Block 2\u003cbr/\u003eOffloaded\"]\n        BlockN[\"Block N\u003cbr/\u003eOffloaded\"]\n        Execute[\"Execute Inference\u003cbr/\u003eActive block\"]\n    end\n    \n    Disk --\u003e LazyLoader\n    LazyLoader --\u003e RAM\n    RAM --\u003e WeightAsync\n    WeightAsync --\u003e VRAM\n    WeightAsync --\u003e Prefetch\n    \n    VRAM --\u003e Block1\n    RAM --\u003e Block2\n    RAM --\u003e BlockN\n    Block1 --\u003e Execute\n    \n    Execute --\u003e |\"Swap after execution\"| RAM\n    Block2 --\u003e |\"Load on demand\"| VRAM\n```\n\n**Key Components:**\n- **Block-level offloading**: Keeps 2 DiT blocks in GPU, streams others from CPU ([README.md:259]())\n- **Phase-level offloading**: Offloads entire phases (multi-block groups) \n- **Async streaming**: `WeightAsyncStreamManager` overlaps transfers with compute\n- **Lazy loading**: Streams from diskCPUGPU for minimal VRAM footprint\n\n### Configuration Examples\n\n**Block-level offloading** (recommended for RTX 4090 24GB):\n```python\npipe.enable_offload(\n    cpu_offload=True,\n    offload_granularity=\"block\",  # 2 blocks resident\n    text_encoder_offload=True,\n    image_encoder_offload=False,\n    vae_offload=False,\n)\n```\n\n**Phase-level offloading** (for 16GB VRAM):\n```python\npipe.enable_offload(\n    cpu_offload=True,\n    offload_granularity=\"phase\",  # Single phase resident\n    text_encoder_offload=True,\n    vae_offload=True,\n)\n```\n\nSources: [README.md:156-165](), [scripts/hunyuan_video_15/README.md:54-61]()\n\n### Performance vs Memory Tradeoff\n\nReal-world measurements on Wan2.1-I2V-14B-480P (40 steps, 81 frames):\n\n| Configuration | VRAM Required | Total RAM | Step Time | Total Time | Relative Speed |\n|--------------|---------------|-----------|-----------|------------|----------------|\n| No offload (H100) | 60GB | - | 0.75s/it | 30s | 1x |\n| Block offload (H100) | 20GB | 32GB | 1.0s/it | 40s | 0.75x |\n| Block offload (4090) | 24GB | 32GB | 4.75s/it | 190s | 0.16x |\n| Phase offload (4090) | 16GB | 32GB | 7.1s/it | 284s | 0.11x |\n| Lazy load (4090) | 8GB | 16GB | ~10s/it | ~400s | 0.08x |\n\n**Key Insight**: Block offloading provides optimal tradeoff - 3x memory reduction with only 30% speed penalty.\n\nSources: [README.md:100-108]()\n\n## Feature Caching and Streaming\n\n### Caching Mechanisms\n\nLightX2V implements three caching strategies to eliminate redundant computations:\n\n| Cache Type | Target | Speedup | Memory Overhead | Applicable Models |\n|-----------|--------|---------|-----------------|-------------------|\n| **TeaCache** | Redundant DiT blocks | 1.2-1.3x | Minimal | Wan, HunyuanVideo |\n| **MagCache** | Feature reuse across steps | 1.15-1.25x | ~5% | Wan, HunyuanVideo |\n| **KV Cache** | Autoregressive tokens | N/A (enables AR) | Per-token storage | WanCausVid, WorldPlay |\n| **VAE Cache** | Temporal features (CACHE_T=2) | 1.1-1.2x decode | 2-frame buffer | Causal VAE models |\n\n### TeaCache and MagCache Operation\n\n**TeaCache**: Skips redundant transformer blocks based on feature similarity\n- Monitors feature changes across diffusion steps\n- Automatically skips blocks with minimal updates in later steps\n- Typical pattern: Steps 1-20 full compute, Steps 21-40 skip 30% of blocks\n- Implementation is model-runner specific\n\n**MagCache**: Reuses intermediate features across timesteps\n- Caches attention outputs and FFN activations\n- Validity checked via similarity threshold\n- Reduces redundant computation in similar timesteps\n\n### KV Cache for Autoregressive Models\n\nWanCausVid uses KV cache for efficient autoregressive video generation:\n\n```mermaid\ngraph LR\n    subgraph \"Timestep t\"\n        Input_t[\"Input frames[0:t]\"]\n        KV_t[\"KV Cache[0:t-1]\"]\n        Compute_t[\"Compute only\u003cbr/\u003enew token t\"]\n        KV_updated[\"KV Cache[0:t]\"]\n        Input_t --\u003e Compute_t\n        KV_t --\u003e Compute_t\n        Compute_t --\u003e KV_updated\n    end\n    \n    subgraph \"Timestep t+1\"\n        Input_t1[\"Input frames[0:t+1]\"]\n        KV_t1[\"KV Cache[0:t]\"]\n        Compute_t1[\"Compute only\u003cbr/\u003enew token t+1\"]\n        KV_updated1[\"KV Cache[0:t+1]\"]\n        Input_t1 --\u003e Compute_t1\n        KV_t1 --\u003e Compute_t1\n        Compute_t1 --\u003e KV_updated1\n    end\n    \n    KV_updated --\u003e KV_t1\n```\n\n**KV Cache Parameters:**\n- `kv_start`: Starting position for cache retrieval\n- `kv_end`: Ending position for cache storage\n- Memory scaling: O(sequence_length  hidden_dim) per layer\n\nSources: [README.md:274-280]()\n\n## Parallel Inference Strategies\n\n### Multi-GPU Parallelism Types\n\nLightX2V implements three orthogonal parallelism strategies:\n\n| Parallelism Type | Scaling Efficiency | Memory Per GPU | Communication Backend | Configuration |\n|-----------------|-------------------|----------------|----------------------|---------------|\n| **CFG Parallelism** | 2x (perfect) | 0.5x (2 GPUs) | Gloo (task), NCCL (compute) | `cfg_p_group` |\n| **Sequence Parallelism (Ulysses)** | 0.8x per GPU | 1/N | NCCL all-to-all | `seq_p_type=\"ulysses\"` |\n| **Sequence Parallelism (Ring)** | 0.7x per GPU | 1/N | NCCL P2P | `seq_p_type=\"ring\"` |\n| **Tensor Parallelism** | 0.6-0.7x per GPU | 1/N | NCCL | For very large models |\n\n### CFG Parallelism Architecture\n\nCFG parallelism doubles throughput by computing conditional and unconditional passes in parallel:\n\n```mermaid\ngraph TB\n    subgraph \"Sequential CFG (1 GPU)\"\n        Input1[\"Input batch\"]\n        Cond1[\"Conditional pass\u003cbr/\u003eT seconds\"]\n        Uncond1[\"Unconditional pass\u003cbr/\u003eT seconds\"]\n        Combine1[\"Combine: cond + scale*(cond-uncond)\"]\n        Input1 --\u003e Cond1\n        Cond1 --\u003e Uncond1\n        Uncond1 --\u003e Combine1\n    end\n    \n    subgraph \"Parallel CFG (2 GPUs)\"\n        Input2[\"Input batch\"]\n        Broadcast[\"Broadcast input\u003cbr/\u003evia Gloo\"]\n        \n        GPU0[\"GPU 0\"]\n        Cond2[\"Conditional pass\u003cbr/\u003eT seconds\"]\n        \n        GPU1[\"GPU 1\"]\n        Uncond2[\"Unconditional pass\u003cbr/\u003eT seconds\"]\n        \n        Gather[\"Gather results\u003cbr/\u003evia NCCL\"]\n        Combine2[\"Combine outputs\"]\n        \n        Input2 --\u003e Broadcast\n        Broadcast --\u003e GPU0\n        Broadcast --\u003e GPU1\n        GPU0 --\u003e Cond2\n        GPU1 --\u003e Uncond2\n        Cond2 --\u003e Gather\n        Uncond2 --\u003e Gather\n        Gather --\u003e Combine2\n    end\n```\n\n**Performance Impact:**\n- Single GPU with CFG: 2T time per step\n- 2 GPUs with CFG parallelism: T time per step (2x speedup)\n- Communication overhead: \u003c5% (Gloo for broadcast, NCCL for compute)\n\nConfiguration example [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:19-22]():\n```json\n{\n    \"parallel\": {\n        \"seq_p_size\": 8,\n        \"seq_p_attn_type\": \"ulysses\"\n    }\n}\n```\n\nSources: [README.md:277]()\n\n### Ring Attention Communication\n\nRing attention for sequence parallelism uses circular P2P communication:\n\nImplementation: [lightx2v/common/ops/attn/utils/ring_comm.py:7-46]()\n\n**Key Components:**\n- `RingComm` class manages circular topology\n- `send_rank = (rank + 1) % world_size`\n- `recv_rank = (rank - 1) % world_size`\n- `send_recv()`: Overlaps send to next GPU with receive from previous\n- `commit()` and `wait()`: Async batch operations via `dist.batch_isend_irecv`\n\n**Sequence Partitioning:**\n- Each GPU processes 1/N of sequence length\n- Ring pattern passes activations in circular fashion\n- Enables sequences exceeding single-GPU memory\n\nSources: [lightx2v/common/ops/attn/utils/ring_comm.py:7-46]()\n\n## Step Distillation and Model Variants\n\n### Distillation Speedup\n\nStep distillation compresses 40-50 step diffusion into 4-step inference:\n\n| Model | Original Steps | CFG Required | Distilled Steps | CFG Required | Total Speedup |\n|-------|---------------|--------------|----------------|--------------|---------------|\n| Wan2.1 Base | 40-50 | Yes (2x compute) | 4 | No (1x compute) | ~25x |\n| Wan2.2 MoE Base | 40-50 | Yes | 4 | No | ~25x |\n| HunyuanVideo-1.5 | 50 | Yes | 4 | No | ~25x |\n| Qwen-Image-Edit | 30 | Yes | 8 | No | ~7.5x |\n\n### Available Distilled Models\n\n**Wan Family:**\n- [Wan2.1-Distill-Models](https://huggingface.co/lightx2v/Wan2.1-Distill-Models): Full model checkpoints\n- [Wan2.1-Distill-Loras](https://huggingface.co/lightx2v/Wan2.1-Distill-Loras): LoRA adaptors for base models\n- [Wan2.2-Distill-Models](https://huggingface.co/lightx2v/Wan2.2-Distill-Models): MoE distilled models\n- [Wan-NVFP4](https://huggingface.co/lightx2v/Wan-NVFP4): 4-step + 4-bit quantization\n\n**Other Models:**\n- [Hy1.5-Distill-Models](https://huggingface.co/lightx2v/Hy1.5-Distill-Models): HunyuanVideo-1.5 4-step\n- [Qwen-Image-Edit-2511-Lightning](https://huggingface.co/lightx2v/Qwen-Image-Edit-2511-Lightning): 8-step Qwen editing\n\n### Combined Optimization Example\n\nMaximum speedup configuration (H100 8-GPU):\n\n```\nBase: 50 steps  2 (CFG)  9.77s/it = 977s total\n\nOptimized:\n- 4-step distillation: 504 steps (12.5x)\n- CFG-free: 2x1x compute (2x)\n- FP8 quantization: 9.77s5.18s per iter (1.9x)\n- 8-GPU parallel: 5.18s0.35s per iter (14.8x)\n\nTotal: 4 steps  1 (no CFG)  0.35s/it = 1.4s\nSpeedup: 977s / 1.4s = ~698x\n```\n\nSources: [README.md:252-254](), [README.md:63]()\n\n## Lightweight VAE/TAE Models\n\n### Encoder/Decoder Acceleration\n\nLightweight autoencoders reduce VAE/TAE decode latency:\n\n| Model | Original Size | Light Version | Decode Speedup | Quality Loss |\n|-------|--------------|---------------|----------------|--------------|\n| Wan VAE | ~600MB | LightVAE (~300MB) | 2-3x | Minimal |\n| HunyuanVideo TAE | ~800MB | LightTAE (~400MB) | 2-4x | Minimal |\n\nConfiguration example [scripts/hunyuan_video_15/README.md:64-69]():\n```python\npipe.enable_lightvae(\n    use_tae=True,\n    tae_path=\"/path/to/lighttaehy1_5.safetensors\",\n    use_lightvae=False,\n    vae_path=None,\n)\n```\n\n**Key Optimizations in LightVAE/TAE:**\n- Reduced channel counts in decoder layers\n- Pruned attention layers\n- Optimized causal convolutions with caching\n- Maintained output quality through knowledge distillation\n\nSources: [README.md:222-223](), [scripts/hunyuan_video_15/README.md:64-69]()\n\n## Performance Benchmark Summary\n\n### Cross-Framework Comparison\n\nWan2.1-I2V-14B-480P (40 steps, 81 frames) on H100:\n\n| Framework | 1 GPU (s/it) | 8 GPUs (s/it) | Speedup vs Diffusers |\n|-----------|-------------|--------------|---------------------|\n| Diffusers | 9.77 | - | 1x |\n| xDiT | 8.93 | 2.70 | 1.1x (1 GPU) |\n| FastVideo | 7.35 | 2.94 | 1.3x (1 GPU) |\n| SGL-Diffusion | 6.13 | 1.19 | 1.6x (1 GPU) |\n| **LightX2V** | **5.18** | **0.75** | **1.9x (1 GPU), 3.9x (8 GPUs)** |\n\n### LightX2V Configuration Ablations\n\nImpact of individual optimizations on H100 8-GPU:\n\n| Configuration | Step Time | Relative Speedup | Enabled Optimizations |\n|--------------|-----------|------------------|----------------------|\n| Base (CFG enabled) | 0.75s/it | 1x | None |\n| CFG-free (distilled) | 0.39s/it | 1.9x | Step distillation |\n| CFG-free + FP8 | 0.35s/it | 2.1x | Distillation + FP8 quantization |\n| CFG-free + FP8 + SageAttn | ~0.30s/it | ~2.5x | All optimizations |\n\nSources: [README.md:72-96](), [README.md:100-108]()\n\n### Hardware Range Support\n\nMinimum to maximum configurations:\n\n| Hardware | VRAM | RAM | Configuration | Performance | Resolution Support |\n|----------|------|-----|---------------|-------------|-------------------|\n| **Minimum** | 8GB | 16GB | INT8 + Lazy Load + Phase Offload | ~10s/it | 480p |\n| **Consumer** | 24GB | 32GB | FP8 + Block Offload + SageAttn | 2.35s/it | 480p/720p |\n| **Workstation** | 48GB | 64GB | FP16 + Block Offload | 1.0s/it | 720p/1080p |\n| **Server (1x H100)** | 80GB | - | FP16 + No Offload | 5.18s/it | 1080p |\n| **Server (8x H100)** | 640GB | - | FP8 + CFG Parallel + Seq Parallel | 0.35s/it | 1080p+ |\n\n**Key Achievement**: 100x VRAM range support (8GB  640GB) through composable optimizations.\n\nSources: [README.md:258]()\n\n## Optimization Configuration Files\n\n### Sample Optimized Configuration\n\nHigh-performance configuration combining multiple optimizations [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:1-74]():\n\n**Key Settings:**\n- `infer_steps: 4` - Step distillation\n- `self_attn_1_type: \"nbhd_attn\"` - Neighborhood attention\n- `cross_attn_1_type: \"flash_attn3\"` - Flash Attention 3\n- `enable_cfg: false` - CFG-free inference\n- `seq_p_size: 8` - 8-way sequence parallelism\n- `dit_quantized: true` / `dit_quant_scheme: \"fp8-sgl\"` - FP8 quantization\n- `compile: true` - PyTorch compilation\n- `compile_shapes` - Pre-compiled resolution grid\n\n**Performance Impact:**\n- Base 50-step inference: ~500s\n- This configuration: ~6s total\n- **~83x speedup** on 8x H100 with all optimizations\n\nSources: [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:1-74](), [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh:1-22]()\n\n## Optimization Selection Guide\n\n### Decision Matrix\n\n```mermaid\ngraph TB\n    Start[\"Start: Choose Optimizations\"]\n    \n    Q1{\"VRAM Available?\"}\n    Start --\u003e Q1\n    \n    Q1 --\u003e|\"\u003c12GB\"| LazyLoad[\"Enable:\u003cbr/\u003e Lazy Load\u003cbr/\u003e Phase Offload\u003cbr/\u003e INT8 Quantization\"]\n    Q1 --\u003e|\"12-24GB\"| BlockOffload[\"Enable:\u003cbr/\u003e Block Offload\u003cbr/\u003e FP8/INT8 Quantization\u003cbr/\u003e SageAttention\"]\n    Q1 --\u003e|\"24-48GB\"| Minimal[\"Enable:\u003cbr/\u003e FP8 Quantization\u003cbr/\u003e SageAttention\u003cbr/\u003e Optional Block Offload\"]\n    Q1 --\u003e|\"\u003e48GB\"| Full[\"Enable:\u003cbr/\u003e FP8 Quantization (optional)\u003cbr/\u003e Flash Attention 3\u003cbr/\u003e Full model in VRAM\"]\n    \n    Q2{\"Priority?\"}\n    LazyLoad --\u003e Q2\n    BlockOffload --\u003e Q2\n    Minimal --\u003e Q2\n    Full --\u003e Q2\n    \n    Q2 --\u003e|\"Speed\"| Speed[\"Add:\u003cbr/\u003e Step Distillation (4-step)\u003cbr/\u003e Lightweight VAE\u003cbr/\u003e TeaCache/MagCache\"]\n    Q2 --\u003e|\"Quality\"| Quality[\"Use:\u003cbr/\u003e Full steps (40-50)\u003cbr/\u003e Higher precision (FP16)\u003cbr/\u003e Standard VAE\"]\n    Q2 --\u003e|\"Balanced\"| Balanced[\"Add:\u003cbr/\u003e 8-step distillation\u003cbr/\u003e Selective caching\u003cbr/\u003e FP8 quantization\"]\n    \n    Q3{\"Multi-GPU?\"}\n    Speed --\u003e Q3\n    Quality --\u003e Q3\n    Balanced --\u003e Q3\n    \n    Q3 --\u003e|\"No\"| Single[\"Single GPU\u003cbr/\u003eConfiguration Complete\"]\n    Q3 --\u003e|\"2-4 GPUs\"| CFGP[\"Enable:\u003cbr/\u003e CFG Parallelism (2 GPUs)\u003cbr/\u003e Sequence Parallel (\u003e2)\"]\n    Q3 --\u003e|\"8+ GPUs\"| FullPar[\"Enable:\u003cbr/\u003e CFG Parallelism\u003cbr/\u003e Sequence Parallelism\u003cbr/\u003e Ulysses attention\"]\n```\n\n### Recommended Configurations by Use Case\n\n| Use Case | Hardware | Recommended Optimizations | Expected Performance |\n|----------|----------|--------------------------|---------------------|\n| **Development/Testing** | RTX 3090 24GB | Block offload + FP16 + FlashAttn2 | ~8s/it (480p) |\n| **Production (Quality)** | A100 80GB | FP8 + FlashAttn3 + 40 steps | ~5s/it (720p) |\n| **Production (Speed)** | A100 80GB | FP8 + 4-step distill + LightVAE | ~0.3s/it (720p) |\n| **Consumer Deployment** | RTX 4090 24GB | INT8 + Block offload + SageAttn + 4-step | ~2.5s/it (480p) |\n| **Low-Resource** | RTX 3060 8GB | INT8 + Lazy load + Phase offload + 4-step | ~12s/it (480p) |\n| **Multi-GPU Server** | 8x H100 | FP8 + CFG parallel + Ulysses + 4-step | ~0.35s/it (1080p) |\n\nSources: [README.md:258](), [README.md:283-287]()"])</script><script>self.__next_f.push([1,"19:T8e8c,"])</script><script>self.__next_f.push([1,"# System Architecture Overview\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [README_zh.md](README_zh.md)\n- [lightx2v/common/modules/weight_module.py](lightx2v/common/modules/weight_module.py)\n- [lightx2v/pipeline.py](lightx2v/pipeline.py)\n- [lightx2v/server/__main__.py](lightx2v/server/__main__.py)\n- [lightx2v/server/config.py](lightx2v/server/config.py)\n- [lightx2v/server/schema.py](lightx2v/server/schema.py)\n- [lightx2v/server/services/distributed_utils.py](lightx2v/server/services/distributed_utils.py)\n- [lightx2v/server/services/generation/base.py](lightx2v/server/services/generation/base.py)\n- [lightx2v/server/services/generation/image.py](lightx2v/server/services/generation/image.py)\n- [lightx2v/server/services/inference/worker.py](lightx2v/server/services/inference/worker.py)\n- [lightx2v_platform/README.md](lightx2v_platform/README.md)\n- [lightx2v_platform/README_zh.md](lightx2v_platform/README_zh.md)\n- [scripts/hunyuan_video_15/README.md](scripts/hunyuan_video_15/README.md)\n- [scripts/server/post_i2i_with_lora.py](scripts/server/post_i2i_with_lora.py)\n- [scripts/server/start_server_i2i_with_loradir.sh](scripts/server/start_server_i2i_with_loradir.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis page describes the high-level architecture of LightX2V, focusing on the layered design, core architectural patterns, and how major components interact during video generation. For detailed information about specific optimizations (quantization, offloading, caching), see sections [6.1](#6.1) through [6.6]. For hardware platform abstraction details, see [1.3](#1.3). For specific model implementations and task types, see section [5](#5).\n\nLightX2V follows a **layered architecture** with clear separation of concerns: user interfaces delegate to a pipeline orchestrator, which uses a registry to instantiate model-specific runners, which in turn coordinate encoders, diffusion models, and decoders. This design enables modular optimization strategies and multi-hardware support without coupling to specific model implementations.\n\n---\n\n## Layered Architecture\n\nLightX2V's architecture consists of six primary layers organized hierarchically:\n\n```mermaid\ngraph TB\n    subgraph \"Layer 1: Entry Points\"\n        PythonAPI[\"LightX2VPipeline\"]\n        CLI[\"lightx2v.infer\"]\n        GradioUI[\"app/gradio_demo.py\"]\n        ComfyUI[\"ComfyUI Nodes\"]\n        Server[\"TorchrunInferenceWorker\"]\n    end\n    \n    subgraph \"Layer 2: Core Framework\"\n        Pipeline[\"LightX2VPipeline\u003cbr/\u003ecreate_generator()\u003cbr/\u003eenable_offload()\u003cbr/\u003egenerate()\"]\n        Registry[\"RUNNER_REGISTER\"]\n        Config[\"set_config()\u003cbr/\u003eLockableDict\"]\n        InputInfo[\"InputInfo\u003cbr/\u003einit_empty_input_info()\"]\n    end\n    \n    subgraph \"Layer 3: Model Runners\"\n        BaseRunner[\"BaseRunner\"]\n        DefaultRunner[\"DefaultRunner\u003cbr/\u003erun_pipeline()\u003cbr/\u003einit_modules()\"]\n        WanAudioRunner[\"WanAudioRunner\u003cbr/\u003eS2V highest importance\"]\n        QwenImageRunner[\"QwenImageRunner\u003cbr/\u003eT2I/I2I\"]\n        ZImageRunner[\"ZImageRunner\u003cbr/\u003efast T2I\"]\n        WanDistillRunner[\"WanDistillRunner\u003cbr/\u003e4-step inference\"]\n        HunyuanRunner[\"HunyuanVideo15Runner\"]\n        LTX2Runner[\"LTX2Runner\u003cbr/\u003eaudio+video\"]\n    end\n    \n    subgraph \"Layer 4: Model Architectures\"\n        WanModel[\"WanModel\u003cbr/\u003eWanAudioModel\"]\n        QwenVLModel[\"Qwen25_VLForConditionalGeneration\"]\n        HunyuanModel[\"HunyuanVideoTransformer3DModel\"]\n        LTX2Model[\"LTX2Model\"]\n    end\n    \n    subgraph \"Layer 5: Components\"\n        TextEncoders[\"T5EncoderModel\u003cbr/\u003eCLIPTextModel\u003cbr/\u003eQwen2VLTextModel\"]\n        ImageEncoders[\"CLIPVisionModel\u003cbr/\u003eVisionModel\"]\n        AudioEncoders[\"SekoAudioEncoderModel\"]\n        VAEs[\"WanVAE\u003cbr/\u003eWan2_2_VAE\u003cbr/\u003eHunyuanVideo3DVAE\"]\n        Schedulers[\"WanScheduler\u003cbr/\u003eWanStepDistillScheduler\u003cbr/\u003eEulerDiscreteScheduler\"]\n    end\n    \n    subgraph \"Layer 6: Optimization \u0026 Hardware\"\n        Quantization[\"MMWeightQuant\u003cbr/\u003eint8/fp8/nvfp4\"]\n        Offloading[\"WeightAsyncStreamManager\u003cbr/\u003eCPU offloading\"]\n        Caching[\"TeaCache\u003cbr/\u003eMagCache\"]\n        Platform[\"lightx2v_platform\u003cbr/\u003ePLATFORM_DEVICE_REGISTER\"]\n    end\n    \n    PythonAPI --\u003e Pipeline\n    CLI --\u003e Pipeline\n    GradioUI --\u003e Pipeline\n    ComfyUI --\u003e Pipeline\n    Server --\u003e Pipeline\n    \n    Pipeline --\u003e Registry\n    Pipeline --\u003e Config\n    Pipeline --\u003e InputInfo\n    \n    Registry --\u003e BaseRunner\n    BaseRunner --\u003e DefaultRunner\n    DefaultRunner --\u003e WanAudioRunner\n    DefaultRunner --\u003e QwenImageRunner\n    DefaultRunner --\u003e ZImageRunner\n    DefaultRunner --\u003e WanDistillRunner\n    DefaultRunner --\u003e HunyuanRunner\n    DefaultRunner --\u003e LTX2Runner\n    \n    WanAudioRunner --\u003e WanModel\n    QwenImageRunner --\u003e QwenVLModel\n    HunyuanRunner --\u003e HunyuanModel\n    LTX2Runner --\u003e LTX2Model\n    \n    WanModel --\u003e TextEncoders\n    WanModel --\u003e ImageEncoders\n    WanModel --\u003e AudioEncoders\n    WanModel --\u003e VAEs\n    WanModel --\u003e Schedulers\n    \n    TextEncoders --\u003e Quantization\n    VAEs --\u003e Quantization\n    WanModel --\u003e Offloading\n    WanModel --\u003e Caching\n    \n    Quantization --\u003e Platform\n    Offloading --\u003e Platform\n```\n\n**Layer 1: Entry Points** - Five distinct interfaces for different use cases:\n- `LightX2VPipeline`: Direct Python API for programmatic access [lightx2v/pipeline.py:62-449]()\n- `lightx2v.infer`: Command-line interface for script-based execution [lightx2v/infer.py:1-173]()\n- `app/gradio_demo.py`: Interactive web UI for demos and prototyping\n- ComfyUI nodes: Workflow-based generation with node graphs\n- `TorchrunInferenceWorker`: Distributed HTTP server for production deployments [lightx2v/server/services/inference/worker.py:16-194]()\n\n**Layer 2: Core Framework** - Orchestration and configuration management:\n- `LightX2VPipeline`: Primary orchestrator that manages model lifecycle, configuration updates, and inference coordination [lightx2v/pipeline.py:62-449]()\n- `RUNNER_REGISTER`: Factory registry mapping `model_cls` strings to runner classes [lightx2v/utils/registry_factory.py]()\n- `set_config()`: Hierarchical configuration merging from defaults, args, JSON files, and model configs [lightx2v/utils/set_config.py:37-166]()\n- `InputInfo`: Type-safe dataclasses (`T2VInputInfo`, `I2VInputInfo`, `S2VInputInfo`) for passing inputs through the pipeline [lightx2v/utils/input_info.py]()\n\n**Layer 3: Model Runners** - Task-specific execution logic:\n- `BaseRunner` and `DefaultRunner`: Define the three-stage pipeline interface [lightx2v/models/runners/default_runner.py:56-524]()\n- `WanAudioRunner`: Audio-to-video generation with highest system importance score (222.45) [lightx2v/models/runners/wan/wan_audio_runner.py:276-733]()\n- `QwenImageRunner`: Text/image-to-image generation with Qwen models (importance: 50.95) [lightx2v/models/runners/qwen_image/qwen_image_runner.py]()\n- `ZImageRunner`: Fast image generation with Z-Image-Turbo (importance: 34.12) [lightx2v/models/runners/z_image/z_image_runner.py]()\n- `WanDistillRunner`: 4-step distilled inference without CFG (importance: 33.82) [lightx2v/models/runners/wan/wan_distill_runner.py]()\n\n**Layer 4: Model Architectures** - Core diffusion/flow model implementations:\n- `WanModel` and `WanAudioModel`: WAN family transformer models with audio support [lightx2v/models/networks/wan/model.py:1-535]()\n- `Qwen25_VLForConditionalGeneration`: Qwen vision-language models for image tasks [lightx2v/models/networks/qwen_image/model.py]()\n- `HunyuanVideoTransformer3DModel`: HunyuanVideo 3D transformer [lightx2v/models/networks/hunyuan_video/model.py]()\n- `LTX2Model`: LTX-2 audio+video generation model [lightx2v/models/networks/ltx2/model.py]()\n\n**Layer 5: Components** - Reusable encoders, decoders, and schedulers:\n- Text encoders: `T5EncoderModel`, `CLIPTextModel`, `Qwen2VLTextModel` [lightx2v/models/input_encoders/]()\n- Image encoders: `CLIPVisionModel`, `VisionModel` for conditional generation\n- Audio encoders: `SekoAudioEncoderModel` for audio-to-video tasks [lightx2v/models/input_encoders/seko_audio_encoder.py]()\n- VAEs: Model-specific autoencoders with different compression ratios [lightx2v/models/vae/]()\n- Schedulers: Manage diffusion timesteps and noise schedules [lightx2v/models/schedulers/]()\n\n**Layer 6: Optimization \u0026 Hardware** - Cross-cutting optimization features:\n- `MMWeightQuant`: Quantized weight wrappers supporting int8, fp8, nvfp4 schemes [lightx2v/common/ops/mm/mm_weight.py:99-238]()\n- `WeightAsyncStreamManager`: Asynchronous CPU offloading with double buffering [lightx2v/common/ops/offload/weight_async_stream_manager.py]()\n- Feature caching: TeaCache, MagCache, TaylorCache for redundant computation elimination [lightx2v/models/networks/wan/infer/]()\n- `lightx2v_platform`: Hardware abstraction supporting 8 backends (CUDA, MLU, DCU, MUSA, AMD, Ascend, GCU, Enflame) [lightx2v_platform/README.md:1-19]()\n\n**Sources:** [lightx2v/pipeline.py:1-449](), [lightx2v/infer.py:1-173](), [lightx2v/server/services/inference/worker.py:16-194](), [lightx2v/models/runners/default_runner.py:56-524](), [lightx2v/models/runners/wan/wan_audio_runner.py:276-733](), [lightx2v/models/runners/qwen_image/qwen_image_runner.py](), [lightx2v/utils/registry_factory.py](), [lightx2v/utils/set_config.py:37-166](), [lightx2v_platform/README.md:1-19]()\n\n---\n\n## Core Architectural Patterns\n\n### Runner Registry Pattern\n\nLightX2V uses a **factory pattern** via `RUNNER_REGISTER` to instantiate model-specific runners without hard-coded dependencies. The registry maps string identifiers to runner classes, enabling dynamic model instantiation:\n\n```mermaid\ngraph TB\n    subgraph \"Registration Phase - Import Time\"\n        D1[\"@RUNNER_REGISTER('wan2.1')\"]\n        C1[\"class WanRunner\"]\n        D1 --\u003e C1\n        \n        D2[\"@RUNNER_REGISTER('wan2.2_moe')\"]\n        C2[\"class Wan22MoeRunner\"]\n        D2 --\u003e C2\n        \n        D3[\"@RUNNER_REGISTER('seko_talk')\"]\n        C3[\"class WanAudioRunner\"]\n        D3 --\u003e C3\n        \n        D4[\"@RUNNER_REGISTER('wan2.2_audio')\"]\n        C4[\"class Wan22AudioRunner\"]\n        D4 --\u003e C4\n        \n        D5[\"@RUNNER_REGISTER('qwen_image')\"]\n        C5[\"class QwenImageRunner\"]\n        D5 --\u003e C5\n        \n        D6[\"@RUNNER_REGISTER('hunyuan_video_1.5')\"]\n        C6[\"class HunyuanVideo15Runner\"]\n        D6 --\u003e C6\n        \n        D7[\"@RUNNER_REGISTER('ltx2')\"]\n        C7[\"class LTX2Runner\"]\n        D7 --\u003e C7\n    end\n    \n    subgraph \"Runtime Instantiation\"\n        Config[\"config = {\u003cbr/\u003e  'model_cls': 'seko_talk',\u003cbr/\u003e  'task': 's2v',\u003cbr/\u003e  ...}\"]\n        Init[\"init_runner(config)\"]\n        Lookup[\"RUNNER_REGISTER[config['model_cls']]\"]\n        Create[\"runner_class(config)\"]\n        Instance[\"WanAudioRunner instance\"]\n        \n        Config --\u003e Init\n        Init --\u003e Lookup\n        Lookup --\u003e Create\n        Create --\u003e Instance\n    end\n    \n    C1 -.-\u003e|\"registered as 'wan2.1'\"| Lookup\n    C2 -.-\u003e|\"registered as 'wan2.2_moe'\"| Lookup\n    C3 -.-\u003e|\"registered as 'seko_talk'\"| Lookup\n    C4 -.-\u003e|\"registered as 'wan2.2_audio'\"| Lookup\n    C5 -.-\u003e|\"registered as 'qwen_image'\"| Lookup\n    C6 -.-\u003e|\"registered as 'hunyuan_video_1.5'\"| Lookup\n    C7 -.-\u003e|\"registered as 'ltx2'\"| Lookup\n```\n\n**Registration Examples:**\n\nThe registry is populated during module import through decorators [lightx2v/pipeline.py:13-27]():\n\n```python\n# Wan family runners\n@RUNNER_REGISTER(\"wan2.1\")\nclass WanRunner(DefaultRunner): ...\n\n@RUNNER_REGISTER(\"wan2.2_moe\")\nclass Wan22MoeRunner(WanRunner): ...\n\n@RUNNER_REGISTER(\"seko_talk\")\n@RUNNER_REGISTER(\"wan2.2_audio\")\nclass WanAudioRunner(WanRunner): ...\n\n# Image generation runners\n@RUNNER_REGISTER(\"qwen_image\")\nclass QwenImageRunner(DefaultRunner): ...\n\n@RUNNER_REGISTER(\"z_image\")\nclass ZImageRunner(DefaultRunner): ...\n\n# Video generation runners\n@RUNNER_REGISTER(\"hunyuan_video_1.5\")\nclass HunyuanVideo15Runner(DefaultRunner): ...\n\n@RUNNER_REGISTER(\"ltx2\")\nclass LTX2Runner(DefaultRunner): ...\n```\n\n**Runtime Instantiation:**\n\nAt runtime, `init_runner()` uses the registry [lightx2v/infer.py:32-36]():\n\n```python\ndef init_runner(config):\n    torch.set_grad_enabled(False)\n    runner = RUNNER_REGISTER[config[\"model_cls\"]](config)\n    runner.init_modules()\n    return runner\n```\n\nThis pattern enables:\n- **Extensibility**: Add new models by creating a runner class and decorating it\n- **Decoupling**: Core framework code never imports specific runner classes\n- **Late binding**: Model selection happens at runtime based on configuration\n\n**Sources:** [lightx2v/pipeline.py:13-27](), [lightx2v/infer.py:32-36](), [lightx2v/utils/registry_factory.py](), [lightx2v/models/runners/wan/wan_runner.py:63](), [lightx2v/models/runners/wan/wan_audio_runner.py:276](), [lightx2v/models/runners/qwen_image/qwen_image_runner.py]()\n\n### Pipeline Orchestration via LightX2VPipeline\n\n`LightX2VPipeline` provides the primary user-facing API for image/video generation. It manages the complete lifecycle from configuration to model loading to inference [lightx2v/pipeline.py:62-449]():\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Pipeline as LightX2VPipeline\n    participant SetConfig as set_config()\n    participant Registry as RUNNER_REGISTER\n    participant Runner as DefaultRunner\n    participant Model as WanModel/QwenModel\n    \n    Note over User,Model: Initialization Phase\n    User-\u003e\u003ePipeline: __init__(model_path, model_cls, task)\n    Pipeline-\u003e\u003ePipeline: Store model_path, model_cls, task\n    Pipeline-\u003e\u003ePipeline: init_empty_input_info(task)\n    \n    Note over User,Model: Configuration Phase\n    User-\u003e\u003ePipeline: enable_offload(cpu_offload=True,\u003cbr/\u003eoffload_granularity=\"block\")\n    Pipeline-\u003e\u003ePipeline: self.cpu_offload = True\u003cbr/\u003eself.offload_granularity = \"block\"\n    \n    User-\u003e\u003ePipeline: enable_quantize(dit_quantized=True,\u003cbr/\u003equant_scheme=\"fp8-sgl\")\n    Pipeline-\u003e\u003ePipeline: self.dit_quantized = True\u003cbr/\u003eself.dit_quant_scheme = \"fp8-sgl\"\n    \n    User-\u003e\u003ePipeline: enable_cache(cache_method=\"Tea\",\u003cbr/\u003eteacache_thresh=0.15)\n    Pipeline-\u003e\u003ePipeline: self.feature_caching = \"Tea\"\u003cbr/\u003eself.teacache_thresh = 0.15\n    \n    User-\u003e\u003ePipeline: create_generator(attn_mode=\"sage_attn2\",\u003cbr/\u003einfer_steps=40, height=480)\n    Pipeline-\u003e\u003ePipeline: self.infer_steps = 40\u003cbr/\u003eself.target_height = 480\u003cbr/\u003eself.attn_type = \"sage_attn2\"\n    Pipeline-\u003e\u003eSetConfig: set_config(self)\n    SetConfig-\u003e\u003eSetConfig: Merge default, args, JSON, model configs\n    SetConfig--\u003e\u003ePipeline: config dict\n    Pipeline-\u003e\u003eRegistry: RUNNER_REGISTER[config[\"model_cls\"]]\n    Registry--\u003e\u003ePipeline: Runner class\n    Pipeline-\u003e\u003eRunner: runner = RunnerClass(config)\n    Pipeline-\u003e\u003eRunner: runner.init_modules()\n    \n    Runner-\u003e\u003eRunner: init_scheduler()\n    Runner-\u003e\u003eRunner: load_model()\n    Runner-\u003e\u003eRunner: load_text_encoder()\n    Runner-\u003e\u003eRunner: load_vae()\n    Runner-\u003e\u003eModel: Initialize model with weights\n    Runner-\u003e\u003eRunner: config.lock()\n    Runner--\u003e\u003ePipeline: Initialized runner\n    \n    Note over User,Model: Inference Phase\n    User-\u003e\u003ePipeline: generate(seed=42, prompt=\"...\",\u003cbr/\u003eimage_path=\"/path/img.jpg\")\n    Pipeline-\u003e\u003ePipeline: init_empty_input_info(task)\n    Pipeline-\u003e\u003ePipeline: update_input_info_from_dict(input_info, self)\n    Pipeline-\u003e\u003eRunner: run_pipeline(input_info)\n    Runner-\u003e\u003eRunner: Stage 1: run_input_encoder()\n    Runner-\u003e\u003eRunner: Stage 2: run_main()\n    Runner-\u003e\u003eRunner: Stage 3: run_vae_decoder()\n    Runner--\u003e\u003ePipeline: Generated video tensor or saved file\n    Pipeline--\u003e\u003eUser: Success\n```\n\n**Key API Methods:**\n\n1. **`__init__(model_path, model_cls, task)`** [lightx2v/pipeline.py:63-128]()\n   - Stores model path and class identifier\n   - Initializes empty `InputInfo` for the specified task\n   - Does **not** load models (delayed initialization)\n\n2. **`create_generator(...)`** [lightx2v/pipeline.py:129-188]()\n   - Updates configuration with inference parameters (`attn_mode`, `infer_steps`, `height`, `width`, etc.)\n   - Calls `set_config()` to merge all configuration sources\n   - Instantiates runner via `RUNNER_REGISTER[model_cls]`\n   - Calls `runner.init_modules()` to load all models\n   - Configuration becomes locked after this point\n\n3. **`enable_offload(cpu_offload, offload_granularity, ...)`** [lightx2v/pipeline.py:298-336]()\n   - Configures CPU offloading: `\"model\"`, `\"block\"`, or `\"phase\"` granularity\n   - Sets encoder-specific offload flags (`text_encoder_offload`, `vae_offload`)\n\n4. **`enable_quantize(dit_quantized, quant_scheme, ...)`** [lightx2v/pipeline.py:260-297]()\n   - Configures quantization: `\"int8-sgl\"`, `\"fp8-sgl\"`, `\"nvfp4\"`, etc.\n   - Sets per-component quantization (DIT, text encoder, image encoder)\n\n5. **`enable_cache(cache_method, ...)`** [lightx2v/pipeline.py:368-391]()\n   - Configures feature caching: `\"Tea\"`, `\"Mag\"`, `\"Taylor\"`, `\"Ada\"`\n   - Sets cache-specific thresholds and retention ratios\n\n6. **`enable_parallel(cfg_p_size, seq_p_size, seq_p_attn_type)`** [lightx2v/pipeline.py:392-397]()\n   - Configures distributed inference: CFG parallel, sequence parallel (Ulysses/Ring)\n\n7. **`generate(seed, prompt, image_path, ...)`** [lightx2v/pipeline.py:399-438]()\n   - Creates `InputInfo` dataclass with all generation parameters\n   - Calls `runner.run_pipeline(input_info)` to execute three-stage inference\n   - Returns generated tensor or saves to file\n\n8. **`switch_lora(lora_path, strength)`** [lightx2v/pipeline.py:358-366]()\n   - Dynamically switches LoRA weights at runtime (requires `lora_dynamic_apply=True`)\n   - Calls `runner.switch_lora()` which delegates to `model._update_lora()` or `model._remove_lora()`\n\n**Delayed Initialization Pattern:**\n\nModels are only loaded when `create_generator()` is called. This enables:\n- Configuration changes before heavy GPU memory allocation\n- Multi-step optimization setup (offload  quantize  cache  compile)\n- Validation of configuration before model loading\n\n**Sources:** [lightx2v/pipeline.py:62-449](), [lightx2v/utils/set_config.py:37-166](), [lightx2v/models/runners/default_runner.py:56-99]()\n\n### Three-Stage Inference Pipeline\n\nEvery runner implements a **three-stage inference pipeline** defined in `DefaultRunner`:\n\n```mermaid\ngraph TB\n    subgraph \"Stage 1: Input Encoding\"\n        ReadInput[\"read_image_input()\u003cbr/\u003eread_audio_input()\"]\n        TextEnc[\"run_text_encoder()\"]\n        ImgEnc[\"run_image_encoder()\"]\n        VAEEnc[\"run_vae_encoder()\"]\n        \n        ReadInput --\u003e TextEnc\n        ReadInput --\u003e ImgEnc\n        ReadInput --\u003e VAEEnc\n        \n        TextEnc --\u003e EncoderOutput[\"Encoder Output Dict\"]\n        ImgEnc --\u003e EncoderOutput\n        VAEEnc --\u003e EncoderOutput\n    end\n    \n    subgraph \"Stage 2: Diffusion Loop\"\n        InitRun[\"init_run()\u003cbr/\u003escheduler.prepare()\"]\n        RunSegment[\"run_segment(segment_idx)\"]\n        DitLoop[\"for step in range(infer_steps):\u003cbr/\u003e  scheduler.step_pre()\u003cbr/\u003e  model.infer(inputs)\u003cbr/\u003e  scheduler.step_post()\"]\n        \n        InitRun --\u003e RunSegment\n        RunSegment --\u003e DitLoop\n        DitLoop --\u003e Latents[\"Final Latents\"]\n    end\n    \n    subgraph \"Stage 3: VAE Decoding\"\n        VAEDec[\"run_vae_decoder(latents)\"]\n        PostProcess[\"process_images_after_vae_decoder()\u003cbr/\u003eVFI, VSR, format conversion\"]\n        \n        Latents --\u003e VAEDec\n        VAEDec --\u003e PostProcess\n        PostProcess --\u003e FinalVideo[\"Final Video Tensor\"]\n    end\n    \n    EncoderOutput -.-\u003e|inputs| InitRun\n    \n    style DitLoop fill:#f9f9f9\n```\n\n**Stage 1** (`run_input_encoder()`) is task-specific:\n- **T2V**: Only text encoding via [lightx2v/models/runners/default_runner.py:291-299]()\n- **I2V**: Text + image + VAE encoding via [lightx2v/models/runners/default_runner.py:280-288]()\n- **S2V** (Audio): Text + image + VAE + audio segmentation via [lightx2v/models/runners/wan/wan_audio_runner.py:440-464]()\n\n**Stage 2** (`run_main()`  `run_segment()`) executes the diffusion denoising loop:\n1. `scheduler.prepare()` initializes latents with noise\n2. For each timestep: `step_pre()`  `model.infer()`  `step_post()`\n3. Returns denoised latents\n\n**Stage 3** (`run_vae_decoder()`) decodes latents to pixel space and applies optional post-processing (video frame interpolation, super-resolution).\n\nThis separation allows:\n- **Task-specific logic** in Stage 1 (different input modalities)\n- **Shared optimization** in Stage 2 (quantization, caching, offloading)\n- **Flexible output processing** in Stage 3 (VFI/VSR plugins)\n\n**Sources:** [lightx2v/models/runners/default_runner.py:362-393](), [lightx2v/models/runners/default_runner.py:174-208](), [lightx2v/models/runners/wan/wan_audio_runner.py:663-733]()\n\n### Weight Management and Inference Separation\n\nLightX2V separates **weight containers** from **inference logic** to enable modular optimization:\n\n```mermaid\ngraph TB\n    subgraph \"Weight Containers\"\n        PreWeight[\"WanPreWeights\u003cbr/\u003epatch_embedding\u003cbr/\u003etext_embedding\u003cbr/\u003etime_embedding\"]\n        TransformerWeights[\"WanTransformerWeights\u003cbr/\u003eblocks: WeightModuleList\u003cbr/\u003enorm, head, modulation\"]\n        PostWeight[\"WanPostWeights\u003cbr/\u003eunpatchify logic\"]\n        \n        BlockWeights[\"WanTransformerAttentionBlock\u003cbr/\u003ecompute_phases:\u003cbr/\u003e  - self_attn phase\u003cbr/\u003e  - cross_attn phase\u003cbr/\u003e  - ffn phase\"]\n        \n        TransformerWeights --\u003e BlockWeights\n    end\n    \n    subgraph \"Inference Logic\"\n        PreInfer[\"WanPreInfer\u003cbr/\u003einfer(weights, inputs)\"]\n        TransformerInfer[\"WanTransformerInfer\u003cbr/\u003einfer_block(block, x)\"]\n        PostInfer[\"WanPostInfer\u003cbr/\u003einfer(x, pre_infer_out)\"]\n    end\n    \n    subgraph \"Model Wrapper\"\n        WanModel[\"WanModel\u003cbr/\u003einfer(inputs):\u003cbr/\u003e  pre  transformer  post\"]\n    end\n    \n    PreInfer -.-\u003e|reads from| PreWeight\n    TransformerInfer -.-\u003e|reads from| TransformerWeights\n    PostInfer -.-\u003e|reads from| PostWeight\n    \n    WanModel --\u003e PreInfer\n    WanModel --\u003e TransformerInfer\n    WanModel --\u003e PostInfer\n    \n    PreWeight -.-\u003e|offload_to_cpu()| PreWeight\n    TransformerWeights -.-\u003e|offload_to_cpu()| TransformerWeights\n```\n\n**Weight Classes** (in `lightx2v/models/networks/wan/weights/`):\n- `WanPreWeights`: Stores patch embedding, text/time embeddings\n- `WanTransformerWeights`: Stores transformer block weights in a `WeightModuleList`\n- Each block contains `compute_phases` (self-attn, cross-attn, FFN) as `WeightModule` instances\n\n**Inference Classes** (in `lightx2v/models/networks/wan/infer/`):\n- `WanPreInfer`: Executes pre-processing (embeddings, positional encoding)\n- `WanTransformerInfer`: Executes transformer blocks in a loop\n- `WanPostInfer`: Executes post-processing (unpatchify)\n\n**Benefits**:\n1. **Memory offloading**: Weight classes implement `to_cpu()`/`to_cuda()` for three-tier memory management\n2. **Quantization**: Weight classes can be swapped with quantized variants (`MMWeightQuant`) without changing inference logic\n3. **LoRA support**: Weight classes handle LoRA weight application/merging independently\n4. **Compilation**: Inference logic can be `torch.compile()`d without recompiling weight loading\n\nThe `WanModel` class coordinates these components:\n```python\ndef infer(self, inputs):\n    pre_infer_out = self.pre_infer.infer(self.pre_weight, inputs)\n    x = self.transformer_infer.infer(self.transformer_weights, pre_infer_out)\n    noise_pred = self.post_infer.infer(x, pre_infer_out)\n    return noise_pred\n```\n\n**Sources:** [lightx2v/models/networks/wan/model.py:39-100](), [lightx2v/models/networks/wan/weights/transformer_weights.py:11-54](), [lightx2v/models/networks/wan/infer/transformer_infer.py:17-98](), [lightx2v/common/ops/mm/mm_weight.py:99-238]()\n\n---\n\n## Component Interaction: End-to-End Inference Flow\n\nThis diagram shows how components interact during a typical image-to-video generation request:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Pipeline as LightX2VPipeline\n    participant Runner as WanRunner\n    participant Scheduler as WanScheduler\n    participant Model as WanModel\n    participant Weights as WanTransformerWeights\n    participant Infer as WanTransformerInfer\n    participant VAE as WanVAE\n    \n    User-\u003e\u003ePipeline: generate(prompt, image_path, seed)\n    Pipeline-\u003e\u003ePipeline: Create I2VInputInfo\n    Pipeline-\u003e\u003eRunner: run_pipeline(input_info)\n    \n    Note over Runner: Stage 1: Input Encoding\n    Runner-\u003e\u003eRunner: read_image_input(image_path)\n    Runner-\u003e\u003eRunner: run_text_encoder(input_info)\n    Runner-\u003e\u003eRunner: run_image_encoder(img)\n    Runner-\u003e\u003eRunner: run_vae_encoder(img)\n    Runner-\u003e\u003eRunner: Store in inputs dict\n    \n    Note over Runner: Stage 2: Diffusion Loop\n    Runner-\u003e\u003eScheduler: prepare(seed, latent_shape, image_encoder_output)\n    Scheduler-\u003e\u003eScheduler: Initialize noisy latents\n    Scheduler--\u003e\u003eRunner: scheduler ready\n    \n    loop For each segment (typically 1)\n        Runner-\u003e\u003eRunner: init_run_segment(segment_idx)\n        \n        loop For each diffusion step (e.g., 40 steps)\n            Runner-\u003e\u003eScheduler: step_pre(step_index)\n            Scheduler-\u003e\u003eScheduler: Update timestep, sigma\n            Scheduler--\u003e\u003eRunner: latents ready for inference\n            \n            Runner-\u003e\u003eModel: infer(inputs)\n            \n            alt CFG enabled\n                Model-\u003e\u003eInfer: _infer_cond_uncond(inputs, cond=True)\n                Infer-\u003e\u003eWeights: Access blocks[block_idx]\n                Weights--\u003e\u003eInfer: Block weights\n                Infer-\u003e\u003eInfer: infer_block() loop\n                Infer--\u003e\u003eModel: noise_pred_cond\n                \n                Model-\u003e\u003eInfer: _infer_cond_uncond(inputs, cond=False)\n                Infer--\u003e\u003eModel: noise_pred_uncond\n                \n                Model-\u003e\u003eModel: Apply CFG formula\n            else No CFG\n                Model-\u003e\u003eInfer: _infer_cond_uncond(inputs, cond=True)\n                Infer--\u003e\u003eModel: noise_pred\n            end\n            \n            Model-\u003e\u003eScheduler: Store noise_pred\n            \n            Runner-\u003e\u003eScheduler: step_post()\n            Scheduler-\u003e\u003eScheduler: Update latents with predicted noise\n        end\n        \n        Runner-\u003e\u003eRunner: run_segment() returns latents\n    end\n    \n    Note over Runner: Stage 3: VAE Decoding\n    Runner-\u003e\u003eVAE: decode(latents)\n    VAE--\u003e\u003eRunner: video frames (B, C, T, H, W)\n    \n    Runner-\u003e\u003eRunner: process_images_after_vae_decoder()\n    Runner-\u003e\u003eRunner: wan_vae_to_comfy() format conversion\n    Runner-\u003e\u003eRunner: Optional: VFI, VSR\n    Runner-\u003e\u003eRunner: save_to_video() if save_result_path\n    \n    Runner--\u003e\u003ePipeline: Final video tensor or None\n    Pipeline--\u003e\u003eUser: Generation complete\n```\n\n**Key Interaction Points**:\n\n1. **Config Flow**: `config` dict flows from `Pipeline`  `Runner`  all components during initialization\n2. **Input Flow**: User inputs  `InputInfo` dataclass  `inputs` dict  Model components\n3. **Scheduler-Model Coupling**: `model.set_scheduler(scheduler)` links them; scheduler stores latents and timesteps, model reads and updates via `scheduler.noise_pred`\n4. **Weight-Infer Coupling**: Inference classes receive weight objects as parameters: `infer.infer(weights, ...)`\n\n**Sources:** [lightx2v/models/runners/default_runner.py:463-478](), [lightx2v/models/runners/default_runner.py:362-393](), [lightx2v/models/networks/wan/model.py:437-500](), [lightx2v/models/schedulers/wan/scheduler.py:51-200]()\n\n---\n\n## Configuration System\n\nLightX2V uses a **hierarchical configuration system** with multiple sources and a locking mechanism:\n\n```mermaid\ngraph TD\n    subgraph \"Configuration Sources Priority: Low to High\"\n        DefaultConfig[\"get_default_config()\u003cbr/\u003eBase defaults\"]\n        ArgsConfig[\"Command-line args\u003cbr/\u003e--model_cls, --task, etc.\"]\n        ConfigJSON[\"--config_json file\u003cbr/\u003eTask-specific settings\"]\n        ModelConfig[\"model_path/config.json\u003cbr/\u003eModel architecture params\"]\n        RuntimeMods[\"Runtime modifications\u003cbr/\u003eenable_offload(), etc.\"]\n    end\n    \n    subgraph \"Configuration Object\"\n        LockableDict[\"LockableDict\u003cbr/\u003ePrevents modification after init\"]\n        Lock[\"config.lock()\u003cbr/\u003eCalled in init_modules()\"]\n        Unlock[\"config.temporarily_unlocked()\u003cbr/\u003eContext manager for updates\"]\n    end\n    \n    DefaultConfig --\u003e Merge1[\"Update\"]\n    ArgsConfig --\u003e Merge1\n    Merge1 --\u003e Merge2[\"Update\"]\n    ConfigJSON --\u003e Merge2\n    Merge2 --\u003e Merge3[\"Update\"]\n    ModelConfig --\u003e Merge3\n    Merge3 --\u003e LockableDict\n    \n    RuntimeMods -.-\u003e|Before lock| LockableDict\n    \n    LockableDict --\u003e Lock\n    Lock --\u003e Unlock\n```\n\n**Configuration Hierarchy**:\n\n1. **Default config** (`get_default_config()` in [lightx2v/utils/set_config.py:14-34]()):\n   - Base settings: `cpu_offload=False`, `feature_caching=\"NoCaching\"`, etc.\n   - Returns a `LockableDict` instance\n\n2. **Command-line arguments** (parsed in `main()` in [lightx2v/infer.py:39-138]()):\n   - `--model_cls`, `--task`, `--model_path`, `--prompt`, etc.\n   - Updates config via `config.update()`\n\n3. **Config JSON file** (`--config_json` in [lightx2v/utils/set_config.py:41-45]()):\n   - Task-specific settings like `infer_steps`, `guidance_scale`, `attn_mode`\n   - Loaded via `json.load()` and merged\n\n4. **Model config** (`model_path/config.json` in [lightx2v/utils/set_config.py:47-105]()):\n   - Architecture parameters: `num_layers`, `dim`, `num_heads`, `vae_stride`\n   - Loaded automatically based on model folder structure\n\n5. **Runtime modifications** (via `LightX2VPipeline` methods):\n   - `enable_offload()`, `enable_quantization()`, `create_generator()` update config before locking\n\n**Locking Mechanism**:\n\nThe `LockableDict` class (in [lightx2v/utils/lockable_dict.py]()) prevents accidental configuration changes after model initialization:\n\n```python\n# In DefaultRunner.init_modules():\nself.config.lock()  # After this, direct updates raise an error\n\n# To update after locking:\nwith self.config.temporarily_unlocked():\n    self.config.update({\"key\": \"value\"})\n```\n\nThis prevents bugs from configuration drift during inference (e.g., accidentally changing `infer_steps` mid-generation).\n\n**Config Validation**:\n\nSpecific validation occurs in [lightx2v/utils/set_config.py:107-127]():\n- Ensures `num_frames` is compatible with `vae_stride`\n- Calculates derived values like `attnmap_frame_num` for transformer blocks\n- Loads diffusers VAE config if present\n\n**Sources:** [lightx2v/utils/set_config.py:14-128](), [lightx2v/infer.py:39-145](), [lightx2v/models/runners/default_runner.py:95]()\n\n---\n\n## Hardware Abstraction Layer\n\nLightX2V supports **8 hardware backends** through the `lightx2v_platform` module, enabling deployment across diverse accelerators without code changes:\n\n```mermaid\ngraph TB\n    subgraph \"Platform Registration System\"\n        Registry[\"PLATFORM_DEVICE_REGISTER\"]\n        \n        CUDAClass[\"CUDADevice\"]\n        MLUClass[\"MLUDevice\"]\n        DCUClass[\"DCUDevice\"]\n        MUSAClass[\"MUSADevice\"]\n        AscendClass[\"AscendDevice\"]\n        AMDClass[\"AMDDevice\"]\n        GCUClass[\"GCUDevice\"]\n        MetaXClass[\"MetaXDevice\"]\n        \n        Registry --\u003e|\"'cuda'\"| CUDAClass\n        Registry --\u003e|\"'mlu'\"| MLUClass\n        Registry --\u003e|\"'dcu'\"| DCUClass\n        Registry --\u003e|\"'musa'\"| MUSAClass\n        Registry --\u003e|\"'ascend'\"| AscendClass\n        Registry --\u003e|\"'amd'\"| AMDClass\n        Registry --\u003e|\"'gcu'\"| GCUClass\n        Registry --\u003e|\"'metax'\"| MetaXClass\n    end\n    \n    subgraph \"Runtime Device Selection\"\n        EnvVar[\"export PLATFORM=mlu\"]\n        GetPlatform[\"os.getenv('PLATFORM', 'cuda')\"]\n        LookupDevice[\"PLATFORM_DEVICE_REGISTER.get(platform)\"]\n        DeviceInstance[\"platform_device instance\"]\n        \n        EnvVar --\u003e GetPlatform\n        GetPlatform --\u003e LookupDevice\n        LookupDevice --\u003e DeviceInstance\n    end\n    \n    subgraph \"Global Device Variables\"\n        AIDevice[\"AI_DEVICE = 'mlu'\"]\n        AIDeviceID[\"AI_DEVICE_ID = 0\"]\n        UseTensor[\"tensor.to(AI_DEVICE)\"]\n        UseDevice[\"torch.device(AI_DEVICE)\"]\n        \n        DeviceInstance --\u003e AIDevice\n        DeviceInstance --\u003e AIDeviceID\n        AIDevice --\u003e UseTensor\n        AIDevice --\u003e UseDevice\n    end\n    \n    subgraph \"Platform-Specific Registries\"\n        RopeReg[\"ROPE_REGISTER\"]\n        MMWeightReg[\"MM_WEIGHT_REGISTER\"]\n        AttnReg[\"Attention Operators\"]\n        \n        RopeCUDA[\"apply_rope_cuda\"]\n        RopeTorch[\"apply_rope_torch_naive\"]\n        RopeDCU[\"apply_rope_dcu_specific\"]\n        \n        MMCuda[\"MMWeightCuda\"]\n        MMQuant[\"MMWeightQuant\"]\n        MMFP8[\"MMFP8Weight\"]\n        \n        RopeReg --\u003e RopeCUDA\n        RopeReg --\u003e RopeTorch\n        RopeReg --\u003e RopeDCU\n        \n        MMWeightReg --\u003e MMCuda\n        MMWeightReg --\u003e MMQuant\n        MMWeightReg --\u003e MMFP8\n    end\n    \n    DeviceInstance --\u003e RopeReg\n    DeviceInstance --\u003e MMWeightReg\n    DeviceInstance --\u003e AttnReg\n```\n\n**Platform Device Classes:**\n\nEach backend implements a device class with platform-specific initialization [lightx2v_platform/]:\n- `CUDADevice`: NVIDIA GPUs (RTX 30/40/50, H100) - default backend\n- `MLUDevice`: Cambricon MLU590 ()\n- `DCUDevice`: Hygon DCU ()\n- `MUSADevice`: MThreads MUSA ()\n- `AscendDevice`: Huawei Ascend 910B ()\n- `AMDDevice`: AMD ROCm\n- `GCUDevice`: Enflame S60 GCU ()\n- `MetaXDevice`: MetaX C500 ()\n\n**Key Abstraction Mechanisms:**\n\n1. **Global Device Constants** [lightx2v_platform/base/global_var.py]():\n   ```python\n   AI_DEVICE = \"cuda\"  # or \"mlu\", \"dcu\", etc.\n   AI_DEVICE_ID = 0\n   ```\n   - Replaces hardcoded `\"cuda\"` strings throughout codebase\n   - All tensor operations use: `tensor.to(AI_DEVICE)`, `torch.device(AI_DEVICE)`\n\n2. **Platform Device API:**\n   Each device class implements:\n   - `init_parallel_env()`: Initialize distributed training/inference\n   - `synchronize()`: Device synchronization\n   - `set_device(device_id)`: Set active device\n   - `current_device()`: Get current device ID\n\n3. **Operation Registries:**\n   - **`ROPE_REGISTER`**: Rotary position embeddings with platform-specific implementations\n     - CUDA: optimized CUDA kernels\n     - DCU: torch-native fallback\n     - MLU: MLU-optimized kernels\n   \n   - **`MM_WEIGHT_REGISTER`**: Matrix multiplication with quantization support\n     - `MMWeightCuda`: Standard CUDA GEMM\n     - `MMWeightQuant`: INT8/FP8 quantized GEMM\n     - `MMFP8Weight`: FP8 per-tensor quantization\n   \n   - **Attention Operators**: Platform-specific flash attention, sage attention, etc.\n\n4. **Automatic Fallbacks:**\n   When platform-specific implementations are unavailable, falls back to PyTorch native operations\n\n**Usage Example:**\n\n```bash\n# Set platform before running\nexport PLATFORM=mlu  # For Cambricon MLU590\nexport PLATFORM=dcu  # For Hygon DCU\nexport PLATFORM=musa # For MThreads MUSA\n\n# Run inference\npython -m lightx2v.infer \\\n    --model_cls wan2.1 \\\n    --task t2v \\\n    --model_path /path/to/model\n```\n\nThe platform is initialized early in the execution flow [lightx2v/infer.py:148-151]():\n```python\nif config[\"parallel\"]:\n    platform_device = PLATFORM_DEVICE_REGISTER.get(os.getenv(\"PLATFORM\", \"cuda\"))\n    platform_device.init_parallel_env()\n```\n\n**Docker Environments:**\n\nPre-built Docker images available for each platform [dockerfiles/platforms/]:\n- `lightx2v-cu128`: NVIDIA CUDA 12.8\n- `lightx2v-mlu590`: Cambricon MLU590\n- `lightx2v-dcu`: Hygon DCU\n- See [lightx2v_platform/README.md:16-18]() for links to platform-specific scripts\n\n**Sources:** [lightx2v_platform/README.md:1-19](), [lightx2v_platform/README_zh.md:1-20](), [lightx2v/infer.py:148-151](), [lightx2v_platform/base/global_var.py](), [lightx2v/models/networks/wan/infer/transformer_infer.py:36-57]()\n\n---\n\n## Summary\n\nLightX2V's architecture achieves **flexibility and modularity** through:\n\n1. **Layered separation**: User interfaces  Core framework  Runners  Models  Hardware\n2. **Registry pattern**: Dynamic model instantiation without hard dependencies\n3. **Three-stage pipeline**: Clear separation of encoding, diffusion, and decoding\n4. **Weight-inference split**: Enables modular optimizations (quantization, offloading, LoRA)\n5. **Configuration hierarchy**: Multiple sources with locking to prevent runtime drift\n6. **Hardware abstraction**: 8+ backends through platform registration\n\nThis design allows LightX2V to support multiple model families (Wan, Qwen, LTX2, HunyuanVideo), diverse task types (T2V, I2V, S2V, T2I), and extensive optimization strategies, all while maintaining clean separation of concerns and extensibility for future models and hardware backends.\n\n**Sources:** [README.md:1-336](), [lightx2v/infer.py:1-173](), [lightx2v/models/runners/default_runner.py:1-524](), [lightx2v/models/networks/wan/model.py:1-535](), [lightx2v/utils/set_config.py:1-166]()"])</script><script>self.__next_f.push([1,"1a:T4038,"])</script><script>self.__next_f.push([1,"# Hardware Platform Support\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [README_zh.md](README_zh.md)\n- [lightx2v_platform/README.md](lightx2v_platform/README.md)\n- [lightx2v_platform/README_zh.md](lightx2v_platform/README_zh.md)\n- [scripts/hunyuan_video_15/README.md](scripts/hunyuan_video_15/README.md)\n\n\u003c/details\u003e\n\n\n\nThis document describes LightX2V's hardware abstraction layer and multi-backend support system. LightX2V supports inference on 8 different hardware platforms through the `lightx2v_platform` abstraction layer, enabling deployment on both NVIDIA GPUs and Chinese domestic AI accelerators.\n\nFor information about optimization features that work across platforms (quantization, offloading, etc.), see [Key Features and Optimization Techniques](#1.1). For deployment-specific guides, see the deployment sections in [Getting Started](#2).\n\n---\n\n## Platform Abstraction Architecture\n\nThe `lightx2v_platform` module provides a unified interface for different hardware backends, abstracting device operations, memory management, and compute primitives. This enables the core LightX2V framework to remain hardware-agnostic while supporting diverse acceleration hardware.\n\n### System Architecture\n\n```mermaid\ngraph TB\n    subgraph \"LightX2V Core Framework\"\n        Pipeline[\"LightX2VPipeline\"]\n        Runners[\"Model Runners\u003cbr/\u003e(WanAudioRunner, QwenImageRunner, etc.)\"]\n        Models[\"Transformer Models\u003cbr/\u003eVAEs, Encoders\"]\n    end\n    \n    subgraph \"lightx2v_platform Abstraction Layer\"\n        PlatformAPI[\"Platform API\u003cbr/\u003eDevice operations\u003cbr/\u003eMemory management\u003cbr/\u003eCompute primitives\"]\n    end\n    \n    subgraph \"Hardware Backends\"\n        CUDA[\"CUDA Backend\u003cbr/\u003envidia.py\"]\n        MLU[\"Cambricon MLU Backend\u003cbr/\u003ecambricon.py\"]\n        MetaX[\"MetaX Backend\u003cbr/\u003emetax.py\"]\n        DCU[\"Hygon DCU Backend\u003cbr/\u003ehygon.py\"]\n        Ascend[\"Ascend Backend\u003cbr/\u003eascend.py\"]\n        ROCm[\"AMD ROCm Backend\u003cbr/\u003eamd.py\"]\n        MUSA[\"MThreads MUSA Backend\u003cbr/\u003emthreads.py\"]\n        GCU[\"Enflame GCU Backend\u003cbr/\u003eenflame.py\"]\n    end\n    \n    Pipeline --\u003e Runners\n    Runners --\u003e Models\n    Models --\u003e PlatformAPI\n    \n    PlatformAPI --\u003e CUDA\n    PlatformAPI --\u003e MLU\n    PlatformAPI --\u003e MetaX\n    PlatformAPI --\u003e DCU\n    PlatformAPI --\u003e Ascend\n    PlatformAPI --\u003e ROCm\n    PlatformAPI --\u003e MUSA\n    PlatformAPI --\u003e GCU\n```\n\n**Sources:** [lightx2v_platform/README.md:1-19]()\n\n### Design Principles\n\nThe `lightx2v_platform` module is independent of the main `lightx2v` framework, following these principles:\n\n1. **Backend Isolation**: Each hardware backend is implemented in its own module within `lightx2v_platform`\n2. **Unified Interface**: All backends expose the same API surface, ensuring consistent behavior across platforms\n3. **Minimal Core Changes**: Adding support for a new hardware platform requires changes only within `lightx2v_platform`, not in the core framework\n4. **Transparent Fallback**: Platform-specific optimizations (e.g., custom kernels) can be added without breaking compatibility\n\n**Sources:** [lightx2v_platform/README.md:1-6]()\n\n---\n\n## Supported Hardware Backends\n\nLightX2V currently supports 8 hardware platforms with varying levels of feature parity:\n\n| Backend | Vendor | Hardware | Status | Added |\n|---------|--------|----------|--------|-------|\n| **NVIDIA CUDA** | NVIDIA | RTX 30/40/50 series\u003cbr/\u003eH100, A100 | Full support (Primary) | Initial |\n| **Cambricon MLU** | Cambricon () | MLU590 | Production ready | Dec 4, 2025 |\n| **MetaX** | MetaX () | C500 | Production ready | Dec 4, 2025 |\n| **Hygon DCU** | Hygon () | DCU | Production ready | Dec 15, 2025 |\n| **Ascend** | Huawei () | 910B | Production ready | Dec 25, 2025 |\n| **AMD ROCm** | AMD | ROCm-compatible GPUs | Production ready | Dec 25, 2025 |\n| **MThreads MUSA** | MThreads () | MUSA architecture | Production ready | Dec 27, 2025 |\n| **Enflame GCU** | Enflame () | S60 GCU | Production ready | Jan 6, 2026 |\n\n### Feature Support Matrix\n\n| Feature | NVIDIA | Other Backends |\n|---------|--------|----------------|\n| Basic Inference |  Full |  Full |\n| Quantization (INT8/FP8) |  Full |  Varies by backend |\n| Flash Attention |  Full |  Platform-specific implementations |\n| CPU Offloading |  Full |  Full |\n| Multi-GPU (TP/SP) |  Full |  Backend-dependent |\n| GGUF Format |  Supported |  MLU590/C500 explicitly supported |\n| Custom Kernels |  Triton, CUDA |  Platform-specific alternatives |\n\n**Sources:** [README.md:51-66](), [README_zh.md:51-66](), [lightx2v_platform/README.md:7-14]()\n\n---\n\n## Platform Detection and Selection\n\n### Automatic Backend Selection\n\nThe platform backend is automatically selected at runtime based on available hardware:\n\n```mermaid\nflowchart TB\n    Start[\"Import lightx2v_platform\"]\n    \n    CheckCUDA{\"Check for\u003cbr/\u003eCUDA availability\"}\n    CheckMLU{\"Check for\u003cbr/\u003eCambricon MLU\"}\n    CheckMetaX{\"Check for\u003cbr/\u003eMetaX\"}\n    CheckDCU{\"Check for\u003cbr/\u003eHygon DCU\"}\n    CheckAscend{\"Check for\u003cbr/\u003eAscend\"}\n    CheckROCm{\"Check for\u003cbr/\u003eAMD ROCm\"}\n    CheckMUSA{\"Check for\u003cbr/\u003eMThreads MUSA\"}\n    CheckGCU{\"Check for\u003cbr/\u003eEnflame GCU\"}\n    \n    LoadNVIDIA[\"Load NVIDIA Backend\u003cbr/\u003ecuda_ops\"]\n    LoadMLU[\"Load MLU Backend\u003cbr/\u003emlu_ops\"]\n    LoadMetaX[\"Load MetaX Backend\u003cbr/\u003emetax_ops\"]\n    LoadDCU[\"Load DCU Backend\u003cbr/\u003edcu_ops\"]\n    LoadAscend[\"Load Ascend Backend\u003cbr/\u003eascend_ops\"]\n    LoadROCm[\"Load ROCm Backend\u003cbr/\u003erocm_ops\"]\n    LoadMUSA[\"Load MUSA Backend\u003cbr/\u003emusa_ops\"]\n    LoadGCU[\"Load GCU Backend\u003cbr/\u003egcu_ops\"]\n    \n    Fallback[\"Fallback to CPU\"]\n    \n    Start --\u003e CheckCUDA\n    CheckCUDA --\u003e|Available| LoadNVIDIA\n    CheckCUDA --\u003e|Not Available| CheckMLU\n    CheckMLU --\u003e|Available| LoadMLU\n    CheckMLU --\u003e|Not Available| CheckMetaX\n    CheckMetaX --\u003e|Available| LoadMetaX\n    CheckMetaX --\u003e|Not Available| CheckDCU\n    CheckDCU --\u003e|Available| LoadDCU\n    CheckDCU --\u003e|Not Available| CheckAscend\n    CheckAscend --\u003e|Available| LoadAscend\n    CheckAscend --\u003e|Not Available| CheckROCm\n    CheckROCm --\u003e|Available| LoadROCm\n    CheckROCm --\u003e|Not Available| CheckMUSA\n    CheckMUSA --\u003e|Available| LoadMUSA\n    CheckMUSA --\u003e|Not Available| CheckGCU\n    CheckGCU --\u003e|Available| LoadGCU\n    CheckGCU --\u003e|Not Available| Fallback\n```\n\nThe platform detection occurs during the import of `lightx2v_platform`, making platform selection transparent to user code. The same LightX2V scripts and API calls work across all platforms without modification.\n\n**Sources:** [lightx2v_platform/README.md:1-6]()\n\n---\n\n## Environment Setup by Platform\n\n### Docker Environments\n\nEach non-NVIDIA platform has a dedicated Docker environment that includes platform-specific drivers, runtimes, and dependencies:\n\n| Platform | Docker Location | Image Base |\n|----------|----------------|------------|\n| NVIDIA CUDA | `dockerfiles/platforms/nvidia/` | CUDA 12.4/12.8 |\n| Cambricon MLU590 | `dockerfiles/platforms/cambricon/` | Neuware SDK |\n| MetaX C500 | `dockerfiles/platforms/metax/` | MetaX Runtime |\n| Hygon DCU | `dockerfiles/platforms/hygon/` | DCU ROCm |\n| Ascend 910B | `dockerfiles/platforms/ascend/` | CANN Toolkit |\n| AMD ROCm | `dockerfiles/platforms/amd/` | ROCm 5.7+ |\n| MThreads MUSA | `dockerfiles/platforms/mthreads/` | MUSA Toolkit |\n| Enflame GCU | `dockerfiles/platforms/enflame/` | TopsSDK |\n\n### Usage Scripts\n\nPlatform-specific usage examples are provided in the scripts directory:\n\n```\nscripts/platforms/\n cambricon/          # MLU590 inference scripts\n metax/              # MetaX C500 scripts\n hygon/              # Hygon DCU scripts\n ascend/             # Ascend 910B scripts\n amd/                # AMD ROCm scripts\n mthreads/           # MThreads MUSA scripts\n enflame/            # Enflame GCU scripts\n```\n\n**Sources:** [lightx2v_platform/README.md:16-18](), [scripts/hunyuan_video_15/README.md:8]()\n\n### Setup Example: Non-NVIDIA Platform\n\nThe following shows the typical setup pattern for using a non-NVIDIA platform:\n\n```bash\n# 1. Pull platform-specific Docker image\ndocker pull lightx2v/lightx2v:platform-specific-tag\n\n# 2. Run container with platform device access\ndocker run --gpus all -itd --ipc=host \\\n    --name lightx2v_container \\\n    -v /path/to/models:/models \\\n    --entrypoint /bin/bash \\\n    lightx2v/lightx2v:platform-specific-tag\n\n# 3. Inside container, use standard LightX2V commands\npython examples/wan/wan_i2v.py\n```\n\nThe platform abstraction ensures that the same Python code runs unchanged across all platforms. Platform-specific optimizations (e.g., custom attention kernels) are selected automatically based on the detected backend.\n\n**Sources:** [scripts/hunyuan_video_15/README.md:6-14]()\n\n---\n\n## Platform API Components\n\n### Core Abstractions\n\nThe `lightx2v_platform` module abstracts the following hardware operations:\n\n```mermaid\ngraph LR\n    subgraph \"Platform API Surface\"\n        DeviceOps[\"Device Operations\u003cbr/\u003e- device allocation\u003cbr/\u003e- device synchronization\u003cbr/\u003e- device properties\"]\n        MemoryOps[\"Memory Operations\u003cbr/\u003e- memory allocation\u003cbr/\u003e- memory copy (H2D, D2H, D2D)\u003cbr/\u003e- memory pools\"]\n        ComputeOps[\"Compute Operations\u003cbr/\u003e- tensor operations\u003cbr/\u003e- attention kernels\u003cbr/\u003e- quantization kernels\"]\n        StreamOps[\"Stream Operations\u003cbr/\u003e- stream creation\u003cbr/\u003e- stream synchronization\u003cbr/\u003e- event handling\"]\n    end\n    \n    subgraph \"Backend Implementation\"\n        NativeAPI[\"Platform Native API\u003cbr/\u003e(CUDA, MLU, ROCm, etc.)\"]\n    end\n    \n    DeviceOps --\u003e NativeAPI\n    MemoryOps --\u003e NativeAPI\n    ComputeOps --\u003e NativeAPI\n    StreamOps --\u003e NativeAPI\n```\n\n### Key Interface Methods\n\nWhile the exact implementation varies by backend, each platform module implements a consistent interface for:\n\n- **Device Management**: Device enumeration, selection, and properties query\n- **Tensor Operations**: Tensor creation, manipulation on accelerator device\n- **Memory Management**: Unified memory allocation and data transfer APIs\n- **Compute Kernels**: Platform-optimized implementations of attention, quantization, and other operations\n- **Stream Control**: Asynchronous execution and synchronization primitives\n\n**Sources:** [lightx2v_platform/README.md:1-6]()\n\n---\n\n## GGUF Format Support\n\nGGUF (GPT-Generated Unified Format) model files are explicitly supported on Cambricon MLU590 and MetaX C500 platforms. This format enables efficient model storage and loading with built-in quantization metadata.\n\n### GGUF Benefits on Chinese Domestic Hardware\n\n1. **Reduced Memory Footprint**: GGUF files store quantized weights efficiently\n2. **Platform Optimization**: Format designed for memory-constrained accelerators\n3. **Simplified Deployment**: Single file contains model weights and configuration\n\nThe GGUF support was added specifically to facilitate deployment on domestic Chinese accelerators with different memory hierarchies compared to NVIDIA GPUs.\n\n**Sources:** [README.md:65](), [README_zh.md:65]()\n\n---\n\n## Adding New Hardware Backends\n\nTo add support for a new hardware platform:\n\n### Implementation Steps\n\n1. **Create Backend Module**: Add a new module in `lightx2v_platform/` implementing the platform API\n2. **Implement Core Operations**: Provide implementations for device, memory, and compute operations\n3. **Add Detection Logic**: Implement platform detection in the initialization code\n4. **Create Docker Environment**: Build a Docker image with platform-specific drivers in `dockerfiles/platforms/`\n5. **Provide Usage Scripts**: Add example scripts in `scripts/platforms/` demonstrating platform usage\n6. **Test Coverage**: Validate all model runners work correctly on the new platform\n\n### Minimal Implementation Requirements\n\nA new backend must provide:\n\n- Device enumeration and selection\n- Memory allocation (device and host)\n- Data transfer primitives (host-to-device, device-to-host, device-to-device)\n- Basic tensor operations\n- Stream/event synchronization\n- (Optional) Platform-optimized attention kernels\n- (Optional) Platform-optimized quantization kernels\n\nThe core LightX2V framework requires no changes when a new backend is properly implemented within `lightx2v_platform`.\n\n**Sources:** [lightx2v_platform/README.md:1-6]()\n\n---\n\n## Platform-Specific Considerations\n\n### NVIDIA CUDA (Primary Platform)\n\nNVIDIA CUDA is the primary development and testing platform for LightX2V, with full support for all features:\n\n- **Attention Operators**: Flash Attention 2/3, Sage Attention, Radial Attention\n- **Quantization**: INT8, FP8, NVFP4, MXFP4/6/8 with custom Triton/CUDA kernels\n- **Memory Management**: Advanced offloading with async streaming and double-buffering\n- **Multi-GPU**: Full support for CFG, tensor, sequence, and pipeline parallelism\n\n**Sources:** [README.md:259]()\n\n### Chinese Domestic Accelerators\n\nChinese domestic accelerators (Cambricon, MetaX, Hygon, Ascend, MThreads, Enflame) have specific characteristics:\n\n- **Ecosystem Integration**: Platform-specific SDKs and development toolchains\n- **Memory Architecture**: Different memory hierarchies requiring adapted offloading strategies\n- **Compute Capabilities**: Platform-specific attention and quantization kernel implementations\n- **Compliance**: Important for strategic domestic chip adoption in China\n\nAll domestic platforms support the core inference pipeline, with platform-optimized kernels provided where available.\n\n**Sources:** [README.md:51-66](), [README_zh.md:51-66](), [lightx2v_platform/README.md:7-14]()\n\n### AMD ROCm\n\nAMD ROCm support enables deployment on AMD GPUs with ROCm 5.7+. The ROCm backend uses HIP (Heterogeneous-compute Interface for Portability) to provide CUDA-compatible APIs.\n\n**Sources:** [README.md:57](), [lightx2v_platform/README.md:13]()\n\n---\n\n## Performance Across Platforms\n\nWhile NVIDIA GPUs are the primary development platform with the most optimizations, other platforms achieve competitive performance through:\n\n1. **Platform-Optimized Kernels**: Backend-specific implementations of attention and quantization\n2. **Memory Hierarchy Adaptation**: Offloading strategies tuned for platform memory characteristics\n3. **Format Optimization**: GGUF support on platforms that benefit from it\n4. **SDK Integration**: Deep integration with platform-specific SDKs and toolchains\n\nPerformance characteristics vary by platform based on hardware capabilities, driver maturity, and available optimizations. Users should refer to platform-specific scripts and documentation for optimal configuration on each backend.\n\n**Sources:** [README.md:72-114](), [lightx2v_platform/README.md:1-19]()\n\n---\n\n## Cross-Platform Testing\n\nThe platform abstraction enables cross-platform testing and validation:\n\n```mermaid\ngraph TB\n    subgraph \"Testing Strategy\"\n        CoreTests[\"Core Framework Tests\u003cbr/\u003ePlatform-agnostic\"]\n        PlatformTests[\"Platform-Specific Tests\u003cbr/\u003eKernel validation\"]\n    end\n    \n    subgraph \"Validation Targets\"\n        Correctness[\"Correctness Validation\u003cbr/\u003eOutput consistency\"]\n        Performance[\"Performance Validation\u003cbr/\u003eSpeed benchmarks\"]\n        Memory[\"Memory Validation\u003cbr/\u003eResource usage\"]\n    end\n    \n    CoreTests --\u003e Correctness\n    PlatformTests --\u003e Correctness\n    PlatformTests --\u003e Performance\n    PlatformTests --\u003e Memory\n```\n\nEach backend undergoes validation to ensure:\n- **Correctness**: Generated outputs match reference implementations\n- **Performance**: Inference speed meets platform capabilities\n- **Memory**: Resource usage stays within platform constraints\n- **Stability**: Long-running inference remains stable\n\n**Sources:** [lightx2v_platform/README.md:1-19]()\n\n---\n\n## Summary\n\nThe `lightx2v_platform` abstraction layer enables LightX2V to support 8 diverse hardware platforms while maintaining a unified user interface. This design allows:\n\n- **Transparent Platform Usage**: Same code works across all platforms\n- **Easy Extension**: New platforms added without core framework changes\n- **Optimized Performance**: Platform-specific optimizations where available\n- **Broad Deployment**: Support for both international and Chinese domestic hardware\n\nThe modular design ensures that LightX2V can adapt to the evolving hardware landscape while providing consistent inference capabilities across platforms.\n\n**Sources:** [lightx2v_platform/README.md:1-19](), [README.md:51-66](), [README_zh.md:51-66]()"])</script><script>self.__next_f.push([1,"1b:T59f0,"])</script><script>self.__next_f.push([1,"# Model Ecosystem and Supported Tasks\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [README_zh.md](README_zh.md)\n- [lightx2v/models/networks/hunyuan_video/model.py](lightx2v/models/networks/hunyuan_video/model.py)\n- [lightx2v/models/networks/longcat_image/model.py](lightx2v/models/networks/longcat_image/model.py)\n- [lightx2v/models/networks/lora_adapter.py](lightx2v/models/networks/lora_adapter.py)\n- [lightx2v/models/networks/ltx2/model.py](lightx2v/models/networks/ltx2/model.py)\n- [lightx2v/models/networks/wan/animate_model.py](lightx2v/models/networks/wan/animate_model.py)\n- [lightx2v/models/runners/longcat_image/longcat_image_runner.py](lightx2v/models/runners/longcat_image/longcat_image_runner.py)\n- [lightx2v/models/runners/ltx2/ltx2_runner.py](lightx2v/models/runners/ltx2/ltx2_runner.py)\n- [lightx2v_platform/README.md](lightx2v_platform/README.md)\n- [lightx2v_platform/README_zh.md](lightx2v_platform/README_zh.md)\n- [scripts/hunyuan_video_15/README.md](scripts/hunyuan_video_15/README.md)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides a comprehensive overview of the model ecosystem supported by LightX2V, including all model families, their capabilities, supported generation tasks, and available optimization variants. It maps high-level model names to their concrete implementations in the codebase and explains how different models are organized and accessed.\n\nFor detailed information about specific model runners and their usage patterns, see [Model Runners and Tasks](#5). For hardware platform support, see [Hardware Platform Support](#1.3). For optimization techniques applied to these models, see [Performance Optimization](#6).\n\n## Model Family Overview\n\nLightX2V supports seven major model families, each with distinct architectures and capabilities:\n\n```mermaid\ngraph TB\n    subgraph WAN[\"WAN Family\"]\n        WAN21[\"wan2.1\u003cbr/\u003eSingle model architecture\"]\n        WAN22[\"wan2.2\u003cbr/\u003eDual guidance scale\"]\n        WAN22MOE[\"wan2.2_moe\u003cbr/\u003eMixture-of-Experts\"]\n    end\n    \n    subgraph Qwen[\"Qwen Family\"]\n        QwenImage[\"qwen-image\u003cbr/\u003eText-to-Image\"]\n        QwenEdit[\"qwen-image-edit\u003cbr/\u003eImage Editing\"]\n        QwenEdit2509[\"qwen-image-edit-2509\"]\n        QwenEdit2511[\"qwen-image-edit-2511\"]\n    end\n    \n    subgraph ZImage[\"Z-Image Family\"]\n        ZImageTurbo[\"z-image-turbo\u003cbr/\u003eFast generation\"]\n    end\n    \n    subgraph Hunyuan[\"HunyuanVideo Family\"]\n        HY15[\"hunyuan_video_1.5\u003cbr/\u003e720p video generation\"]\n    end\n    \n    subgraph LTX[\"LTX Family\"]\n        LTX2[\"ltx2\u003cbr/\u003eAudio+Video synthesis\"]\n    end\n    \n    subgraph LongCat[\"LongCat Family\"]\n        LongCatImg[\"longcat_image\u003cbr/\u003eHigh-resolution images\"]\n    end\n    \n    subgraph Other[\"Other Models\"]\n        WorldPlay[\"worldplay\u003cbr/\u003eVideo generation\"]\n        CausVid[\"wan2.1-causvid\u003cbr/\u003eAutoregressive video\"]\n    end\n```\n\n**Sources:** [README.md:207-235](), [README_zh.md:206-233]()\n\n### WAN Models\n\nThe WAN (Wanx AI) family represents the most versatile model ecosystem in LightX2V, with multiple architectural variants:\n\n| Model Variant | Architecture | Guidance Scale | Primary Use Cases |\n|--------------|--------------|----------------|-------------------|\n| `wan2.1` | Single unified model | Scalar (e.g., 5.0) | T2V, I2V, S2V |\n| `wan2.2` | Dual model system | Dual values [high, low] | T2V, I2V, S2V |\n| `wan2.2_moe` | Mixture-of-Experts | Dual values [high, low] | High/low noise specialization |\n\nThe WAN family uniquely supports **Speech-to-Video (S2V)** generation, enabling audio-driven video synthesis. This is the highest-importance feature in LightX2V according to the system analysis.\n\n**Sources:** [README.md:212](), [lightx2v/models/networks/wan/animate_model.py:1-23]()\n\n### Qwen Models\n\nThe Qwen family specializes in image generation and editing tasks, leveraging vision-language models:\n\n- **qwen-image**: Base text-to-image generation with support for various aspect ratios\n- **qwen-image-edit**: Instruction-based image editing capabilities\n- **qwen-image-edit-2509/2511**: Successive improvements with better editing fidelity\n\nQwen models use the `Qwen25_VLForConditionalGeneration` architecture for vision-language understanding.\n\n**Sources:** [README.md:213-217](), [lightx2v/models/runners/longcat_image/longcat_image_runner.py:1-413]()\n\n### HunyuanVideo Models\n\nHunyuanVideo-1.5 provides high-quality video generation with 720p support:\n\n- Native 720p resolution support\n- 121 frame generation (approximately 5 seconds at 24fps)\n- Integrated with LightTAE for fast VAE decoding\n\n**Sources:** [README.md:211](), [scripts/hunyuan_video_15/README.md:1-104](), [lightx2v/models/networks/hunyuan_video/model.py:1-129]()\n\n### LTX Models\n\nLTX-2 is unique in generating synchronized audio and video simultaneously:\n\n- Multi-stage pipeline with optional upsampling\n- Separate video and audio VAEs\n- Support for both T2AV (text-to-audio+video) and I2AV (image-to-audio+video)\n\n**Sources:** [README.md:210](), [lightx2v/models/runners/ltx2/ltx2_runner.py:1-554]()\n\n### LongCat Models\n\nLongCat specializes in high-resolution image generation:\n\n- Support for custom aspect ratios and resolutions\n- 10 double-stream + 20 single-stream transformer blocks\n- Support for both T2I and I2I (image editing with vision-language conditioning)\n\n**Sources:** [lightx2v/models/runners/longcat_image/longcat_image_runner.py:1-413](), [lightx2v/models/networks/longcat_image/model.py:1-132]()\n\n## Task Type Classification\n\n```mermaid\ngraph LR\n    subgraph VideoTasks[\"Video Generation Tasks\"]\n        T2V[\"T2V\u003cbr/\u003eText  Video\"]\n        I2V[\"I2V\u003cbr/\u003eImage  Video\"]\n        S2V[\"S2V\u003cbr/\u003eSpeech/Audio  Video\"]\n        T2AV[\"T2AV\u003cbr/\u003eText  Audio+Video\"]\n        I2AV[\"I2AV\u003cbr/\u003eImage  Audio+Video\"]\n    end\n    \n    subgraph ImageTasks[\"Image Generation Tasks\"]\n        T2I[\"T2I\u003cbr/\u003eText  Image\"]\n        I2I[\"I2I\u003cbr/\u003eImage  Image\u003cbr/\u003eImage Editing\"]\n    end\n    \n    T2V --\u003e T2VDesc[\"Generate video from text prompt\"]\n    I2V --\u003e I2VDesc[\"Animate still image\"]\n    S2V --\u003e S2VDesc[\"Generate talking head video\u003cbr/\u003efrom audio\"]\n    T2I --\u003e T2IDesc[\"Generate image from text\"]\n    I2I --\u003e I2IDesc[\"Edit image based on instructions\"]\n    T2AV --\u003e T2AVDesc[\"Generate synchronized\u003cbr/\u003eaudio and video\"]\n    I2AV --\u003e I2AVDesc[\"Generate audio+video\u003cbr/\u003efrom image\"]\n```\n\n**Sources:** [README.md:19](), [README_zh.md:19]()\n\n### Task Implementation Details\n\nEach task type is implemented through specific runner classes and inference pipelines:\n\n| Task | Input Modalities | Output | Key Components |\n|------|-----------------|--------|----------------|\n| T2V | Text prompt | Video frames | Text encoder  DiT  VAE decoder |\n| I2V | Image + Text prompt | Video frames | Image encoder + Text encoder  DiT  VAE decoder |\n| S2V | Audio + Text prompt | Video frames | Audio encoder + Text encoder  DiT  VAE decoder |\n| T2I | Text prompt | Single image | Text encoder  DiT  VAE decoder |\n| I2I | Image + Text instruction | Edited image | Vision-language encoder  DiT  VAE decoder |\n| T2AV | Text prompt | Video + Audio | Text encoder  DiT  Video VAE + Audio VAE |\n| I2AV | Image + Text prompt | Video + Audio | Image encoder  DiT  Video VAE + Audio VAE |\n\n**Sources:** [lightx2v/models/runners/longcat_image/longcat_image_runner.py:94-101](), [lightx2v/models/runners/ltx2/ltx2_runner.py:169-192]()\n\n## Model-to-Task Mapping\n\nThe following diagram shows which model families support which generation tasks:\n\n```mermaid\ngraph TB\n    WAN[\"WAN Family\u003cbr/\u003ewan2.1, wan2.2, wan2.2_moe\"]\n    Qwen[\"Qwen Family\u003cbr/\u003eqwen-image variants\"]\n    ZImage[\"Z-Image Family\u003cbr/\u003ez-image-turbo\"]\n    Hunyuan[\"HunyuanVideo 1.5\"]\n    LTX[\"LTX-2\"]\n    LongCat[\"LongCat\"]\n    WorldPlay[\"WorldPlay\"]\n    \n    T2V[\"T2V\u003cbr/\u003eText-to-Video\"]\n    I2V[\"I2V\u003cbr/\u003eImage-to-Video\"]\n    S2V[\"S2V\u003cbr/\u003eSpeech-to-Video\u003cbr/\u003eUNIQUE TO WAN\"]\n    T2I[\"T2I\u003cbr/\u003eText-to-Image\"]\n    I2I[\"I2I\u003cbr/\u003eImage-to-Image\"]\n    T2AV[\"T2AV\u003cbr/\u003eText-to-Audio+Video\"]\n    I2AV[\"I2AV\u003cbr/\u003eImage-to-Audio+Video\"]\n    \n    WAN --\u003e T2V\n    WAN --\u003e I2V\n    WAN --\u003e S2V\n    \n    Qwen --\u003e T2I\n    Qwen --\u003e I2I\n    \n    ZImage --\u003e T2I\n    ZImage --\u003e I2I\n    \n    Hunyuan --\u003e T2V\n    Hunyuan --\u003e I2V\n    \n    LTX --\u003e T2AV\n    LTX --\u003e I2AV\n    \n    LongCat --\u003e T2I\n    LongCat --\u003e I2I\n    \n    WorldPlay --\u003e T2V\n    WorldPlay --\u003e I2V\n```\n\n**Sources:** [README.md:207-235]()\n\n### Task Configuration in Code\n\nTasks are specified in the configuration and determine which runner methods are invoked:\n\n```mermaid\ngraph TB\n    Config[\"task configuration parameter\"]\n    \n    subgraph RunnerInit[\"Runner Initialization\"]\n        CheckTask[\"Check task type in init_modules\"]\n        SetEncoder[\"Set appropriate run_input_encoder method\"]\n        SetDiT[\"Set appropriate run_dit method\"]\n    end\n    \n    subgraph LongCatExample[\"LongCat Example\"]\n        T2IPath[\"task='t2i'\u003cbr/\u003e _run_input_encoder_local_t2i\u003cbr/\u003e _run_dit_local\"]\n        I2IPath[\"task='i2i'\u003cbr/\u003e _run_input_encoder_local_i2i\u003cbr/\u003e _run_dit_local_i2i\"]\n    end\n    \n    subgraph LTXExample[\"LTX2 Example\"]\n        T2AVPath[\"task='t2av'\u003cbr/\u003e _run_input_encoder_local_t2av\"]\n        I2AVPath[\"task='i2av'\u003cbr/\u003e _run_input_encoder_local_i2av\u003cbr/\u003e run_vae_encoder\"]\n    end\n    \n    Config --\u003e CheckTask\n    CheckTask --\u003e SetEncoder\n    CheckTask --\u003e SetDiT\n    \n    SetEncoder --\u003e T2IPath\n    SetEncoder --\u003e I2IPath\n    SetEncoder --\u003e T2AVPath\n    SetEncoder --\u003e I2AVPath\n```\n\n**Sources:** [lightx2v/models/runners/longcat_image/longcat_image_runner.py:94-101](), [lightx2v/models/runners/ltx2/ltx2_runner.py:169-192]()\n\n## Runner Class Mapping\n\nEach model family is implemented through a specific runner class registered in the `RUNNER_REGISTER`:\n\n```mermaid\ngraph TB\n    Registry[\"RUNNER_REGISTER\u003cbr/\u003eRegistry Factory\"]\n    \n    subgraph Runners[\"Runner Classes\"]\n        WanAudioRunner[\"WanAudioRunner\u003cbr/\u003e@RUNNER_REGISTER'wan_audio'\"]\n        WanRunner[\"WanRunner\u003cbr/\u003e@RUNNER_REGISTER'wan'\"]\n        WanDistillRunner[\"WanDistillRunner\u003cbr/\u003e@RUNNER_REGISTER'wan_distill'\"]\n        Wan22AudioRunner[\"Wan22AudioRunner\u003cbr/\u003e@RUNNER_REGISTER'wan22_audio'\"]\n        WanCausVidRunner[\"WanCausVidRunner\u003cbr/\u003e@RUNNER_REGISTER'wan_causvid'\"]\n        \n        QwenImageRunner[\"QwenImageRunner\u003cbr/\u003e@RUNNER_REGISTER'qwen_image'\"]\n        ZImageRunner[\"ZImageRunner\u003cbr/\u003e@RUNNER_REGISTER'z_image'\"]\n        \n        HunyuanVideo15Runner[\"HunyuanVideo15Runner\u003cbr/\u003e@RUNNER_REGISTER'hunyuan_video_1.5'\"]\n        \n        LTX2Runner[\"LTX2Runner\u003cbr/\u003e@RUNNER_REGISTER'ltx2'\"]\n        \n        LongCatImageRunner[\"LongCatImageRunner\u003cbr/\u003e@RUNNER_REGISTER'longcat_image'\"]\n    end\n    \n    subgraph Models[\"Model Networks\"]\n        WanModel[\"WanModel\"]\n        WanAnimateModel[\"WanAnimateModel\"]\n        QwenModels[\"QwenImageTransformerModel\"]\n        HYModel[\"HunyuanVideo15Model\"]\n        LTXModel[\"LTX2Model\"]\n        LongCatModel[\"LongCatImageTransformerModel\"]\n    end\n    \n    Registry --\u003e WanAudioRunner\n    Registry --\u003e WanRunner\n    Registry --\u003e WanDistillRunner\n    Registry --\u003e Wan22AudioRunner\n    Registry --\u003e WanCausVidRunner\n    Registry --\u003e QwenImageRunner\n    Registry --\u003e ZImageRunner\n    Registry --\u003e HunyuanVideo15Runner\n    Registry --\u003e LTX2Runner\n    Registry --\u003e LongCatImageRunner\n    \n    WanAudioRunner --\u003e WanModel\n    WanAudioRunner --\u003e WanAnimateModel\n    WanRunner --\u003e WanModel\n    WanDistillRunner --\u003e WanModel\n    Wan22AudioRunner --\u003e WanModel\n    WanCausVidRunner --\u003e WanModel\n    QwenImageRunner --\u003e QwenModels\n    ZImageRunner --\u003e QwenModels\n    HunyuanVideo15Runner --\u003e HYModel\n    LTX2Runner --\u003e LTXModel\n    LongCatImageRunner --\u003e LongCatModel\n```\n\n**Sources:** [lightx2v/models/runners/longcat_image/longcat_image_runner.py:60](), [lightx2v/models/runners/ltx2/ltx2_runner.py:24](), [lightx2v/models/networks/hunyuan_video/model.py:17](), [lightx2v/models/networks/ltx2/model.py:25]()\n\n### Model Class Specification\n\nModels are specified using the `model_cls` parameter in the pipeline configuration:\n\n| `model_cls` Value | Runner Class | Model Network | Supported Tasks |\n|-------------------|--------------|---------------|-----------------|\n| `wan2.1` | `WanRunner` or `WanAudioRunner` | `WanModel` | T2V, I2V, S2V |\n| `wan2.2_moe` | `Wan22AudioRunner` | `WanModel` | T2V, I2V, S2V |\n| `wan_distill` | `WanDistillRunner` | `WanModel` | T2V, I2V (4-step) |\n| `wan_causvid` | `WanCausVidRunner` | `WanModel` | Autoregressive T2V |\n| `qwen_image` | `QwenImageRunner` | `QwenImageTransformerModel` | T2I, I2I |\n| `z_image` | `ZImageRunner` | `QwenImageTransformerModel` | T2I, I2I |\n| `hunyuan_video_1.5` | `HunyuanVideo15Runner` | `HunyuanVideo15Model` | T2V, I2V |\n| `ltx2` | `LTX2Runner` | `LTX2Model` | T2AV, I2AV |\n| `longcat_image` | `LongCatImageRunner` | `LongCatImageTransformerModel` | T2I, I2I |\n\n**Sources:** [lightx2v/models/runners/longcat_image/longcat_image_runner.py:60](), [lightx2v/models/runners/ltx2/ltx2_runner.py:24]()\n\n## Optimization Variants\n\nLightX2V provides three major categories of optimization variants for supported models:\n\n### Step Distillation Models\n\nStep distillation compresses inference from 40-50 steps to just 4 steps, achieving approximately **25x speedup**:\n\n```mermaid\ngraph LR\n    subgraph Standard[\"Standard Models\"]\n        Std40[\"40-50 inference steps\u003cbr/\u003ewith CFG\"]\n    end\n    \n    subgraph Distilled[\"Distilled Models\"]\n        Dist4[\"4 inference steps\u003cbr/\u003eno CFG required\"]\n    end\n    \n    subgraph Performance[\"Performance Gain\"]\n        Speedup[\"~25x faster\u003cbr/\u003egeneration\"]\n    end\n    \n    Std40 --\u003e|\"Step distillation\u003cbr/\u003etraining\"| Dist4\n    Dist4 --\u003e Speedup\n```\n\n**Available Distilled Models:**\n\n| Model Family | HuggingFace Repository | Format |\n|--------------|------------------------|--------|\n| Wan2.1 | `lightx2v/Wan2.1-Distill-Models` | Full weights |\n| Wan2.1 | `lightx2v/Wan2.1-Distill-Loras` | LoRA weights |\n| Wan2.1 NVFP4 | `lightx2v/Wan-NVFP4` | Quantization-aware 4-step |\n| Wan2.2 | `lightx2v/Wan2.2-Distill-Models` | Full weights |\n| Wan2.2 | `lightx2v/Wan2.2-Distill-Loras` | LoRA weights |\n| HunyuanVideo-1.5 | `lightx2v/Hy1.5-Distill-Models` | Full weights + FP8 |\n| Qwen-Image-Edit-2511 | `lightx2v/Qwen-Image-Edit-2511-Lightning` | CFG/step distilled LoRA |\n| Qwen-Image-2512 | `lightx2v/Qwen-Image-2512-Lightning` | CFG/step distilled LoRA |\n\n**Sources:** [README.md:218-224](), [README.md:67]()\n\n### Quantized Models\n\nQuantization reduces weight precision to decrease memory usage and increase throughput:\n\n| Quantization Scheme | Precision | Typical Speedup | Models Available |\n|---------------------|-----------|-----------------|------------------|\n| INT8 | 8-bit integer | 1.5-2x | WAN, Qwen, HunyuanVideo |\n| FP8 | 8-bit floating point | 1.5-2x | WAN, Qwen, HunyuanVideo, LTX |\n| NVFP4 | 4-bit NVIDIA format | 3-4x | Wan2.1 (quantization-aware) |\n| MXFP4/6/8 | Mixed precision | Varies | Experimental |\n\n**Quantized Model Repositories:**\n\n- `lightx2v/Hy1.5-Quantized-Models` - HunyuanVideo-1.5 quantized variants\n- `lightx2v/Wan-NVFP4` - Wan2.1 NVFP4 quantized models\n- `lightx2v/Qwen-Image-Edit-2511-Lightning` - Includes FP8 weights\n\n**Sources:** [README.md:218-224](), [README.md:62](), [README.md:69]()\n\n### Lightweight Autoencoder Models\n\nLightweight VAE/TAE models provide faster encoding/decoding with reduced memory footprint:\n\n| Model | Target | Speedup | Quality Trade-off |\n|-------|--------|---------|-------------------|\n| LightTAE for HunyuanVideo-1.5 | Video decoding | 2-3x | Minimal |\n| LightVAE variants | Image/Video encoding | Varies | Model-specific |\n\n**Repository:** `lightx2v/Autoencoders`\n\n**Usage Pattern:**\n```python\npipe.enable_lightvae(\n    use_tae=True,\n    tae_path=\"/path/to/lighttaehy1_5.safetensors\",\n    use_lightvae=False,\n    vae_path=None,\n)\n```\n\n**Sources:** [README.md:226-227](), [scripts/hunyuan_video_15/README.md:63-69]()\n\n## Model Distribution and Storage\n\n```mermaid\ngraph TB\n    subgraph Sources[\"Model Distribution Channels\"]\n        HF[\"HuggingFace Hub\u003cbr/\u003ehuggingface.co\"]\n        MS[\"ModelScope\u003cbr/\u003emodelscope.cn\"]\n        QC[\"Quark Cloud\u003cbr/\u003eWindows packages\"]\n        Local[\"Local Storage\u003cbr/\u003eCustom paths\"]\n    end\n    \n    subgraph Official[\"Official Model Repos\"]\n        WanOfficial[\"Wan-AI/\"]\n        QwenOfficial[\"Qwen/\"]\n        TencentOfficial[\"tencent/\"]\n        LightricksOfficial[\"Lightricks/\"]\n    end\n    \n    subgraph LightX2V[\"LightX2V Model Repos\"]\n        LightX2VHub[\"lightx2v/\"]\n        DistillModels[\"Distill-Models\"]\n        DistillLoras[\"Distill-Loras\"]\n        QuantModels[\"Quantized-Models\"]\n        AutoEncoders[\"Autoencoders\"]\n    end\n    \n    subgraph LocalStorage[\"Local Storage Structure\"]\n        ModelPath[\"model_path/\u003cbr/\u003eBase directory\"]\n        Transformer[\"transformer/\u003cbr/\u003eDiT weights\"]\n        TextEncoder[\"text_encoder/\u003cbr/\u003eor gemma/\"]\n        VAE[\"vae/\u003cbr/\u003eor video_vae, audio_vae\"]\n        Upsampler[\"latent_upsampler/\u003cbr/\u003eoptional\"]\n    end\n    \n    HF --\u003e Official\n    HF --\u003e LightX2V\n    MS --\u003e Official\n    \n    Official --\u003e WanOfficial\n    Official --\u003e QwenOfficial\n    Official --\u003e TencentOfficial\n    Official --\u003e LightricksOfficial\n    \n    LightX2V --\u003e DistillModels\n    LightX2V --\u003e DistillLoras\n    LightX2V --\u003e QuantModels\n    LightX2V --\u003e AutoEncoders\n    \n    HF --\u003e LocalStorage\n    MS --\u003e LocalStorage\n    QC --\u003e LocalStorage\n    Local --\u003e LocalStorage\n    \n    ModelPath --\u003e Transformer\n    ModelPath --\u003e TextEncoder\n    ModelPath --\u003e VAE\n    ModelPath --\u003e Upsampler\n```\n\n**Sources:** [README.md:207-235](), [lightx2v/models/runners/ltx2/ltx2_runner.py:56-67]()\n\n### Model Path Configuration\n\nModels are loaded from the `model_path` directory, which contains standardized subdirectories:\n\n**Standard Structure:**\n```\nmodel_path/\n transformer/          # Main DiT model weights\n text_encoder/        # Text encoder weights (T5, CLIP, Qwen)\n vae/                 # VAE encoder/decoder weights\n [optional components]\n     audio_vae/       # LTX2: separate audio VAE\n     video_vae/       # LTX2: separate video VAE\n     latent_upsampler/ # LTX2: upsampling model\n     gemma/           # LTX2: Gemma text encoder\n```\n\n**Configuration Keys:**\n\n| Config Key | Purpose | Default |\n|------------|---------|---------|\n| `model_path` | Base model directory | Required |\n| `dit_original_ckpt` | Override DiT checkpoint path | `{model_path}/transformer` |\n| `dit_quantized_ckpt` | Use quantized DiT weights | None |\n| `gemma_original_ckpt` | Override text encoder path | `{model_path}` |\n| `upsampler_original_ckpt` | Override upsampler path | `{model_path}/latent_upsampler` |\n\n**Sources:** [lightx2v/models/runners/ltx2/ltx2_runner.py:77-88]()\n\n## LoRA and Adapter System\n\nLightX2V supports dynamic LoRA (Low-Rank Adaptation) application for model customization:\n\n```mermaid\ngraph TB\n    subgraph LoRAConfig[\"LoRA Configuration\"]\n        Config[\"lora_configs list\u003cbr/\u003ein model config\"]\n        Path[\"path: LoRA file path\"]\n        Strength[\"strength: 0.0-1.0\u003cbr/\u003eapplication strength\"]\n    end\n    \n    subgraph Loading[\"LoRA Loading Process\"]\n        Adapter[\"LoraAdapter\u003cbr/\u003elightx2v/models/networks/lora_adapter.py\"]\n        Loader[\"LoRALoader\u003cbr/\u003ewith model_prefix\"]\n        LoadFile[\"_load_lora_file\u003cbr/\u003esafetensors format\"]\n    end\n    \n    subgraph Application[\"LoRA Application\"]\n        OrigWeights[\"original_weight_dict\u003cbr/\u003eBase model weights\"]\n        ApplyLora[\"apply_lora\u003cbr/\u003eMerge LoRA with base\"]\n        ModifiedWeights[\"Modified weight_dict\"]\n        ApplyWeights[\"_apply_weights\u003cbr/\u003eUpdate model\"]\n    end\n    \n    Config --\u003e Path\n    Config --\u003e Strength\n    \n    Path --\u003e Adapter\n    Strength --\u003e Adapter\n    \n    Adapter --\u003e Loader\n    Adapter --\u003e LoadFile\n    \n    OrigWeights --\u003e ApplyLora\n    LoadFile --\u003e ApplyLora\n    Strength --\u003e ApplyLora\n    \n    ApplyLora --\u003e ModifiedWeights\n    ModifiedWeights --\u003e ApplyWeights\n```\n\n**Supported Models:**\n- WAN models (all variants)\n- Qwen models\n- LTX2 models\n- HunyuanVideo models\n\n**Usage Example:**\n```python\n# Configuration with multiple LoRAs\nconfig = {\n    \"lora_configs\": [\n        {\"path\": \"/path/to/style_lora.safetensors\", \"strength\": 1.0},\n        {\"path\": \"/path/to/detail_lora.safetensors\", \"strength\": 0.8}\n    ]\n}\n\n# LoRAs are automatically applied during model initialization\nmodel = LTX2Model(model_path, config, device)\n```\n\n**Sources:** [lightx2v/models/networks/lora_adapter.py:1-47](), [lightx2v/models/runners/ltx2/ltx2_runner.py:40-53]()\n\n## Autoregressive and Specialized Models\n\n### CausVid: Autoregressive Video Generation\n\nWAN CausVid models support autoregressive video generation with KV cache management:\n\n- **Model:** `wan_causvid`\n- **Architecture:** Causal attention with KV caching\n- **Use Case:** Long-form video generation through sequential frame prediction\n- **Key Parameters:** `kv_start`, `kv_end` for cache window management\n\n**Sources:** [README.md:229-232]()\n\n### Wan Animate Models\n\nSpecialized models for character animation with motion and face encoders:\n\n- **Components:** Motion encoder + Face encoder + Base WAN model\n- **Architecture:** `WanAnimateModel` extends `WanModel`\n- **Features:** Facial expression control, motion guidance\n\n**Sources:** [lightx2v/models/networks/wan/animate_model.py:1-23]()\n\n### WorldPlay Models\n\nVideo generation models with additional capabilities:\n\n- **Tasks:** T2V, I2V\n- **Features:** Integration with LightX2V runner system\n\n**Sources:** [README.md:207-235]()\n\n## Model Selection Guide\n\nUse this decision tree to select the appropriate model for your use case:\n\n```mermaid\ngraph TB\n    Start[\"What do you want to generate?\"]\n    \n    Start --\u003e Video{\"Video or Image?\"}\n    \n    Video --\u003e VideoType{\"Type of video?\"}\n    Video --\u003e NeedAudio{\"Need audio?\"}\n    \n    VideoType --\u003e Standard[\"Standard video\u003cbr/\u003efrom text/image\"]\n    VideoType --\u003e AudioDriven[\"Audio-driven\u003cbr/\u003etalking head\"]\n    VideoType --\u003e LongForm[\"Long-form\u003cbr/\u003eautoregressive\"]\n    \n    Standard --\u003e Resolution{\"Resolution?\"}\n    Resolution --\u003e R480[\"480p  Wan2.1/2.2\"]\n    Resolution --\u003e R720[\"720p  HunyuanVideo-1.5\"]\n    \n    AudioDriven --\u003e WanAudio[\"Wan2.1/2.2\u003cbr/\u003eS2V task\u003cbr/\u003eUNIQUE CAPABILITY\"]\n    \n    LongForm --\u003e WanCausVid[\"Wan CausVid\u003cbr/\u003ewith KV caching\"]\n    \n    NeedAudio --\u003e LTXModel[\"LTX-2\u003cbr/\u003eT2AV or I2AV\"]\n    \n    Video --\u003e Image{\"Video or Image?\"}\n    Image --\u003e ImageType{\"Type of image?\"}\n    \n    ImageType --\u003e CreateImage[\"Create new image\"]\n    ImageType --\u003e EditImage[\"Edit existing image\"]\n    \n    CreateImage --\u003e ImageModel{\"Model preference?\"}\n    ImageModel --\u003e QwenImg[\"Qwen-Image\u003cbr/\u003eVision-language\"]\n    ImageModel --\u003e ZImg[\"Z-Image\u003cbr/\u003eFast generation\"]\n    ImageModel --\u003e LCImg[\"LongCat\u003cbr/\u003eHigh resolution\"]\n    \n    EditImage --\u003e QwenEdit[\"Qwen-Image-Edit\u003cbr/\u003eInstruction-based\"]\n    EditImage --\u003e LCEdit[\"LongCat\u003cbr/\u003eVision-language edit\"]\n```\n\n**Sources:** [README.md:19](), [README.md:207-235]()"])</script><script>self.__next_f.push([1,"1c:T493b,"])</script><script>self.__next_f.push([1,"# Getting Started\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json](configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json)\n- [docs/EN/source/getting_started/quickstart.md](docs/EN/source/getting_started/quickstart.md)\n- [docs/ZH_CN/source/getting_started/quickstart.md](docs/ZH_CN/source/getting_started/quickstart.md)\n- [lightx2v/__init__.py](lightx2v/__init__.py)\n- [lightx2v/common/ops/attn/utils/ring_comm.py](lightx2v/common/ops/attn/utils/ring_comm.py)\n- [lightx2v/common/ops/mm/triton_kernels.py](lightx2v/common/ops/mm/triton_kernels.py)\n- [pyproject.toml](pyproject.toml)\n- [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh](scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh)\n\n\u003c/details\u003e\n\n\n\n**Purpose and Scope**: This document provides an entry point for new users to install LightX2V, download models, and run their first inference. It covers system requirements, installation methods, model preparation, and basic usage patterns. This page provides an overview with links to detailed sub-pages for each major step.\n\nFor detailed information about model architectures and task-specific runners, see [Model Runners and Tasks](#5). For optimization techniques and performance tuning, see [Performance Optimization](#6). For production deployment options, see [HTTP Server and Production Deployment](#3.4).\n\n## System Requirements\n\nLightX2V requires the following minimum specifications:\n\n| Component | Requirement | Notes |\n|-----------|-------------|-------|\n| **Operating System** | Linux (Ubuntu 18.04+) or Windows 10/11 | Docker available on Linux only |\n| **Python** | 3.10 or higher | Python 3.11-3.12 recommended |\n| **GPU** | NVIDIA GPU with CUDA support | Minimum 8GB VRAM |\n| **Memory** | 16GB RAM or more | 32GB+ recommended for large models |\n| **Storage** | 50GB+ available space | SSD strongly recommended for model storage |\n| **CUDA** | CUDA 12.4 or 12.8 | cu128 recommended for best performance |\n\nThe framework also supports multiple Chinese domestic GPU platforms through the `lightx2v_platform` abstraction layer. See [Hardware Platform Support](#1.3) for details.\n\n**Sources**: [docs/EN/source/getting_started/quickstart.md:14-20](), [docs/ZH_CN/source/getting_started/quickstart.md:14-20]()\n\n## Installation Overview\n\nLightX2V provides three primary installation paths, each suited to different use cases:\n\n```mermaid\ngraph TB\n    Start[\"User Begins Installation\"]\n    \n    OSChoice{\"Operating\u003cbr/\u003eSystem?\"}\n    \n    LinuxPath[\"Linux Installation\"]\n    WindowsPath[\"Windows Installation\"]\n    \n    LinuxMethod{\"Installation\u003cbr/\u003eMethod?\"}\n    \n    Docker[\"Docker Environment\u003cbr/\u003e(Recommended)\"]\n    Conda[\"Conda Environment\u003cbr/\u003e(Manual Setup)\"]\n    \n    DockerPull[\"Pull Docker Image:\u003cbr/\u003elightx2v/lightx2v:26011201-cu128\"]\n    DockerRun[\"Run Container:\u003cbr/\u003edocker run --gpus all\"]\n    \n    CondaCreate[\"Create Conda Env:\u003cbr/\u003econda create -n lightx2v python=3.11\"]\n    CondaInstall[\"Install Dependencies:\u003cbr/\u003epip install -v -e .\"]\n    AttentionOps[\"Install Attention Operators:\u003cbr/\u003eFlash Attn 2/3 or SageAttention\"]\n    QuantOps[\"Optional: Install Quantization Operators:\u003cbr/\u003evllm, sgl-kernel, or q8_kernels\"]\n    \n    WindowsConda[\"Create Conda Env:\u003cbr/\u003ePython 3.12 recommended\"]\n    WindowsPyTorch[\"Install PyTorch:\u003cbr/\u003eMatch CUDA version from nvidia-smi\"]\n    WindowsLightX2V[\"Install LightX2V:\u003cbr/\u003epip install -r requirements_win.txt\"]\n    WindowsAttn[\"Install SageAttention:\u003cbr/\u003eWindows-specific wheel\"]\n    WindowsQuant[\"Optional: Install vLLM Windows Build\"]\n    \n    Ready[\"Environment Ready\"]\n    \n    Start --\u003e OSChoice\n    OSChoice --\u003e|\"Linux\"| LinuxPath\n    OSChoice --\u003e|\"Windows\"| WindowsPath\n    \n    LinuxPath --\u003e LinuxMethod\n    LinuxMethod --\u003e|\"Simple, Fast\"| Docker\n    LinuxMethod --\u003e|\"Customizable\"| Conda\n    \n    Docker --\u003e DockerPull\n    DockerPull --\u003e DockerRun\n    DockerRun --\u003e Ready\n    \n    Conda --\u003e CondaCreate\n    CondaCreate --\u003e CondaInstall\n    CondaInstall --\u003e AttentionOps\n    AttentionOps --\u003e QuantOps\n    QuantOps --\u003e Ready\n    \n    WindowsPath --\u003e WindowsConda\n    WindowsConda --\u003e WindowsPyTorch\n    WindowsPyTorch --\u003e WindowsLightX2V\n    WindowsLightX2V --\u003e WindowsAttn\n    WindowsAttn --\u003e WindowsQuant\n    WindowsQuant --\u003e Ready\n```\n\n**Installation Path Comparison**\n\n| Path | Advantages | Disadvantages | Best For |\n|------|-----------|---------------|----------|\n| **Docker (Linux)** | Pre-configured, fastest setup, consistent environment | Less flexibility, larger disk footprint | Quick evaluation, production deployment |\n| **Conda (Linux)** | Full control, latest features, custom builds | More setup steps, dependency management | Development, custom optimization |\n| **Conda (Windows)** | Native Windows support, no Docker needed | More complex setup, limited operator support | Windows users, desktop applications |\n\n**Sources**: [docs/EN/source/getting_started/quickstart.md:22-151](), [docs/ZH_CN/source/getting_started/quickstart.md:22-150]()\n\n## Core Dependencies\n\nLightX2V's dependencies are defined in [pyproject.toml:33-70]() and can be categorized into several functional groups:\n\n### Required Core Dependencies\n\n```mermaid\ngraph LR\n    subgraph \"Deep Learning Framework\"\n        PyTorch[\"torch\u003c=2.8.0\"]\n        TorchVision[\"torchvision\u003c=0.23.0\"]\n        TorchAudio[\"torchaudio\u003c=2.8.0\"]\n    end\n    \n    subgraph \"Diffusion Libraries\"\n        Diffusers[\"diffusers\"]\n        Transformers[\"transformers\"]\n        Tokenizers[\"tokenizers\"]\n        Accelerate[\"accelerate\"]\n    end\n    \n    subgraph \"Weight Management\"\n        Safetensors[\"safetensors\"]\n        GGUF[\"gguf\"]\n    end\n    \n    subgraph \"Video Processing\"\n        OpenCV[\"opencv-python\"]\n        Imageio[\"imageio\"]\n        ImageioFFmpeg[\"imageio-ffmpeg\"]\n        Decord[\"decord\"]\n        AV[\"av\"]\n    end\n    \n    subgraph \"Web Interface\"\n        Gradio[\"gradio\"]\n        FastAPI[\"fastapi\"]\n        Uvicorn[\"uvicorn\"]\n    end\n    \n    subgraph \"Optimization\"\n        Einops[\"einops\"]\n        QTorch[\"qtorch\"]\n        TorchAda[\"torchada\u003e=0.1.10\"]\n    end\n    \n    LightX2V[\"LightX2VPipeline\"]\n    \n    PyTorch --\u003e LightX2V\n    Diffusers --\u003e LightX2V\n    Safetensors --\u003e LightX2V\n    OpenCV --\u003e LightX2V\n    Gradio --\u003e LightX2V\n    Einops --\u003e LightX2V\n```\n\n### Optional Operator Dependencies\n\nThese dependencies are not in `pyproject.toml` but must be installed separately for specific optimization features:\n\n| Operator | Purpose | Installation | Platforms |\n|----------|---------|--------------|-----------|\n| **Flash Attention 2** | Efficient attention computation | `cd flash-attention \u0026\u0026 python setup.py install` | Linux, Windows |\n| **Flash Attention 3** | Hopper GPU optimization (H100) | `cd flash-attention/hopper \u0026\u0026 python setup.py install` | Linux only |\n| **SageAttention 2** | Memory-efficient attention (recommended) | See [Installation and Environment Setup](#2.1) | Linux, Windows |\n| **vLLM** | Quantization kernels (FP8, INT8) | `pip install vllm` | Linux, Windows (special build) |\n| **SGL Kernels** | SGL quantization scheme | `pip install sgl-kernel --upgrade` | Linux (requires torch==2.8.0) |\n| **Q8 Kernels** | INT8 kernels for Ada architecture | See [Installation and Environment Setup](#2.1) | Linux, Windows |\n\n**Sources**: [pyproject.toml:33-70](), [docs/EN/source/getting_started/quickstart.md:86-143](), [docs/ZH_CN/source/getting_started/quickstart.md:86-144]()\n\n## Package Structure and Entry Points\n\nAfter installation, the `lightx2v` package exposes several key components:\n\n```mermaid\ngraph TB\n    Package[\"lightx2v package\u003cbr/\u003elightx2v/__init__.py\"]\n    \n    subgraph \"Primary API\"\n        Pipeline[\"LightX2VPipeline\u003cbr/\u003elightx2v.pipeline.LightX2VPipeline\"]\n        SetConfig[\"set_config\u003cbr/\u003elightx2v.common.set_config\"]\n    end\n    \n    subgraph \"CLI Entry Points\"\n        InferModule[\"lightx2v.infer module\u003cbr/\u003epython -m lightx2v.infer\"]\n        TorchrunInfer[\"torchrun -m lightx2v.infer\u003cbr/\u003eDistributed inference\"]\n    end\n    \n    subgraph \"Sub-Packages\"\n        Models[\"models\u003cbr/\u003eModel architectures\"]\n        Common[\"common\u003cbr/\u003eShared utilities\"]\n        Deploy[\"deploy\u003cbr/\u003eDeployment tools\"]\n        Utils[\"utils\u003cbr/\u003eHelper functions\"]\n    end\n    \n    subgraph \"Platform Layer\"\n        Platform[\"lightx2v_platform\u003cbr/\u003eHardware abstraction\"]\n    end\n    \n    Package --\u003e Pipeline\n    Package --\u003e SetConfig\n    Package --\u003e InferModule\n    Package --\u003e TorchrunInfer\n    \n    Package --\u003e Models\n    Package --\u003e Common\n    Package --\u003e Deploy\n    Package --\u003e Utils\n    \n    Package --\u003e Platform\n```\n\nThe main entry point is `LightX2VPipeline`, which provides a unified interface for all model types and tasks. See [Python API (LightX2VPipeline)](#3.2) for complete API documentation.\n\n**Sources**: [lightx2v/__init__.py:1-18]()\n\n## End-to-End Setup and First Inference Flow\n\nThis diagram shows the complete workflow from installation to generating your first video:\n\n```mermaid\nflowchart TD\n    Start[\"Start: Fresh System\"]\n    \n    InstallEnv[\"Install Environment\u003cbr/\u003eDocker or Conda\"]\n    VerifyInstall[\"Verify Installation:\u003cbr/\u003eimport lightx2v\"]\n    \n    DownloadModel[\"Download Model Files\u003cbr/\u003eHuggingFace or ModelScope\"]\n    OrganizeModels[\"Organize Model Directory\u003cbr/\u003emodel_path structure\"]\n    \n    ChooseInterface{\"Choose\u003cbr/\u003eInterface\"}\n    \n    PythonAPI[\"Python API Approach\"]\n    CLIApproach[\"CLI Script Approach\"]\n    GradioApproach[\"Gradio Web UI\"]\n    \n    subgraph \"Python API Flow\"\n        CreatePipeline[\"Create LightX2VPipeline:\u003cbr/\u003emodel_path, model_cls, task\"]\n        CreateGenerator[\"Call create_generator:\u003cbr/\u003eattn_mode, infer_steps, height, width\"]\n        CallGenerate[\"Call generate:\u003cbr/\u003eprompt, seed, save_result_path\"]\n    end\n    \n    subgraph \"CLI Flow\"\n        PrepareConfig[\"Prepare config JSON:\u003cbr/\u003econfigs/*.json\"]\n        PrepareScript[\"Prepare shell script:\u003cbr/\u003escripts/*.sh\"]\n        RunScript[\"Execute: bash scripts/run_*.sh\"]\n    end\n    \n    subgraph \"Gradio Flow\"\n        LaunchGradio[\"Launch: bash scripts/run_gradio.sh\"]\n        UseWebUI[\"Configure in Web UI:\u003cbr/\u003eSelect model, set parameters\"]\n        GenerateInUI[\"Click Generate\"]\n    end\n    \n    Output[\"Output: Generated Video/Image\"]\n    \n    Start --\u003e InstallEnv\n    InstallEnv --\u003e VerifyInstall\n    VerifyInstall --\u003e DownloadModel\n    DownloadModel --\u003e OrganizeModels\n    \n    OrganizeModels --\u003e ChooseInterface\n    \n    ChooseInterface --\u003e|\"Programmatic\"| PythonAPI\n    ChooseInterface --\u003e|\"Reproducible\"| CLIApproach\n    ChooseInterface --\u003e|\"Interactive\"| GradioApproach\n    \n    PythonAPI --\u003e CreatePipeline\n    CreatePipeline --\u003e CreateGenerator\n    CreateGenerator --\u003e CallGenerate\n    CallGenerate --\u003e Output\n    \n    CLIApproach --\u003e PrepareConfig\n    PrepareConfig --\u003e PrepareScript\n    PrepareScript --\u003e RunScript\n    RunScript --\u003e Output\n    \n    GradioApproach --\u003e LaunchGradio\n    LaunchGradio --\u003e UseWebUI\n    UseWebUI --\u003e GenerateInUI\n    GenerateInUI --\u003e Output\n```\n\n**Sources**: [docs/EN/source/getting_started/quickstart.md:291-357](), [docs/ZH_CN/source/getting_started/quickstart.md:282-345]()\n\n## Minimal Python Example\n\nHere is a minimal example to verify your installation and run your first inference:\n\n```python\nfrom lightx2v import LightX2VPipeline\n\n# Initialize pipeline\npipe = LightX2VPipeline(\n    model_path=\"/path/to/Wan2.1-T2V-14B\",\n    model_cls=\"wan2.1\",\n    task=\"t2v\",\n)\n\n# Configure generation parameters\npipe.create_generator(\n    attn_mode=\"sage_attn2\",\n    infer_steps=50,\n    height=480,\n    width=832,\n    num_frames=81,\n    guidance_scale=5.0,\n    sample_shift=5.0,\n)\n\n# Generate video\npipe.generate(\n    seed=42,\n    prompt=\"Two anthropomorphic cats in comfy boxing gear fight intensely.\",\n    negative_prompt=\"camera shake, overexposed, static, blurry details\",\n    save_result_path=\"output.mp4\",\n)\n```\n\nThis example demonstrates the three-step pattern used throughout LightX2V:\n\n1. **Pipeline Initialization**: Specify `model_path`, `model_cls`, and `task`\n2. **Generator Configuration**: Set inference parameters via `create_generator()`\n3. **Generation**: Call `generate()` with prompt and output path\n\n**Key Parameters**:\n\n- `model_cls`: Model family identifier (e.g., `\"wan2.1\"`, `\"wan2.2\"`, `\"qwen-image\"`, `\"z-image-turbo\"`)\n- `task`: Task type (`\"t2v\"`, `\"i2v\"`, `\"s2v\"`, `\"t2i\"`, `\"i2i\"`)\n- `attn_mode`: Attention operator (`\"sage_attn2\"`, `\"flash_attn2\"`, `\"flash_attn3\"`)\n- `infer_steps`: Number of diffusion steps (50 for standard, 4 for distilled models)\n\n**Sources**: [docs/EN/source/getting_started/quickstart.md:324-355](), [docs/ZH_CN/source/getting_started/quickstart.md:315-344]()\n\n## CLI Example with Configuration Files\n\nFor reproducible experiments and production use, LightX2V provides configuration files and shell scripts:\n\n```bash\n#!/bin/bash\n\nlightx2v_path=/path/to/LightX2V\nmodel_path=/path/to/SekoTalk-Distill-fp8\n\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n\n# Set environment variables\nsource ${lightx2v_path}/scripts/base/base.sh\n\ntorchrun --nproc-per-node 8 -m lightx2v.infer \\\n--model_cls seko_talk \\\n--task s2v \\\n--model_path $model_path \\\n--config_json ${lightx2v_path}/configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json \\\n--prompt \"A male speaking to the camera with arms spread out.\" \\\n--image_path ${lightx2v_path}/assets/inputs/audio/seko_input.png \\\n--audio_path ${lightx2v_path}/assets/inputs/audio/seko_input.mp3 \\\n--save_result_path ${lightx2v_path}/save_results/output.mp4\n```\n\n**Configuration File Structure** (example from [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:1-74]()):\n\n```json\n{\n    \"infer_steps\": 4,\n    \"target_fps\": 16,\n    \"target_video_length\": 81,\n    \"sample_guide_scale\": 1.0,\n    \"sample_shift\": 5,\n    \"enable_cfg\": false,\n    \"self_attn_1_type\": \"nbhd_attn\",\n    \"cross_attn_1_type\": \"flash_attn3\",\n    \"parallel\": {\n        \"seq_p_size\": 8,\n        \"seq_p_attn_type\": \"ulysses\"\n    },\n    \"dit_quantized\": true,\n    \"dit_quant_scheme\": \"fp8-sgl\",\n    \"compile\": true,\n    \"compile_shapes\": [[480, 832], [720, 1280]]\n}\n```\n\nThe configuration file controls:\n- **Inference parameters**: `infer_steps`, `sample_shift`, `guidance_scale`\n- **Attention operators**: `self_attn_1_type`, `cross_attn_1_type`, `cross_attn_2_type`\n- **Quantization**: `dit_quantized`, `dit_quant_scheme`, `t5_quantized`\n- **Parallelism**: `seq_p_size`, `seq_p_attn_type`\n- **Compilation**: `compile`, `compile_shapes`\n\nSee [Configuration System](#8.2) for complete configuration documentation.\n\n**Sources**: [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh:1-22](), [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:1-74]()\n\n## Docker Installation (Recommended for Linux)\n\nDocker provides the fastest and most reliable installation method for Linux users:\n\n### 1. Pull Docker Image\n\n```bash\n# Latest CUDA 12.8 image (recommended)\ndocker pull lightx2v/lightx2v:26011201-cu128\n\n# Or CUDA 12.4 image\ndocker pull lightx2v/lightx2v:25101501-cu124\n\n# China mirror (Alibaba Cloud)\ndocker pull registry.cn-hangzhou.aliyuncs.com/yongyang/lightx2v:26011201-cu128\n```\n\nAvailable Docker images are published to:\n- **Docker Hub**: `lightx2v/lightx2v`\n- **Alibaba Cloud (China)**: `registry.cn-hangzhou.aliyuncs.com/yongyang/lightx2v`\n\nImage tags follow the format `YYMMDDVV-cuXXX` where:\n- `YYMMDDVV`: Date and version (e.g., `26011201` = 2026-01-12, version 01)\n- `cuXXX`: CUDA version (`cu124` or `cu128`)\n\n### 2. Run Container\n\n```bash\ndocker run --gpus all -itd \\\n  --ipc=host \\\n  --name lightx2v_container \\\n  -v /path/to/models:/models \\\n  -v /path/to/outputs:/outputs \\\n  --entrypoint /bin/bash \\\n  lightx2v/lightx2v:26011201-cu128\n```\n\n**Container Arguments**:\n- `--gpus all`: Enable GPU access\n- `--ipc=host`: Required for PyTorch multiprocessing\n- `-v /host/path:/container/path`: Mount model and output directories\n\n### 3. Verify Installation Inside Container\n\n```bash\ndocker exec -it lightx2v_container bash\npython -c \"import lightx2v; print(lightx2v.__version__)\"\n```\n\n**Sources**: [docs/EN/source/getting_started/quickstart.md:24-58](), [docs/ZH_CN/source/getting_started/quickstart.md:24-58]()\n\n## Common Issues and Troubleshooting\n\n| Issue | Symptom | Solution |\n|-------|---------|----------|\n| **CUDA version mismatch** | `RuntimeError: CUDA error: no kernel image available` | Verify CUDA version with `nvidia-smi` and use matching Docker image or PyTorch version |\n| **Out of memory** | `RuntimeError: CUDA out of memory` | Enable CPU offloading (`cpu_offload=True`), reduce resolution, or use quantization |\n| **Missing attention operator** | `ImportError: cannot import name 'flash_attn_func'` | Install Flash Attention 2/3 or SageAttention following [Installation and Environment Setup](#2.1) |\n| **Slow inference** | Long generation time | Enable quantization, use distilled models, enable torch.compile, or use SageAttention |\n| **Model not found** | `FileNotFoundError: model_path does not exist` | Verify `model_path` points to directory containing model weights, see [Model Download and Organization](#2.2) |\n| **Import error on Windows** | `ImportError: DLL load failed` | Install Visual C++ Redistributable and ensure PyTorch CUDA version matches GPU driver |\n\nFor additional help:\n1. Search [GitHub Issues](https://github.com/ModelTC/LightX2V/issues)\n2. Check documentation at [https://lightx2v-en.readthedocs.io](https://lightx2v-en.readthedocs.io)\n3. Submit a new issue with error logs and system information\n\n**Sources**: [docs/EN/source/getting_started/quickstart.md:359-365](), [docs/ZH_CN/source/getting_started/quickstart.md:348-354]()\n\n## Next Steps\n\nAfter completing the basic setup:\n\n1. **Installation Details**: See [Installation and Environment Setup](#2.1) for detailed instructions on attention operators, quantization kernels, and platform-specific configurations.\n\n2. **Model Management**: See [Model Download and Organization](#2.2) for information on downloading models from HuggingFace/ModelScope, model directory structure, and storage recommendations.\n\n3. **Quick Start Tutorial**: See [Quick Start Tutorial](#2.3) for step-by-step examples of different model types (WAN, Qwen, Z-Image) and tasks (T2V, I2V, S2V, T2I).\n\n4. **Advanced Features**: Explore [Performance Optimization](#6) for quantization, CPU offloading, feature caching, and distributed inference techniques that achieve ~20-25x speedup.\n\n5. **Production Deployment**: See [HTTP Server and Production Deployment](#3.4) for setting up distributed inference services with `TorchrunInferenceWorker`.\n\n**Sources**: [docs/EN/source/getting_started/quickstart.md:1-369](), [docs/ZH_CN/source/getting_started/quickstart.md:1-358]()"])</script><script>self.__next_f.push([1,"1d:T55ca,"])</script><script>self.__next_f.push([1,"# Installation and Environment Setup\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json](configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json)\n- [docs/EN/source/getting_started/quickstart.md](docs/EN/source/getting_started/quickstart.md)\n- [docs/ZH_CN/source/getting_started/quickstart.md](docs/ZH_CN/source/getting_started/quickstart.md)\n- [lightx2v/__init__.py](lightx2v/__init__.py)\n- [lightx2v/common/ops/attn/utils/ring_comm.py](lightx2v/common/ops/attn/utils/ring_comm.py)\n- [lightx2v/common/ops/mm/triton_kernels.py](lightx2v/common/ops/mm/triton_kernels.py)\n- [pyproject.toml](pyproject.toml)\n- [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh](scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document covers the installation and environment setup procedures for LightX2V, including system requirements, installation methods (Docker and Conda), dependency management, and hardware platform configuration. For information about model download and organization after installation, see [Model Download and Organization](#2.2). For a complete quick start tutorial including first generation, see [Quick Start Tutorial](#2.3).\n\n## System Requirements\n\nLightX2V requires the following minimum specifications:\n\n| Component | Requirement |\n|-----------|------------|\n| **Operating System** | Linux (Ubuntu 18.04+) or Windows 10/11 |\n| **Python** | 3.10 or higher (3.11-3.12 recommended) |\n| **GPU** | NVIDIA GPU with CUDA support, minimum 8GB VRAM |\n| **Memory** | 16GB RAM or more recommended |\n| **Storage** | At least 50GB available space (SSD recommended) |\n| **CUDA** | CUDA 12.4 or 12.8 (12.8 recommended for best performance) |\n\nSources: [docs/EN/source/getting_started/quickstart.md:14-20](), [docs/ZH_CN/source/getting_started/quickstart.md:14-20]()\n\n## Installation Methods Overview\n\n```mermaid\ngraph TB\n    Start[\"Installation Decision\"]\n    \n    Docker[\"Docker Installation\u003cbr/\u003e(Recommended)\"]\n    CondaLinux[\"Conda Installation\u003cbr/\u003eLinux\"]\n    CondaWindows[\"Conda Installation\u003cbr/\u003eWindows\"]\n    \n    PullImage[\"docker pull lightx2v/lightx2v:25111101-cu128\"]\n    RunContainer[\"docker run --gpus all -itd\"]\n    \n    CloneRepo[\"git clone https://github.com/ModelTC/LightX2V.git\"]\n    CreateEnv[\"conda create -n lightx2v python=3.11\"]\n    InstallCore[\"pip install -v -e .\"]\n    \n    InstallAttn[\"Install Attention Operators\u003cbr/\u003eflash-attention/SageAttention\"]\n    InstallQuant[\"Install Quantization Operators\u003cbr/\u003evllm/sgl-kernel/q8_kernels\"]\n    \n    WindowsTorch[\"Install PyTorch\u003cbr/\u003etorch==2.6.0+cu124\"]\n    WindowsVLLM[\"Install vllm-windows\"]\n    WindowsReqs[\"pip install -r requirements_win.txt\"]\n    \n    Verify[\"Verify Installation\u003cbr/\u003eimport lightx2v\"]\n    \n    Start --\u003e Docker\n    Start --\u003e CondaLinux\n    Start --\u003e CondaWindows\n    \n    Docker --\u003e PullImage\n    PullImage --\u003e RunContainer\n    RunContainer --\u003e Verify\n    \n    CondaLinux --\u003e CloneRepo\n    CloneRepo --\u003e CreateEnv\n    CreateEnv --\u003e InstallCore\n    InstallCore --\u003e InstallAttn\n    InstallAttn --\u003e InstallQuant\n    InstallQuant --\u003e Verify\n    \n    CondaWindows --\u003e CloneRepo\n    CloneRepo --\u003e WindowsTorch\n    WindowsTorch --\u003e WindowsVLLM\n    WindowsVLLM --\u003e WindowsReqs\n    WindowsReqs --\u003e InstallAttn\n    InstallAttn --\u003e Verify\n    \n    classDef recommended fill:#f9f9f9,stroke:#333,stroke-width:2px\n    class Docker recommended\n```\n\n**Installation Methods Comparison**\n\n| Method | Complexity | Platform | Use Case |\n|--------|-----------|----------|----------|\n| Docker | Low | Linux | Production, quick start, recommended for all users |\n| Conda (Linux) | Medium | Linux | Development, customization |\n| Conda (Windows) | High | Windows | Windows-specific deployment |\n\nSources: [docs/EN/source/getting_started/quickstart.md:22-60](), [README.md:115-130]()\n\n## Docker Installation (Recommended)\n\nDocker provides the simplest and fastest installation method with pre-configured environments including all necessary dependencies.\n\n### Step 1: Pull Docker Image\n\nLightX2V provides official Docker images on Docker Hub with different CUDA versions:\n\n```bash\n# CUDA 12.8 (Recommended for best performance)\ndocker pull lightx2v/lightx2v:25111101-cu128\n\n# CUDA 12.4 (Alternative)\ndocker pull lightx2v/lightx2v:25101501-cu124\n```\n\nFor users in mainland China with network access issues, Alibaba Cloud mirrors are available:\n\n```bash\n# CUDA 12.8\ndocker pull registry.cn-hangzhou.aliyuncs.com/yongyang/lightx2v:25111101-cu128\n\n# CUDA 12.4\ndocker pull registry.cn-hangzhou.aliyuncs.com/yongyang/lightx2v:25101501-cu124\n```\n\nThe Docker images include:\n- Python 3.11 environment\n- PyTorch 2.6+ with CUDA support\n- Pre-installed attention operators (Flash Attention 2/3, SageAttention)\n- Pre-installed quantization operators (vllm, Triton kernels)\n- LightX2V core framework\n\nSources: [docs/EN/source/getting_started/quickstart.md:28-58](), [README.md:115-116]()\n\n### Step 2: Run Container\n\n```bash\ndocker run --gpus all -itd \\\n  --ipc=host \\\n  --name lightx2v_container \\\n  -v /path/to/models:/models \\\n  -v /path/to/outputs:/outputs \\\n  --entrypoint /bin/bash \\\n  [image_id]\n```\n\n**Parameter Explanation:**\n- `--gpus all`: Enable access to all GPUs\n- `--ipc=host`: Use host IPC namespace (required for multi-GPU communication)\n- `-v`: Mount host directories for models and outputs\n\nSources: [docs/EN/source/getting_started/quickstart.md:44-46](), [scripts/hunyuan_video_15/README.md:12-14]()\n\n## Conda Installation - Linux\n\nFor users who prefer manual environment setup or need customization, Conda installation provides full control over dependencies.\n\n### Step 1: Clone Repository\n\n```bash\ngit clone https://github.com/ModelTC/LightX2V.git\ncd LightX2V\n```\n\nSources: [docs/EN/source/getting_started/quickstart.md:66-69](), [README.md:124-126]()\n\n### Step 2: Create Virtual Environment\n\n```bash\nconda create -n lightx2v python=3.11 -y\nconda activate lightx2v\n```\n\nPython 3.11 is recommended for optimal compatibility, though Python 3.10 and 3.12 are also supported as specified in [pyproject.toml:19-28]().\n\nSources: [docs/EN/source/getting_started/quickstart.md:74-77]()\n\n### Step 3: Install Core Dependencies\n\n```bash\npip install -v -e .\n```\n\nThis installs the core LightX2V package along with dependencies defined in [pyproject.toml:33-70](), including:\n- PyTorch (`torch\u003c=2.8.0`)\n- Diffusers, Transformers, Tokenizers\n- Image/video processing: `opencv-python`, `imageio`, `ffmpeg`\n- Utilities: `einops`, `loguru`, `safetensors`\n- Server components: `gradio`, `fastapi`, `uvicorn`\n\nSources: [docs/EN/source/getting_started/quickstart.md:82-84](), [pyproject.toml:1-70]()\n\n### Step 4: Install Attention Operators\n\nAttention operators are critical for performance. LightX2V supports multiple implementations:\n\n#### Option A: Flash Attention 2\n\n```bash\ngit clone https://github.com/Dao-AILab/flash-attention.git --recursive\ncd flash-attention \u0026\u0026 python setup.py install\n```\n\nFlash Attention 2 is the standard implementation providing 2-4x speedup for attention computation.\n\n#### Option B: Flash Attention 3 (Hopper Architecture)\n\n```bash\ncd flash-attention/hopper \u0026\u0026 python setup.py install\n```\n\nFlash Attention 3 is optimized for NVIDIA H100 and newer GPUs with Hopper architecture.\n\n#### Option C: SageAttention 2 (Recommended)\n\n```bash\ngit clone https://github.com/thu-ml/SageAttention.git\ncd SageAttention \u0026\u0026 \\\n  CUDA_ARCHITECTURES=\"8.0,8.6,8.9,9.0,12.0\" \\\n  EXT_PARALLEL=4 \\\n  NVCC_APPEND_FLAGS=\"--threads 8\" \\\n  MAX_JOBS=32 \\\n  pip install -v -e .\n```\n\nSageAttention provides optimal performance across a wide range of GPUs and is integrated throughout the codebase via the `ATTN_WEIGHT_REGISTER` system in [lightx2v/utils/registry_factory.py]().\n\nSources: [docs/EN/source/getting_started/quickstart.md:86-103]()\n\n### Step 5: Install Quantization Operators (Optional)\n\nQuantization operators enable memory-efficient inference with INT8, FP8, and NVFP4 quantization schemes.\n\n#### Option A: VLLM Kernels (Recommended)\n\n```bash\npip install vllm\n```\n\nOr install from source for latest features:\n\n```bash\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\nuv pip install -e .\n```\n\nVLLM provides comprehensive quantization support including FP8 quantization used in [lightx2v/common/ops/mm/triton_kernels.py:38-65]().\n\n#### Option B: SGL Kernels\n\n```bash\npip install sgl-kernel --upgrade\n```\n\nSGL kernels require `torch==2.8.0` and provide optimized quantization operators. Configuration examples can be found in [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:24-30]().\n\n#### Option C: Q8 Kernels (Ada Architecture)\n\n```bash\ngit clone https://github.com/KONAKONA666/q8_kernels.git\ncd q8_kernels \u0026\u0026 git submodule init \u0026\u0026 git submodule update\npython setup.py install\n```\n\nQ8 kernels are optimized for Ada architecture GPUs (RTX 4090, L40S, etc.).\n\nSources: [docs/EN/source/getting_started/quickstart.md:105-143]()\n\n## Conda Installation - Windows\n\nWindows installation requires manual configuration of PyTorch and compatible operators.\n\n### Step 1: Verify CUDA Version\n\n```cmd\nnvidia-smi\n```\n\nRecord the **CUDA Version** displayed in the output. All subsequent installations must match this CUDA version.\n\nSources: [docs/EN/source/getting_started/quickstart.md:158-165]()\n\n### Step 2: Create Python Environment\n\n```cmd\nconda create -n lightx2v python=3.12 -y\nconda activate lightx2v\n```\n\nPython 3.12 is recommended for Windows as it has the best compatibility with available pre-built packages.\n\nSources: [docs/EN/source/getting_started/quickstart.md:169-175]()\n\n### Step 3: Install PyTorch\n\n**Method 1: Download Official Wheel (Recommended)**\n\n1. Visit [PyTorch Official Download Page](https://download.pytorch.org/whl/torch/)\n2. Select wheel matching: Python version, CUDA version, Windows platform\n\nExample for Python 3.12 + CUDA 12.4:\n\n```cmd\npip install torch-2.6.0+cu124-cp312-cp312-win_amd64.whl\npip install torchvision==0.21.0 torchaudio==2.6.0\n```\n\n**Method 2: Direct pip Installation**\n\n```cmd\npip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 \\\n  --index-url https://download.pytorch.org/whl/cu124\n```\n\nSources: [docs/EN/source/getting_started/quickstart.md:180-205]()\n\n### Step 4: Clone Repository and Install Dependencies\n\n```cmd\ngit clone https://github.com/ModelTC/LightX2V.git\ncd LightX2V\npip install -r requirements_win.txt\npip install -v -e .\n```\n\nThe `requirements_win.txt` file contains Windows-specific dependencies including `triton-windows` for Triton kernel support.\n\nSources: [docs/EN/source/getting_started/quickstart.md:243-251]()\n\n### Step 5: Install Attention Operators\n\n#### Option A: Flash Attention 2\n\n```cmd\npip install flash-attn==2.7.2.post1\n```\n\n#### Option B: SageAttention 2 (Strongly Recommended)\n\nDownload pre-built wheels from:\n- [Windows Version 1](https://github.com/woct0rdho/SageAttention/releases)\n- [Windows Version 2](https://github.com/sdbds/SageAttention-for-windows/releases)\n\n```cmd\npip install sageattention-2.1.1+cu126torch2.6.0-cp312-cp312-win_amd64.whl\n```\n\n**Note:** SageAttention CUDA version doesn't need strict alignment, but Python and PyTorch versions must match exactly.\n\nSources: [docs/EN/source/getting_started/quickstart.md:220-240]()\n\n### Step 6: Install Quantization Operators (Optional)\n\n**Default: Triton Kernels**\n\nBy default, LightX2V uses Triton kernels for quantization (implemented in [lightx2v/common/ops/mm/triton_kernels.py]()), which are efficient and require no additional dependencies beyond `triton-windows`.\n\n**Optional: vLLM Windows Version**\n\nDownload from [vllm-windows releases](https://github.com/SystemPanic/vllm-windows/releases) matching your Python, PyTorch, and CUDA versions:\n\n```cmd\npip install vllm-0.9.1+cu124-cp312-cp312-win_amd64.whl\n```\n\n**Optional: Q8 Kernels**\n\nFor RTX 40 series GPUs, install `q8_kernel==0.1.0`:\n\n```bash\ngit clone https://github.com/KONAKONA666/q8_kernels.git\ncd q8_kernels \u0026\u0026 git submodule init \u0026\u0026 git submodule update\npython setup.py install\n```\n\nFor other GPUs, install `q8_kernel==0.5.0` from [LTX-Video-Q8-Kernels](https://github.com/Lightricks/LTX-Video-Q8-Kernels).\n\nSources: [docs/EN/source/getting_started/quickstart.md:254-289]()\n\n## Dependency Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Core Dependencies\"\n        PyTorch[\"torch\u003c=2.8.0\u003cbr/\u003etorchvision/torchaudio\"]\n        Diffusers[\"diffusers\u003cbr/\u003etransformers\u003cbr/\u003etokenizers\"]\n        Utils[\"einops\u003cbr/\u003eloguru\u003cbr/\u003esafetensors\"]\n        Media[\"opencv-python\u003cbr/\u003eimageio\u003cbr/\u003eimageio-ffmpeg\u003cbr/\u003edecord\u003cbr/\u003eav\"]\n    end\n    \n    subgraph \"Attention Operators (Optional)\"\n        FlashAttn2[\"Flash Attention 2\u003cbr/\u003eATTN_WEIGHT_REGISTER: flash_attn2\"]\n        FlashAttn3[\"Flash Attention 3\u003cbr/\u003eATTN_WEIGHT_REGISTER: flash_attn3\"]\n        SageAttn2[\"SageAttention 2\u003cbr/\u003eATTN_WEIGHT_REGISTER: sage_attn2\"]\n        SageAttn3[\"SageAttention 3\u003cbr/\u003eATTN_WEIGHT_REGISTER: sage_attn3\"]\n        NbhdAttn[\"Neighborhood Attention\u003cbr/\u003eATTN_WEIGHT_REGISTER: nbhd_attn\"]\n    end\n    \n    subgraph \"Quantization Operators (Optional)\"\n        VLLM[\"vllm\u003cbr/\u003eFP8/INT8 kernels\"]\n        SGL[\"sgl-kernel\u003cbr/\u003eFP8 quantization\"]\n        Q8[\"q8_kernels\u003cbr/\u003eINT8 for Ada GPUs\"]\n        Triton[\"triton-windows\u003cbr/\u003eCustom kernels\"]\n    end\n    \n    subgraph \"Server Dependencies\"\n        Gradio[\"gradio\u003cbr/\u003eWeb interface\"]\n        FastAPI[\"fastapi\u003cbr/\u003euvicorn\u003cbr/\u003eAPI server\"]\n    end\n    \n    subgraph \"Platform Backends\"\n        CUDA[\"CUDA Backend\u003cbr/\u003eNVIDIA GPUs\"]\n        Cambricon[\"Cambricon MLU590\u003cbr/\u003elightx2v_platform\"]\n        MetaX[\"MetaX C500\u003cbr/\u003elightx2v_platform\"]\n        Hygon[\"Hygon DCU\u003cbr/\u003elightx2v_platform\"]\n        Ascend[\"Ascend 910B\u003cbr/\u003elightx2v_platform\"]\n        ROCm[\"AMD ROCm\u003cbr/\u003elightx2v_platform\"]\n        MUSA[\"MThreads MUSA\u003cbr/\u003elightx2v_platform\"]\n        Enflame[\"Enflame S60 GCU\u003cbr/\u003elightx2v_platform\"]\n    end\n    \n    PyTorch --\u003e FlashAttn2\n    PyTorch --\u003e FlashAttn3\n    PyTorch --\u003e SageAttn2\n    PyTorch --\u003e SageAttn3\n    PyTorch --\u003e NbhdAttn\n    \n    PyTorch --\u003e VLLM\n    PyTorch --\u003e SGL\n    PyTorch --\u003e Q8\n    PyTorch --\u003e Triton\n    \n    PyTorch --\u003e CUDA\n    PyTorch --\u003e Cambricon\n    PyTorch --\u003e MetaX\n    PyTorch --\u003e Hygon\n    PyTorch --\u003e Ascend\n    PyTorch --\u003e ROCm\n    PyTorch --\u003e MUSA\n    PyTorch --\u003e Enflame\n```\n\nThe dependency architecture shows the relationship between core dependencies, optional operators, and platform backends. All components are registered through the framework's registry system defined in [lightx2v/utils/registry_factory.py]().\n\nSources: [pyproject.toml:33-70](), [lightx2v/__init__.py:1-19]()\n\n## Hardware Platform Support\n\nLightX2V provides a unified hardware abstraction layer through `lightx2v_platform`, enabling deployment across diverse hardware platforms.\n\n```mermaid\ngraph TB\n    subgraph \"Application Layer\"\n        LightX2V[\"lightx2v\u003cbr/\u003eMain Application\"]\n        Config[\"Configuration System\u003cbr/\u003eJSON configs\"]\n    end\n    \n    subgraph \"Platform Abstraction Layer\"\n        PlatformCore[\"lightx2v_platform\u003cbr/\u003eset_ai_device\"]\n        BackendRegistry[\"Backend Registry\u003cbr/\u003eDevice selection\"]\n    end\n    \n    subgraph \"Supported Backends\"\n        NVIDIA[\"NVIDIA CUDA\u003cbr/\u003eDefault backend\"]\n        Cambricon[\"Cambricon MLU590\u003cbr/\u003eChinese AI chip\"]\n        MetaX[\"MetaX C500\u003cbr/\u003eChinese AI chip\"]\n        Hygon[\"Hygon DCU\u003cbr/\u003eChinese AI chip\"]\n        Ascend[\"Ascend 910B\u003cbr/\u003eHuawei NPU\"]\n        ROCm[\"AMD ROCm\u003cbr/\u003eAMD GPUs\"]\n        MUSA[\"MThreads MUSA\u003cbr/\u003eChinese AI chip\"]\n        Enflame[\"Enflame S60 GCU\u003cbr/\u003eChinese AI chip\"]\n    end\n    \n    subgraph \"Docker Environments\"\n        DockerNV[\"dockerfiles/platforms/nvidia\"]\n        DockerCam[\"dockerfiles/platforms/cambricon\"]\n        DockerMeta[\"dockerfiles/platforms/metax\"]\n        DockerOthers[\"dockerfiles/platforms/...\u003cbr/\u003ehygon/ascend/rocm/musa/enflame\"]\n    end\n    \n    subgraph \"Platform Scripts\"\n        ScriptNV[\"scripts/platforms/nvidia\"]\n        ScriptCam[\"scripts/platforms/cambricon\"]\n        ScriptMeta[\"scripts/platforms/metax\"]\n        ScriptOthers[\"scripts/platforms/...\u003cbr/\u003eplatform-specific tests\"]\n    end\n    \n    LightX2V --\u003e PlatformCore\n    Config --\u003e PlatformCore\n    PlatformCore --\u003e BackendRegistry\n    \n    BackendRegistry --\u003e NVIDIA\n    BackendRegistry --\u003e Cambricon\n    BackendRegistry --\u003e MetaX\n    BackendRegistry --\u003e Hygon\n    BackendRegistry --\u003e Ascend\n    BackendRegistry --\u003e ROCm\n    BackendRegistry --\u003e MUSA\n    BackendRegistry --\u003e Enflame\n    \n    DockerNV -.-\u003e|provides env| NVIDIA\n    DockerCam -.-\u003e|provides env| Cambricon\n    DockerMeta -.-\u003e|provides env| MetaX\n    DockerOthers -.-\u003e|provides env| Hygon\n    DockerOthers -.-\u003e|provides env| Ascend\n    DockerOthers -.-\u003e|provides env| ROCm\n    \n    ScriptNV -.-\u003e|tests| NVIDIA\n    ScriptCam -.-\u003e|tests| Cambricon\n    ScriptMeta -.-\u003e|tests| MetaX\n    ScriptOthers -.-\u003e|tests| Hygon\n```\n\n**Platform Backend Status**\n\n| Backend | Status | Platform Type | Docker Support | Notes |\n|---------|--------|---------------|----------------|-------|\n| NVIDIA CUDA |  Stable | GPU | Yes | Default, best support |\n| Cambricon MLU590 |  Stable | NPU | Yes | GGUF format support |\n| MetaX C500 |  Stable | NPU | Yes | GGUF format support |\n| Hygon DCU |  Stable | GPU | Yes | AMD-compatible |\n| Ascend 910B |  Stable | NPU | Yes | Huawei NPU |\n| AMD ROCm |  Stable | GPU | Yes | AMD GPUs |\n| MThreads MUSA |  Stable | GPU | No | Chinese GPU |\n| Enflame S60 (GCU) |  Stable | GPU | No | Chinese GPU |\n\nThe platform abstraction is initialized at import time via [lightx2v/__init__.py:5]() which imports `lightx2v_platform.set_ai_device`. Backend-specific Docker environments are available in `dockerfiles/platforms/` directory, and platform-specific test scripts are in `scripts/platforms/`.\n\nSources: [lightx2v_platform/README.md:1-19](), [lightx2v/__init__.py:5]()\n\n## Verification\n\nAfter installation, verify that LightX2V is correctly installed:\n\n```python\n# Check version and imports\nimport lightx2v\nprint(f\"LightX2V Version: {lightx2v.__version__}\")\n\n# Verify core components\nfrom lightx2v import LightX2VPipeline\nfrom lightx2v.models import wan\nfrom lightx2v.common.ops import attn\n\n# Check GPU availability\nimport torch\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\nprint(f\"CUDA Version: {torch.version.cuda}\")\nprint(f\"GPU Count: {torch.cuda.device_count()}\")\nif torch.cuda.is_available():\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n\n# Verify attention operators\ntry:\n    import flash_attn\n    print(\"Flash Attention: \")\nexcept ImportError:\n    print(\"Flash Attention: \")\n\ntry:\n    import sageattention\n    print(\"SageAttention: \")\nexcept ImportError:\n    print(\"SageAttention: \")\n\n# Verify quantization operators\ntry:\n    import vllm\n    print(\"VLLM: \")\nexcept ImportError:\n    print(\"VLLM: \")\n\ntry:\n    import triton\n    print(\"Triton: \")\nexcept ImportError:\n    print(\"Triton: \")\n```\n\nExpected output should show:\n- LightX2V version (e.g., \"0.1.0\")\n- CUDA available: True\n- At least one attention operator installed\n- At least one quantization operator (Triton minimum)\n\nSources: [lightx2v/__init__.py:1-3](), [docs/EN/source/getting_started/quickstart.md:147-150]()\n\n## Environment Variables and Configuration\n\nLightX2V supports several environment variables for configuration:\n\n| Variable | Purpose | Example |\n|----------|---------|---------|\n| `CUDA_VISIBLE_DEVICES` | Specify visible GPUs | `0,1,2,3` |\n| `LIGHTX2V_CACHE_DIR` | Model cache directory | `/path/to/cache` |\n| `TORCH_COMPILE_DEBUG` | Enable PyTorch compile debug | `1` |\n| `TRITON_CACHE_DIR` | Triton kernel cache | `/path/to/triton_cache` |\n\nExample environment setup script at [scripts/base/base.sh]() (referenced in [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh:9]()):\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\nexport LIGHTX2V_CACHE_DIR=/path/to/cache\n```\n\nSources: [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh:6-9]()\n\n## Troubleshooting Common Installation Issues\n\n### CUDA Version Mismatch\n\n**Symptom:** `RuntimeError: CUDA error: no kernel image is available`\n\n**Solution:** Ensure PyTorch CUDA version matches your driver CUDA version. Check with `nvidia-smi` and reinstall PyTorch with matching CUDA version.\n\n### Attention Operator Installation Fails\n\n**Symptom:** Compilation errors during flash-attention or SageAttention installation\n\n**Solution:** \n1. Ensure `ninja` is installed: `pip install ninja`\n2. Set CUDA architecture explicitly: `CUDA_ARCHITECTURES=\"8.0,8.6,8.9,9.0\" pip install ...`\n3. Reduce parallel jobs if out of memory: `MAX_JOBS=4 pip install ...`\n\n### Windows vLLM Compatibility\n\n**Symptom:** `ModuleNotFoundError: No module named 'vllm'` or import errors\n\n**Solution:** \n1. Use pre-built wheels from [vllm-windows releases](https://github.com/SystemPanic/vllm-windows/releases)\n2. Ensure exact version matching for Python, PyTorch, and CUDA\n3. Fall back to Triton kernels if vLLM is not available\n\n### Multi-GPU Communication Issues\n\n**Symptom:** `RuntimeError: NCCL error` or hanging during distributed inference\n\n**Solution:**\n1. Ensure `--ipc=host` is set when running Docker containers\n2. Verify all GPUs are visible: `nvidia-smi`\n3. Check NCCL configuration: `export NCCL_DEBUG=INFO`\n\nSources: Documentation experience and common patterns from [docs/EN/source/getting_started/quickstart.md]()\n\n## Next Steps\n\nAfter successfully installing LightX2V:\n\n1. **Download Models**: Proceed to [Model Download and Organization](#2.2) to download required model files\n2. **Quick Start**: Follow [Quick Start Tutorial](#2.3) for your first video generation\n3. **Advanced Configuration**: Explore [Configuration System](#8.2) for detailed parameter tuning\n4. **Hardware Optimization**: See [Hardware Auto-Configuration](#6.6) for automatic optimization settings\n\nSources: Document structure from table of contents"])</script><script>self.__next_f.push([1,"1e:T86a0,"])</script><script>self.__next_f.push([1,"# Model Download and Organization\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [README_zh.md](README_zh.md)\n- [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json](configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json)\n- [docs/EN/source/getting_started/quickstart.md](docs/EN/source/getting_started/quickstart.md)\n- [docs/ZH_CN/source/getting_started/quickstart.md](docs/ZH_CN/source/getting_started/quickstart.md)\n- [lightx2v/__init__.py](lightx2v/__init__.py)\n- [lightx2v/common/ops/attn/utils/ring_comm.py](lightx2v/common/ops/attn/utils/ring_comm.py)\n- [lightx2v/common/ops/mm/triton_kernels.py](lightx2v/common/ops/mm/triton_kernels.py)\n- [lightx2v_platform/README.md](lightx2v_platform/README.md)\n- [lightx2v_platform/README_zh.md](lightx2v_platform/README_zh.md)\n- [pyproject.toml](pyproject.toml)\n- [scripts/hunyuan_video_15/README.md](scripts/hunyuan_video_15/README.md)\n- [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh](scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document explains how LightX2V discovers, downloads, and organizes model files on the local filesystem. It covers:\n\n- Official model repositories and naming conventions\n- Expected local directory structure for models\n- The model discovery and caching system\n- Methods for downloading models (Gradio UI and manual)\n- File format organization (single files vs. split storage)\n\nFor information about model architecture types and their capabilities, see [Model Variants and Tasks](#5). For details on quantization formats referenced in model names, see [Quantization System](#6.1). For configuration of model paths in different interfaces, see [Python API](#3.2) and [Gradio Web Interface](#3.1).\n\n---\n\n## Model Repository Structure\n\nLightX2V models are hosted on both HuggingFace and ModelScope, organized into repositories by model family. The system caches repository contents at startup in the `HF_MODELS_CACHE` dictionary to avoid repeated API calls.\n\n### Repository Organization and Caching\n\n```mermaid\ngraph TB\n    subgraph \"Remote Repositories\"\n        WanDistill[\"lightx2v/Wan2.1-Distill-Models\u003cbr/\u003elightx2v/Wan2.2-Distill-Models\"]\n        Encoders[\"lightx2v/Encoders\"]\n        Autoencoders[\"lightx2v/Autoencoders\"]\n        QwenLightning[\"lightx2v/Qwen-Image-Edit-2511-Lightning\"]\n    end\n    \n    subgraph \"Cache: HF_MODELS_CACHE dict\"\n        CacheEntry[\"repo_id  list[model_names]\"]\n    end\n    \n    subgraph \"Component Types\"\n        DiT[\"DiT Transformers:\u003cbr/\u003ewan2.1_i2v_*.safetensors\u003cbr/\u003ewan2.2_*_high_noise_*.safetensors\u003cbr/\u003ewan2.2_*_low_noise_*.safetensors\"]\n        TextEnc[\"Text Encoders:\u003cbr/\u003et5_*.safetensors\u003cbr/\u003egoogle/ tokenizer dir\"]\n        ImageEnc[\"Image Encoders:\u003cbr/\u003eclip_*.safetensors\u003cbr/\u003exlm-roberta-large/ tokenizer dir\"]\n        VAEDec[\"VAE Decoders:\u003cbr/\u003e*vae*.safetensors\u003cbr/\u003elightvae*.safetensors\u003cbr/\u003elighttae*.safetensors\"]\n    end\n    \n    WanDistill --\u003e|\"load_hf_models_cache()\"| CacheEntry\n    Encoders --\u003e|\"load_hf_models_cache()\"| CacheEntry\n    Autoencoders --\u003e|\"load_hf_models_cache()\"| CacheEntry\n    QwenLightning --\u003e|\"load_hf_models_cache()\"| CacheEntry\n    \n    CacheEntry --\u003e DiT\n    CacheEntry --\u003e TextEnc\n    CacheEntry --\u003e ImageEnc\n    CacheEntry --\u003e VAEDec\n```\n\n**Sources:** [app/gradio_demo.py:120-203](), [docs/EN/source/deploy_guides/deploy_gradio.md:79-86]()\n\u003c/thinking\u003e\n\nThe `load_hf_models_cache()` function populates the cache by querying ModelScope API (primary) with HuggingFace fallback (30s timeout). It processes file lists to extract model names and handles both single files and `_split` directories.\n\n### Repository Contents Table\n\n| Repository | Component Type | File Patterns | Notes |\n|-----------|---------------|---------------|-------|\n| `lightx2v/Wan2.1-Distill-Models` | DiT weights | `wan2.1_{task}_{res}_*.safetensors` | 4-step distilled, quantized variants |\n| `lightx2v/Wan2.2-Distill-Models` | DiT weights (MoE) | `wan2.2_*_high_noise_*.safetensors`\u003cbr/\u003e`wan2.2_*_low_noise_*.safetensors` | Separate high/low noise experts |\n| `lightx2v/Encoders` | Text/Image encoders | `t5_*.safetensors`, `clip_*.safetensors`\u003cbr/\u003e`google/`, `xlm-roberta-large/` | Includes tokenizer directories |\n| `lightx2v/Autoencoders` | VAE decoders | `wan2_1_vae_decoder.safetensors`\u003cbr/\u003e`lightvae*.safetensors`, `lighttae*.safetensors` | Multiple speed/quality options |\n| `lightx2v/Qwen-Image-Edit-2511-Lightning` | Qwen DiT | `qwen_image_edit_2511_*_lightning.safetensors` | 4-step distilled image models |\n\n**Sources:** [docs/EN/source/deploy_guides/deploy_gradio.md:47-86](), [docs/ZH_CN/source/deploy_guides/deploy_gradio.md:48-78]()\n\n---\n\n## Model Naming Conventions\n\nModel files follow a structured naming convention that encodes model family, task type, resolution, quantization, and storage format.\n\n### Naming Pattern Breakdown\n\n```\nwan2.1_i2v_720p_scaled_fp8_e4m3_lightx2v_4step_split\n                                             \n                                              Storage: _split (block-based)\n                                        Steps: 4step (distilled)\n                               Framework: lightx2v\n                         Quantization: fp8 E4M3 format\n                   Scaling: scaled (per-tensor scale)\n            Resolution: 720p\n        Task: i2v (image-to-video)\n   Model Family: wan2.1\n```\n\n### Component Identification Table\n\n| Component | Values | Meaning |\n|-----------|--------|---------|\n| **Model Family** | `wan2.1`, `wan2.2`, `qwen_image_edit_2511` | Architecture version |\n| **Task Type** | `i2v`, `t2v` | Image-to-video or text-to-video |\n| **Resolution** | `480p`, `540p`, `720p`, `1280p` | Output resolution |\n| **Quantization** | `int8`, `fp8`, `scaled_fp8_e4m3` | Weight precision format |\n| **Distillation** | `lightx2v_4step`, `lightning` | Indicates distilled models |\n| **Noise Level** | `high_noise`, `low_noise` | For Wan2.2 MoE models |\n| **Storage Format** | `_split` suffix | Block-based storage directory |\n\n**Sources:** [app/gradio_demo.py:302-389](), [app/gradio_demo.py:391-457]()\n\n### Model Discovery Flow: get_dit_choices()\n\n```mermaid\ngraph TD\n    Start[\"get_dit_choices(model_path, model_type, task_type)\"] --\u003e CheckCache[\"Access HF_MODELS_CACHE[repo_id]\"]\n    CheckCache --\u003e FilterFamily[\"Filter by model_type:\u003cbr/\u003e'wan2.1', 'wan2.2', 'qwen'\"]\n    FilterFamily --\u003e FilterTask[\"Filter by task_type:\u003cbr/\u003e'i2v', 't2v' (optional)\"]\n    FilterTask --\u003e CheckFP8[\"is_fp8_supported_gpu()\"]\n    CheckFP8 --\u003e|\"True\"| AllModels[\"Include all models\"]\n    CheckFP8 --\u003e|\"False\"| ExcludeFP8[\"Exclude models with 'fp8' in name\"]\n    AllModels --\u003e ExcludeKeywords[\"Exclude keywords:\u003cbr/\u003e'vae', 'tae', 'clip', 't5'\u003cbr/\u003e'high_noise', 'low_noise' (non-MoE)\"]\n    ExcludeFP8 --\u003e ExcludeKeywords\n    ExcludeKeywords --\u003e ValidateFormat[\"Validate:\u003cbr/\u003eends with '.safetensors' OR\u003cbr/\u003eends with '_split'\"]\n    ValidateFormat --\u003e SortPriority[\"sort_model_choices():\u003cbr/\u003eFP8+split \u003e INT8+split \u003e FP8 \u003e INT8\"]\n    SortPriority --\u003e CheckLocal[\"check_model_exists(model_path, name)\u003cbr/\u003efor each model\"]\n    CheckLocal --\u003e FormatStatus[\"format_model_choice(name, exists):\u003cbr/\u003eAdd  or  prefix\"]\n    FormatStatus --\u003e Return[\"Return list of formatted choices\"]\n```\n\n**Sources:** [app/gradio_demo.py:302-389](), [app/gradio_demo.py:267-299](), [app/gradio_demo.py:221-239]()\n\nThe `get_dit_choices()` function coordinates model discovery by combining remote cache data with local filesystem checks. Hardware capability is detected via `is_fp8_supported_gpu()` which checks CUDA compute capability for (8,9), (9,0), or (12,0).\n\n---\n\n## Local Directory Structure\n\nLightX2V expects models to be organized in a flat directory structure where all model files and component directories coexist.\n\n### Expected Directory Layout\n\nLightX2V uses a flat directory structure where all model components coexist at the top level. This design enables the `scan_model_path_contents()` function to discover all components in a single pass.\n\n```\nmodel_path/  # Provided via --model_path argument\n wan2.1_i2v_720p_lightx2v_4step.safetensors          # DiT single file\n wan2.1_i2v_720p_int8_lightx2v_4step_split/          # DiT split directory\n    block_0.safetensors                              # Pre-processing + Block 0\n    block_1.safetensors                              # Transformer block 1\n    ...\n    block_40.safetensors                             # Post-processing\n wan2.2_i2v_A14b_high_noise_lightx2v_4step.safetensors  # MoE expert (high)\n wan2.2_i2v_A14b_low_noise_lightx2v_4step.safetensors   # MoE expert (low)\n t5_wan2.1_scaled_fp8_e4m3.safetensors               # T5 encoder weights\n clip_wan2.1_int8.safetensors                        # CLIP encoder weights\n google/                                              # T5 tokenizer files\n    tokenizer_config.json\n    spiece.model\n xlm-roberta-large/                                   # CLIP tokenizer files\n    tokenizer_config.json\n    sentencepiece.bpe.model\n Wan2.1_VAE.safetensors                              # VAE encoder\n wan2_1_vae_decoder.safetensors                      # VAE decoder (standard)\n lightvaewy2_1.safetensors                           # LightVAE (optimized)\n lighttaewy2_1.safetensors                           # LightTAE (temporal)\n qwen_image_edit_2511_lightning.safetensors          # Qwen DiT\n vae/                                                 # Qwen VAE directory\n    diffusion_pytorch_model.safetensors\n lora_style_anime_v1.safetensors                     # LoRA adaptation weights\n```\n\n**Directory Requirements:**\n- **Flat Structure**: All components at same level except tokenizers (`google/`, `xlm-roberta-large/`) and Qwen VAE (`vae/`)\n- **Storage Medium**: SSD strongly recommended; HDD causes slow block loading in `load_state_dict_from_disk()`\n- **Mixed Formats**: `.safetensors` files and `_split/` directories can coexist for same model (system detects either)\n\n**Sources:** [docs/EN/source/deploy_guides/deploy_gradio.md:47-86](), [app/run_gradio.sh:9-13](), [lightx2v/common/offload/manager.py:92-104]()\n\n### Directory Scanning: scan_model_path_contents()\n\nThe `scan_model_path_contents()` function categorizes model directory contents for efficient component discovery:\n\n```mermaid\ngraph TD\n    ScanStart[\"scan_model_path_contents(model_path)\"] --\u003e PathCheck{\"os.path.exists(model_path)?\"}\n    PathCheck --\u003e|\"False\"| ReturnEmpty[\"Return {\u003cbr/\u003edirs: [],\u003cbr/\u003efiles: [],\u003cbr/\u003esafetensors_dirs: [],\u003cbr/\u003epth_files: []\u003cbr/\u003e}\"]\n    PathCheck --\u003e|\"True\"| ListDir[\"os.listdir(model_path)\"]\n    ListDir --\u003e ForEachItem[\"for item in items\"]\n    ForEachItem --\u003e CheckType{\"os.path.isdir(item_path)?\"}\n    \n    CheckType --\u003e|\"True\"| AddDir[\"dirs.append(item)\"]\n    AddDir --\u003e GlobCheck[\"glob.glob(item_path/*.safetensors)\"]\n    GlobCheck --\u003e|\"len \u003e 0\"| AddSafetensorsDir[\"safetensors_dirs.append(item)\"]\n    GlobCheck --\u003e|\"len == 0\"| NextItem\n    AddSafetensorsDir --\u003e NextItem\n    \n    CheckType --\u003e|\"False\"| CheckExt{\"File extension?\"}\n    CheckExt --\u003e|\"'.safetensors'\"| AddFile[\"files.append(item)\"]\n    CheckExt --\u003e|\"'.pth'\"| AddPth[\"pth_files.append(item)\"]\n    CheckExt --\u003e|\"other\"| NextItem[\"Continue\"]\n    AddFile --\u003e NextItem\n    AddPth --\u003e NextItem\n    \n    NextItem --\u003e MoreItems{\"More items?\"}\n    MoreItems --\u003e|\"Yes\"| ForEachItem\n    MoreItems --\u003e|\"No\"| SortLists[\"Sort each list:\u003cbr/\u003edirs.sort()\u003cbr/\u003efiles.sort()\u003cbr/\u003esafetensors_dirs.sort()\u003cbr/\u003epth_files.sort()\"]\n    SortLists --\u003e ReturnDict[\"Return categorized dict\"]\n```\n\n**Return Value Structure:**\n```python\n{\n    \"dirs\": [\"google\", \"xlm-roberta-large\", \"wan2.1_i2v_720p_int8_lightx2v_4step_split\", ...],\n    \"files\": [\"wan2.1_i2v_720p_lightx2v_4step.safetensors\", \"t5_wan2.1_scaled_fp8_e4m3.safetensors\", ...],\n    \"safetensors_dirs\": [\"wan2.1_i2v_720p_int8_lightx2v_4step_split\", ...],  # Directories with *.safetensors\n    \"pth_files\": []  # Legacy format, rarely used\n}\n```\n\nThis categorization allows component-specific discovery functions like `get_t5_model_choices()`, `get_clip_model_choices()`, and `get_vae_decoder_choices()` to filter by file patterns without repeated directory traversals.\n\n**Sources:** [app/gradio_demo.py:101-127]()\n\n---\n\n## Model Discovery and Caching System\n\nThe Gradio interface implements a two-tier caching system: remote repository cache and local filesystem status cache.\n\n### Caching Architecture\n\n```mermaid\nsequenceDiagram\n    participant App as Gradio App Startup\n    participant Cache as load_hf_models_cache()\n    participant MS as ModelScope API\n    participant HF as HuggingFace API\n    participant Disk as Local Filesystem\n    participant UI as UI Model Dropdowns\n    \n    App-\u003e\u003eCache: Initialize\n    Cache-\u003e\u003eMS: Try ModelScope first\u003cbr/\u003eapi.get_model_files(repo_id)\n    alt ModelScope Success\n        MS--\u003e\u003eCache: Return file list\n    else ModelScope Fails\n        Cache-\u003e\u003eHF: Fallback to HuggingFace\u003cbr/\u003elist_repo_files(repo_id)\n        HF--\u003e\u003eCache: Return file list (30s timeout)\n    end\n    Cache-\u003e\u003eCache: process_files()\u003cbr/\u003eExtract model names\u003cbr/\u003eFilter _split dirs, .safetensors\n    Cache-\u003e\u003eCache: Update HF_MODELS_CACHE[repo_id]\n    Cache--\u003e\u003eApp: Cache loaded\n    \n    App-\u003e\u003eUI: User selects model type\n    UI-\u003e\u003eDisk: scan_model_path_contents()\n    Disk--\u003e\u003eUI: Local model list\n    UI-\u003e\u003eUI: Merge remote cache + local\u003cbr/\u003eDeduplicate\u003cbr/\u003eSort by priority\n    UI-\u003e\u003eDisk: check_model_exists() for each\n    Disk--\u003e\u003eUI: Existence status\n    UI-\u003e\u003eUI: format_model_choice()\u003cbr/\u003eAdd / prefix\n    UI--\u003e\u003eApp: Display model choices\n```\n\n**Sources:** [app/gradio_demo.py:130-203](), [app/gradio_demo.py:221-240]()\n\n### Cache Loading Implementation\n\nThe `load_hf_models_cache()` function implements a concurrent loading strategy with fallback:\n\n1. **Primary Source (ModelScope):** Attempts to fetch file lists using ModelScope API with no timeout\n2. **Fallback Source (HuggingFace):** If ModelScope fails, tries HuggingFace API with 30-second timeout\n3. **File Processing:** Extracts model names from file paths, handling both single files and `_split` directories\n4. **Special Handling:** For `Qwen/Qwen-Image-Edit-2511`, preserves `vae` and `scheduler` directories\n\n**Sources:** [app/gradio_demo.py:130-203]()\n\n### Model Existence Checking\n\n```mermaid\ngraph TD\n    CheckStart[\"check_model_exists(model_path, model_name)\"] --\u003e PathExists{\"os.path.exists\u003cbr/\u003e(model_path)?\"}\n    PathExists --\u003e|No| ReturnFalse1[\"Return False\"]\n    PathExists --\u003e|Yes| JoinPath[\"full_path = os.path.join\u003cbr/\u003e(model_path, model_name)\"]\n    JoinPath --\u003e ExistsCheck{\"os.path.exists\u003cbr/\u003e(full_path)?\"}\n    ExistsCheck --\u003e|Yes| ReturnTrue[\"Return True\"]\n    ExistsCheck --\u003e|No| IsSafetensors{\"model_name.endswith\u003cbr/\u003e('.safetensors')?\"}\n    IsSafetensors --\u003e|No| ReturnFalse2[\"Return False\"]\n    IsSafetensors --\u003e|Yes| CheckSplit[\"Check for\u003cbr/\u003ebase_name + '_split' directory\"]\n    CheckSplit --\u003e SplitExists{\"Split dir exists?\"}\n    SplitExists --\u003e|Yes| ReturnTrue\n    SplitExists --\u003e|No| ReturnFalse3[\"Return False\"]\n```\n\n**Sources:** [app/gradio_demo.py:221-239]()\n\nThis function handles the equivalence between single `.safetensors` files and their `_split` directory counterparts for memory-constrained environments.\n\n---\n\n## Download Methods\n\nLightX2V provides two primary methods for obtaining models: automated Gradio interface download and manual download from repositories.\n\n### Gradio Interface Download\n\nThe Gradio UI implements one-click downloads from both HuggingFace and ModelScope:\n\n```mermaid\ngraph TB\n    UserSelect[\"User selects model\u003cbr/\u003ewith  status\"] --\u003e ClickDownload[\"Click 'Download' button\"]\n    ClickDownload --\u003e ExtractName[\"extract_model_name()\u003cbr/\u003eRemove / prefix\"]\n    ExtractName --\u003e ParsePath[\"Parse repo_id and filename\"]\n    ParsePath --\u003e SelectSource{\"User-selected\u003cbr/\u003edownload source?\"}\n    SelectSource --\u003e|HuggingFace| HFDownload[\"hf_hub_download() or\u003cbr/\u003ehf_snapshot_download()\"]\n    SelectSource --\u003e|ModelScope| MSDownload[\"ms_snapshot_download()\"]\n    HFDownload --\u003e IsFile{\"Single file or\u003cbr/\u003edirectory?\"}\n    MSDownload --\u003e IsFile\n    IsFile --\u003e|Single file| DownloadFile[\"Download .safetensors\u003cbr/\u003eto model_path/\"]\n    IsFile --\u003e|Directory| DownloadDir[\"Download entire directory\u003cbr/\u003eto model_path/dir_name/\"]\n    DownloadFile --\u003e UpdateStatus[\"Update UI:\u003cbr/\u003e  \"]\n    DownloadDir --\u003e UpdateStatus\n    UpdateStatus --\u003e RefreshChoices[\"Refresh model dropdown\u003cbr/\u003ewith new status\"]\n```\n\n**Sources:** [app/gradio_demo_zh.py:1-100](), [app/gradio_demo.py:37-38](), [app/gradio_demo.py:42-43]()\n\nThe download functionality is integrated into the Gradio interface and supports:\n- **Status Tracking:** Real-time display of download status with emoji indicators\n- **Dual Sources:** Fallback between HuggingFace and ModelScope based on accessibility\n- **Directory Handling:** Automatically detects whether to download a single file or entire directory (e.g., `_split` directories, tokenizer directories)\n\n### Manual Download Process\n\nFor manual downloads, users can directly fetch models from repositories:\n\n**For DiT Models:**\n```bash\n# Single file download\nhuggingface-cli download lightx2v/Wan2.1-Distill-Models \\\n    wan2.1_i2v_720p_int8_lightx2v_4step.safetensors \\\n    --local-dir /path/to/models\n\n# Directory download (split format)\nhuggingface-cli download lightx2v/Wan2.1-Distill-Models \\\n    wan2.1_i2v_720p_int8_lightx2v_4step_split \\\n    --local-dir /path/to/models\n```\n\n**For Encoders and VAE:**\n```bash\n# T5 encoder\nhuggingface-cli download lightx2v/Encoders \\\n    t5_wan2.1_scaled_fp8_e4m3.safetensors \\\n    --local-dir /path/to/models\n\n# CLIP encoder\nhuggingface-cli download lightx2v/Encoders \\\n    clip_wan2.1_int8.safetensors \\\n    --local-dir /path/to/models\n\n# Tokenizers (directories)\nhuggingface-cli download lightx2v/Encoders google \\\n    --local-dir /path/to/models\n\nhuggingface-cli download lightx2v/Encoders xlm-roberta-large \\\n    --local-dir /path/to/models\n\n# VAE decoder\nhuggingface-cli download lightx2v/Autoencoders \\\n    wan2_1_vae_decoder.safetensors \\\n    --local-dir /path/to/models\n```\n\n**Sources:** [README.md:202-229](), [docs/EN/source/deploy_guides/deploy_gradio.md:43-86]()\n\n---\n\n## Model File Formats\n\nLightX2V supports two storage formats for large models: single `.safetensors` files and block-split directories. This design enables deployment on systems with varying memory constraints.\n\n### Storage Format Comparison\n\n| Format | Structure | Use Case | Memory Requirement | Download Method |\n|--------|-----------|----------|-------------------|-----------------|\n| **Single File** | `model_name.safetensors` | High-RAM systems (32GB+) | Full model loaded to RAM | Direct file download |\n| **Split Directory** | `model_name_split/` | Low-RAM systems (16GB) | Block-wise lazy loading | Directory download with multiple files |\n\n**Sources:** [lightx2v/common/offload/manager.py:1-141](), [docs/EN/source/deploy_guides/deploy_local_windows.md:14](), [docs/EN/source/deploy_guides/deploy_gradio.md:86]()\n\n### Split Directory Structure and Block Organization\n\nFor memory-constrained environments (especially systems with 16GB RAM), models are stored in block-wise split directories. Each directory contains sequentially numbered block files:\n\n```\nwan2.1_i2v_720p_int8_lightx2v_4step_split/\n block_0.safetensors     # Pre-processing weights + Block 0\n block_1.safetensors     # Transformer block 1\n block_2.safetensors     # Transformer block 2\n ...\n block_39.safetensors    # Transformer block 39\n block_40.safetensors    # Post-processing weights (if exists)\n```\n\n**Block Content Organization:**\n- **block_0.safetensors**: Contains pre-processing layers and the first transformer block\n- **block_1 to block_39**: Each contains one complete transformer block (self-attention + cross-attention + FFN)\n- **block_40.safetensors**: Contains post-processing layers (e.g., final layer norm, output projection)\n\nThe number of blocks corresponds to the model's `num_layers` parameter. For Wan models with 40 transformer layers, there are typically 41 block files (block_0 through block_40).\n\n**Sources:** [lightx2v/common/offload/manager.py:28-48](), [app/gradio_demo_zh.py:54-88]()\n\n### File Format Equivalence and Detection\n\nThe system treats single files and split directories as equivalent representations of the same model:\n\n```mermaid\ngraph LR\n    Query[\"check_model_exists(model_path, model_name)\"] --\u003e CheckDirect{\"Direct path exists?\"}\n    CheckDirect --\u003e|\"Yes:\u003cbr/\u003efile or dir found\"| ReturnTrue[\"Return True\"]\n    CheckDirect --\u003e|No| IsSafetensors{\"model_name ends with\u003cbr/\u003e.safetensors?\"}\n    IsSafetensors --\u003e|No| ReturnFalse[\"Return False\"]\n    IsSafetensors --\u003e|Yes| ExtractBase[\"base_name =\u003cbr/\u003emodel_name.replace('.safetensors', '')\"]\n    ExtractBase --\u003e CheckSplit[\"Check for\u003cbr/\u003ebase_name + '_split' directory\"]\n    CheckSplit --\u003e SplitExists{\"Split dir exists?\"}\n    SplitExists --\u003e|Yes| ReturnTrue\n    SplitExists --\u003e|No| ReturnFalse\n```\n\n**Detection Examples:**\n- Query: `wan2.1_i2v_720p_int8_lightx2v_4step.safetensors`\n  - Checks: `wan2.1_i2v_720p_int8_lightx2v_4step.safetensors` (file)\n  - Fallback: `wan2.1_i2v_720p_int8_lightx2v_4step_split/` (directory)\n  - Result: Returns `True` if either exists\n\nThis equivalence allows users to download either format based on their system constraints, and the framework will detect and use whichever is available.\n\n**Sources:** [app/gradio_demo.py:221-239]()\n\n### WeightAsyncStreamManager: Lazy Loading Implementation\n\nThe `WeightAsyncStreamManager` class implements the core lazy loading mechanism for split models:\n\n```mermaid\ngraph TD\n    Init[\"WeightAsyncStreamManager.__init__(offload_granularity)\"] --\u003e CreateStreams[\"Create CUDA streams:\u003cbr/\u003einit_stream (priority 0)\u003cbr/\u003ecuda_load_stream (priority 1)\u003cbr/\u003ecompute_stream (priority -1)\"]\n    CreateStreams --\u003e InitCPUBuffer[\"init_cpu_buffer(blocks_cpu_buffer)\u003cbr/\u003eOR\u003cbr/\u003einit_cuda_buffer(blocks_cuda_buffer)\"]\n    InitCPUBuffer --\u003e InitFirstBuffer[\"init_first_buffer(blocks, adapter_block_idx):\u003cbr/\u003eLoad block_0 to cuda_buffers[0]\u003cbr/\u003eusing init_stream\"]\n    InitFirstBuffer --\u003e InferLoop[\"Inference Loop: for block_idx in range(1, num_blocks)\"]\n    \n    InferLoop --\u003e Prefetch[\"prefetch_weights(block_idx, blocks):\u003cbr/\u003eLoad block_N to cuda_buffers[1]\u003cbr/\u003eusing cuda_load_stream (async)\"]\n    Prefetch --\u003e Compute[\"Compute on cuda_buffers[0]\u003cbr/\u003eusing compute_stream (async)\"]\n    Compute --\u003e SwapBlock[\"swap_blocks():\u003cbr/\u003eSynchronize streams\u003cbr/\u003eSwap cuda_buffers[0]  cuda_buffers[1]\"]\n    SwapBlock --\u003e NextBlock{\"More blocks?\"}\n    NextBlock --\u003e|Yes| InferLoop\n    NextBlock --\u003e|No| Complete[\"Inference Complete\"]\n```\n\n**Stream Priority Strategy:**\n- **PyTorch 2.7 on CUDA**: `cuda_load_stream` gets priority=1, `compute_stream` gets priority=1\n- **PyTorch \u003c2.7 or non-CUDA**: `cuda_load_stream` gets priority=0, `compute_stream` gets priority=-1 (higher priority)\n\nThis strategy ensures that:\n1. The next block is prefetched to GPU memory while the current block is computing\n2. Disk I/O, CPU-to-GPU transfer, and GPU computation overlap for maximum throughput\n3. The compute stream has sufficient priority to avoid stalling on memory transfers\n\n**Sources:** [lightx2v/common/offload/manager.py:14-90]()\n\n### Double Buffering Architecture\n\n```mermaid\nsequenceDiagram\n    participant Disk as Disk Storage\u003cbr/\u003e(block_N.safetensors)\n    participant CPU as CPU Buffer\u003cbr/\u003e(Optional)\n    participant Buffer0 as cuda_buffers[0]\n    participant Buffer1 as cuda_buffers[1]\n    participant GPU as GPU Compute\n    \n    Note over Buffer0,Buffer1: Initial State\n    Disk-\u003e\u003eBuffer0: Load block_0 (init_stream)\n    \n    Note over Buffer0,Buffer1: Iteration N\n    Disk-\u003e\u003eCPU: Prefetch block_N+1 (if lazy_load enabled)\n    CPU-\u003e\u003eBuffer1: Transfer block_N+1 (cuda_load_stream)\n    Buffer0-\u003e\u003eGPU: Compute block_N (compute_stream)\n    \n    Note over Buffer0,Buffer1: Synchronization Point\n    Buffer0--\u003e\u003eBuffer1: swap_blocks()\n    \n    Note over Buffer0,Buffer1: After Swap\n    Note right of Buffer0: Now contains block_N+1\n    Note right of Buffer1: Now contains block_N (stale)\n    \n    Note over Buffer0,Buffer1: Iteration N+1\n    Disk-\u003e\u003eCPU: Prefetch block_N+2\n    CPU-\u003e\u003eBuffer1: Transfer block_N+2\n    Buffer0-\u003e\u003eGPU: Compute block_N+1\n```\n\n**Buffer Management:**\n1. **Two GPU Buffers**: `cuda_buffers[0]` and `cuda_buffers[1]` alternate roles\n2. **Current Buffer**: Holds the currently executing block weights\n3. **Prefetch Buffer**: Receives the next block while current block computes\n4. **Swap Operation**: After compute and prefetch complete, buffers swap roles via pointer exchange\n\n**Sources:** [lightx2v/common/offload/manager.py:79-90]()\n\n### Disk-Based Lazy Loading with ThreadPoolExecutor\n\nFor extreme memory constraints, the system supports disk-based lazy loading where blocks are loaded from disk on-demand:\n\n```mermaid\ngraph TD\n    InitLazy[\"init_lazy_load(num_workers=6)\"] --\u003e CreateExecutor[\"ThreadPoolExecutor(max_workers=6)\"]\n    CreateExecutor --\u003e InferStart[\"Inference starts\"]\n    InferStart --\u003e StartPrefetch[\"start_prefetch_block(block_idx):\u003cbr/\u003eSubmit disk load tasks to executor\"]\n    StartPrefetch --\u003e AsyncDiskLoad[\"cpu_buffers[1].load_state_dict_from_disk(block_idx)\u003cbr/\u003e(runs in background thread)\"]\n    AsyncDiskLoad --\u003e ComputeCurrent[\"Compute current block\u003cbr/\u003e(GPU work overlaps disk I/O)\"]\n    ComputeCurrent --\u003e SwapCPU[\"swap_cpu_buffers():\u003cbr/\u003eWait for prefetch completion\u003cbr/\u003ef.result() for all futures\u003cbr/\u003eSwap cpu_buffers[0]  cpu_buffers[1]\"]\n    SwapCPU --\u003e TransferGPU[\"Transfer new cpu_buffers[0] to GPU\"]\n    TransferGPU --\u003e NextBlock{\"More blocks?\"}\n    NextBlock --\u003e|Yes| StartPrefetch\n    NextBlock --\u003e|No| Cleanup[\"Cleanup: executor.shutdown()\"]\n```\n\n**Lazy Loading Parameters:**\n- **`num_workers=6`**: Default thread pool size for parallel disk reads\n- **CPU Buffers**: Two CPU-side buffers mirror the GPU double-buffering pattern\n- **Prefetch Futures**: List of `Future` objects tracking async disk loads\n\n**Performance Characteristics:**\n- **Memory Footprint**: Only 2-3 blocks in RAM at any time (compared to full model ~10-20GB)\n- **Latency**: Disk read latency (~100-500ms per block on SSD) overlapped with GPU compute\n- **Throughput**: Effective if GPU compute time  disk read time (typical for large models)\n\n**Sources:** [lightx2v/common/offload/manager.py:106-132](), [docs/EN/source/deploy_guides/deploy_local_windows.md:14]()\n\n### Warm-up Process for CPU Buffers\n\nWhen using disk-based lazy loading, the system performs a warm-up phase to populate OS page cache:\n\n```python\n# Simplified from lightx2v/common/offload/manager.py:92-104\ndef warm_up_cpu_buffers(self, blocks_num):\n    # Touch all block files to populate page cache\n    for i in range(blocks_num):\n        for phase in self.cpu_buffers[0]:\n            phase.load_state_dict_from_disk(i, None)\n        for phase in self.cpu_buffers[1]:\n            phase.load_state_dict_from_disk(i, None)\n    \n    # Reset to initial state\n    for phase in self.cpu_buffers[0]:\n        phase.load_state_dict_from_disk(0, None)\n    for phase in self.cpu_buffers[1]:\n        phase.load_state_dict_from_disk(1, None)\n```\n\nThis warm-up reduces subsequent disk read latencies by ensuring blocks are cached by the operating system.\n\n**Sources:** [lightx2v/common/offload/manager.py:92-104]()\n\n---\n\n## Component Organization\n\nModel components follow specific naming patterns that enable discovery functions to filter by file patterns and keywords.\n\n### Text Encoder Organization: get_t5_model_choices() and get_clip_model_choices()\n\n```mermaid\ngraph TB\n    T5Weight[\"T5 Weight File\"] --\u003e|\"Filter: 't5' in name\"| T5Pattern[\"t5_*.safetensors\"]\n    T5Pattern --\u003e|\"Exclude: 'google', 'comfyui'\"| T5Files[\"t5_wan2.1_scaled_fp8_e4m3.safetensors\u003cbr/\u003et5_wan2.1_int8.safetensors\"]\n    \n    T5Tokenizer[\"T5 Tokenizer\"] --\u003e|\"Directory name == 'google'\"| GoogleDir[\"google/\"]\n    GoogleDir --\u003e GoogleFiles[\"tokenizer_config.json\u003cbr/\u003espiece.model\"]\n    \n    CLIPWeight[\"CLIP Weight File\"] --\u003e|\"Filter: 'clip' in name\"| CLIPPattern[\"clip_*.safetensors\"]\n    CLIPPattern --\u003e|\"Exclude: 'xlm-roberta-large', 'comfyui'\"| CLIPFiles[\"clip_wan2.1_int8.safetensors\u003cbr/\u003eclip_wan2.1_fp8.safetensors\"]\n    \n    CLIPTokenizer[\"CLIP Tokenizer\"] --\u003e|\"Directory name == 'xlm-roberta-large'\"| XLMDir[\"xlm-roberta-large/\"]\n    XLMDir --\u003e XLMFiles[\"tokenizer_config.json\u003cbr/\u003esentencepiece.bpe.model\"]\n```\n\n**Filter Logic:**\n- **get_t5_model_choices()**: `\"t5\" in name.lower()` AND `.endswith(\".safetensors\")` AND `\"google\" not in name` AND `\"comfyui\" not in name`\n- **get_t5_tokenizer_choices()**: `\"google\" in dirs`\n- **get_clip_model_choices()**: `\"clip\" in name.lower()` AND `.endswith(\".safetensors\")` AND `\"xlm-roberta-large\" not in name` AND `\"comfyui\" not in name`\n- **get_clip_tokenizer_choices()**: `\"xlm-roberta-large\" in dirs`\n\n**Sources:** [app/gradio_demo.py:529-591]()\n\n### VAE Organization: get_vae_encoder_choices() and get_vae_decoder_choices()\n\n```mermaid\ngraph TB\n    VAEEnc[\"VAE Encoder\"] --\u003e|\"Fixed filename\"| EncFile[\"Wan2.1_VAE.safetensors\"]\n    \n    VAEDec[\"VAE Decoder\"] --\u003e|\"Filter: ('vae' OR 'tae' OR 'lightvae' OR 'lighttae') in name\"| DecFilter\n    DecFilter --\u003e|\"AND: ('2_1' OR '2.1') in name\"| DecPattern\n    DecPattern --\u003e DecChoices[\"wan2_1_vae_decoder.safetensors\u003cbr/\u003elightvaewy2_1.safetensors\u003cbr/\u003elighttaewy2_1.safetensors\"]\n    DecChoices --\u003e|\"sort_model_choices():\u003cbr/\u003eFP8 \u003e INT8 \u003e original\"| SortedDec[\"Priority-sorted list\"]\n```\n\n**Filter Logic:**\n- **get_vae_encoder_choices()**: Always returns `[\"Wan2.1_VAE.safetensors\"]`\n- **get_vae_decoder_choices()**: \n  - Must match: `(\"vae\" in name OR \"tae\" in name OR \"lightvae\" in name OR \"lighttae\" in name)` \n  - AND: `(\"2_1\" in name OR \"2.1\" in name)`\n  - AND: `.endswith(\".safetensors\")` OR name in `safetensors_dirs`\n  - Sort by quantization priority\n\n**Sources:** [app/gradio_demo.py:659-740]()\n\n### Qwen Model Organization\n\nQwen models use a nested directory structure for VAE and scheduler components:\n\n```mermaid\ngraph TB\n    DiT[\"Qwen DiT\"] --\u003e|\"get_qwen_image_dit_choices()\"| DiTFilter[\"Filter: 'qwen_image_edit_2511' in name\u003cbr/\u003eAND 'lightning' in name\"]\n    DiTFilter --\u003e DiTFiles[\"qwen_image_edit_2511_lightning.safetensors\u003cbr/\u003eqwen_image_edit_2511_int8_lightning_split/\"]\n    \n    VAE[\"Qwen VAE\"] --\u003e|\"get_qwen_image_vae_choices()\"| VAEFilter[\"Check for 'vae/' subdirectory\"]\n    VAEFilter --\u003e VAEPath[\"model_path/vae/diffusion_pytorch_model.safetensors\"]\n    \n    Scheduler[\"Qwen Scheduler\"] --\u003e|\"get_qwen_image_scheduler_choices()\"| SchedFilter[\"Check for 'scheduler/' subdirectory\"]\n    SchedFilter --\u003e SchedPath[\"model_path/scheduler/scheduler_config.json\"]\n```\n\n**Organization Differences:**\n- DiT weights follow standard flat structure\n- VAE and scheduler use subdirectories (`vae/`, `scheduler/`) unlike other models\n- VAE path must be: `os.path.join(model_path, \"vae\", \"diffusion_pytorch_model.safetensors\")`\n- Scheduler path must be: `os.path.join(model_path, \"scheduler\")`\n\n**Sources:** [app/gradio_demo.py:743-825]()\n\n### LoRA Organization\n\nLoRA (Low-Rank Adaptation) weights are stored as standalone `.safetensors` files that can be dynamically loaded:\n\n```mermaid\ngraph TB\n    LoRAFiles[\"LoRA Weight Files\"] --\u003e NamingPattern[\"lora_*.safetensors\u003cbr/\u003eOR {style_name}_lora.safetensors\"]\n    NamingPattern --\u003e Usage[\"Usage in config:\u003cbr/\u003elora_configs = [\u003cbr/\u003e  {path: 'lora_style_anime_v1.safetensors',\u003cbr/\u003e   strength: 0.8,\u003cbr/\u003e   name: 'main_lora'}\u003cbr/\u003e]\"]\n    Usage --\u003e Runtime[\"Runtime: switch_lora(lora_path, strength)\u003cbr/\u003eDynamic loading without model reload\"]\n```\n\n**LoRA File Characteristics:**\n- **Location**: Same flat directory as other components\n- **Format**: Standard `.safetensors` format containing delta weights\n- **Configuration**: Specified via `lora_configs` list in runner config or `--lora_path` CLI argument\n- **Dynamic Loading**: Can be swapped at runtime using `runner.switch_lora()` method without full model reload\n- **MoE Models**: Wan2.2 supports separate LoRA for high/low noise models via `lora_configs` with `name: \"high_noise_model\"` or `name: \"low_noise_model\"`\n\n**Sources:** [app/gradio_demo.py:232-234](), [app/gradio_demo.py:276-321]()\n\n---\n\n## Hardware-Aware Model Filtering\n\nThe discovery system automatically filters models based on detected hardware capabilities.\n\n### FP8 Capability Detection\n\n```mermaid\ngraph TD\n    DetectStart[\"is_fp8_supported_gpu()\"] --\u003e CheckCUDA{\"torch.cuda.is_available()?\"}\n    CheckCUDA --\u003e|No| ReturnFalse[\"Return False\"]\n    CheckCUDA --\u003e|Yes| GetCapability[\"capability = torch.cuda.get_device_capability(0)\"]\n    GetCapability --\u003e CheckArch{\"Compute capability\u003cbr/\u003ein supported list?\"}\n    CheckArch --\u003e|\"(8,9), (9,0)\": Ada/Hopper| ReturnTrue[\"Return True\"]\n    CheckArch --\u003e|\"(12,0)\": Blackwell| ReturnTrue\n    CheckArch --\u003e|Others| ReturnFalse\n```\n\n**Sources:** [app/gradio_demo.py:267-299](), [lightx2v/common/ops/attn/sage_attn.py:8]()\n\nThis detection influences model sorting and filtering:\n- **FP8 Supported:** Priority order is `fp8_split \u003e int8_split \u003e fp8 \u003e int8 \u003e others`\n- **FP8 Not Supported:** Models containing \"fp8\" in their names are excluded entirely, priority becomes `int8_split \u003e int8 \u003e others`\n\n### Prioritization Logic\n\nThe `sort_model_choices()` function implements a priority-based sorting algorithm:\n\n```python\n# Simplified logic from app/gradio_demo.py:267-299\ndef get_priority(name):\n    if fp8_supported:\n        if \"fp8\" in name and \"_split\" in name: return 0\n        elif \"int8\" in name and \"_split\" in name: return 1\n        elif \"fp8\" in name: return 2\n        elif \"int8\" in name: return 3\n        else: return 4\n    else:\n        if \"int8\" in name and \"_split\" in name: return 0\n        elif \"int8\" in name: return 1\n        else: return 2\n```\n\nThis ensures that users see the most appropriate models for their hardware first in dropdown lists, with split versions prioritized for memory-constrained systems.\n\n**Sources:** [app/gradio_demo.py:267-299]()"])</script><script>self.__next_f.push([1,"1f:T49eb,"])</script><script>self.__next_f.push([1,"# Quick Start Tutorial\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [README_zh.md](README_zh.md)\n- [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json](configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json)\n- [docs/EN/source/getting_started/quickstart.md](docs/EN/source/getting_started/quickstart.md)\n- [docs/ZH_CN/source/getting_started/quickstart.md](docs/ZH_CN/source/getting_started/quickstart.md)\n- [lightx2v/__init__.py](lightx2v/__init__.py)\n- [lightx2v/common/ops/attn/utils/ring_comm.py](lightx2v/common/ops/attn/utils/ring_comm.py)\n- [lightx2v/common/ops/mm/triton_kernels.py](lightx2v/common/ops/mm/triton_kernels.py)\n- [lightx2v_platform/README.md](lightx2v_platform/README.md)\n- [lightx2v_platform/README_zh.md](lightx2v_platform/README_zh.md)\n- [pyproject.toml](pyproject.toml)\n- [scripts/hunyuan_video_15/README.md](scripts/hunyuan_video_15/README.md)\n- [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh](scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh)\n\n\u003c/details\u003e\n\n\n\nThis page provides minimal working examples for running your first inference tasks using LightX2V. It assumes you have completed installation ([2.1](#2.1)) and downloaded models ([2.2](#2.2)). For deployment-specific configurations (low-resource, low-latency, production servers), see the deployment guides ([3](#3)). For detailed explanations of optimization techniques (quantization, offloading, caching), see the method tutorials ([6](#6)).\n\n## Overview\n\nLightX2V supports multiple generation tasks through a unified `LightX2VPipeline` interface. This tutorial demonstrates the basic inference workflow for three common tasks:\n\n- **Text-to-Video (T2V)**: Generate video from text prompts\n- **Image-to-Video (I2V)**: Animate static images with text guidance\n- **Text-to-Image (T2I)**: Generate images from text descriptions\n\nEach example follows the same three-stage pattern: **initialize pipeline**  **configure generation**  **execute inference**.\n\n## Basic Inference Workflow\n\nThe following diagram shows how high-level user concepts map to concrete code entities in LightX2V:\n\n```mermaid\ngraph TB\n    subgraph \"User Concepts\"\n        Task[\"Task Type\u003cbr/\u003e(T2V, I2V, T2I, A2V)\"]\n        Model[\"Model Architecture\u003cbr/\u003e(Wan2.1, HunyuanVideo, Qwen-Image)\"]\n        Optimize[\"Optimization Strategy\u003cbr/\u003e(offload, quantization, attention)\"]\n        Input[\"Input Data\u003cbr/\u003e(prompts, images, audio)\"]\n    end\n    \n    subgraph \"Code Entry Points\"\n        Pipeline[\"LightX2VPipeline\u003cbr/\u003elightx2v/pipeline.py\"]\n        CLI[\"CLI Module\u003cbr/\u003elightx2v.infer\"]\n    end\n    \n    subgraph \"Configuration Methods\"\n        Init[\"__init__()\u003cbr/\u003emodel_path, model_cls, task\"]\n        EnableOff[\"enable_offload()\u003cbr/\u003ecpu_offload, offload_granularity\"]\n        CreateGen[\"create_generator()\u003cbr/\u003eattn_mode, infer_steps, height, width\"]\n        Generate[\"generate()\u003cbr/\u003eprompt, seed, image_path, save_result_path\"]\n    end\n    \n    subgraph \"Execution Layer\"\n        Runner[\"Model Runner\u003cbr/\u003eWanRunner, QwenImageRunner, etc.\"]\n        Registry[\"RUNNER_REGISTER\u003cbr/\u003elightx2v/models/*/runner.py\"]\n        ConfigJSON[\"Config JSON\u003cbr/\u003econfigs/*.json\"]\n    end\n    \n    Task --\u003e Init\n    Model --\u003e Init\n    Optimize --\u003e EnableOff\n    Optimize --\u003e CreateGen\n    Input --\u003e Generate\n    \n    Pipeline --\u003e Init\n    Pipeline --\u003e EnableOff\n    Pipeline --\u003e CreateGen\n    Pipeline --\u003e Generate\n    \n    CLI --\u003e ConfigJSON\n    CLI --\u003e Runner\n    \n    Init --\u003e Registry\n    CreateGen --\u003e Runner\n    Generate --\u003e Runner\n    \n    style Pipeline fill:#f9f9f9,stroke:#333,stroke-width:2px\n    style CLI fill:#f9f9f9,stroke:#333,stroke-width:2px\n```\n\n**Sources**: [README.md:135-193](), [lightx2v/__init__.py:1-18](), [docs/EN/source/getting_started/quickstart.md:291-356]()\n\n## Text-to-Video (T2V) Example\n\n### Python API Usage\n\nThe following example generates a 480p video using the Wan2.1 model:\n\n```python\nfrom lightx2v import LightX2VPipeline\n\n# Stage 1: Initialize pipeline\npipe = LightX2VPipeline(\n    model_path=\"/path/to/Wan2.1-T2V-14B\",\n    model_cls=\"wan2.1\",\n    task=\"t2v\",\n)\n\n# Stage 2: Configure generation parameters\npipe.create_generator(\n    attn_mode=\"sage_attn2\",\n    infer_steps=50,\n    height=480,\n    width=832,\n    num_frames=81,\n    guidance_scale=5.0,\n    sample_shift=5.0,\n)\n\n# Stage 3: Execute inference\nseed = 42\nprompt = \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\nnegative_prompt = \"\"\nsave_result_path = \"/path/to/output.mp4\"\n\npipe.generate(\n    seed=seed,\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    save_result_path=save_result_path,\n)\n```\n\n**Parameter Explanation**:\n\n| Parameter | Purpose | Common Values | Notes |\n|-----------|---------|---------------|-------|\n| `model_cls` | Model architecture | `\"wan2.1\"`, `\"wan2.2_moe\"`, `\"hunyuan_video_1.5\"` | Must match downloaded model |\n| `task` | Generation task | `\"t2v\"`, `\"i2v\"`, `\"t2i\"`, `\"s2v\"` | Determines input requirements |\n| `attn_mode` | Attention operator | `\"sage_attn2\"`, `\"flash_attn2\"`, `\"flash_attn3\"` | See [2.1](#2.1) for installation |\n| `infer_steps` | Diffusion steps | 4-50 | 4 for distilled models, 40-50 for base |\n| `height`  `width` | Output resolution | 480832, 7201280 | Must align with model training |\n| `num_frames` | Video length | 81, 121 | Determines duration (81 frames  3.4s at 24fps) |\n| `guidance_scale` | CFG strength | 5.0-7.0 | Higher = stronger prompt adherence |\n| `sample_shift` | Noise schedule | 5.0-9.0 | Model-specific, see config files |\n\n**Sources**: [README.md:135-193](), [docs/EN/source/getting_started/quickstart.md:325-355]()\n\n### CLI Usage\n\nThe same T2V generation can be executed via command line:\n\n```bash\n# Linux\nbash scripts/wan/run_wan_t2v.sh\n\n# Windows\nscripts\\win\\run_wan_t2v.bat\n```\n\nThe script invokes the `lightx2v.infer` module with arguments:\n\n```bash\npython -m lightx2v.infer \\\n    --model_cls wan2.1 \\\n    --task t2v \\\n    --model_path /path/to/Wan2.1-T2V-14B \\\n    --config_json configs/wan/wan_t2v.json \\\n    --prompt \"Two anthropomorphic cats in comfy boxing gear...\" \\\n    --negative_prompt \"...\" \\\n    --save_result_path /path/to/output.mp4 \\\n    --seed 42\n```\n\n**Configuration Hierarchy**: Parameters are merged from three sources (priority order):\n1. **Command-line arguments** (highest priority)\n2. **Config JSON file** specified by `--config_json`\n3. **Default values** in runner implementation\n\n**Sources**: [docs/EN/source/getting_started/quickstart.md:309-321](), [README.md:135-193]()\n\n## Image-to-Video (I2V) Example\n\n### Python API Usage\n\nI2V adds image conditioning to the T2V workflow:\n\n```python\nfrom lightx2v import LightX2VPipeline\n\n# Initialize for I2V task\npipe = LightX2VPipeline(\n    model_path=\"/path/to/Wan2.2-I2V-A14B\",\n    model_cls=\"wan2.2_moe\",\n    task=\"i2v\",\n)\n\n# Enable CPU offloading for memory efficiency\npipe.enable_offload(\n    cpu_offload=True,\n    offload_granularity=\"block\",\n    text_encoder_offload=True,\n    image_encoder_offload=False,\n    vae_offload=False,\n)\n\n# Configure generation\npipe.create_generator(\n    attn_mode=\"sage_attn2\",\n    infer_steps=40,\n    height=480,\n    width=832,\n    num_frames=81,\n    guidance_scale=[3.5, 3.5],  # Wan2.2 uses list for dual CFG\n    sample_shift=5.0,\n)\n\n# Generate with image input\npipe.generate(\n    seed=42,\n    image_path=\"/path/to/input_image.jpg\",\n    prompt=\"Summer beach vacation style, a white cat wearing sunglasses...\",\n    negative_prompt=\"...\",\n    save_result_path=\"/path/to/output.mp4\",\n)\n```\n\n**Key Differences from T2V**:\n\n| Aspect | T2V | I2V |\n|--------|-----|-----|\n| Input requirement | Text prompt only | Text prompt + image |\n| `image_path` argument | Not used | Required in `generate()` |\n| First frame | Generated from noise | Conditioned on input image |\n| Temporal consistency | Natural motion | Anchored to initial frame |\n\n**Sources**: [README.md:136-193](), [docs/EN/source/getting_started/quickstart.md:325-355]()\n\n### Memory Optimization with Offloading\n\nThe `enable_offload()` call demonstrates LightX2V's memory management capabilities:\n\n```python\npipe.enable_offload(\n    cpu_offload=True,              # Enable CPU offloading\n    offload_granularity=\"block\",   # Offload at block level\n    text_encoder_offload=True,     # Offload text encoder after use\n    image_encoder_offload=False,   # Keep image encoder on GPU\n    vae_offload=False,             # Keep VAE on GPU\n)\n```\n\n**Offloading Strategies**:\n\n| Strategy | VRAM Reduction | Speed Impact | Use Case |\n|----------|----------------|--------------|----------|\n| No offload | 0% | 0% (baseline) | High-VRAM GPUs (A100, H100) |\n| `offload_granularity=\"model\"` | ~60% | ~2x slower | 8GB VRAM + lazy loading |\n| `offload_granularity=\"block\"` | ~40% | ~30% slower | 24GB VRAM (RTX 4090) |\n| `offload_granularity=\"phase\"` | ~50% | ~50% slower | 16GB VRAM |\n\nFor detailed offloading configuration, see [6.3](#6.3).\n\n**Sources**: [README.md:157-165](), [docs/EN/source/getting_started/quickstart.md:1-369]()\n\n## Text-to-Image (T2I) Example\n\n### Python API Usage\n\nT2I uses different model architectures (Qwen-Image, Z-Image) optimized for single-frame generation:\n\n```python\nfrom lightx2v import LightX2VPipeline\n\n# Initialize for T2I with Qwen-Image\npipe = LightX2VPipeline(\n    model_path=\"/path/to/Qwen-Image-2512\",\n    model_cls=\"qwen_image\",\n    task=\"t2i\",\n)\n\n# Configure image generation\npipe.create_generator(\n    attn_mode=\"sage_attn2\",\n    infer_steps=30,\n    aspect_ratio=\"1:1\",  # T2I uses aspect ratio instead of height/width\n    guidance_scale=4.0,\n)\n\n# Generate image\npipe.generate(\n    seed=42,\n    prompt=\"A serene Japanese garden with cherry blossoms in full bloom\",\n    negative_prompt=\"blurry, low quality, distorted\",\n    save_result_path=\"/path/to/output.png\",\n)\n```\n\n**T2I-Specific Parameters**:\n\n| Parameter | Purpose | Common Values |\n|-----------|---------|---------------|\n| `aspect_ratio` | Output dimensions | `\"1:1\"`, `\"16:9\"`, `\"9:16\"`, `\"4:3\"` |\n| `infer_steps` | Diffusion steps | 8-30 (T2I typically faster than T2V) |\n| `guidance_scale` | CFG strength | 3.0-5.0 (lower than T2V) |\n\n**Supported T2I Models**:\n\n- **Qwen-Image**: Multi-resolution, supports aspect ratios\n- **Z-Image-Turbo**: Fast inference, Ada architecture optimized\n- **Qwen-Image-Edit**: Image-to-image editing (I2I task)\n\n**Sources**: [README.md:206-212](), [scripts/hunyuan_video_15/README.md:1-104]()\n\n## Pipeline Execution Flow\n\nThis diagram shows the complete data flow from user invocation to generated output:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Pipeline as LightX2VPipeline\n    participant Registry as RUNNER_REGISTER\n    participant Runner as DefaultRunner\n    participant Components as Encoders/VAE/Scheduler\n    participant Model as Transformer Model\n    \n    User-\u003e\u003ePipeline: LightX2VPipeline(model_path, model_cls, task)\n    Pipeline-\u003e\u003ePipeline: Load config, validate paths\n    Pipeline-\u003e\u003eRegistry: Get runner class by model_cls\n    Registry--\u003e\u003ePipeline: Return runner constructor\n    \n    User-\u003e\u003ePipeline: enable_offload(cpu_offload=True, ...)\n    Pipeline-\u003e\u003ePipeline: Set offload config\n    \n    User-\u003e\u003ePipeline: create_generator(attn_mode, infer_steps, ...)\n    Pipeline-\u003e\u003eRunner: Initialize runner with config\n    Runner-\u003e\u003eComponents: Load text_encoder, vae, scheduler\n    Runner-\u003e\u003eModel: Load transformer weights\n    Runner--\u003e\u003ePipeline: Runner ready\n    \n    User-\u003e\u003ePipeline: generate(prompt, image_path, seed, ...)\n    Pipeline-\u003e\u003eRunner: run_pipeline(input_info)\n    \n    Runner-\u003e\u003eComponents: Encode text prompt  text_embeddings\n    \n    alt I2V/T2I with image input\n        Runner-\u003e\u003eComponents: Encode image  image_embeddings\n    end\n    \n    Runner-\u003e\u003eModel: Initialize latents from seed\n    \n    loop Diffusion steps (4-50 iterations)\n        Runner-\u003e\u003eModel: Forward pass with latents + embeddings\n        Model--\u003e\u003eRunner: Predicted noise\n        Runner-\u003e\u003eComponents: Scheduler.step(noise)  updated latents\n    end\n    \n    Runner-\u003e\u003eComponents: VAE.decode(final_latents)  pixels\n    Runner-\u003e\u003eRunner: Save video/image to disk\n    Runner--\u003e\u003ePipeline: save_result_path\n    Pipeline--\u003e\u003eUser: Generated file path\n```\n\n**Sources**: [README.md:135-193](), [docs/EN/source/getting_started/quickstart.md:291-356]()\n\n## Common Pitfalls and Solutions\n\n### Model Path Configuration\n\n**Problem**: `FileNotFoundError` when loading models\n\n**Solution**: Verify model directory structure matches expected layout:\n\n```\n/path/to/Wan2.1-T2V-14B/\n DIT/                    # Transformer weights\n    dit.safetensors\n T5-Encoder/             # Text encoder\n    model.safetensors\n CLIP-Encoder/           # Image encoder (I2V only)\n    model.safetensors\n VAE/                    # Video autoencoder\n    vae.safetensors\n config.json             # Model config\n```\n\nThe `model_path` should point to the root directory containing these subdirectories.\n\n**Sources**: [docs/EN/source/getting_started/quickstart.md:293-300]()\n\n### Attention Operator Mismatch\n\n**Problem**: `ImportError` or runtime errors with attention operators\n\n**Solution**: Verify operator installation matches `attn_mode` parameter:\n\n| `attn_mode` | Required Installation | Verification |\n|-------------|----------------------|--------------|\n| `\"sage_attn2\"` | SageAttention 2.x | `import sageattention` |\n| `\"flash_attn2\"` | Flash Attention 2.x | `import flash_attn` |\n| `\"flash_attn3\"` | Flash Attention 3.x (Hopper only) | Requires H100/H800 |\n| `\"vanilla\"` | None (PyTorch native) | Always available |\n\n**Sources**: [docs/EN/source/getting_started/quickstart.md:86-103]()\n\n### Resolution Constraints\n\n**Problem**: Generated video has artifacts or fails to generate\n\n**Solution**: Use resolutions aligned with model training:\n\n| Model | Supported Resolutions | Constraint |\n|-------|----------------------|------------|\n| Wan2.1/2.2 | 480832, 7201280 | Height/width must be multiples of 32 |\n| HunyuanVideo | 544960, 7201280 | Height/width must be multiples of 16 |\n| Qwen-Image | Aspect ratios | Uses `aspect_ratio` instead of explicit dimensions |\n\n**Sources**: [README.md:135-193](), [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:32-72]()\n\n### Memory Exhaustion (OOM)\n\n**Problem**: `torch.cuda.OutOfMemoryError` during generation\n\n**Solution**: Enable offloading or reduce resolution:\n\n```python\n# Option 1: Enable block-level offloading\npipe.enable_offload(\n    cpu_offload=True,\n    offload_granularity=\"block\",\n    text_encoder_offload=True,\n    vae_offload=False,\n)\n\n# Option 2: Reduce resolution\npipe.create_generator(\n    height=480,  # Instead of 720\n    width=832,   # Instead of 1280\n    num_frames=81,\n    # ... other params\n)\n\n# Option 3: Use quantized models\n# Download from https://huggingface.co/lightx2v/\n# and specify in config_json\n```\n\nFor 8GB VRAM deployment, see [6.3](#6.3) for advanced offloading strategies.\n\n**Sources**: [README.md:157-165](), [README.md:258-260]()\n\n## Alternative Configuration via JSON\n\nInstead of calling `create_generator()` with parameters, you can load pre-configured settings from JSON files:\n\n```python\npipe = LightX2VPipeline(\n    model_path=\"/path/to/Wan2.1-T2V-14B\",\n    model_cls=\"wan2.1\",\n    task=\"t2v\",\n)\n\n# Load all generation settings from JSON\npipe.create_generator(\n    config_json=\"configs/wan/wan_t2v.json\"\n)\n\n# JSON overrides individual parameters\npipe.generate(\n    prompt=\"...\",\n    seed=42,\n    save_result_path=\"/path/to/output.mp4\",\n)\n```\n\n**JSON Configuration Structure** (example from [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:1-74]()):\n\n```json\n{\n    \"infer_steps\": 4,\n    \"target_fps\": 16,\n    \"video_duration\": 360,\n    \"self_attn_1_type\": \"nbhd_attn\",\n    \"sample_guide_scale\": 1.0,\n    \"sample_shift\": 5,\n    \"enable_cfg\": false,\n    \"cpu_offload\": false,\n    \"dit_quantized\": true,\n    \"dit_quant_scheme\": \"fp8-sgl\",\n    \"compile\": true\n}\n```\n\nConfiguration files for all supported models are available in the `configs/` directory.\n\n**Sources**: [docs/EN/source/getting_started/quickstart.md:302-306](), [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:1-74]()\n\n## Distributed Multi-GPU Execution\n\nFor multi-GPU inference, use `torchrun` with the CLI interface:\n\n```bash\n# 8-GPU inference with CFG parallelism\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\ntorchrun --nproc-per-node 8 -m lightx2v.infer \\\n    --model_cls seko_talk \\\n    --task s2v \\\n    --model_path /path/to/model \\\n    --config_json configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json \\\n    --prompt \"...\" \\\n    --image_path /path/to/input.png \\\n    --audio_path /path/to/input.mp3 \\\n    --save_result_path /path/to/output.mp4\n```\n\nThe `parallel` section in config JSON controls parallelism strategy:\n\n```json\n{\n    \"parallel\": {\n        \"seq_p_size\": 8,\n        \"seq_p_attn_type\": \"ulysses\"\n    }\n}\n```\n\n| Parameter | Purpose | Values |\n|-----------|---------|--------|\n| `seq_p_size` | Number of GPUs for sequence parallelism | 1-8 |\n| `seq_p_attn_type` | Parallelism algorithm | `\"ulysses\"`, `\"ring\"` |\n\nFor detailed parallel inference setup, see [6.5](#6.5).\n\n**Sources**: [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh:1-22](), [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:19-22]()\n\n## Expected Output and Validation\n\nAfter successful generation, verify the output:\n\n```bash\n# Check file existence\nls -lh /path/to/output.mp4\n\n# Verify video properties\nffprobe -v error -show_entries \\\n    stream=width,height,r_frame_rate,duration \\\n    -of default=noprint_wrappers=1 \\\n    /path/to/output.mp4\n```\n\n**Expected Properties**:\n\n| Property | Wan2.1 480p T2V | HunyuanVideo 720p |\n|----------|----------------|-------------------|\n| Resolution | 480832 | 7201280 |\n| Frame count | 81 | 121 |\n| Duration | ~3.4s @ 24fps | ~5.0s @ 24fps |\n| File size | ~2-5MB | ~5-10MB |\n| Codec | H.264 | H.264 |\n\n**Sources**: [README.md:135-193](), [scripts/hunyuan_video_15/README.md:97-100]()\n\n## Next Steps\n\nAfter completing these basic examples, explore:\n\n- **Optimization techniques** ([6](#6)): Quantization, caching, advanced offloading\n- **Production deployment** ([3.4](#3.4)): HTTP server, dynamic LoRA switching\n- **Advanced tasks** ([5](#5)): Audio-to-video, autoregressive models, distilled inference\n- **Custom configurations** ([8.2](#8.2)): Fine-tuning generation parameters\n\nFor more examples including quantization, FP8 inference, and distilled models, see the `examples/` directory in the repository.\n\n**Sources**: [README.md:199-200](), [docs/EN/source/getting_started/quickstart.md:357-369]()"])</script><script>self.__next_f.push([1,"20:T4061,"])</script><script>self.__next_f.push([1,"# User Interfaces\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [README_zh.md](README_zh.md)\n- [lightx2v_platform/README.md](lightx2v_platform/README.md)\n- [lightx2v_platform/README_zh.md](lightx2v_platform/README_zh.md)\n- [scripts/hunyuan_video_15/README.md](scripts/hunyuan_video_15/README.md)\n\n\u003c/details\u003e\n\n\n\nThis page provides an overview of the five different interfaces available for interacting with LightX2V. Each interface is designed for different use cases, from interactive experimentation to production deployment. For detailed documentation of each interface's configuration and advanced features, see the respective sub-pages [3.1](#3.1) through [3.5](#3.5).\n\n## Interface Architecture Overview\n\nLightX2V provides multiple entry points that all converge on the core `LightX2VPipeline` class, which handles model orchestration and execution. The interfaces range from simple Python API calls to complex distributed inference services.\n\n```mermaid\ngraph TB\n    subgraph \"User Entry Points\"\n        PythonAPI[\"Python API\u003cbr/\u003eLightX2VPipeline\u003cbr/\u003eDirect programmatic access\"]\n        CLI[\"Command Line Interface\u003cbr/\u003elightx2v.infer\u003cbr/\u003eBash script execution\"]\n        GradioUI[\"Gradio Web Interface\u003cbr/\u003eapp/gradio_demo.py\u003cbr/\u003eInteractive web UI\"]\n        ComfyUI[\"ComfyUI Integration\u003cbr/\u003eNode-based workflow\"]\n        HTTPServer[\"HTTP Server\u003cbr/\u003eTorchrunInferenceWorker\u003cbr/\u003eMulti-worker distributed\"]\n    end\n    \n    subgraph \"Core Framework\"\n        Pipeline[\"LightX2VPipeline\u003cbr/\u003elightx2v/pipeline/lightx2v_pipeline.py\"]\n        Config[\"Configuration System\u003cbr/\u003eset_config, InputInfo\"]\n        Registry[\"RUNNER_REGISTER\u003cbr/\u003eModel routing\"]\n    end\n    \n    subgraph \"Execution Layer\"\n        Runners[\"Model Runners\u003cbr/\u003eWanAudioRunner, QwenImageRunner, etc.\"]\n        Models[\"Model Implementations\u003cbr/\u003eTransformers, Encoders, VAEs\"]\n    end\n    \n    PythonAPI --\u003e Pipeline\n    CLI --\u003e Pipeline\n    GradioUI --\u003e Pipeline\n    ComfyUI --\u003e Pipeline\n    HTTPServer --\u003e Pipeline\n    \n    Pipeline --\u003e Config\n    Pipeline --\u003e Registry\n    Registry --\u003e Runners\n    Runners --\u003e Models\n```\n\n**Diagram 1: LightX2V Interface Architecture** - All interfaces converge on the `LightX2VPipeline` class, which manages configuration and routes to appropriate model runners.\n\nSources: [README.md:238-252](), Diagram 1 from high-level overview\n\n## Interface Comparison Matrix\n\n| Interface | Use Case | Complexity | Deployment | Configuration | Best For |\n|-----------|----------|------------|------------|---------------|----------|\n| **Python API** | Direct code integration | Low | Embedded in apps | Programmatic | Developers, automated pipelines |\n| **CLI** | Script automation | Low | Shell scripts | JSON + args | Batch processing, CI/CD |\n| **Gradio** | Interactive testing | Medium | Web server | UI + config files | Prototyping, demos, non-technical users |\n| **HTTP Server** | Production services | High | Distributed | JSON config + API | High-throughput, multi-tenant |\n| **ComfyUI** | Visual workflows | Medium | Plugin integration | Node-based | Complex pipelines, visual composition |\n\n## Interface Details\n\n### Python API (`LightX2VPipeline`)\n\nThe Python API provides direct programmatic access to LightX2V through the `LightX2VPipeline` class. This is the foundational interface that all other interfaces ultimately wrap.\n\n**Key characteristics:**\n- Direct instantiation of `LightX2VPipeline` class\n- Method-based configuration: `enable_offload()`, `enable_quantization()`, `create_generator()`\n- Synchronous generation: `pipe.generate()` blocks until completion\n- Full control over all optimization features\n\n**Typical usage pattern:**\n```python\nfrom lightx2v import LightX2VPipeline\n\npipe = LightX2VPipeline(\n    model_path=\"/path/to/model\",\n    model_cls=\"wan2.2_moe\",\n    task=\"i2v\"\n)\npipe.enable_offload(cpu_offload=True)\npipe.create_generator(infer_steps=40, height=480)\npipe.generate(seed=42, prompt=\"...\", save_result_path=\"output.mp4\")\n```\n\nThe pipeline instance maintains state across multiple generations, allowing efficient reuse of loaded models and compiled kernels.\n\n**For detailed API reference, see [3.2](#3.2).**\n\nSources: [README.md:138-197]()\n\n### Command Line Interface\n\nThe CLI provides shell-based access through the `lightx2v.infer` command, accepting configuration via JSON files and command-line arguments. This interface is designed for batch processing and script automation.\n\n**Key characteristics:**\n- Entry point: `python -m lightx2v.infer` or `lightx2v.infer` \n- Configuration hierarchy: JSON config file + command-line overrides\n- Single-shot execution: process exits after generation\n- Shell script friendly for automation\n\n**Typical usage pattern:**\n```bash\npython -m lightx2v.infer \\\n    --config_json configs/wan22/wan_moe_i2v.json \\\n    --model_path /path/to/model \\\n    --prompt \"video description\" \\\n    --save_result_path output.mp4\n```\n\nThe CLI interface is particularly useful for batch processing multiple prompts, integration with build systems, and remote execution over SSH.\n\n**For detailed CLI documentation, see [3.3](#3.3).**\n\nSources: [README.md:122-133]()\n\n### Gradio Web Interface\n\nThe Gradio interface provides an interactive web-based UI for real-time parameter tuning and visual feedback. It includes model selection, parameter sliders, and preview capabilities.\n\n**Key characteristics:**\n- Entry point: `app/gradio_demo.py`\n- Configuration: `run_gradio.sh` or `run_gradio.bat` scripts\n- Interactive parameter adjustment with immediate visual feedback\n- Built-in model auto-configuration\n- Support for multiple models in single interface\n\n**Typical workflow:**\n1. Start Gradio server: `bash run_gradio.sh`\n2. Open web browser to displayed URL\n3. Select model and task type\n4. Adjust parameters via sliders and dropdowns\n5. Click generate and preview results\n\nThe Gradio interface is recommended for first-time users, prototyping, and demonstrations to non-technical stakeholders. It automatically applies sensible defaults and provides guidance for parameter ranges.\n\n**For Gradio setup and configuration, see [3.1](#3.1).**\n\nSources: [README.md:238-243]()\n\n### HTTP Server and Production Deployment\n\nThe HTTP server provides a distributed inference service using `TorchrunInferenceWorker` for multi-GPU, multi-worker deployments. This interface is designed for production environments requiring high throughput and horizontal scaling.\n\n**Key characteristics:**\n- Entry point: `TorchrunInferenceWorker` class\n- Distributed architecture: multiple worker processes, optional load balancer\n- Task queue system: asynchronous request handling\n- Dynamic LoRA loading: serve multiple LoRA variants from directory\n- RESTful API: JSON request/response format\n\n**Architecture pattern:**\n```mermaid\ngraph LR\n    subgraph \"Client Layer\"\n        Client1[\"Client 1\"]\n        Client2[\"Client 2\"]\n        Client3[\"Client N\"]\n    end\n    \n    subgraph \"Load Balancer (Optional)\"\n        LB[\"Load Balancer\u003cbr/\u003eRequest distribution\"]\n    end\n    \n    subgraph \"Worker Pool\"\n        Worker1[\"Worker 1\u003cbr/\u003eTorchrunInferenceWorker\u003cbr/\u003eGPU 0-1\"]\n        Worker2[\"Worker 2\u003cbr/\u003eTorchrunInferenceWorker\u003cbr/\u003eGPU 2-3\"]\n        WorkerN[\"Worker N\u003cbr/\u003eTorchrunInferenceWorker\u003cbr/\u003eGPU N-M\"]\n    end\n    \n    subgraph \"Shared Storage\"\n        Models[\"Model Repository\u003cbr/\u003eBase models + LoRAs\"]\n        Queue[\"Task Queue\u003cbr/\u003eRequest buffering\"]\n    end\n    \n    Client1 --\u003e LB\n    Client2 --\u003e LB\n    Client3 --\u003e LB\n    \n    LB --\u003e Worker1\n    LB --\u003e Worker2\n    LB --\u003e WorkerN\n    \n    Worker1 --\u003e Queue\n    Worker2 --\u003e Queue\n    WorkerN --\u003e Queue\n    \n    Worker1 --\u003e Models\n    Worker2 --\u003e Models\n    WorkerN --\u003e Models\n```\n\n**Diagram 2: Production HTTP Server Architecture** - Distributed workers share model storage and coordinate through task queue. Each worker can utilize multiple GPUs with parallelism strategies.\n\nThe HTTP server supports dynamic LoRA switching, allowing clients to specify LoRA variants in requests without reloading base models. This is particularly useful for serving personalized models in multi-tenant environments.\n\n**For production deployment guide, see [3.4](#3.4).**\n\nSources: [README.md:286-291](), Diagram 1 from high-level overview\n\n### ComfyUI Integration\n\nComfyUI integration provides node-based workflow composition, allowing visual construction of complex generation pipelines. LightX2V models appear as nodes that can be connected with other ComfyUI nodes for preprocessing, postprocessing, and composition.\n\n**Key characteristics:**\n- Entry point: ComfyUI custom node plugin\n- Visual workflow editor: drag-and-drop node composition\n- Pipeline composition: chain multiple models and operations\n- Reusable workflows: save and share node graphs\n- Integration with ComfyUI ecosystem\n\n**Typical workflow structure:**\n```mermaid\ngraph LR\n    Input[\"Input Node\u003cbr/\u003eText/Image\"]\n    LightX2VNode[\"LightX2V Node\u003cbr/\u003eModel: wan2.2_moe\u003cbr/\u003eTask: i2v\"]\n    Postprocess[\"Postprocess Node\u003cbr/\u003eFrame interpolation\"]\n    Output[\"Output Node\u003cbr/\u003eSave video\"]\n    \n    Input --\u003e LightX2VNode\n    LightX2VNode --\u003e Postprocess\n    Postprocess --\u003e Output\n```\n\n**Diagram 3: Example ComfyUI Workflow** - Nodes represent processing stages that can be visually connected and configured through the UI.\n\nComfyUI is recommended for users who need to compose complex pipelines (e.g., prompt preprocessing  generation  upscaling  frame interpolation) and want visual feedback during workflow construction.\n\n**For ComfyUI installation and node documentation, see [3.5](#3.5).**\n\nSources: [README.md:244-245]()\n\n## Choosing the Right Interface\n\nThe following decision tree helps select the appropriate interface based on your requirements:\n\n```mermaid\ngraph TB\n    Start{{\"What is your use case?\"}}\n    \n    Start --\u003e|\"Development/Integration\"| DevPath{{\"Programming language?\"}}\n    Start --\u003e|\"Interactive exploration\"| InteractivePath{{\"Technical background?\"}}\n    Start --\u003e|\"Production deployment\"| ProdPath{{\"Throughput requirements?\"}}\n    Start --\u003e|\"Complex pipelines\"| ComplexPath{{\"Visual vs code?\"}}\n    \n    DevPath --\u003e|\"Python\"| PythonAPI[\"Python API\u003cbr/\u003eDirect LightX2VPipeline\"]\n    DevPath --\u003e|\"Shell/Bash\"| CLI[\"Command Line Interface\u003cbr/\u003elightx2v.infer\"]\n    \n    InteractivePath --\u003e|\"Technical\"| Gradio[\"Gradio Interface\u003cbr/\u003eWeb UI with full control\"]\n    InteractivePath --\u003e|\"Non-technical\"| WindowsGradio[\"Windows One-Click\u003cbr/\u003ePackaged Gradio\"]\n    \n    ProdPath --\u003e|\"High (multi-worker)\"| HTTPServer[\"HTTP Server\u003cbr/\u003eTorchrunInferenceWorker\"]\n    ProdPath --\u003e|\"Low (single instance)\"| PythonAPI2[\"Python API\u003cbr/\u003eEmbedded in service\"]\n    \n    ComplexPath --\u003e|\"Visual\"| ComfyUI[\"ComfyUI Integration\u003cbr/\u003eNode-based workflow\"]\n    ComplexPath --\u003e|\"Code\"| PythonAPI3[\"Python API\u003cbr/\u003eOrchestration code\"]\n    \n    style PythonAPI fill:#e1f5ff\n    style CLI fill:#e1f5ff\n    style Gradio fill:#e1f5ff\n    style HTTPServer fill:#e1f5ff\n    style ComfyUI fill:#e1f5ff\n```\n\n**Diagram 4: Interface Selection Decision Tree**\n\n### Interface Selection Guidelines\n\n**Use Python API when:**\n- Integrating LightX2V into existing Python applications\n- Building automated generation pipelines\n- Need fine-grained control over optimization features\n- Developing custom workflows programmatically\n\n**Use CLI when:**\n- Running batch generation jobs\n- Scripting with Bash/shell\n- Integrating with CI/CD pipelines\n- Remote execution over SSH\n- Quick testing without writing Python code\n\n**Use Gradio when:**\n- First-time users learning LightX2V\n- Prototyping and exploring model capabilities\n- Demonstrating to non-technical users\n- Need immediate visual feedback during experimentation\n- Prefer UI-based parameter adjustment\n\n**Use HTTP Server when:**\n- Production deployment with multiple clients\n- High-throughput requirements (multiple simultaneous generations)\n- Multi-tenant environments requiring isolation\n- Need horizontal scaling across multiple machines\n- Serving multiple LoRA variants dynamically\n\n**Use ComfyUI when:**\n- Building complex multi-stage pipelines\n- Need visual workflow construction\n- Want to combine LightX2V with other ComfyUI nodes\n- Prefer node-based composition over code\n- Need to share reusable workflow templates\n\n## Common Configuration Patterns\n\nAll interfaces share common configuration patterns, though syntax varies by interface:\n\n| Configuration Aspect | Python API | CLI | Gradio | HTTP Server | ComfyUI |\n|---------------------|------------|-----|--------|-------------|---------|\n| Model selection | `model_cls` parameter | `--model_cls` argument | Dropdown menu | JSON `model_cls` field | Node property |\n| Task type | `task` parameter | `--task` argument | Radio buttons | JSON `task` field | Node type |\n| Optimization flags | `enable_offload()` method | JSON config file | Checkboxes | JSON config | Node settings |\n| Generation params | `create_generator()` method | `--infer_steps` etc. | Sliders | JSON request body | Node inputs |\n| Output path | `save_result_path` in `generate()` | `--save_result_path` | Save dialog | Response download | Output node |\n\n### Configuration Hierarchy\n\nAll interfaces follow the same configuration precedence (highest to lowest):\n\n1. **Runtime parameters**: Direct arguments to `generate()` call\n2. **Generator configuration**: Settings from `create_generator()`\n3. **JSON config file**: Configuration loaded from file (if specified)\n4. **Model defaults**: Auto-configuration based on `model_cls` and `task`\n5. **System defaults**: Framework-level defaults\n\nThis hierarchy allows mixing configuration methods. For example, you can load base settings from a JSON file and override specific parameters at runtime.\n\n## Interface Implementation Flow\n\nThe following diagram shows how each interface internally calls the core pipeline:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Interface as \"Interface Layer\u003cbr/\u003e(API/CLI/Gradio/Server/ComfyUI)\"\n    participant Pipeline as \"LightX2VPipeline\"\n    participant Config as \"Configuration System\"\n    participant Runner as \"Model Runner\"\n    \n    User-\u003e\u003eInterface: Request generation\u003cbr/\u003e(prompt, params)\n    Interface-\u003e\u003eConfig: Load/merge configuration\n    Config-\u003e\u003ePipeline: Initialize with merged config\n    Pipeline-\u003e\u003ePipeline: create_generator()\u003cbr/\u003e(infer_steps, resolution, etc.)\n    Interface-\u003e\u003ePipeline: generate(seed, prompt, ...)\n    Pipeline-\u003e\u003eConfig: Prepare InputInfo\n    Pipeline-\u003e\u003eRunner: Execute pipeline\u003cbr/\u003e(encode  denoise  decode)\n    Runner--\u003e\u003ePipeline: Generated output\n    Pipeline--\u003e\u003eInterface: Result (video/image)\n    Interface--\u003e\u003eUser: Return/save output\n```\n\n**Diagram 5: Interface Execution Sequence** - All interfaces follow the same internal flow: configuration  pipeline initialization  generation  result handling.\n\nSources: Diagram 4 from high-level overview\n\n## Environment-Specific Considerations\n\n### Docker Deployment\n\nAll interfaces work within Docker containers. The recommended Docker images include all necessary dependencies:\n\n- `lightx2v/lightx2v:25111101-cu128` - CUDA 12.8 with all operators\n- `lightx2v/lightx2v:25111101-cu124` - CUDA 12.4 variant\n\n**Port mapping for web interfaces:**\n- Gradio: Default port 7860 (`-p 7860:7860`)\n- HTTP Server: Custom port (configure in server script)\n\n### Windows One-Click Installer\n\nThe Windows One-Click installer provides a packaged Gradio interface with automatic environment setup. This is the recommended starting point for Windows users without technical background.\n\n**Features:**\n- Automatic dependency installation\n- Pre-configured model paths\n- Intelligent parameter optimization\n- Desktop shortcut creation\n\nSources: [README.md:246-252]()\n\n## Next Steps\n\nFor detailed documentation of each interface:\n\n- **[3.1 Gradio Web Interface](#3.1)**: Setup, configuration, UI features, and model selection\n- **[3.2 Python API (LightX2VPipeline)](#3.2)**: Complete API reference and method documentation\n- **[3.3 Command Line Interface](#3.3)**: Arguments, config files, and script patterns\n- **[3.4 HTTP Server and Production Deployment](#3.4)**: Distributed architecture, task queue, and API endpoints\n- **[3.5 ComfyUI Integration](#3.5)**: Node installation, workflow creation, and integration patterns\n\nFor information about the underlying runner system that all interfaces utilize, see [4.1 Runner System and Registry Pattern](#4.1).\n\nSources: Table of contents structure"])</script><script>self.__next_f.push([1,"21:T7261,"])</script><script>self.__next_f.push([1,"# Gradio Web Interface\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [app/README.md](app/README.md)\n- [app/gradio_demo.py](app/gradio_demo.py)\n- [app/run_gradio.sh](app/run_gradio.sh)\n- [app/run_gradio_win.bat](app/run_gradio_win.bat)\n- [docs/EN/source/deploy_guides/deploy_gradio.md](docs/EN/source/deploy_guides/deploy_gradio.md)\n- [docs/EN/source/deploy_guides/deploy_local_windows.md](docs/EN/source/deploy_guides/deploy_local_windows.md)\n- [docs/ZH_CN/source/deploy_guides/deploy_gradio.md](docs/ZH_CN/source/deploy_guides/deploy_gradio.md)\n- [docs/ZH_CN/source/deploy_guides/deploy_local_windows.md](docs/ZH_CN/source/deploy_guides/deploy_local_windows.md)\n- [lightx2v/common/offload/manager.py](lightx2v/common/offload/manager.py)\n- [scripts/win/run_wan_i2v.bat](scripts/win/run_wan_i2v.bat)\n- [scripts/win/run_wan_t2v.bat](scripts/win/run_wan_t2v.bat)\n\n\u003c/details\u003e\n\n\n\nThe Gradio Web Interface provides a browser-based UI for model management, configuration, and video generation inference. It handles model discovery from remote repositories (HuggingFace/ModelScope), local filesystem scanning, automatic quantization detection, and hardware-aware configuration. The interface is implemented in [app/gradio_demo.py]() (English) and [app/gradio_demo_zh.py]() (Chinese), with identical functionality but different language labels.\n\nFor programmatic API usage, see [Python API (LightX2VPipeline)](#3.2). For command-line batch processing, see [Command Line Interface](#3.3).\n\n---\n\n## System Architecture\n\nThe Gradio interface consists of five major subsystems: model discovery, local scanning, download orchestration, UI generation, and inference execution.\n\n```mermaid\ngraph TB\n    subgraph \"Startup Phase\"\n        LoadCache[\"load_hf_models_cache()\u003cbr/\u003eLines 130-203\"]\n        HFApi[\"HuggingFace API\u003cbr/\u003elist_repo_files()\"]\n        MSApi[\"ModelScope API\u003cbr/\u003eget_model_files()\"]\n        Cache[\"HF_MODELS_CACHE\u003cbr/\u003eDict[repo_id, models]\u003cbr/\u003eLines 89-98\"]\n        \n        LoadCache --\u003e MSApi\n        LoadCache --\u003e HFApi\n        MSApi --\u003e Cache\n        HFApi --\u003e Cache\n    end\n    \n    subgraph \"Model Selection Phase\"\n        LocalScan[\"scan_model_path_contents()\u003cbr/\u003eLines 101-127\"]\n        GetChoices[\"get_dit_choices()\u003cbr/\u003eget_t5_model_choices()\u003cbr/\u003eetc.\"]\n        Sort[\"sort_model_choices()\u003cbr/\u003eLines 267-299\"]\n        Format[\"format_model_choice()\u003cbr/\u003eLines 242-254\"]\n        CheckExists[\"check_model_exists()\u003cbr/\u003eLines 221-239\"]\n        \n        Cache --\u003e GetChoices\n        LocalScan --\u003e GetChoices\n        GetChoices --\u003e Sort\n        Sort --\u003e Format\n        CheckExists --\u003e Format\n    end\n    \n    subgraph \"Download Phase\"\n        DownloadHF[\"download_model_from_hf()\u003cbr/\u003eLines 886-953\"]\n        DownloadMS[\"download_model_from_ms()\u003cbr/\u003eLines 956-1053\"]\n        SnapshotDL[\"hf_snapshot_download()\u003cbr/\u003ems_snapshot_download()\"]\n        \n        Format --\u003e DownloadHF\n        Format --\u003e DownloadMS\n        DownloadHF --\u003e SnapshotDL\n        DownloadMS --\u003e SnapshotDL\n    end\n    \n    subgraph \"Configuration Phase\"\n        DetectQuant[\"detect_quant_scheme()\u003cbr/\u003eLines 866-883\"]\n        DetectHW[\"is_fp8_supported_gpu()\u003cbr/\u003edetect_available_ops()\"]\n        OptimalConfig[\"get_optimal_config()\u003cbr/\u003eLines 1166-1374\"]\n        \n        Format --\u003e DetectQuant\n        DetectHW --\u003e Sort\n        DetectHW --\u003e OptimalConfig\n    end\n    \n    subgraph \"Inference Phase\"\n        Pipeline[\"LightX2VPipeline\"]\n        EnableOps[\"enable_cpu_offload()\u003cbr/\u003eenable_vae_tiling()\u003cbr/\u003eetc.\"]\n        Generate[\"generate()\"]\n        \n        OptimalConfig --\u003e EnableOps\n        EnableOps --\u003e Pipeline\n        Pipeline --\u003e Generate\n    end\n```\n\n**Architecture Flow**: On startup, `load_hf_models_cache()` queries remote repositories and populates `HF_MODELS_CACHE`. When users select model types, `get_*_choices()` functions combine cached remote models with locally scanned files, sort by hardware capability, and format with status indicators. Downloads use `hf_snapshot_download()`/`ms_snapshot_download()` APIs. Configuration auto-detects quantization schemes from filenames and GPU capabilities. The pipeline is configured via `enable_*` methods before calling `generate()`.\n\n**Sources**: [app/gradio_demo.py:130-203](), [app/gradio_demo.py:89-98](), [app/gradio_demo.py:267-299](), [app/gradio_demo.py:886-953]()\n\n---\n\n## Model Discovery and Caching System\n\n### Repository Configuration\n\nThe interface queries multiple HuggingFace and ModelScope repositories on startup:\n\n| Repository ID | Content Type | Purpose |\n|--------------|--------------|---------|\n| `lightx2v/wan2.1-Distill-Models` | Distilled diffusion models | 4-step Wan 2.1 models |\n| `lightx2v/wan2.1-Official-Models` | Official diffusion models | Standard Wan 2.1 models |\n| `lightx2v/wan2.2-Distill-Models` | Distilled MoE models | 4-step Wan 2.2 models |\n| `lightx2v/wan2.2-Official-Models` | Official MoE models | Standard Wan 2.2 models |\n| `lightx2v/Encoders` | Text/image encoders | T5, CLIP, tokenizers |\n| `lightx2v/Autoencoders` | VAE models | Wan2.1_VAE, LightVAE, TAE |\n| `lightx2v/Qwen-Image-Edit-2511-Lightning` | Qwen diffusion models | Distilled Qwen models |\n| `Qwen/Qwen-Image-Edit-2511` | Qwen VAE/scheduler | vae, scheduler directories |\n\n**Sources**: [app/gradio_demo.py:89-98]()\n\n### Cache Loading Logic\n\n```mermaid\nsequenceDiagram\n    participant Startup\n    participant LoadCache as load_hf_models_cache()\n    participant MSApi as ModelScope API\n    participant HFApi as HuggingFace API\n    participant ProcessFiles as process_files()\n    participant Cache as HF_MODELS_CACHE\n    \n    Startup-\u003e\u003eLoadCache: On app launch\n    \n    loop For each repo_id\n        LoadCache-\u003e\u003eMSApi: api.get_model_files(repo_id)\n        alt ModelScope success\n            MSApi--\u003e\u003eProcessFiles: file list\n            ProcessFiles--\u003e\u003eCache: model names\n        else ModelScope fails\n            LoadCache-\u003e\u003eHFApi: list_repo_files(repo_id, timeout=30s)\n            alt HuggingFace success\n                HFApi--\u003e\u003eProcessFiles: file list\n                ProcessFiles--\u003e\u003eCache: model names\n            else Timeout\n                LoadCache--\u003e\u003eCache: empty list\n            end\n        end\n    end\n    \n    Note over ProcessFiles: Filters:\u003cbr/\u003e- Exclude \"comfyui\"\u003cbr/\u003e- Extract top-level dirs\u003cbr/\u003e- Keep .safetensors files\u003cbr/\u003e- Keep _split directories\n```\n\n**Fallback Strategy**: ModelScope is attempted first (no timeout). If it fails, HuggingFace API is tried with a 30-second timeout ([app/gradio_demo.py:166]()). This prioritizes Chinese users while maintaining global accessibility.\n\n**File Processing Rules** ([app/gradio_demo.py:134-163]()):\n- Top-level `.safetensors` files are preserved\n- Directories containing `.safetensors` files become choices\n- Directories with `_split` suffix are included (block-level storage)\n- Files/directories with \"comfyui\" are excluded\n- For `Qwen/Qwen-Image-Edit-2511`, `vae` and `scheduler` directories are explicitly preserved\n\n**Sources**: [app/gradio_demo.py:130-203](), [app/gradio_demo.py:134-163]()\n\n---\n\n## Local Model Scanning\n\n### Filesystem Scanner\n\nThe `scan_model_path_contents()` function examines the local model directory and categorizes contents:\n\n```python\n{\n    \"dirs\": [\"wan2.1_i2v_720p_int8_split\", \"t5\", \"google\", ...],\n    \"files\": [\"wan2.1_i2v_720p.safetensors\", \"config.json\", ...],\n    \"safetensors_dirs\": [\"wan2.1_i2v_720p_int8_split\", ...],  # dirs containing *.safetensors\n    \"pth_files\": [\"legacy_model.pth\", ...]\n}\n```\n\n**Implementation** ([app/gradio_demo.py:101-127]()):\n- Iterates `os.listdir(model_path)`\n- Identifies directories vs files\n- Uses `glob.glob(os.path.join(dir, \"*.safetensors\"))` to detect safetensors directories\n- Sorts all lists alphabetically\n\n**Sources**: [app/gradio_demo.py:101-127]()\n\n### Model Existence Checking\n\nThe `check_model_exists()` function validates whether a model is locally available:\n\n**Check Logic** ([app/gradio_demo.py:221-239]()):\n1. Direct path check: `os.path.exists(os.path.join(model_path, model_name))`\n2. For `.safetensors` files: also checks for `{base_name}_split` directory\n3. Returns `True` if either exists\n\n**Example**:\n- Model name: `wan2.1_i2v_720p_int8.safetensors`\n- Checks both:\n  - `models/wan2.1_i2v_720p_int8.safetensors` (single file)\n  - `models/wan2.1_i2v_720p_int8_split/` (block directory)\n\n**Sources**: [app/gradio_demo.py:221-239]()\n\n### Status Formatting\n\nThe `format_model_choice()` function adds visual indicators:\n\n```python\n# Examples:\n\" wan2.1_i2v_720p_int8.safetensors\"  # Locally available\n\" wan2.1_i2v_720p_fp8.safetensors\"   # Not downloaded\n```\n\n**Sources**: [app/gradio_demo.py:242-254]()\n\n---\n\n## Hardware-Aware Model Filtering\n\n### GPU Capability Detection\n\nThe system filters models based on detected GPU capabilities:\n\n```mermaid\ngraph LR\n    Detect[\"is_fp8_supported_gpu()\"]\n    FilterModels[\"sort_model_choices()\u003cbr/\u003eget_*_choices()\"]\n    \n    Detect --\u003e|\"fp8_supported=True\"| Priority1[\"Priority 0: fp8+split\u003cbr/\u003ePriority 1: int8+split\u003cbr/\u003ePriority 2: fp8\u003cbr/\u003ePriority 3: int8\u003cbr/\u003ePriority 4: others\"]\n    Detect --\u003e|\"fp8_supported=False\"| Priority2[\"Priority 0: int8+split\u003cbr/\u003ePriority 1: int8\u003cbr/\u003ePriority 2: others\u003cbr/\u003e(fp8 models excluded)\"]\n    \n    Priority1 --\u003e FilterModels\n    Priority2 --\u003e FilterModels\n```\n\n**Sorting Algorithm** ([app/gradio_demo.py:267-299]()):\n\nFor **FP8-capable GPUs** (Ada, Hopper architectures):\n1. `fp8` + `_split`: Highest memory efficiency\n2. `int8` + `_split`: Good memory efficiency\n3. `fp8`: Standard quantization\n4. `int8`: Standard quantization\n5. Others: Non-quantized models\n\nFor **Non-FP8 GPUs**:\n1. `int8` + `_split`: Highest memory efficiency\n2. `int8`: Standard quantization\n3. Others: Non-quantized models (FP8 models excluded)\n\n**Filtering in `get_dit_choices()`** ([app/gradio_demo.py:302-388]()):\n```python\ndef is_valid(name):\n    name_lower = name.lower()\n    if \"comfyui\" in name_lower:\n        return False\n    if not fp8_supported and \"fp8\" in name_lower:\n        return False  # Exclude FP8 models on unsupported hardware\n    return not any(kw in name_lower for kw in excluded_keywords)\n```\n\n**Sources**: [app/gradio_demo.py:267-299](), [app/gradio_demo.py:302-388]()\n\n---\n\n## Dynamic Dropdown Generation\n\n### Diffusion Model Choices\n\nThe `get_dit_choices()` function generates dropdown options for diffusion models:\n\n**Parameters**:\n- `model_path`: Local directory path\n- `model_type`: `\"wan2.1\"` or `\"wan2.2\"`\n- `task_type`: `\"i2v\"` or `\"t2v\"` (optional filter)\n- `is_distill`: `True`/`False`/`None` (distilled vs official vs both)\n\n**Repository Selection Logic** ([app/gradio_demo.py:315-340]()):\n```python\nif model_type == \"wan2.1\":\n    if is_distill is True:\n        repo_id = \"lightx2v/wan2.1-Distill-Models\"\n    elif is_distill is False:\n        repo_id = \"lightx2v/wan2.1-Official-Models\"\n    else:  # Get both\n        hf_models = get_hf_models(\"lightx2v/wan2.1-Distill-Models\", prefix_filter=\"wan2.1\") + \\\n                    get_hf_models(\"lightx2v/wan2.1-Official-Models\", prefix_filter=\"wan2.1\")\n```\n\n**Filtering Rules**:\n- Must contain `model_type` (e.g., \"wan2.1\")\n- Excludes: `[\"vae\", \"tae\", \"clip\", \"t5\", \"high_noise\", \"low_noise\", \"comfyui\"]`\n- If `task_type` specified, must contain \"i2v\" or \"t2v\"\n- FP8 models excluded on non-FP8 GPUs\n\n**Sources**: [app/gradio_demo.py:302-388]()\n\n### MoE Model Choices (Wan 2.2)\n\nFor Wan 2.2 MoE models, separate functions handle high-noise and low-noise components:\n\n**`get_high_noise_choices()`** ([app/gradio_demo.py:391-457]()):\n- Filters for `\"high_noise\"` or `\"high-noise\"` keywords\n- Used for Wan 2.2 MoE dual-model architecture\n\n**`get_low_noise_choices()`** ([app/gradio_demo.py:460-526]()):\n- Filters for `\"low_noise\"` or `\"low-noise\"` keywords\n- Paired with high-noise model at `boundary_step_index`\n\n**Sources**: [app/gradio_demo.py:391-457](), [app/gradio_demo.py:460-526]()\n\n### Encoder Choices\n\n| Function | Repository | Keyword Filter | Tokenizer Dir |\n|----------|-----------|----------------|---------------|\n| `get_t5_model_choices()` | `lightx2v/Encoders` | `\"t5\"` | `\"google\"` |\n| `get_clip_model_choices()` | `lightx2v/Encoders` | `\"clip\"` | `\"xlm-roberta-large\"` |\n| `get_t5_tokenizer_choices()` | `lightx2v/Encoders` | - | Returns `[\"google\"]` |\n| `get_clip_tokenizer_choices()` | `lightx2v/Encoders` | - | Returns `[\"xlm-roberta-large\"]` |\n\n**Encoder Filtering** ([app/gradio_demo.py:538-546]()):\n```python\ndef is_valid_hf(name):\n    name_lower = name.lower()\n    if \"comfyui\" in name_lower or name == \"google\":  # Exclude tokenizer dir\n        return False\n    if not fp8_supported and \"fp8\" in name_lower:\n        return False\n    return (\"t5\" in name_lower) and name.endswith(\".safetensors\")\n```\n\n**Sources**: [app/gradio_demo.py:529-574](), [app/gradio_demo.py:594-639]()\n\n### VAE Choices\n\n**`get_vae_encoder_choices()`** ([app/gradio_demo.py:659-677]()):\n- Returns fixed value: `[\"Wan2.1_VAE.safetensors\"]` (for I2V tasks)\n\n**`get_vae_decoder_choices()`** ([app/gradio_demo.py:680-740]()):\n- Filters for `[\"vae\", \"tae\", \"lightvae\", \"lighttae\"]` keywords\n- Additional filter: must contain `\"2_1\"` or `\"2.1\"` (Wan 2.1 architecture)\n- Includes both `.safetensors` files and `_split` directories\n\n**Sources**: [app/gradio_demo.py:659-677](), [app/gradio_demo.py:680-740]()\n\n### Qwen Image Model Choices\n\n**`get_qwen_image_dit_choices()`** ([app/gradio_demo.py:743-783]()):\n- Repository: `lightx2v/Qwen-Image-Edit-2511-Lightning`\n- Must contain: `\"qwen_image_edit_2511\"`\n- Must end with: `\"lightning.safetensors\"` or `\"_split\"` or contain `\"lightning_split\"`\n\n**`get_qwen_image_vae_choices()`** ([app/gradio_demo.py:786-811]()):\n- Repository: `Qwen/Qwen-Image-Edit-2511`\n- Returns: `[\"vae\"]` directory\n\n**`get_qwen_image_scheduler_choices()`** ([app/gradio_demo.py:814-839]()):\n- Repository: `Qwen/Qwen-Image-Edit-2511`\n- Returns: `[\"scheduler\"]` directory\n\n**Sources**: [app/gradio_demo.py:743-839]()\n\n---\n\n## Download Orchestration\n\n### Download Workflow\n\n```mermaid\nsequenceDiagram\n    participant UI as Gradio UI\n    participant DownloadBtn as Download Button\n    participant DetectSource as Model Source Detection\n    participant DownloadHF as download_model_from_hf()\n    participant DownloadMS as download_model_from_ms()\n    participant Progress as gr.Progress()\n    participant Snapshot as snapshot_download()\n    participant Filesystem as Local Filesystem\n    \n    UI-\u003e\u003eDownloadBtn: User clicks download\n    DownloadBtn-\u003e\u003eDetectSource: Extract model name\n    DetectSource-\u003e\u003eDetectSource: Determine repo_id from model type\n    \n    alt Download from HuggingFace\n        DetectSource-\u003e\u003eDownloadHF: repo_id, model_name\n        DownloadHF-\u003e\u003eProgress: 0% - Starting download\n        DownloadHF-\u003e\u003eDownloadHF: Check if file or directory\n        \n        alt Is directory\n            DownloadHF-\u003e\u003eSnapshot: hf_snapshot_download(allow_patterns=[model_name/**])\n            Snapshot--\u003e\u003eFilesystem: Download to temp location\n            DownloadHF-\u003e\u003eFilesystem: Move to target path\n        else Is file (.safetensors/.pth)\n            DownloadHF-\u003e\u003eSnapshot: hf_hub_download(filename=model_name)\n            Snapshot--\u003e\u003eFilesystem: Download directly\n        end\n        \n        DownloadHF-\u003e\u003eProgress: 100% - Complete\n        DownloadHF--\u003e\u003eUI: Update dropdown with  status\n    else Download from ModelScope\n        DetectSource-\u003e\u003eDownloadMS: repo_id, model_name\n        DownloadMS-\u003e\u003eProgress: 0% - Starting download\n        DownloadMS-\u003e\u003eSnapshot: ms_snapshot_download(allow_patterns=[model_name/**])\n        Snapshot--\u003e\u003eFilesystem: Download to temp location\n        DownloadMS-\u003e\u003eFilesystem: Move and cleanup\n        DownloadMS-\u003e\u003eProgress: 100% - Complete\n        DownloadMS--\u003e\u003eUI: Update dropdown with  status\n    end\n```\n\n**Sources**: [app/gradio_demo.py:886-1053]()\n\n### HuggingFace Download Implementation\n\n**File vs Directory Detection** ([app/gradio_demo.py:899]()):\n```python\nis_directory = not (model_name.endswith(\".safetensors\") or model_name.endswith(\".pth\"))\n```\n\n**Directory Download** ([app/gradio_demo.py:901-931]()):\n```python\nhf_snapshot_download(\n    repo_id=repo_id,\n    allow_patterns=[f\"{model_name}/**\"],  # Download entire subdirectory\n    local_dir=model_path,\n    local_dir_use_symlinks=False,\n    repo_type=\"model\",\n)\n# Post-processing: move from repo_id subdirectory to model_path/model_name\n```\n\n**File Download** ([app/gradio_demo.py:933-950]()):\n```python\nhf_hub_download(\n    repo_id=repo_id,\n    filename=model_name,\n    local_dir=model_path,\n    local_dir_use_symlinks=False,\n    repo_type=\"model\",\n)\n```\n\n**Sources**: [app/gradio_demo.py:886-953]()\n\n### ModelScope Download Implementation\n\n**Implementation Differences** ([app/gradio_demo.py:956-1053]()):\n- Uses temporary directory: `temp_dir = os.path.join(model_path, f\".temp_{model_name}\")`\n- Always uses `ms_snapshot_download()` (no separate file API)\n- Manual file search: walks downloaded directory to find target file\n- Cleanup: removes temporary directory after moving files\n\n**Sources**: [app/gradio_demo.py:956-1053]()\n\n---\n\n## Automatic Quantization Detection\n\n### Detection Logic\n\nThe `detect_quant_scheme()` function infers quantization from model filenames:\n\n```python\ndef detect_quant_scheme(model_name):\n    \"\"\"\n    Returns:\n        \"int8\" if \"int8\" in model_name\n        \"fp8\" if \"fp8\" in model_name and GPU supports FP8\n        None otherwise (no quantization or not supported)\n    \"\"\"\n    if not model_name:\n        return None\n    name_lower = model_name.lower()\n    if \"int8\" in name_lower:\n        return \"int8\"\n    elif \"fp8\" in name_lower:\n        if is_fp8_supported_gpu():\n            return \"fp8\"\n        else:\n            return None  # FP8 model but GPU doesn't support it\n    return None\n```\n\n**Usage in UI** ([app/gradio_demo.py:866-883]()):\n- Called when user selects a diffusion model\n- Automatically populates the quantization dropdown\n- Falls back to `None` (no quantization) if unsupported\n\n**Sources**: [app/gradio_demo.py:866-883]()\n\n---\n\n## Inference Configuration and Execution\n\n### Configuration Generation\n\nThe `get_optimal_config()` function auto-configures optimization settings based on detected hardware:\n\n**GPU Memory Tiers** ([app/gradio_demo.py:1166-1374]()):\n\n| GPU VRAM | Offloading | VAE Tiling | Lazy Load | Block Granularity |\n|----------|-----------|------------|-----------|-------------------|\n| \u003c 12 GB | CPU offload | Enabled | Enabled | Block-level |\n| 12-16 GB | CPU offload | Enabled | Disabled | Phase-level |\n| 16-24 GB | Minimal | Conditional | Disabled | Phase-level |\n| \u003e 24 GB | None | Disabled | Disabled | N/A |\n\n**CPU Memory Tiers**:\n\n| System RAM | Lazy Load | Module Unloading | Block Granularity |\n|-----------|-----------|------------------|-------------------|\n| \u003c 16 GB | Enabled | Aggressive | Block-level |\n| 16-32 GB | Enabled | Moderate | Phase-level |\n| \u003e 32 GB | Disabled | Minimal | Phase-level |\n\n**Operator Selection** ([app/gradio_demo.py:1196-1247]()):\n```python\n# Priority order for attention operators:\navailable_attns = []\nif \"sageattention\" in installed_ops:\n    available_attns.append(\"sageattn\")\nif \"flash_attn\" in installed_ops:\n    available_attns.append(\"flash_attn\")\nif \"natten\" in installed_ops:\n    available_attns.append(\"neighborhood\")\n# Default to \"vanilla\" if none installed\n\n# Priority order for quantized matrix multiplication:\navailable_qmms = []\nif \"vllm\" in installed_ops:\n    available_qmms.append(\"vllm\")\nif \"sglang\" in installed_ops:\n    available_qmms.append(\"sgl\")\nif \"q8_kernels\" in installed_ops:\n    available_qmms.append(\"q8\")\nif \"torchao\" in installed_ops:\n    available_qmms.append(\"torchao\")\n# Default to \"triton\"\n```\n\n**Sources**: [app/gradio_demo.py:1166-1374](), [app/gradio_demo.py:1196-1247]()\n\n### Pipeline Initialization and Generation\n\n```mermaid\nsequenceDiagram\n    participant UI as Gradio Generate Button\n    participant GenFunc as generate_video()\n    participant Config as get_optimal_config()\n    participant Pipeline as LightX2VPipeline\n    participant SetInputInfo as set_input_info()\n    participant Generate as pipeline.generate()\n    \n    UI-\u003e\u003eGenFunc: User clicks generate\n    GenFunc-\u003e\u003eConfig: GPU/CPU memory, installed ops\n    Config--\u003e\u003eGenFunc: optimal_config dict\n    \n    GenFunc-\u003e\u003ePipeline: LightX2VPipeline(model_path, model_cls)\n    GenFunc-\u003e\u003ePipeline: enable_cpu_offload(granularity, lazy_load)\n    GenFunc-\u003e\u003ePipeline: enable_vae_tiling(tile_size)\n    GenFunc-\u003e\u003ePipeline: enable_feature_caching(strategy)\n    GenFunc-\u003e\u003ePipeline: create_generator(seed)\n    \n    GenFunc-\u003e\u003eSetInputInfo: Task type, prompt, image, etc.\n    SetInputInfo--\u003e\u003eGenFunc: input_info dataclass\n    \n    GenFunc-\u003e\u003eGenerate: pipeline.generate(input_info)\n    Generate--\u003e\u003eGenFunc: output video path\n    GenFunc--\u003e\u003eUI: Display video + metrics\n```\n\n**Configuration Application** ([app/gradio_demo.py:1415-1485]()):\n```python\npipe = LightX2VPipeline(model_path=model_path, model_cls=model_cls)\n\n# Apply optimizations from optimal_config\nif optimal_config[\"cpu_offload\"]:\n    pipe.enable_cpu_offload(\n        granularity=optimal_config[\"offload_granularity\"],\n        lazy_load=optimal_config[\"lazy_load\"]\n    )\n\nif optimal_config[\"vae_tiling\"]:\n    pipe.enable_vae_tiling(tile_sample_min_size=optimal_config[\"vae_tile_size\"])\n\nif optimal_config[\"feature_caching\"]:\n    pipe.enable_feature_caching(cache_type=optimal_config[\"cache_strategy\"])\n\npipe.enable_quantization(\n    quant_precision=optimal_config[\"quant_precision\"],\n    qmm_type=optimal_config[\"qmm_type\"]\n)\n\npipe.enable_attention(attn_mode=optimal_config[\"attn_mode\"])\n\ngenerator = pipe.create_generator(seed=seed)\n```\n\n**Input Info Construction** ([app/gradio_demo.py:1487-1520]()):\n```python\nfrom lightx2v.utils.input_info import set_input_info\n\ninput_info = set_input_info(\n    task_type=task_type,  # \"t2v\" or \"i2v\"\n    model_type=model_cls,  # \"wan2.1\" or \"wan2.2\"\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    image_path=image_path if task_type == \"i2v\" else None,\n    height=height,\n    width=width,\n    num_frames=num_frames,\n    fps=fps,\n    num_inference_steps=num_inference_steps,\n    guidance_scale=guidance_scale,\n    flow_shift=flow_shift,\n)\n```\n\n**Generation Call** ([app/gradio_demo.py:1522-1530]()):\n```python\noutput = pipe.generate(\n    input_info=input_info,\n    generator=generator,\n    save_result_path=save_path,\n    enable_profiling=True  # Logs timing metrics\n)\n```\n\n**Sources**: [app/gradio_demo.py:1415-1530](), [app/gradio_demo.py:1487-1520]()\n\n---\n\n## Deployment Methods\n\n### Linux Startup Script\n\nThe [app/run_gradio.sh]() script provides parameter parsing and environment setup:\n\n**Configuration Variables** ([app/run_gradio.sh:16-31]()):\n```bash\nlightx2v_path=/data/video_gen/lightx2v_debug/LightX2V\nmodel_path=/models/\nserver_name=\"0.0.0.0\"\nserver_port=8033\noutput_dir=\"./outputs\"\ngpu_id=0\n```\n\n**Environment Variables** ([app/run_gradio.sh:34-38]()):\n```bash\nexport CUDA_VISIBLE_DEVICES=$gpu_id\nexport CUDA_LAUNCH_BLOCKING=1\nexport PYTHONPATH=${lightx2v_path}:$PYTHONPATH\nexport PROFILING_DEBUG_LEVEL=2\nexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n```\n\n**Parameter Parsing** ([app/run_gradio.sh:44-98]()):\n- `--lang zh|en`: Interface language selection\n- `--port PORT`: Server port override\n- `--gpu GPU_ID`: GPU device selection\n- `--model_path PATH`: Model directory override\n- `--output_dir DIR`: Output save directory\n\n**Launch Command** ([app/run_gradio.sh:171-175]()):\n```bash\npython $demo_file \\\n    --model_path \"$model_path\" \\\n    --server_name \"$server_name\" \\\n    --server_port \"$server_port\" \\\n    --output_dir \"$output_dir\"\n```\n\n**Sources**: [app/run_gradio.sh:1-182]()\n\n### Windows Startup Script\n\nThe [app/run_gradio_win.bat]() provides equivalent functionality for Windows:\n\n**Key Differences**:\n- Uses `set` instead of `export` for environment variables\n- Uses `^` for line continuation instead of `\\`\n- Uses `chcp 65001` for UTF-8 encoding support\n- Uses `wmic` for system resource display\n\n**Sources**: [app/run_gradio_win.bat:1-197]()\n\n### One-Click Windows Installer\n\nFor Windows users, a pre-packaged distribution is available with automatic environment setup:\n\n**Directory Structure** (from [docs/EN/source/deploy_guides/deploy_local_windows.md:35-45]()):\n```\n env/                        # LightX2V environment directory\n LightX2V/                   # LightX2V project directory\n start_lightx2v.bat          # One-click startup script\n lightx2v_config.txt         # Configuration file\n outputs/                    # Generated video save directory\n models/                     # Model storage directory\n```\n\n**Configuration File Format** ([docs/EN/source/deploy_guides/deploy_local_windows.md:59-71]()):\n```ini\n# Interface language (zh: Chinese, en: English)\nlang=en\n\n# Server port\nport=8032\n\n# GPU device ID (0, 1, 2...)\ngpu=0\n\n# Model path\nmodel_path=models/\n```\n\n**Auto-Detection Features**:\n- GPU capability detection (FP8 support)\n- Memory profiling (GPU VRAM + system RAM)\n- Optimal configuration selection based on hardware\n- Automatic model dropdown sorting by device capability\n\n**Sources**: [docs/EN/source/deploy_guides/deploy_local_windows.md:35-86]()\n\n---\n\n## Code-to-Concept Mapping\n\nThe following diagram maps high-level UI concepts to their implementing functions:\n\n```mermaid\ngraph TB\n    subgraph \"UI Dropdowns\"\n        DiffusionDD[\"Diffusion Model Dropdown\"]\n        HighNoiseDD[\"High Noise Model Dropdown\"]\n        LowNoiseDD[\"Low Noise Model Dropdown\"]\n        T5DD[\"T5 Encoder Dropdown\"]\n        CLIPDD[\"CLIP Encoder Dropdown\"]\n        VAEDD[\"VAE Decoder Dropdown\"]\n    end\n    \n    subgraph \"Choice Generation Functions\"\n        GetDit[\"get_dit_choices()\u003cbr/\u003eLines 302-388\"]\n        GetHighNoise[\"get_high_noise_choices()\u003cbr/\u003eLines 391-457\"]\n        GetLowNoise[\"get_low_noise_choices()\u003cbr/\u003eLines 460-526\"]\n        GetT5[\"get_t5_model_choices()\u003cbr/\u003eLines 529-574\"]\n        GetCLIP[\"get_clip_model_choices()\u003cbr/\u003eLines 594-639\"]\n        GetVAE[\"get_vae_decoder_choices()\u003cbr/\u003eLines 680-740\"]\n    end\n    \n    subgraph \"Model Discovery\"\n        Cache[\"HF_MODELS_CACHE\u003cbr/\u003eLines 89-98\"]\n        LoadCache[\"load_hf_models_cache()\u003cbr/\u003eLines 130-203\"]\n        ScanLocal[\"scan_model_path_contents()\u003cbr/\u003eLines 101-127\"]\n    end\n    \n    subgraph \"Filtering and Sorting\"\n        Sort[\"sort_model_choices()\u003cbr/\u003eLines 267-299\"]\n        DetectFP8[\"is_fp8_supported_gpu()\"]\n        DetectQuant[\"detect_quant_scheme()\u003cbr/\u003eLines 866-883\"]\n    end\n    \n    subgraph \"Download Management\"\n        CheckExists[\"check_model_exists()\u003cbr/\u003eLines 221-239\"]\n        FormatChoice[\"format_model_choice()\u003cbr/\u003eLines 242-254\"]\n        DownloadHF[\"download_model_from_hf()\u003cbr/\u003eLines 886-953\"]\n        DownloadMS[\"download_model_from_ms()\u003cbr/\u003eLines 956-1053\"]\n    end\n    \n    DiffusionDD --\u003e GetDit\n    HighNoiseDD --\u003e GetHighNoise\n    LowNoiseDD --\u003e GetLowNoise\n    T5DD --\u003e GetT5\n    CLIPDD --\u003e GetCLIP\n    VAEDD --\u003e GetVAE\n    \n    GetDit --\u003e Cache\n    GetDit --\u003e ScanLocal\n    GetHighNoise --\u003e Cache\n    GetLowNoise --\u003e Cache\n    GetT5 --\u003e Cache\n    GetCLIP --\u003e Cache\n    GetVAE --\u003e Cache\n    \n    Cache --\u003e LoadCache\n    \n    GetDit --\u003e Sort\n    GetHighNoise --\u003e Sort\n    GetLowNoise --\u003e Sort\n    Sort --\u003e DetectFP8\n    \n    GetDit --\u003e FormatChoice\n    FormatChoice --\u003e CheckExists\n    \n    FormatChoice --\u003e DownloadHF\n    FormatChoice --\u003e DownloadMS\n    \n    GetDit --\u003e DetectQuant\n```\n\n**Sources**: [app/gradio_demo.py:89-1053]()\n\n---\n\n## Model Configuration Table Reference\n\n| Model Component | Function | Repository | Keyword Filter | File Types |\n|----------------|----------|------------|----------------|-----------|\n| Wan 2.1 DiT | `get_dit_choices()` | `lightx2v/wan2.1-*-Models` | `\"wan2.1\"`, excludes VAE/encoders | `.safetensors`, `_split` dirs |\n| Wan 2.2 DiT | `get_dit_choices()` | `lightx2v/wan2.2-*-Models` | `\"wan2.2\"`, excludes VAE/encoders | `.safetensors`, `_split` dirs |\n| Wan 2.2 High Noise | `get_high_noise_choices()` | `lightx2v/wan2.2-*-Models` | `\"high_noise\"` | `.safetensors`, `_split` dirs |\n| Wan 2.2 Low Noise | `get_low_noise_choices()` | `lightx2v/wan2.2-*-Models` | `\"low_noise\"` | `.safetensors`, `_split` dirs |\n| T5 Encoder | `get_t5_model_choices()` | `lightx2v/Encoders` | `\"t5\"` | `.safetensors` only |\n| T5 Tokenizer | `get_t5_tokenizer_choices()` | `lightx2v/Encoders` | - | `google` directory |\n| CLIP Encoder | `get_clip_model_choices()` | `lightx2v/Encoders` | `\"clip\"` | `.safetensors` only |\n| CLIP Tokenizer | `get_clip_tokenizer_choices()` | `lightx2v/Encoders` | - | `xlm-roberta-large` directory |\n| VAE Encoder | `get_vae_encoder_choices()` | `lightx2v/Autoencoders` | - | Fixed: `Wan2.1_VAE.safetensors` |\n| VAE Decoder | `get_vae_decoder_choices()` | `lightx2v/Autoencoders` | `\"vae\"/\"tae\"/\"lightvae\"`, must contain `\"2_1\"` | `.safetensors`, `_split` dirs |\n| Qwen DiT | `get_qwen_image_dit_choices()` | `lightx2v/Qwen-Image-Edit-2511-Lightning` | `\"qwen_image_edit_2511\"`, ends with `\"lightning\"` | `.safetensors`, `_split` dirs |\n| Qwen VAE | `get_qwen_image_vae_choices()` | `Qwen/Qwen-Image-Edit-2511` | - | `vae` directory |\n| Qwen Scheduler | `get_qwen_image_scheduler_choices()` | `Qwen/Qwen-Image-Edit-2511` | - | `scheduler` directory |\n\n**Sources**: [app/gradio_demo.py:302-839]()"])</script><script>self.__next_f.push([1,"22:T7409,"])</script><script>self.__next_f.push([1,"# Python API (LightX2VPipeline)\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [README_zh.md](README_zh.md)\n- [lightx2v/common/modules/weight_module.py](lightx2v/common/modules/weight_module.py)\n- [lightx2v/pipeline.py](lightx2v/pipeline.py)\n- [lightx2v/server/__main__.py](lightx2v/server/__main__.py)\n- [lightx2v/server/config.py](lightx2v/server/config.py)\n- [lightx2v/server/schema.py](lightx2v/server/schema.py)\n- [lightx2v/server/services/distributed_utils.py](lightx2v/server/services/distributed_utils.py)\n- [lightx2v/server/services/generation/base.py](lightx2v/server/services/generation/base.py)\n- [lightx2v/server/services/generation/image.py](lightx2v/server/services/generation/image.py)\n- [lightx2v/server/services/inference/worker.py](lightx2v/server/services/inference/worker.py)\n- [lightx2v_platform/README.md](lightx2v_platform/README.md)\n- [lightx2v_platform/README_zh.md](lightx2v_platform/README_zh.md)\n- [scripts/hunyuan_video_15/README.md](scripts/hunyuan_video_15/README.md)\n- [scripts/server/post_i2i_with_lora.py](scripts/server/post_i2i_with_lora.py)\n- [scripts/server/start_server_i2i_with_loradir.sh](scripts/server/start_server_i2i_with_loradir.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis page documents the `LightX2VPipeline` class, which serves as the primary Python API for programmatic control of LightX2V. The pipeline provides a high-level interface for initializing models, configuring optimization features, and executing inference tasks.\n\nFor information about:\n- Gradio web interface usage, see [3.1](#3.1)\n- Command-line scripts and shell interface, see [3.3](#3.3)\n- Underlying runner architecture, see [4.1](#4.1)\n- Model-specific configurations, see [5](#5)\n\n---\n\n## Overview and Architecture\n\nThe `LightX2VPipeline` class orchestrates the entire inference workflow by:\n1. Managing model lifecycle (loading, configuration, execution)\n2. Providing a unified interface across different model families (Wan, LTX2, Qwen, etc.)\n3. Exposing optimization features through `enable_*` configuration methods\n4. Delegating execution to model-specific runners via the `RUNNER_REGISTER` factory\n\n### Pipeline Architecture\n\n```mermaid\ngraph TB\n    User[\"User Code\"]\n    \n    subgraph \"LightX2VPipeline Class\"\n        Init[\"__init__()\u003cbr/\u003eInitialize with task,\u003cbr/\u003emodel_path, model_cls\"]\n        CreateGen[\"create_generator()\u003cbr/\u003eConfigure inference\u003cbr/\u003eparameters\"]\n        EnableMethods[\"enable_* methods\u003cbr/\u003equantize, offload,\u003cbr/\u003ecache, parallel, etc.\"]\n        Generate[\"generate()\u003cbr/\u003eExecute inference\"]\n        SwitchLora[\"switch_lora()\u003cbr/\u003eDynamic LoRA\"]\n    end\n    \n    subgraph \"Configuration\"\n        ConfigJSON[\"config.json files\"]\n        InputInfo[\"InputInfo dataclass\"]\n        SetConfig[\"set_config()\"]\n    end\n    \n    subgraph \"Execution Backend\"\n        RunnerFactory[\"RUNNER_REGISTER\u003cbr/\u003eFactory Pattern\"]\n        Runner[\"Model-Specific Runner\u003cbr/\u003eWanRunner, LTX2Runner, etc.\"]\n        Scheduler[\"Scheduler\"]\n        Model[\"Transformer Model\"]\n        VAE[\"VAE Encoder/Decoder\"]\n    end\n    \n    User --\u003e Init\n    Init --\u003e CreateGen\n    CreateGen --\u003e EnableMethods\n    EnableMethods --\u003e Generate\n    \n    CreateGen -.-\u003e ConfigJSON\n    ConfigJSON --\u003e SetConfig\n    Generate --\u003e InputInfo\n    InputInfo --\u003e SetConfig\n    \n    SetConfig --\u003e RunnerFactory\n    RunnerFactory --\u003e Runner\n    Runner --\u003e Scheduler\n    Runner --\u003e Model\n    Runner --\u003e VAE\n    \n    Generate -.-\u003e SwitchLora\n```\n\n**Sources:** [lightx2v/pipeline.py:1-443]()\n\n---\n\n## Class Initialization\n\nThe `LightX2VPipeline` constructor accepts core model configuration and prepares the pipeline for later generator creation.\n\n### Constructor Signature\n\n```python\nLightX2VPipeline(\n    task: str,\n    model_path: str,\n    model_cls: str,\n    sf_model_path: str = None,\n    dit_original_ckpt: str = None,\n    low_noise_original_ckpt: str = None,\n    high_noise_original_ckpt: str = None,\n    transformer_model_name: str = None,\n)\n```\n\n### Key Parameters\n\n| Parameter | Type | Description | Example Values |\n|-----------|------|-------------|----------------|\n| `task` | `str` | Task type | `\"t2v\"`, `\"i2v\"`, `\"t2i\"`, `\"i2i\"`, `\"t2av\"`, `\"i2av\"`, `\"s2v\"` |\n| `model_path` | `str` | Path to model directory or checkpoint | `/path/to/Wan2.2-I2V-A14B` |\n| `model_cls` | `str` | Model class identifier | `\"wan2.1\"`, `\"wan2.2_moe\"`, `\"ltx2\"`, `\"qwen_image\"`, `\"hunyuan_video_1.5\"` |\n| `transformer_model_name` | `str` | Specific transformer variant (optional) | `\"720p_t2v\"` for HunyuanVideo |\n\n### Initialization Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Pipeline as LightX2VPipeline\n    participant Config as Internal Config\n    \n    User-\u003e\u003ePipeline: __init__(task, model_path, model_cls)\n    \n    Pipeline-\u003e\u003eConfig: Set vae_stride based on model_cls\n    Note over Config: wan2.1: (4,8,8)\u003cbr/\u003ewan2.2: (4,16,16)\u003cbr/\u003ehunyuan: (4,16,16)\u003cbr/\u003eltx2: custom\n    \n    Pipeline-\u003e\u003eConfig: Set model-specific attributes\n    Note over Config: num_channels_latents,\u003cbr/\u003eaudio_mel_bins,\u003cbr/\u003eUSE_IMAGE_ID_IN_PROMPT, etc.\n    \n    Pipeline-\u003e\u003eConfig: Initialize empty InputInfo\n    Note over Config: Creates task-specific\u003cbr/\u003edataclass structure\n    \n    Pipeline--\u003e\u003eUser: Pipeline instance ready\n```\n\n### Usage Example\n\n```python\nfrom lightx2v import LightX2VPipeline\n\n# Initialize for Wan2.2 I2V task\npipe = LightX2VPipeline(\n    model_path=\"/path/to/Wan2.2-I2V-A14B\",\n    model_cls=\"wan2.2_moe\",\n    task=\"i2v\",\n)\n\n# Initialize for LTX2 audio-video generation\npipe = LightX2VPipeline(\n    model_path=\"Lightricks/LTX-2\",\n    model_cls=\"ltx2\",\n    task=\"t2av\",\n)\n```\n\n**Sources:** [lightx2v/pipeline.py:59-124](), [examples/ltx2/ltxt_t2av_distilled_fp8.py:1-44](), [README.md:135-193]()\n\n---\n\n## Configuration Methods\n\nThe `LightX2VPipeline` exposes multiple `enable_*` methods to configure optimization features before generator creation. These methods modify the internal configuration state that will be passed to the runner during initialization.\n\n### Configuration Method Architecture\n\n```mermaid\ngraph LR\n    subgraph \"Configuration Methods\"\n        EnableQuant[\"enable_quantize()\u003cbr/\u003eINT8/FP8/NVFP4\"]\n        EnableOffload[\"enable_offload()\u003cbr/\u003eCPU/Disk offload\"]\n        EnableLora[\"enable_lora()\u003cbr/\u003eLoRA adapters\"]\n        EnableCache[\"enable_cache()\u003cbr/\u003eTeaCache/MagCache\"]\n        EnableParallel[\"enable_parallel()\u003cbr/\u003eCFG/Sequence parallel\"]\n        EnableLightVAE[\"enable_lightvae()\u003cbr/\u003eFast VAE decoders\"]\n        EnableCompile[\"enable_compile()\u003cbr/\u003etorch.compile()\"]\n    end\n    \n    subgraph \"Internal State\"\n        ConfigDict[\"Pipeline Config\u003cbr/\u003eDictionary\"]\n    end\n    \n    subgraph \"Runtime\"\n        CreateGen[\"create_generator()\"]\n        SetConfig[\"set_config()\"]\n        Runner[\"Runner Initialization\"]\n    end\n    \n    EnableQuant --\u003e ConfigDict\n    EnableOffload --\u003e ConfigDict\n    EnableLora --\u003e ConfigDict\n    EnableCache --\u003e ConfigDict\n    EnableParallel --\u003e ConfigDict\n    EnableLightVAE --\u003e ConfigDict\n    EnableCompile --\u003e ConfigDict\n    \n    ConfigDict --\u003e CreateGen\n    CreateGen --\u003e SetConfig\n    SetConfig --\u003e Runner\n```\n\n### enable_quantize()\n\nConfigures quantization schemes for model weights to reduce memory usage.\n\n```python\npipe.enable_quantize(\n    dit_quantized: bool = False,\n    text_encoder_quantized: bool = False,\n    image_encoder_quantized: bool = False,\n    dit_quantized_ckpt: str = None,\n    low_noise_quantized_ckpt: str = None,\n    high_noise_quantized_ckpt: str = None,\n    text_encoder_quantized_ckpt: str = None,\n    image_encoder_quantized_ckpt: str = None,\n    quant_scheme: str = \"fp8-sgl\",\n    text_encoder_quant_scheme: str = None,\n    skip_fp8_block_index: list = [0, 43, 44, 45, 46, 47],\n)\n```\n\n**Supported Quantization Schemes:**\n\n| Scheme | Description | Typical Memory Reduction |\n|--------|-------------|-------------------------|\n| `\"int8\"` | INT8 quantization via Triton kernels | ~2x |\n| `\"fp8-sgl\"` | FP8 via SGL kernels (recommended) | ~2x |\n| `\"fp8-vllm\"` | FP8 via VLLM kernels | ~2x |\n| `\"fp8-pertensor\"` | Per-tensor FP8 (for LTX2) | ~2x |\n| `\"nvfp4\"` | NVIDIA FP4 quantization | ~4x |\n\n**Example:**\n\n```python\n# FP8 quantization for LTX2\npipe.enable_quantize(\n    dit_quantized=True,\n    dit_quantized_ckpt=\"Lightricks/LTX-2/ltx-2-19b-distilled-fp8.safetensors\",\n    quant_scheme=\"fp8-pertensor\",\n    skip_fp8_block_index=[0, 43, 44, 45, 46, 47],\n)\n```\n\n**Sources:** [lightx2v/pipeline.py:257-294](), [examples/ltx2/ltxt_t2av_distilled_fp8.py:5-10]()\n\n### enable_offload()\n\nConfigures CPU/disk offloading to reduce GPU memory requirements.\n\n```python\npipe.enable_offload(\n    cpu_offload: bool = False,\n    offload_granularity: str = \"block\",  # \"block\" or \"phase\"\n    text_encoder_offload: bool = False,\n    image_encoder_offload: bool = False,\n    vae_offload: bool = False,\n)\n```\n\n**Offload Granularities:**\n\n| Granularity | Description | Suitable For |\n|-------------|-------------|--------------|\n| `\"block\"` | Per-transformer-block offloading | Most models, fine-grained control |\n| `\"phase\"` | Per-diffusion-phase offloading | Wan models, coarser control |\n\n**Example:**\n\n```python\npipe.enable_offload(\n    cpu_offload=True,\n    offload_granularity=\"block\",\n    text_encoder_offload=True,\n    vae_offload=False,\n)\n```\n\n**Sources:** [lightx2v/pipeline.py:295-333](), [README.md:159-165]()\n\n### enable_lora()\n\nConfigures LoRA (Low-Rank Adaptation) weights for fine-tuning.\n\n```python\npipe.enable_lora(\n    lora_configs: list[dict],\n    lora_dynamic_apply: bool = False,\n)\n```\n\n**LoRA Configuration Format:**\n\n```python\nlora_configs = [\n    {\n        \"path\": \"/path/to/lora1.safetensors\",\n        \"strength\": 1.0,\n    },\n    {\n        \"path\": \"/path/to/lora2.safetensors\",\n        \"strength\": 0.8,\n    },\n]\n```\n\n**Dynamic vs Static Application:**\n\n| Mode | Behavior | Use Case |\n|------|----------|----------|\n| `lora_dynamic_apply=False` | LoRA merged into base weights before inference | Single LoRA, faster inference |\n| `lora_dynamic_apply=True` | LoRA applied at runtime, switchable via `switch_lora()` | Multiple LoRAs, flexible switching |\n\n**Example:**\n\n```python\npipe.enable_lora(\n    lora_configs=[\n        {\"path\": \"lightx2v/distill-lora.safetensors\", \"strength\": 0.8}\n    ],\n    lora_dynamic_apply=True,\n)\n\n# Later, switch LoRA dynamically\npipe.switch_lora(\"/path/to/another-lora.safetensors\", strength=1.0)\n```\n\n**Sources:** [lightx2v/pipeline.py:351-360](), [configs/ltx2/ltx2_lora.json:1-25]()\n\n### enable_cache()\n\nConfigures feature caching to skip redundant computations.\n\n```python\npipe.enable_cache(\n    cache_method: str = \"Tea\",  # \"Tea\" or \"Mag\"\n    coefficients: list = [],\n    teacache_thresh: float = 0.15,\n    use_ret_steps: bool = False,\n    magcache_calibration: bool = False,\n    magcache_K: int = 6,\n    magcache_thresh: float = 0.24,\n    magcache_retention_ratio: float = 0.2,\n    magcache_ratios: list = [],\n)\n```\n\n**Caching Methods:**\n\n| Method | Description | Configuration |\n|--------|-------------|---------------|\n| `\"Tea\"` | TeaCache - threshold-based caching | `teacache_thresh`, `use_ret_steps` |\n| `\"Mag\"` | MagCache - magnitude-based caching | `magcache_thresh`, `magcache_K` |\n\n**Sources:** [lightx2v/pipeline.py:362-384]()\n\n### enable_parallel()\n\nConfigures parallel processing strategies for multi-GPU inference.\n\n```python\npipe.enable_parallel(\n    cfg_p_size: int = 1,\n    seq_p_size: int = 1,\n    seq_p_attn_type: str = \"ulysses\",\n)\n```\n\n**Parallelism Types:**\n\n| Parameter | Description | Typical Values |\n|-----------|-------------|----------------|\n| `cfg_p_size` | CFG (classifier-free guidance) parallelism | 2 for dual-GPU CFG split |\n| `seq_p_size` | Sequence parallelism for long sequences | 2, 4, 8 for large videos |\n| `seq_p_attn_type` | Sequence parallel attention type | `\"ulysses\"` or `\"ring\"` |\n\n**Sources:** [lightx2v/pipeline.py:386-391]()\n\n### enable_lightvae()\n\nConfigures lightweight VAE decoders for faster decoding.\n\n```python\npipe.enable_lightvae(\n    use_lightvae: bool = False,\n    use_tae: bool = False,\n    vae_path: str = None,\n    tae_path: str = None,\n)\n```\n\n**Sources:** [lightx2v/pipeline.py:242-255]()\n\n### enable_compile()\n\nEnables `torch.compile()` for model optimization (experimental).\n\n```python\npipe.enable_compile()\n```\n\n**Sources:** [lightx2v/pipeline.py:334-349]()\n\n---\n\n## Generator Creation\n\nAfter configuring the pipeline, `create_generator()` finalizes the inference configuration and initializes the model runner.\n\n### create_generator() Signature\n\n```python\npipe.create_generator(\n    attn_mode: str = \"flash_attn2\",\n    infer_steps: int = 50,\n    num_frames: int = 81,\n    height: int = 480,\n    width: int = 832,\n    guidance_scale: float = 5.0,\n    sample_shift: float = 5.0,\n    fps: int = 16,\n    aspect_ratio: str = \"16:9\",\n    boundary: float = 0.900,\n    boundary_step_index: int = 2,\n    denoising_step_list: list = [1000, 750, 500, 250],\n    config_json: str = None,\n    rope_type: str = \"torch\",\n    resize_mode: str = None,\n    audio_fps: int = 24000,\n    double_precision_rope: bool = True,\n    norm_modulate_backend: str = \"torch\",\n    distilled_sigma_values: list = None,\n)\n```\n\n### Configuration Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Pipeline as LightX2VPipeline\n    participant SetConfig as set_config()\n    participant Platform as PLATFORM_DEVICE_REGISTER\n    participant RunnerFactory as RUNNER_REGISTER\n    participant Runner as Model Runner\n    \n    User-\u003e\u003ePipeline: create_generator(params) or\u003cbr/\u003ecreate_generator(config_json=path)\n    \n    alt config_json provided\n        Pipeline-\u003e\u003ePipeline: set_infer_config_json(config_json)\n        Note over Pipeline: Load JSON, update internal state\n    else parameters provided\n        Pipeline-\u003e\u003ePipeline: set_infer_config(params)\n        Note over Pipeline: Set individual parameters\n    end\n    \n    Pipeline-\u003e\u003eSetConfig: set_config(self)\n    Note over SetConfig: Build config dict from\u003cbr/\u003epipeline attributes\n    \n    SetConfig-\u003e\u003eSetConfig: validate_config_paths(config)\n    Note over SetConfig: Check model paths exist\n    \n    alt parallel enabled\n        SetConfig-\u003e\u003ePlatform: init_parallel_env()\n        Note over Platform: Initialize NCCL/distributed\n        SetConfig-\u003e\u003eSetConfig: set_parallel_config(config)\n    end\n    \n    Pipeline-\u003e\u003eRunnerFactory: RUNNER_REGISTER[model_cls](config)\n    RunnerFactory-\u003e\u003eRunner: Create runner instance\n    \n    Runner-\u003e\u003eRunner: init_modules()\n    Note over Runner: Load transformer, text encoder,\u003cbr/\u003eVAE, scheduler\n    \n    Runner--\u003e\u003ePipeline: Return initialized runner\n    Pipeline--\u003e\u003eUser: Pipeline ready for generate()\n```\n\n### Key Parameters\n\n| Category | Parameter | Description | Model Support |\n|----------|-----------|-------------|---------------|\n| **Inference** | `infer_steps` | Number of diffusion steps | All models |\n| | `guidance_scale` | CFG guidance scale | All models |\n| | `sample_shift` | Sigma schedule shift | Wan, LTX2, HunyuanVideo |\n| **Output** | `num_frames` | Video length in frames | T2V, I2V, T2AV, I2AV |\n| | `height` | Output height in pixels | All models |\n| | `width` | Output width in pixels | All models |\n| | `fps` | Frames per second | T2V, I2V, T2AV, I2AV |\n| **Attention** | `attn_mode` | Attention implementation | `\"sage_attn2\"`, `\"flash_attn2\"`, `\"flash_attn3\"` |\n| **Advanced** | `distilled_sigma_values` | Custom sigma schedule for distilled models | LTX2 distilled |\n| | `double_precision_rope` | Use FP64 for RoPE | LTX2 |\n| | `norm_modulate_backend` | Normalization backend | `\"torch\"`, `\"triton\"` |\n\n### Configuration from JSON\n\nInstead of passing individual parameters, you can load configuration from a JSON file:\n\n```python\npipe.create_generator(\n    config_json=\"configs/ltx2/ltx2_distill_fp8.json\"\n)\n```\n\n**Example JSON Configuration:**\n\n```json\n{\n    \"infer_steps\": 8,\n    \"target_video_length\": 121,\n    \"target_height\": 512,\n    \"target_width\": 768,\n    \"attn_type\": \"sage_attn2\",\n    \"sample_guide_scale\": 1,\n    \"sample_shift\": [2.05, 0.95],\n    \"enable_cfg\": false,\n    \"fps\": 24,\n    \"audio_fps\": 24000,\n    \"distilled_sigma_values\": [1.0, 0.99375, 0.9875, 0.98125, 0.975, 0.909375, 0.725, 0.421875, 0.0]\n}\n```\n\n**Sources:** [lightx2v/pipeline.py:126-184](), [configs/ltx2/ltx2_distill_fp8.json:1-22](), [lightx2v/utils/set_config.py]()\n\n---\n\n## Execution\n\nThe `generate()` method executes the inference pipeline and produces outputs.\n\n### generate() Signature\n\n```python\npipe.generate(\n    seed: int,\n    prompt: str,\n    negative_prompt: str,\n    save_result_path: str,\n    image_path: str = None,\n    image_strength: float | list[float] = None,\n    last_frame_path: str = None,\n    audio_path: str = None,\n    src_ref_images: str = None,\n    src_video: str = None,\n    src_mask: str = None,\n    return_result_tensor: bool = False,\n    target_shape: list = [],\n)\n```\n\n### Parameter Details\n\n| Parameter | Type | Description | Task Support |\n|-----------|------|-------------|--------------|\n| `seed` | `int` | Random seed for reproducibility | All |\n| `prompt` | `str` | Text prompt describing desired output | All |\n| `negative_prompt` | `str` | Text describing what to avoid | All (optional) |\n| `save_result_path` | `str` | Output file path (`.mp4`, `.png`, etc.) | All |\n| `image_path` | `str` | Conditioning image path(s), comma-separated | I2V, I2I, I2AV |\n| `image_strength` | `float` or `list[float]` | Conditioning strength (0.0-1.0) | I2V, I2AV |\n| `audio_path` | `str` | Audio file path for audio-driven generation | S2V |\n| `return_result_tensor` | `bool` | Return tensors instead of saving to disk | All |\n| `target_shape` | `list[int]` | Override target resolution `[height, width]` | All |\n\n### Execution Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Pipeline as LightX2VPipeline\n    participant InputInfo as InputInfo\n    participant Runner as Model Runner\n    participant Encoder as Input Encoders\n    participant Model as Diffusion Model\n    participant VAE as VAE Decoder\n    \n    User-\u003e\u003ePipeline: generate(seed, prompt, ...)\n    \n    Pipeline-\u003e\u003ePipeline: seed_all(seed)\n    Note over Pipeline: Set random seed\n    \n    Pipeline-\u003e\u003eInputInfo: init_empty_input_info(task)\n    Pipeline-\u003e\u003eInputInfo: update_input_info_from_dict(input_info, self)\n    Note over InputInfo: Populate with generation params\n    \n    Pipeline-\u003e\u003eRunner: run_pipeline(input_info)\n    \n    Runner-\u003e\u003eEncoder: run_input_encoder()\n    Note over Encoder: Text encoding\u003cbr/\u003eImage encoding (I2V)\u003cbr/\u003eAudio processing (S2V)\n    Encoder--\u003e\u003eRunner: encoder_outputs\n    \n    Runner-\u003e\u003eModel: run_main()\n    Note over Model: Prepare scheduler\u003cbr/\u003eDiffusion loop\u003cbr/\u003eUpsampling (if enabled)\n    Model--\u003e\u003eRunner: latents\n    \n    Runner-\u003e\u003eVAE: run_vae_decoder(latents)\n    VAE--\u003e\u003eRunner: video_frames / image\n    \n    alt return_result_tensor\n        Runner--\u003e\u003ePipeline: tensors\n        Pipeline--\u003e\u003eUser: {\"video\": tensor, \"audio\": tensor}\n    else save to disk\n        Runner-\u003e\u003eRunner: save_video() / save_image()\n        Runner--\u003e\u003ePipeline: None\n        Pipeline--\u003e\u003eUser: File saved\n    end\n```\n\n### Usage Examples\n\n**Text-to-Video:**\n\n```python\npipe.generate(\n    seed=42,\n    prompt=\"A beautiful sunset over the ocean\",\n    negative_prompt=\"blurry, low quality\",\n    save_result_path=\"/path/to/output.mp4\",\n)\n```\n\n**Image-to-Video:**\n\n```python\npipe.generate(\n    seed=42,\n    prompt=\"The cat starts playing with a ball\",\n    negative_prompt=\"static, blurry\",\n    image_path=\"/path/to/cat.jpg\",\n    save_result_path=\"/path/to/output.mp4\",\n)\n```\n\n**Image-to-Audio-Video (LTX2) with Multiple Conditioning Images:**\n\n```python\n# Multiple images: frame 0, frame 60, frame 120\npipe.generate(\n    seed=42,\n    prompt=\"A person singing joyfully\",\n    negative_prompt=\"distorted audio, out of sync\",\n    image_path=\"/path/to/img1.jpg,/path/to/img2.jpg,/path/to/img3.jpg\",\n    image_strength=[1.0, 0.8, 0.6],  # Different strengths per image\n    save_result_path=\"/path/to/output.mp4\",\n)\n```\n\n**Return Tensors Instead of Saving:**\n\n```python\nresult = pipe.generate(\n    seed=42,\n    prompt=\"A dancing robot\",\n    negative_prompt=\"\",\n    save_result_path=None,\n    return_result_tensor=True,\n)\n# result = {\"video\": torch.Tensor, \"audio\": torch.Tensor}\n```\n\n**Sources:** [lightx2v/pipeline.py:393-432](), [README.md:186-192](), [examples/ltx2/ltxt_i2av_distilled_fp8.py:48-55]()\n\n---\n\n## Advanced Features\n\n### Dynamic LoRA Switching\n\nWhen `lora_dynamic_apply=True` is set during `enable_lora()`, you can switch LoRA weights at runtime without reloading the base model.\n\n```python\n# Enable dynamic LoRA\npipe.enable_lora(\n    lora_configs=[{\"path\": \"lora1.safetensors\", \"strength\": 1.0}],\n    lora_dynamic_apply=True,\n)\npipe.create_generator(...)\n\n# Switch to different LoRA\npipe.switch_lora(\"/path/to/lora2.safetensors\", strength=0.8)\npipe.generate(...)\n\n# Switch again\npipe.switch_lora(\"/path/to/lora3.safetensors\", strength=1.2)\npipe.generate(...)\n```\n\n**Implementation:**\n\nThe `switch_lora()` method delegates to the runner's `switch_lora()` implementation, which updates the model weights dynamically.\n\n**Sources:** [lightx2v/pipeline.py:355-360]()\n\n### Configuration Persistence\n\nThe `LightX2VPipeline` instance stores all configuration in its attributes, which can be accessed as a dictionary-like object:\n\n```python\n# Access configuration\nprint(pipe.infer_steps)\nprint(pipe.target_height)\n\n# Modify configuration before create_generator()\npipe.infer_steps = 25\npipe.target_width = 1024\npipe.create_generator()\n```\n\n**Sources:** [lightx2v/pipeline.py:32-55]()\n\n---\n\n## Complete Workflow Example\n\n### Full Configuration Example\n\n```python\nfrom lightx2v import LightX2VPipeline\n\n# 1. Initialize pipeline\npipe = LightX2VPipeline(\n    model_path=\"/path/to/Wan2.2-I2V-A14B\",\n    model_cls=\"wan2.2_moe\",\n    task=\"i2v\",\n)\n\n# 2. Enable optimizations\npipe.enable_quantize(\n    dit_quantized=True,\n    quant_scheme=\"fp8-sgl\",\n)\n\npipe.enable_offload(\n    cpu_offload=True,\n    offload_granularity=\"block\",\n    text_encoder_offload=True,\n)\n\npipe.enable_lora(\n    lora_configs=[\n        {\"path\": \"/path/to/style-lora.safetensors\", \"strength\": 0.8}\n    ],\n    lora_dynamic_apply=False,\n)\n\npipe.enable_cache(\n    cache_method=\"Tea\",\n    teacache_thresh=0.15,\n)\n\npipe.enable_parallel(\n    cfg_p_size=2,  # Split CFG across 2 GPUs\n    seq_p_size=1,\n)\n\n# 3. Create generator\npipe.create_generator(\n    attn_mode=\"sage_attn2\",\n    infer_steps=40,\n    height=720,\n    width=1280,\n    num_frames=81,\n    guidance_scale=[3.5, 3.5],\n    sample_shift=5.0,\n)\n\n# 4. Generate video\npipe.generate(\n    seed=42,\n    prompt=\"A cat playing with yarn\",\n    negative_prompt=\"blurry, static\",\n    image_path=\"/path/to/cat_start.jpg\",\n    save_result_path=\"/path/to/output.mp4\",\n)\n```\n\n### Configuration from JSON Example\n\n```python\nfrom lightx2v import LightX2VPipeline\n\n# 1. Initialize\npipe = LightX2VPipeline(\n    model_path=\"Lightricks/LTX-2\",\n    model_cls=\"ltx2\",\n    task=\"t2av\",\n)\n\n# 2. Enable optimizations\npipe.enable_quantize(\n    dit_quantized=True,\n    dit_quantized_ckpt=\"Lightricks/LTX-2/ltx-2-19b-distilled-fp8.safetensors\",\n    quant_scheme=\"fp8-pertensor\",\n    skip_fp8_block_index=[0, 43, 44, 45, 46, 47],\n)\n\n# 3. Create generator from JSON\npipe.create_generator(\n    config_json=\"configs/ltx2/ltx2_distill_fp8.json\"\n)\n\n# 4. Generate\npipe.generate(\n    seed=42,\n    prompt=\"A beautiful sunset over the ocean\",\n    negative_prompt=\"blurry, distorted\",\n    save_result_path=\"/path/to/output.mp4\",\n)\n```\n\n**Sources:** [README.md:135-193](), [examples/ltx2/ltxt_t2av_distilled_fp8.py:1-44](), [configs/ltx2/ltx2_distill_fp8.json:1-22]()\n\n---\n\n## Configuration Reference\n\n### Model Class Identifiers\n\n| `model_cls` | Model Family | Supported Tasks | Notes |\n|-------------|--------------|-----------------|-------|\n| `\"wan2.1\"` | Wan 2.1 | `t2v`, `i2v` | Base model |\n| `\"wan2.1_distill\"` | Wan 2.1 Distilled | `t2v`, `i2v` | 4-step inference |\n| `\"wan2.2_moe\"` | Wan 2.2 MoE | `t2v`, `i2v` | Mixture-of-experts |\n| `\"wan2.2_moe_distill\"` | Wan 2.2 MoE Distilled | `t2v`, `i2v` | 4-step + MoE |\n| `\"seko_talk\"` | Seko Talk | `s2v` | Speech-to-video |\n| `\"wan2.2_audio\"` | Wan 2.2 Audio | `s2v` | Audio-driven |\n| `\"hunyuan_video_1.5\"` | HunyuanVideo 1.5 | `t2v` | 720p generation |\n| `\"hunyuan_video_1.5_distill\"` | HunyuanVideo 1.5 Distilled | `t2v` | 4-step inference |\n| `\"ltx2\"` | LTX-2 | `t2av`, `i2av` | Audio-video generation |\n| `\"qwen_image\"` | Qwen Image | `t2i`, `i2i` | Text-to-image, editing |\n| `\"longcat_image\"` | LongCat Image | `t2i` | Long-context images |\n| `\"z_image\"` | Z Image | `t2i` | Image generation |\n\n### Task Identifiers\n\n| `task` | Description | Required Inputs |\n|--------|-------------|-----------------|\n| `\"t2v\"` | Text-to-video | `prompt` |\n| `\"i2v\"` | Image-to-video | `prompt`, `image_path` |\n| `\"t2av\"` | Text-to-audio-video | `prompt` |\n| `\"i2av\"` | Image-to-audio-video | `prompt`, `image_path` |\n| `\"s2v\"` | Speech/audio-to-video | `prompt`, `audio_path` |\n| `\"t2i\"` | Text-to-image | `prompt` |\n| `\"i2i\"` | Image-to-image (editing) | `prompt`, `image_path` |\n\n### Attention Modes\n\n| `attn_mode` | Description | Hardware | Recommendation |\n|-------------|-------------|----------|----------------|\n| `\"sage_attn2\"` | SageAttention 2 | NVIDIA |  Recommended for most cases |\n| `\"flash_attn2\"` | Flash Attention 2 | NVIDIA | Good for Ampere/Ada |\n| `\"flash_attn3\"` | Flash Attention 3 | H100 | Best for Hopper |\n| `\"radial_attn\"` | Radial Attention | NVIDIA | Memory-efficient |\n| `\"nbhd_attn\"` | Neighborhood Attention | NVIDIA | Sparse patterns |\n\n### Internal Configuration Flow\n\n```mermaid\ngraph TB\n    subgraph \"Pipeline Attributes\"\n        TaskAttr[\"task\"]\n        ModelPathAttr[\"model_path\"]\n        ModelClsAttr[\"model_cls\"]\n        EnableAttrs[\"enable_* method\u003cbr/\u003econfigurations\"]\n        GeneratorAttrs[\"create_generator()\u003cbr/\u003eparameters\"]\n    end\n    \n    subgraph \"set_config() Processing\"\n        BuildDict[\"Build config dict\"]\n        ModelSpecific[\"Apply model-specific\u003cbr/\u003edefaults\"]\n        Validation[\"validate_config_paths()\"]\n        ParallelSetup[\"set_parallel_config()\u003cbr/\u003e(if parallel enabled)\"]\n    end\n    \n    subgraph \"Runner Registry\"\n        FactoryLookup[\"RUNNER_REGISTER[model_cls]\"]\n        RunnerInit[\"Runner.__init__(config)\"]\n    end\n    \n    subgraph \"Runner Initialization\"\n        LoadTransformer[\"load_transformer()\"]\n        LoadEncoder[\"load_text_encoder()\"]\n        LoadVAE[\"load_vae()\"]\n        InitScheduler[\"init_scheduler()\"]\n    end\n    \n    TaskAttr --\u003e BuildDict\n    ModelPathAttr --\u003e BuildDict\n    ModelClsAttr --\u003e BuildDict\n    EnableAttrs --\u003e BuildDict\n    GeneratorAttrs --\u003e BuildDict\n    \n    BuildDict --\u003e ModelSpecific\n    ModelSpecific --\u003e Validation\n    Validation --\u003e ParallelSetup\n    \n    ParallelSetup --\u003e FactoryLookup\n    FactoryLookup --\u003e RunnerInit\n    \n    RunnerInit --\u003e LoadTransformer\n    RunnerInit --\u003e LoadEncoder\n    RunnerInit --\u003e LoadVAE\n    RunnerInit --\u003e InitScheduler\n```\n\n**Sources:** [lightx2v/pipeline.py:126-184](), [lightx2v/utils/set_config.py]()\n\n---\n\n## Error Handling and Validation\n\n### Path Validation\n\nThe `validate_config_paths()` function checks that model paths exist before runner initialization:\n\n```python\n# Checks performed:\n# 1. model_path exists\n# 2. dit_original_ckpt exists (if specified)\n# 3. dit_quantized_ckpt exists (if specified)\n# 4. vae_path exists (if specified)\n# 5. tae_path exists (if specified)\n```\n\n### Common Errors\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| `KeyError: model_cls` in `RUNNER_REGISTER` | Invalid `model_cls` value | Use valid identifier from table above |\n| `FileNotFoundError` | Model path doesn't exist | Check `model_path` and download models |\n| `CUDA out of memory` | Insufficient GPU memory | Enable offloading and/or quantization |\n| `LoRA dynamic apply not enabled` | Calling `switch_lora()` without `lora_dynamic_apply=True` | Set `lora_dynamic_apply=True` in `enable_lora()` |\n\n**Sources:** [lightx2v/utils/set_config.py](), [lightx2v/pipeline.py:355-360]()\n\n---\n\n## Integration with Other Components\n\n### Relationship to Runners\n\n```mermaid\ngraph LR\n    Pipeline[\"LightX2VPipeline\"]\n    Registry[\"RUNNER_REGISTER\"]\n    \n    subgraph \"Runner Implementations\"\n        WanRunner[\"WanRunner\"]\n        LTX2Runner[\"LTX2Runner\"]\n        QwenRunner[\"QwenImageRunner\"]\n        HyRunner[\"HunyuanVideo15Runner\"]\n    end\n    \n    Pipeline --\u003e|\"model_cls lookup\"| Registry\n    Registry --\u003e|\"wan2.1, wan2.2_moe\"| WanRunner\n    Registry --\u003e|\"ltx2\"| LTX2Runner\n    Registry --\u003e|\"qwen_image\"| QwenRunner\n    Registry --\u003e|\"hunyuan_video_1.5\"| HyRunner\n    \n    WanRunner -.-\u003e|\"inherits\"| DefaultRunner\n    LTX2Runner -.-\u003e|\"inherits\"| DefaultRunner\n    QwenRunner -.-\u003e|\"inherits\"| DefaultRunner\n    HyRunner -.-\u003e|\"inherits\"| DefaultRunner\n```\n\nFor details on runner architecture, see [4.1 Runner System and Model Registry](#4.1).\n\n### Relationship to Input Encoders\n\nThe pipeline initializes `InputInfo` dataclasses that flow to runners, which then call appropriate encoders:\n\n```mermaid\ngraph TB\n    Pipeline[\"LightX2VPipeline.generate()\"]\n    InputInfo[\"InputInfo dataclass\"]\n    Runner[\"Runner.run_pipeline()\"]\n    \n    subgraph \"Encoders\"\n        TextEnc[\"run_text_encoder()\"]\n        ImageEnc[\"run_image_encoder()\"]\n        VAEEnc[\"run_vae_encoder()\"]\n        AudioProc[\"run_audio_processor()\"]\n    end\n    \n    Pipeline --\u003e InputInfo\n    InputInfo --\u003e Runner\n    Runner --\u003e TextEnc\n    Runner --\u003e ImageEnc\n    Runner --\u003e VAEEnc\n    Runner --\u003e AudioProc\n```\n\nFor details on encoder architecture, see [4.4 Input Encoder Architecture](#4.4).\n\n**Sources:** [lightx2v/pipeline.py:393-432](), [lightx2v/utils/input_info.py]()"])</script><script>self.__next_f.push([1,"23:T5b4a,"])</script><script>self.__next_f.push([1,"# Command Line Interface\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json](configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json)\n- [docs/EN/source/getting_started/quickstart.md](docs/EN/source/getting_started/quickstart.md)\n- [docs/ZH_CN/source/getting_started/quickstart.md](docs/ZH_CN/source/getting_started/quickstart.md)\n- [lightx2v/__init__.py](lightx2v/__init__.py)\n- [lightx2v/common/ops/attn/utils/ring_comm.py](lightx2v/common/ops/attn/utils/ring_comm.py)\n- [lightx2v/common/ops/mm/triton_kernels.py](lightx2v/common/ops/mm/triton_kernels.py)\n- [pyproject.toml](pyproject.toml)\n- [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh](scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh)\n\n\u003c/details\u003e\n\n\n\nThis page documents the shell script interface and direct command-line execution of LightX2V, including the `lightx2v.infer` module invocation, configuration file usage, and distributed execution with `torchrun`. For programmatic usage, see [Python API](#3.2). For interactive usage, see [Gradio Web Interface](#3.1).\n\n## Overview\n\nLightX2V provides shell scripts and direct command-line execution for batch processing and distributed inference. The CLI follows a two-tier configuration pattern: command-line arguments specify model selection and input/output paths, while JSON configuration files define optimization strategies (quantization, offloading, parallelism, attention operators).\n\n**Sources:** [docs/EN/source/getting_started/quickstart.md:301-322](), [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh:1-22]()\n\n## Invocation Architecture\n\n**CLI Execution Flow (Code Entity Map)**\n\n```mermaid\ngraph TB\n    ShellScript[\"Shell Script\u003cbr/\u003erun_*.sh\"]\n    Torchrun[\"torchrun\u003cbr/\u003e--nproc-per-node N\"]\n    InferMain[\"lightx2v.infer.main()\u003cbr/\u003elightx2v/infer.py:36\"]\n    \n    subgraph \"Argument Processing\"\n        ArgParser[\"argparse.ArgumentParser\u003cbr/\u003elightx2v/infer.py:37-123\"]\n        SetConfig[\"set_config(args)\u003cbr/\u003eutils/set_config.py:37\"]\n        ConfigJSON[\"config.json\u003cbr/\u003econfigs/*/\"]\n        MergeConfig[\"config.update()\u003cbr/\u003eLockableDict\"]\n    end\n    \n    subgraph \"Runner Initialization\"\n        InitRunner[\"init_runner(config)\u003cbr/\u003elightx2v/infer.py:29\"]\n        RunnerRegister[\"RUNNER_REGISTER\u003cbr/\u003eregistry_factory.py\"]\n        RunnerClass[\"WanRunner\u003cbr/\u003eWanAudioRunner\u003cbr/\u003eQwenImageRunner\"]\n        InitModules[\"runner.init_modules()\u003cbr/\u003edefault_runner.py:71\"]\n    end\n    \n    subgraph \"Pipeline Execution\"\n        SetInputInfo[\"set_input_info(args)\u003cbr/\u003eutils/input_info.py\"]\n        RunPipeline[\"runner.run_pipeline()\u003cbr/\u003edefault_runner.py:460\"]\n        RunMain[\"runner.run_main()\u003cbr/\u003edefault_runner.py:359\"]\n        RunSegment[\"runner.run_segment()\u003cbr/\u003edefault_runner.py:171\"]\n    end\n    \n    ShellScript --\u003e Torchrun\n    Torchrun --\u003e InferMain\n    InferMain --\u003e ArgParser\n    ArgParser --\u003e SetConfig\n    ConfigJSON --\u003e SetConfig\n    SetConfig --\u003e MergeConfig\n    \n    MergeConfig --\u003e InitRunner\n    InitRunner --\u003e RunnerRegister\n    RunnerRegister --\u003e RunnerClass\n    RunnerClass --\u003e InitModules\n    \n    InitModules --\u003e SetInputInfo\n    SetInputInfo --\u003e RunPipeline\n    RunPipeline --\u003e RunMain\n    RunMain --\u003e RunSegment\n```\n\n**Execution Flow:**\n1. Shell script invokes `torchrun` with `--nproc-per-node` for distributed execution\n2. `torchrun` spawns N processes, each calling `lightx2v.infer.main()` [lightx2v/infer.py:36-150]()\n3. `argparse.ArgumentParser` parses CLI arguments [lightx2v/infer.py:37-123]()\n4. `set_config(args)` merges CLI args with JSON config [lightx2v/utils/set_config.py:37-102]()\n5. `init_runner(config)` retrieves runner class from `RUNNER_REGISTER` [lightx2v/infer.py:29-33]()\n6. Runner's `init_modules()` loads model, encoders, VAE, scheduler [lightx2v/models/runners/default_runner.py:71-94]()\n7. `run_pipeline()` executes inference via `run_main()`  `run_segment()` loop [lightx2v/models/runners/default_runner.py:359-389]()\n\n**Sources:** [lightx2v/infer.py:1-151](), [lightx2v/models/runners/default_runner.py:56-489](), [lightx2v/utils/set_config.py:1-130](), [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh:1-22]()\n\n## Shell Script Structure\n\n### Standard Script Template\n\nShell scripts follow a consistent pattern across model families:\n\n```bash\n#!/bin/bash\n\n# Path configuration\nlightx2v_path=/path/to/Lightx2v\nmodel_path=/path/to/Model\n\n# GPU configuration\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n\n# Environment setup\nsource ${lightx2v_path}/scripts/base/base.sh\n\n# Execution (single-GPU or distributed)\ntorchrun --nproc-per-node 8 -m lightx2v.infer \\\n--model_cls seko_talk \\\n--task s2v \\\n--model_path $model_path \\\n--config_json ${lightx2v_path}/configs/seko_talk/config.json \\\n--prompt \"Your prompt here\" \\\n--negative_prompt \"Negative prompt\" \\\n--image_path ${lightx2v_path}/assets/inputs/image.png \\\n--audio_path ${lightx2v_path}/assets/inputs/audio.mp3 \\\n--save_result_path ${lightx2v_path}/save_results/output.mp4\n```\n\n**Script Organization:**\n- **Wan models**: `scripts/wan/run_wan_t2v.sh`, `run_wan_i2v.sh`\n- **Audio models**: `scripts/seko_talk/run_seko_talk_*.sh`\n- **HunyuanVideo**: `scripts/hunyuan_video/run_*.sh`\n- **Windows**: `scripts/win/*.bat` (batch file equivalents)\n\n**Sources:** [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh:1-22](), [docs/EN/source/getting_started/quickstart.md:309-322]()\n\n## Command-Line Arguments\n\n**Argument Parser Definition: [lightx2v/infer.py:37-123]()**\n\n### Core Arguments\n\n| Argument | Type | Required | Parser Line | Description |\n|----------|------|----------|-------------|-------------|\n| `--model_cls` | str | Yes | [infer.py:40-62]() | Model class key in `RUNNER_REGISTER`: `wan2.1`, `wan2.1_distill`, `seko_talk`, `wan2.2_moe`, `wan2.2_audio`, `hunyuan_video_1.5`, `qwen_image`, `z_image` |\n| `--task` | str | Yes | [infer.py:65]() | Task type: `t2v`, `i2v`, `t2i`, `i2i`, `flf2v`, `vace`, `animate`, `s2v` |\n| `--model_path` | str | Yes | [infer.py:66]() | Path to model weights directory |\n| `--config_json` | str | Yes | [infer.py:68]() | Path to JSON config (loaded via `set_config()`) |\n| `--save_result_path` | str | Conditional | [infer.py:121]() | Output file path (omit for ComfyUI tensor return) |\n\n### Input Arguments\n\n| Argument | Type | Required | Parser Line | Description |\n|----------|------|----------|-------------|-------------|\n| `--prompt` | str | Conditional | [infer.py:71]() | Text prompt passed to `input_info.prompt` |\n| `--negative_prompt` | str | No | [infer.py:72]() | Negative prompt for `context_null` in CFG |\n| `--image_path` | str | Conditional | [infer.py:74]() | Reference image for i2v/i2i/s2v tasks |\n| `--audio_path` | str | Conditional | [infer.py:76]() | Audio file for s2v task (processed by `AudioProcessor`) |\n| `--seed` | int | No | [infer.py:38]() | Random seed (default 42, passed to `seed_all()`) |\n\n### Task Validation Logic\n\nThe function `validate_task_arguments(args)` [lightx2v/utils/utils.py:379-403]() enforces task-specific requirements:\n\n- **t2v**: Requires `--prompt`\n- **i2v/i2i**: Requires `--prompt` and `--image_path`\n- **s2v**: Requires `--prompt`, `--image_path`, and `--audio_path`\n- **vace**: Requires `--src_video`, `--src_mask`, optionally `--src_ref_images`\n\n**Sources:** [lightx2v/infer.py:37-123](), [lightx2v/utils/utils.py:379-403](), [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh:12-21]()\n\n### Optional Override Arguments\n\nWhile most configuration is specified in JSON files, certain parameters can be overridden at command-line:\n\n| Argument | Type | Description |\n|----------|------|-------------|\n| `--height` | int | Override video height from JSON config |\n| `--width` | int | Override video width from JSON config |\n| `--num_frames` | int | Override number of frames |\n| `--infer_steps` | int | Override inference steps |\n| `--guidance_scale` | float | Override CFG guidance scale |\n\n**Priority Order:** Command-line arguments \u003e JSON config \u003e Model defaults\n\n**Sources:** [docs/EN/source/getting_started/quickstart.md:324-355]()\n\n## Configuration File System\n\n**Configuration Loading Flow (Code Entity Map)**\n\n```mermaid\ngraph TB\n    CLIArg[\"--config_json\u003cbr/\u003eargs.config_json\"]\n    \n    subgraph \"Config Loading Pipeline\"\n        SetConfig[\"set_config(args)\u003cbr/\u003eset_config.py:37\"]\n        GetDefault[\"get_default_config()\u003cbr/\u003eset_config.py:14\"]\n        LoadJSON[\"json.load(f)\u003cbr/\u003eset_config.py:43\"]\n        UpdateDict[\"config.update()\u003cbr/\u003eLockableDict\"]\n        LoadModelConfig[\"Load model_path/config.json\u003cbr/\u003eset_config.py:54-73\"]\n    end\n    \n    subgraph \"Config Categories\"\n        InferParams[\"infer_steps\u003cbr/\u003etarget_video_length\u003cbr/\u003esample_guide_scale\"]\n        QuantConfig[\"dit_quantized\u003cbr/\u003edit_quant_scheme\u003cbr/\u003eMM_WEIGHT_REGISTER\"]\n        OffloadConfig[\"cpu_offload\u003cbr/\u003eoffload_granularity\u003cbr/\u003eOffloadManager\"]\n        ParallelConfig[\"parallel.seq_p_size\u003cbr/\u003edevice_mesh\u003cbr/\u003eset_parallel_config\"]\n        AttnConfig[\"self_attn_1_type\u003cbr/\u003eATTN_WEIGHT_REGISTER\"]\n        VAEConfig[\"vae_cpu_offload\u003cbr/\u003euse_tiling_vae\"]\n    end\n    \n    subgraph \"Configuration Consumers\"\n        InitScheduler[\"init_scheduler()\u003cbr/\u003eWanScheduler\"]\n        InitRunner[\"init_runner()\u003cbr/\u003eRUNNER_REGISTER\"]\n        LoadTransformer[\"load_transformer()\u003cbr/\u003eWanModel.__init__\"]\n        LoadEncoders[\"load_text_encoder()\u003cbr/\u003eload_image_encoder()\"]\n        SetParallel[\"set_parallel_config()\u003cbr/\u003einit_device_mesh\"]\n    end\n    \n    CLIArg --\u003e SetConfig\n    SetConfig --\u003e GetDefault\n    SetConfig --\u003e LoadJSON\n    LoadJSON --\u003e UpdateDict\n    UpdateDict --\u003e LoadModelConfig\n    \n    LoadModelConfig --\u003e InferParams\n    LoadModelConfig --\u003e QuantConfig\n    LoadModelConfig --\u003e OffloadConfig\n    LoadModelConfig --\u003e ParallelConfig\n    LoadModelConfig --\u003e AttnConfig\n    LoadModelConfig --\u003e VAEConfig\n    \n    InferParams --\u003e InitScheduler\n    QuantConfig --\u003e LoadTransformer\n    OffloadConfig --\u003e LoadTransformer\n    ParallelConfig --\u003e SetParallel\n    AttnConfig --\u003e LoadTransformer\n    VAEConfig --\u003e LoadEncoders\n    \n    ParallelConfig --\u003e InitRunner\n```\n\n**Configuration Merge Priority:**\n1. Default config from `get_default_config()` [lightx2v/utils/set_config.py:14-34]()\n2. CLI arguments merged via `config.update()` [lightx2v/utils/set_config.py:39]()\n3. JSON file via `--config_json` [lightx2v/utils/set_config.py:42-45]()\n4. Model-specific `config.json` in `model_path/` [lightx2v/utils/set_config.py:54-80]()\n5. Quantized model config if `dit_quantized_ckpt` specified [lightx2v/utils/set_config.py:75-80]()\n\n### Configuration Schema\n\nConfiguration files in `configs/` directory use JSON format with the following top-level structure:\n\n**Inference Parameters:**\n```json\n{\n  \"infer_steps\": 50,\n  \"target_fps\": 16,\n  \"video_duration\": 5,\n  \"target_video_length\": 81,\n  \"sample_guide_scale\": 5.0,\n  \"sample_shift\": 5.0,\n  \"enable_cfg\": false\n}\n```\n\n**Quantization Configuration (Consumed by `MM_WEIGHT_REGISTER`)**\n```json\n{\n  \"dit_quantized\": true,\n  \"dit_quant_scheme\": \"int8-q8f\",\n  \"t5_quantized\": true,\n  \"t5_quant_scheme\": \"int8-q8f\",\n  \"clip_quantized\": true,\n  \"clip_quant_scheme\": \"fp8-sgl\",\n  \"adapter_quantized\": true,\n  \"adapter_quant_scheme\": \"int8-q8f\"\n}\n```\n\nMaps to weight classes in `MM_WEIGHT_REGISTER` [lightx2v/utils/registry_factory.py:22]():\n- `\"Default\"`  `MMWeight` [lightx2v/common/ops/mm/mm_weight.py:149]()\n- `\"int8-q8f\"`  `Q8FINTMMWeight` [lightx2v/common/ops/mm/mm_weight.py:761]()\n- `\"fp8-sgl\"`  `SGLFPMMWeight` [lightx2v/common/ops/mm/mm_weight.py:880]()\n\n**CPU Offloading (Controls `OffloadManager`)**\n```json\n{\n  \"cpu_offload\": false,\n  \"offload_granularity\": \"block\",\n  \"lazy_load\": false,\n  \"t5_cpu_offload\": true,\n  \"clip_cpu_offload\": true,\n  \"vae_cpu_offload\": true,\n  \"audio_encoder_cpu_offload\": true,\n  \"audio_adapter_cpu_offload\": true\n}\n```\n\nControls offloading behavior in `WanModel` [lightx2v/models/networks/wan/model.py:43-98]():\n- `offload_granularity: \"model\"`  Full model swap per step\n- `offload_granularity: \"block\"`  2-block rotation via `offload_block_cuda_buffers`\n- `offload_granularity: \"phase\"`  Attention/FFN phase-level swapping\n- `lazy_load: true`  Enables `lazy_load_path` for on-demand weight loading from disk\n\n**Parallelism Configuration (Sets up `device_mesh`)**\n```json\n{\n  \"parallel\": {\n    \"seq_p_size\": 8,\n    \"seq_p_fp8_comm\": true,\n    \"seq_p_head_parallel\": false,\n    \"seq_p_attn_type\": \"ulysses\"\n  }\n}\n```\n\nProcessed by `set_parallel_config(config)` [lightx2v/utils/set_config.py:105-120]():\n- Creates `device_mesh` via `init_device_mesh(AI_DEVICE, (cfg_p_size, seq_p_size))` [set_config.py:110]()\n- `seq_p_size` must equal `--nproc-per-node` in `torchrun` command\n- Enables sequence parallel attention in `WanTransformerInfer` [lightx2v/models/networks/wan/infer/transformer_infer.py:63-70]()\n- `seq_p_attn_type` selects parallel attention implementation:\n  - `\"ulysses\"`  `UlyssesAttention` [lightx2v/common/ops/attn/ulysses_attn.py:13]()\n  - `\"ring\"`  `RingAttention` with `RingComm` [lightx2v/common/ops/attn/utils/ring_comm.py:7]()\n\n**Attention Operators (Consumed by `ATTN_WEIGHT_REGISTER`)**\n```json\n{\n  \"self_attn_1_type\": \"sage_attn3\",\n  \"cross_attn_1_type\": \"flash_attn3\",\n  \"cross_attn_2_type\": \"flash_attn3\",\n  \"nbhd_attn_setting\": {\n    \"coefficient\": [1.0, 0.25, 0.056]\n  }\n}\n```\n\nMaps to attention classes in `ATTN_WEIGHT_REGISTER` [lightx2v/utils/registry_factory.py:25]():\n- `\"flash_attn2\"`  `FlashAttentionWeights` (Flash Attention 2)\n- `\"flash_attn3\"`  `FlashAttention3Weights` (Flash Attention 3 for Hopper)\n- `\"sage_attn3\"`  `SageAttention3Weights` (Sage Attention 3)\n- `\"nbhd_attn\"`  `NbhdAttnWeights` (Neighborhood Attention) [common/ops/attn/nbhd_attn.py]()\n- `\"radial_attn\"`  `RadialAttnWeights` (Radial Attention) [common/ops/attn/radial_attn.py]()\n\nUsed by `WanTransformerInfer` to instantiate attention in `infer_self_attn()` [lightx2v/models/networks/wan/infer/transformer_infer.py:170-253]()\n\n**Sources:** [configs/seko_talk/seko_talk_25_int8_dist_fp8_comm.json:1-42](), [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:1-75](), [configs/hunyuan_video_15/fp8comm/hy15_i2v_480p_int8_offload_dist_fp8_comm.json:1-24]()\n\n## Distributed Execution with torchrun\n\n### Single-GPU vs Multi-GPU Invocation\n\n**Single GPU:**\n```bash\npython -m lightx2v.infer \\\n--model_cls wan2.1 \\\n--task t2v \\\n--model_path /path/to/model \\\n--config_json config.json \\\n--prompt \"prompt text\" \\\n--save_result_path output.mp4\n```\n\n**Multi-GPU (8 GPUs):**\n```bash\ntorchrun --nproc-per-node 8 -m lightx2v.infer \\\n--model_cls wan2.1 \\\n--task t2v \\\n--model_path /path/to/model \\\n--config_json config.json \\\n--prompt \"prompt text\" \\\n--save_result_path output.mp4\n```\n\n### Distributed Configuration Requirements\n\n**Distributed Setup Flow (Code Entity Map)**\n\n```mermaid\ngraph TB\n    Torchrun[\"torchrun\u003cbr/\u003e--nproc-per-node N\u003cbr/\u003e--master-port 29500\"]\n    \n    subgraph \"Process Group Initialization\"\n        DistInit[\"dist.is_initialized()\u003cbr/\u003etorch.distributed\"]\n        EnvVars[\"RANK\u003cbr/\u003eLOCAL_RANK\u003cbr/\u003eWORLD_SIZE\"]\n        SetParallel[\"set_parallel_config(config)\u003cbr/\u003eset_config.py:105\"]\n        InitMesh[\"init_device_mesh(AI_DEVICE)\u003cbr/\u003eset_config.py:110\"]\n    end\n    \n    subgraph \"Config Validation\"\n        SeqPSize[\"parallel.seq_p_size\u003cbr/\u003e(must equal N)\"]\n        CfgPSize[\"parallel.cfg_p_size\u003cbr/\u003e(optional)\"]\n        Assertion[\"assert cfg_p * seq_p == world_size\u003cbr/\u003eset_config.py:109\"]\n    end\n    \n    subgraph \"Device Mesh Setup\"\n        MeshDims[\"mesh_dim_names=('cfg_p', 'seq_p')\u003cbr/\u003eset_config.py:110\"]\n        GetSeqPGroup[\"device_mesh.get_group('seq_p')\u003cbr/\u003eWanModel.__init__\"]\n        GetCfgPGroup[\"device_mesh.get_group('cfg_p')\u003cbr/\u003eWanModel.infer\"]\n    end\n    \n    subgraph \"Sequence Parallel Execution\"\n        UlyssesAttn[\"UlyssesAttention\u003cbr/\u003eulysses_attn.py:13\"]\n        AllToAll[\"all_to_all_varlen\u003cbr/\u003eulysses_attn.py:138\"]\n        RingAttn[\"RingAttention\u003cbr/\u003ering_attn.py\"]\n        RingComm[\"RingComm.send_recv\u003cbr/\u003ering_comm.py:22\"]\n    end\n    \n    Torchrun --\u003e DistInit\n    DistInit --\u003e EnvVars\n    EnvVars --\u003e SetParallel\n    \n    SetParallel --\u003e SeqPSize\n    SetParallel --\u003e CfgPSize\n    SeqPSize --\u003e Assertion\n    CfgPSize --\u003e Assertion\n    \n    Assertion --\u003e InitMesh\n    InitMesh --\u003e MeshDims\n    MeshDims --\u003e GetSeqPGroup\n    MeshDims --\u003e GetCfgPGroup\n    \n    GetSeqPGroup --\u003e UlyssesAttn\n    GetSeqPGroup --\u003e RingAttn\n    UlyssesAttn --\u003e AllToAll\n    RingAttn --\u003e RingComm\n```\n\n**Key Requirements:**\n- `parallel.seq_p_size` must equal `--nproc-per-node` [set_config.py:109]()\n- If using CFG parallelism: `cfg_p_size * seq_p_size == world_size`\n- `CUDA_VISIBLE_DEVICES` must specify N available GPUs\n- Config sets `config[\"seq_parallel\"] = True` when `seq_p_size \u003e 1` [set_config.py:112-113]()\n\n**Sequence Parallel Attention Types:**\n- `\"ulysses\"`  `UlyssesAttention` using `all_to_all_varlen()` [lightx2v/common/ops/attn/ulysses_attn.py:138-234]()\n- `\"ulysses-4090\"`  Load-balanced variant for RTX 4090 with custom all-to-all\n- `\"ring\"`  `RingAttention` using `RingComm` for peer-to-peer exchange [lightx2v/common/ops/attn/utils/ring_comm.py:7-47]()\n\n**Sources:** [lightx2v/utils/set_config.py:105-120](), [lightx2v/common/ops/attn/ulysses_attn.py:13-234](), [lightx2v/common/ops/attn/utils/ring_comm.py:7-47](), [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh:6-16]()\n\n## Environment Variables\n\n### CUDA Device Configuration\n\n```bash\n# Single GPU\nexport CUDA_VISIBLE_DEVICES=0\n\n# Multiple GPUs (sequential)\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n\n# Specific GPU selection\nexport CUDA_VISIBLE_DEVICES=1,3,5,7\n\n# All available GPUs (default if not set)\n# No export needed\n```\n\n**Behavior:**\n- `torchrun` distributes processes across visible devices in order\n- Device indices are 0-based relative to visible devices\n- `LOCAL_RANK` maps to visible device index\n\n### Additional Environment Variables\n\n| Variable | Purpose | Set By |\n|----------|---------|--------|\n| `RANK` | Global process rank (0 to world_size-1) | torchrun |\n| `LOCAL_RANK` | Local device index on node | torchrun |\n| `WORLD_SIZE` | Total number of processes | torchrun |\n| `MASTER_ADDR` | Master node address for multi-node | User |\n| `MASTER_PORT` | Master node port for multi-node | User |\n\n**Sources:** [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh:6-9](), [lightx2v/common/ops/attn/ulysses_attn.py:52-54]()\n\n## Common Usage Patterns\n\n### Pattern 1: Quick Single-GPU Inference\n\n**Scenario:** Test generation on single GPU with minimal optimization\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0\n\npython -m lightx2v.infer \\\n--model_cls wan2.1 \\\n--task t2v \\\n--model_path /models/Wan2.1-T2V-14B \\\n--config_json configs/wan/wan21_t2v_basic.json \\\n--prompt \"A cat playing piano\" \\\n--save_result_path output.mp4\n```\n\n**Configuration:** Use configs with `cpu_offload: true` for memory efficiency\n\n**Sources:** [docs/EN/source/getting_started/quickstart.md:324-355]()\n\n### Pattern 2: Multi-GPU with Quantization\n\n**Scenario:** 8-GPU distributed inference with INT8 quantization\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n\ntorchrun --nproc-per-node 8 -m lightx2v.infer \\\n--model_cls seko_talk \\\n--task s2v \\\n--model_path /models/SekoTalk-Distill-int8 \\\n--config_json configs/seko_talk/seko_talk_25_int8_dist_fp8_comm.json \\\n--prompt \"A person speaking to camera\" \\\n--image_path reference.png \\\n--audio_path speech.mp3 \\\n--save_result_path output.mp4\n```\n\n**Configuration Requirements:**\n- `parallel.seq_p_size: 8` in JSON\n- `dit_quantized: true` and `dit_quant_scheme: \"int8-q8f\"`\n- `parallel.seq_p_fp8_comm: true` for FP8 communication\n\n**Sources:** [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh:1-22](), [configs/seko_talk/seko_talk_25_int8_dist_fp8_comm.json:1-42]()\n\n### Pattern 3: High-Resolution with Graph Compilation\n\n**Scenario:** Multiple resolutions with compiled graphs for maximum speed\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n\ntorchrun --nproc-per-node 8 -m lightx2v.infer \\\n--model_cls seko_talk \\\n--task s2v \\\n--model_path /models/SekoTalk-Distill-fp8 \\\n--config_json configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json \\\n--prompt \"Dynamic scene\" \\\n--image_path input.png \\\n--audio_path audio.mp3 \\\n--save_result_path output.mp4\n```\n\n**Configuration Features:**\n- `compile: true` enables PyTorch compilation\n- `compile_shapes: [[480,832], [720,1280], ...]` pre-compiles graphs for multiple resolutions\n- `resize_mode: \"adaptive\"` allows dynamic resolution selection\n\n**First Run:** Compilation takes extra time (5-10 minutes)\n**Subsequent Runs:** Use cached compiled graphs for 2-3x speedup\n\n**Sources:** [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:1-75]()\n\n### Pattern 4: Memory-Constrained Setup\n\n**Scenario:** Limited VRAM (12-16GB per GPU)\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0\n\npython -m lightx2v.infer \\\n--model_cls wan2.1 \\\n--task i2v \\\n--model_path /models/Wan2.1-I2V-14B \\\n--config_json configs/wan/wan21_i2v_offload.json \\\n--prompt \"Animation from image\" \\\n--image_path reference.png \\\n--save_result_path output.mp4 \\\n--height 480 \\\n--width 832 \\\n--num_frames 41\n```\n\n**Configuration for Low VRAM:**\n- `cpu_offload: true` and `offload_granularity: \"block\"`\n- `t5_cpu_offload: true`, `clip_cpu_offload: true`, `vae_cpu_offload: true`\n- `dit_quantized: true` with INT8 quantization\n- Reduce resolution and frame count via command-line overrides\n\n**Sources:** [configs/hunyuan_video_15/fp8comm/hy15_i2v_480p_int8_offload_dist_fp8_comm.json:1-24]()\n\n## Windows Batch Script Execution\n\n### Windows-Specific Invocation\n\nWindows uses `.bat` files with similar structure:\n\n```batch\n@echo off\nset lightx2v_path=C:\\path\\to\\LightX2V\nset model_path=C:\\models\\Wan2.1-T2V-14B\n\nset CUDA_VISIBLE_DEVICES=0,1,2,3\n\npython -m lightx2v.infer ^\n--model_cls wan2.1 ^\n--task t2v ^\n--model_path %model_path% ^\n--config_json %lightx2v_path%\\configs\\wan\\config.json ^\n--prompt \"Your prompt\" ^\n--save_result_path %lightx2v_path%\\output.mp4\n```\n\n**Windows Notes:**\n- Line continuation uses `^` instead of `\\`\n- Path separators are `\\` instead of `/`\n- Environment variables use `%VAR%` instead of `$VAR`\n- `torchrun` not required for multi-GPU (PyTorch handles automatically on Windows)\n\n**Sources:** [docs/EN/source/getting_started/quickstart.md:316-321](), [docs/ZH_CN/source/getting_started/quickstart.md:307-312]()\n\n## Troubleshooting\n\n### Common Issues\n\n**Issue:** `seq_p_size` mismatch error\n```\nAssertionError: parallel.seq_p_size (8) must match world_size (4)\n```\n**Solution:** Ensure JSON `parallel.seq_p_size` equals `torchrun --nproc-per-node` value\n\n**Issue:** CUDA out of memory\n**Solution:** \n1. Enable CPU offloading: `cpu_offload: true`\n2. Use quantization: `dit_quantized: true`\n3. Reduce batch size or resolution via command-line overrides\n4. Use tiling VAE: `use_tiling_vae: true`\n\n**Issue:** Attention operator not found\n**Solution:** Verify installation of Flash Attention or SageAttention (see [Installation](#2.1))\n\n**Issue:** Configuration file not found\n**Solution:** Use absolute paths or paths relative to execution directory\n\n**Sources:** [docs/EN/source/getting_started/quickstart.md:145-151](), [lightx2v/common/ops/attn/ulysses_attn.py:52-76]()"])</script><script>self.__next_f.push([1,"24:T6344,"])</script><script>self.__next_f.push([1,"# HTTP Server and Production Deployment\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/common/modules/weight_module.py](lightx2v/common/modules/weight_module.py)\n- [lightx2v/pipeline.py](lightx2v/pipeline.py)\n- [lightx2v/server/__main__.py](lightx2v/server/__main__.py)\n- [lightx2v/server/config.py](lightx2v/server/config.py)\n- [lightx2v/server/schema.py](lightx2v/server/schema.py)\n- [lightx2v/server/services/distributed_utils.py](lightx2v/server/services/distributed_utils.py)\n- [lightx2v/server/services/generation/base.py](lightx2v/server/services/generation/base.py)\n- [lightx2v/server/services/generation/image.py](lightx2v/server/services/generation/image.py)\n- [lightx2v/server/services/inference/worker.py](lightx2v/server/services/inference/worker.py)\n- [scripts/server/post_i2i_with_lora.py](scripts/server/post_i2i_with_lora.py)\n- [scripts/server/start_server_i2i_with_loradir.sh](scripts/server/start_server_i2i_with_loradir.sh)\n\n\u003c/details\u003e\n\n\n\nThis document describes the HTTP server infrastructure for deploying LightX2V models in production environments. The server provides a REST API for task submission, supports distributed multi-GPU inference via `torchrun`, and enables dynamic LoRA switching for customization without reloading models.\n\nFor information about the Python API interface, see [Python API (LightX2VPipeline)](#3.2). For CLI-based inference, see [Command Line Interface](#3.3). For distributed optimization techniques, see [Distributed and Parallel Inference](#6.5).\n\n---\n\n## System Architecture\n\nThe HTTP server is built on a multi-worker architecture where a master process handles HTTP requests and distributes tasks to inference workers via PyTorch distributed communication. This enables efficient utilization of multiple GPUs while maintaining a simple REST API interface.\n\n```mermaid\ngraph TB\n    subgraph \"Client Layer\"\n        HTTPClient[\"HTTP Client\"]\n        RESTRequest[\"POST /v1/tasks/video/\u003cbr/\u003ePOST /v1/tasks/image/\"]\n    end\n    \n    subgraph \"Master Process (Rank 0)\"\n        FastAPI[\"FastAPI Server\u003cbr/\u003elightx2v.server.__main__\"]\n        FileService[\"FileService\u003cbr/\u003efile uploads \u0026 downloads\"]\n        GenService[\"GenerationService\u003cbr/\u003ebase.py, image.py, video.py\"]\n        TaskQueue[\"AsyncIO Task Queue\"]\n    end\n    \n    subgraph \"Distributed Workers\"\n        Worker0[\"TorchrunInferenceWorker\u003cbr/\u003eRank 0\u003cbr/\u003eworker.py\"]\n        Worker1[\"TorchrunInferenceWorker\u003cbr/\u003eRank 1\u003cbr/\u003eworker.py\"]\n        WorkerN[\"TorchrunInferenceWorker\u003cbr/\u003eRank N\u003cbr/\u003eworker.py\"]\n    end\n    \n    subgraph \"Coordination Layer\"\n        DistManager[\"DistributedManager\u003cbr/\u003edistributed_utils.py\"]\n        TaskBroadcast[\"broadcast_task_data()\u003cbr/\u003epickle serialization\"]\n        Barrier[\"barrier()\u003cbr/\u003esynchronization\"]\n    end\n    \n    subgraph \"Inference Engine\"\n        Runner[\"Runner\u003cbr/\u003eRUNNER_REGISTER\"]\n        LoRASwitch[\"switch_lora()\u003cbr/\u003edynamic LoRA loading\"]\n    end\n    \n    HTTPClient --\u003e RESTRequest\n    RESTRequest --\u003e FastAPI\n    FastAPI --\u003e FileService\n    FastAPI --\u003e GenService\n    GenService --\u003e TaskQueue\n    TaskQueue --\u003e Worker0\n    \n    Worker0 --\u003e DistManager\n    Worker1 --\u003e DistManager\n    WorkerN --\u003e DistManager\n    \n    DistManager --\u003e TaskBroadcast\n    DistManager --\u003e Barrier\n    \n    Worker0 --\u003e Runner\n    Worker1 --\u003e Runner\n    WorkerN --\u003e Runner\n    \n    Runner --\u003e LoRASwitch\n```\n\n**Sources:** [lightx2v/server/__main__.py:1-30](), [lightx2v/server/services/inference/worker.py:1-194](), [lightx2v/server/services/distributed_utils.py:1-157](), [lightx2v/server/services/generation/base.py:1-148]()\n\n---\n\n## TorchrunInferenceWorker\n\nThe `TorchrunInferenceWorker` class manages inference execution on each GPU worker process. It initializes model runners, processes incoming tasks, and handles dynamic LoRA loading.\n\n### Worker Initialization\n\n```mermaid\nsequenceDiagram\n    participant Main as \"__main__.py\"\n    participant Worker as \"TorchrunInferenceWorker\"\n    participant DistMgr as \"DistributedManager\"\n    participant Runner as \"Runner\"\n    participant LoRA as \"LoRA Directory\"\n    \n    Main-\u003e\u003eWorker: __init__()\n    Note over Worker: Read LOCAL_RANK, WORLD_SIZE\u003cbr/\u003efrom environment\n    \n    Main-\u003e\u003eWorker: init(args)\n    Worker-\u003e\u003eDistMgr: init_process_group()\n    Note over DistMgr: Set up NCCL backend\u003cbr/\u003eCreate gloo process group\n    \n    Worker-\u003e\u003eWorker: Set lora_dir from args\n    Worker-\u003e\u003eRunner: set_config(args)\n    Worker-\u003e\u003eRunner: init_runner(config)\n    Note over Runner: Load model weights\u003cbr/\u003eInitialize encoders, VAE, scheduler\n    \n    Worker-\u003e\u003eWorker: init_empty_input_info(args.task)\n    \n    alt Rank 0\n        Worker-\u003e\u003eMain: Log \"initialization completed\"\n    end\n    \n    alt Multi-worker\n        Worker-\u003e\u003eDistMgr: barrier()\n        Note over DistMgr: Synchronize all workers\n    end\n```\n\n**Sources:** [lightx2v/server/services/inference/worker.py:16-64](), [lightx2v/server/services/distributed_utils.py:22-50]()\n\n### Key Components\n\n| Component | Type | Purpose |\n|-----------|------|---------|\n| `rank` | `int` | Worker rank from `LOCAL_RANK` environment variable |\n| `world_size` | `int` | Total number of workers from `WORLD_SIZE` |\n| `runner` | `BaseRunner` | Model runner initialized from config |\n| `dist_manager` | `DistributedManager` | Handles distributed communication |\n| `lora_dir` | `Path` | Directory containing LoRA weight files |\n| `current_lora_name` | `str` | Currently loaded LoRA filename |\n| `current_lora_strength` | `float` | Current LoRA strength value |\n\n**Sources:** [lightx2v/server/services/inference/worker.py:17-25]()\n\n---\n\n## Task Processing Flow\n\nTasks submitted via HTTP API are processed through an asynchronous pipeline that handles file uploads, broadcasts task data to workers, executes inference, and returns results.\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant API as \"FastAPI Endpoint\"\n    participant GenSvc as \"GenerationService\"\n    participant FileSvc as \"FileService\"\n    participant Worker0 as \"Worker (Rank 0)\"\n    participant WorkerN as \"Workers (Rank 1..N)\"\n    participant DistMgr as \"DistributedManager\"\n    \n    Client-\u003e\u003eAPI: POST /v1/tasks/video/\u003cbr/\u003e{prompt, image_path, lora_name}\n    API-\u003e\u003eGenSvc: generate_with_stop_event(request)\n    \n    GenSvc-\u003e\u003eGenSvc: Extract task_data from request\n    \n    alt image_path is URL\n        GenSvc-\u003e\u003eFileSvc: download_image(url)\n        FileSvc--\u003e\u003eGenSvc: local_path\n    else image_path is base64\n        GenSvc-\u003e\u003eFileSvc: save_base64_image(data)\n        FileSvc--\u003e\u003eGenSvc: local_path\n    end\n    \n    GenSvc-\u003e\u003eGenSvc: _prepare_output_path()\n    Note over GenSvc: Set save_result_path\u003cbr/\u003ewith auto extension\n    \n    GenSvc-\u003e\u003eWorker0: submit_task_async(task_data)\n    Worker0-\u003e\u003eDistMgr: broadcast_task_data(task_data)\n    Note over DistMgr: Serialize with pickle\u003cbr/\u003eBroadcast in 1MB chunks\n    \n    DistMgr-\u003e\u003eWorkerN: Broadcast task to all workers\n    \n    Worker0-\u003e\u003eWorker0: process_request(task_data)\n    WorkerN-\u003e\u003eWorkerN: process_request(task_data)\n    \n    alt LoRA requested\n        Worker0-\u003e\u003eWorker0: switch_lora(lora_name, strength)\n        WorkerN-\u003e\u003eWorkerN: switch_lora(lora_name, strength)\n    end\n    \n    Worker0-\u003e\u003eWorker0: update_input_info_from_dict()\n    Worker0-\u003e\u003eWorker0: runner.run_pipeline()\n    WorkerN-\u003e\u003eWorkerN: update_input_info_from_dict()\n    WorkerN-\u003e\u003eWorkerN: runner.run_pipeline()\n    \n    Worker0-\u003e\u003eDistMgr: barrier()\n    WorkerN-\u003e\u003eDistMgr: barrier()\n    Note over DistMgr: Synchronize completion\n    \n    Worker0--\u003e\u003eGenSvc: {status: \"success\", save_result_path}\n    GenSvc--\u003e\u003eAPI: TaskResponse\n    API--\u003e\u003eClient: {task_id, task_status, save_result_path}\n```\n\n**Sources:** [lightx2v/server/services/generation/base.py:99-148](), [lightx2v/server/services/inference/worker.py:66-124](), [lightx2v/server/services/distributed_utils.py:120-156]()\n\n### process_request Method\n\nThe `process_request` method is the core task handler executed on each worker:\n\n| Step | Action | Code Reference |\n|------|--------|----------------|\n| 1. Broadcast | Rank 0 broadcasts task_data to all workers | [worker.py:71-72]() |\n| 2. LoRA Handling | Extract `lora_name` and `lora_strength` from task_data | [worker.py:75-76]() |\n| 3. LoRA Switch | Call `switch_lora()` if `lora_dir` is configured | [worker.py:78-79]() |\n| 4. Task Setup | Set `task`, `negative_prompt`, `return_result_tensor` | [worker.py:81-83]() |\n| 5. VFI Config | Handle `target_fps` for video frame interpolation | [worker.py:85-91]() |\n| 6. Input Update | Update `input_info` from task_data | [worker.py:93]() |\n| 7. Runner Config | Update runner configuration | [worker.py:95]() |\n| 8. Inference | Execute `runner.run_pipeline(input_info)` | [worker.py:96]() |\n| 9. Barrier | Synchronize all workers | [worker.py:105-106]() |\n| 10. Response | Rank 0 returns success/failure response | [worker.py:108-124]() |\n\n**Sources:** [lightx2v/server/services/inference/worker.py:66-124]()\n\n---\n\n## Dynamic LoRA Loading\n\nThe server supports dynamic LoRA switching at runtime, allowing different customizations per request without reloading the base model. This is particularly useful for serving multiple customized variants of the same base model.\n\n### LoRA Switch Mechanism\n\n```mermaid\nflowchart TB\n    Request[\"Task Request\u003cbr/\u003elora_name: 'anime.safetensors'\u003cbr/\u003elora_strength: 1.0\"]\n    \n    CheckLoRA{\"LoRA dir\u003cbr/\u003econfigured?\"}\n    \n    CompareLoRA{\"Same LoRA\u003cbr/\u003eand strength?\"}\n    \n    RemoveCurrent[\"Remove Current LoRA\u003cbr/\u003emodel._remove_lora()\"]\n    \n    FindPath[\"Resolve LoRA Path\u003cbr/\u003elora_dir / lora_name\"]\n    \n    CheckExists{\"LoRA file\u003cbr/\u003eexists?\"}\n    \n    ApplyLoRA[\"Apply LoRA Weights\u003cbr/\u003emodel._update_lora(path, strength)\"]\n    \n    UpdateState[\"Update\u003cbr/\u003ecurrent_lora_name\u003cbr/\u003ecurrent_lora_strength\"]\n    \n    RunInference[\"Run Inference\u003cbr/\u003ewith LoRA applied\"]\n    \n    Request --\u003e CheckLoRA\n    CheckLoRA --\u003e|No| RunInference\n    CheckLoRA --\u003e|Yes| CompareLoRA\n    \n    CompareLoRA --\u003e|Same| RunInference\n    CompareLoRA --\u003e|Different| RemoveCurrent\n    \n    RemoveCurrent --\u003e FindPath\n    FindPath --\u003e CheckExists\n    \n    CheckExists --\u003e|Not Found| RunInference\n    CheckExists --\u003e|Found| ApplyLoRA\n    \n    ApplyLoRA --\u003e UpdateState\n    UpdateState --\u003e RunInference\n```\n\n**Sources:** [lightx2v/server/services/inference/worker.py:126-157]()\n\n### switch_lora Implementation\n\nThe `switch_lora` method handles three scenarios:\n\n**1. Remove LoRA (lora_name=None)**\n```python\n# Removes currently applied LoRA weights\nif lora_name is None:\n    if self.current_lora_name is not None:\n        self.runner.model._remove_lora()\n        self.current_lora_name = None\n```\n[worker.py:128-136]()\n\n**2. Apply New LoRA**\n```python\n# Applies LoRA if name or strength changed\nif lora_name != self.current_lora_name or lora_strength != current_strength:\n    lora_path = self._lora_path(lora_name)  # Resolves to lora_dir / lora_name\n    self.runner.model._update_lora(lora_path, lora_strength)\n    self.current_lora_name = lora_name\n    self.current_lora_strength = lora_strength\n```\n[worker.py:140-151]()\n\n**3. Keep Existing LoRA**\nNo action if same LoRA and strength already applied.\n\nThe underlying `_update_lora` and `_remove_lora` methods delegate to the `WeightModule` system:\n- `model.register_lora(weight_dict, strength)` - Register LoRA weights\n- `model.update_lora(weight_dict, strength)` - Update LoRA weights dynamically\n- `model.remove_lora()` - Remove LoRA weights\n\n**Sources:** [lightx2v/server/services/inference/worker.py:126-165](), [lightx2v/common/modules/weight_module.py:35-60]()\n\n---\n\n## API Schema and Request Format\n\nThe server exposes REST endpoints for video and image generation tasks. Request schemas are defined using Pydantic models.\n\n### Request Schema Hierarchy\n\n```mermaid\nclassDiagram\n    class BaseTaskRequest {\n        +str task_id\n        +str prompt\n        +bool use_prompt_enhancer\n        +str negative_prompt\n        +str image_path\n        +str save_result_path\n        +int infer_steps\n        +int seed\n        +list target_shape\n        +Optional[str] lora_name\n        +float lora_strength\n        +get(key, default)\n    }\n    \n    class VideoTaskRequest {\n        +int num_fragments\n        +int target_video_length\n        +str audio_path\n        +int video_duration\n        +Optional[list[TalkObject]] talk_objects\n        +Optional[int] target_fps\n        +Optional[str] resize_mode\n    }\n    \n    class ImageTaskRequest {\n        +str aspect_ratio\n    }\n    \n    class TaskRequest {\n        +int num_fragments\n        +int target_video_length\n        +str audio_path\n        +int video_duration\n        +Optional[list[TalkObject]] talk_objects\n        +str aspect_ratio\n        +Optional[int] target_fps\n    }\n    \n    class TalkObject {\n        +str audio\n        +str mask\n    }\n    \n    BaseTaskRequest \u003c|-- VideoTaskRequest\n    BaseTaskRequest \u003c|-- ImageTaskRequest\n    BaseTaskRequest \u003c|-- TaskRequest\n    TaskRequest *-- TalkObject\n    VideoTaskRequest *-- TalkObject\n```\n\n**Sources:** [lightx2v/server/schema.py:1-77]()\n\n### LoRA Request Parameters\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `lora_name` | `Optional[str]` | `None` | Filename of LoRA in `lora_dir` (e.g., `\"anime.safetensors\"`) |\n| `lora_strength` | `float` | `1.0` | LoRA weight strength (0.0-2.0 typically) |\n\n**Note:** Setting `lora_name=None` or omitting it removes any currently applied LoRA.\n\n**Sources:** [lightx2v/server/schema.py:28-29]()\n\n### Example: Image Generation with LoRA\n\n```python\nimport requests\nimport base64\n\n# Convert image to base64\nwith open(\"input.png\", \"rb\") as f:\n    image_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n\n# Submit request with LoRA\nresponse = requests.post(\n    \"http://localhost:8000/v1/tasks/image/\",\n    json={\n        \"prompt\": \"turn the style to anime\",\n        \"image_path\": image_b64,\n        \"lora_name\": \"anime_style.safetensors\",\n        \"lora_strength\": 1.0,\n        \"seed\": 42,\n        \"infer_steps\": 5\n    }\n)\n\nresult = response.json()\n# {\"task_id\": \"abc123\", \"task_status\": \"completed\", \"save_result_path\": \"output.png\"}\n```\n\n**Sources:** [scripts/server/post_i2i_with_lora.py:1-29]()\n\n---\n\n## Distributed Communication\n\nThe `DistributedManager` handles PyTorch distributed communication between workers, using a dual-process-group architecture for robustness.\n\n### Process Group Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Initialization\"\n        EnvVars[\"Environment Variables\u003cbr/\u003eLOCAL_RANK, WORLD_SIZE\u003cbr/\u003eMASTER_ADDR, MASTER_PORT\"]\n        InitPG[\"dist.init_process_group()\u003cbr/\u003ebackend='nccl'\"]\n        TaskPG[\"dist.new_group()\u003cbr/\u003ebackend='gloo'\u003cbr/\u003etimeout=30 days\"]\n    end\n    \n    subgraph \"NCCL Process Group\"\n        NCCLBarrier[\"barrier()\u003cbr/\u003eFast GPU-to-GPU sync\"]\n        NCCLOps[\"Tensor operations\u003cbr/\u003eAll-reduce, broadcast\"]\n    end\n    \n    subgraph \"Gloo Task Process Group\"\n        GlooPG[\"task_pg\u003cbr/\u003eLong timeout for inference\"]\n        BroadcastTask[\"broadcast_task_data()\u003cbr/\u003ePickle + chunked transfer\"]\n        SignalStop[\"Broadcast stop signal\u003cbr/\u003etensor([1])\"]\n    end\n    \n    EnvVars --\u003e InitPG\n    InitPG --\u003e TaskPG\n    \n    InitPG --\u003e NCCLBarrier\n    InitPG --\u003e NCCLOps\n    \n    TaskPG --\u003e GlooPG\n    GlooPG --\u003e BroadcastTask\n    GlooPG --\u003e SignalStop\n```\n\n**Sources:** [lightx2v/server/services/distributed_utils.py:22-50](), [lightx2v/server/services/distributed_utils.py:120-156]()\n\n### Task Broadcasting Protocol\n\nThe server broadcasts task data from rank 0 to all workers using a chunked protocol to handle large payloads:\n\n**Broadcast Protocol Steps:**\n\n| Step | Rank 0 (Master) | Rank 1..N (Workers) |\n|------|-----------------|---------------------|\n| 1. Signal | Broadcast `tensor([0])` if task, `tensor([1])` if stop | Receive stop signal |\n| 2. Length | Broadcast `tensor([len(pickle_data)])` | Receive data length |\n| 3. Chunks | Broadcast data in 1MB chunks via `_broadcast_byte_chunks()` | Receive chunks via `_receive_byte_chunks()` |\n| 4. Deserialize | - | `pickle.loads(received_bytes)` |\n\n**Chunk Size:** `CHUNK_SIZE = 1024 * 1024` (1MB)\n\n**Sources:** [lightx2v/server/services/distributed_utils.py:87-156]()\n\n---\n\n## Server Configuration\n\nServer behavior is controlled by the `ServerConfig` class, which supports both programmatic and environment variable configuration.\n\n### ServerConfig Parameters\n\n```python\n@dataclass\nclass ServerConfig:\n    host: str = \"0.0.0.0\"                    # Bind address\n    port: int = 8000                          # HTTP port\n    max_queue_size: int = 10                  # Max pending tasks\n    task_timeout: int = 300                   # Task timeout (seconds)\n    task_history_limit: int = 1000            # Max task history\n    http_timeout: int = 30                    # HTTP request timeout\n    http_max_retries: int = 3                 # Max HTTP retries\n    cache_dir: str = \"server_cache\"           # Temp file storage\n    max_upload_size: int = 500 * 1024 * 1024  # 500MB upload limit\n    lora_dir: str = None                      # LoRA directory path\n```\n\n**Sources:** [lightx2v/server/config.py:8-23]()\n\n### Environment Variables\n\n| Variable | Maps To | Example |\n|----------|---------|---------|\n| `LIGHTX2V_HOST` | `host` | `\"0.0.0.0\"` |\n| `LIGHTX2V_PORT` | `port` | `\"8000\"` |\n| `LIGHTX2V_MAX_QUEUE_SIZE` | `max_queue_size` | `\"20\"` |\n| `LIGHTX2V_CACHE_DIR` | `cache_dir` | `\"/tmp/lightx2v\"` |\n\n**Sources:** [lightx2v/server/config.py:26-49]()\n\n---\n\n## Deployment Patterns\n\n### Single-GPU Deployment\n\nFor single-GPU deployment, start the server without `torchrun`:\n\n```bash\n# Set model paths\nexport LIGHTX2V_PATH=/path/to/lightx2v\nexport MODEL_PATH=/path/to/model\nexport LORA_DIR=/path/to/loras\n\n# Set environment\nexport CUDA_VISIBLE_DEVICES=0\nsource ${LIGHTX2V_PATH}/scripts/base/base.sh\n\n# Start server\npython -m lightx2v.server \\\n  --model_cls qwen_image \\\n  --task i2i \\\n  --model_path $MODEL_PATH \\\n  --lora_dir $LORA_DIR \\\n  --config_json ${LIGHTX2V_PATH}/configs/qwen_image/qwen_image_i2i_2511.json \\\n  --port 8000\n```\n\n**Sources:** [scripts/server/start_server_i2i_with_loradir.sh:1-23]()\n\n### Multi-GPU Deployment with torchrun\n\nFor multi-GPU deployment with distributed inference:\n\n```bash\n# Set visible GPUs\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\n\n# Launch with torchrun\ntorchrun \\\n  --nproc_per_node=4 \\\n  --master_addr=localhost \\\n  --master_port=29500 \\\n  -m lightx2v.server \\\n  --model_cls wan2.2_audio \\\n  --task s2v \\\n  --model_path $MODEL_PATH \\\n  --lora_dir $LORA_DIR \\\n  --config_json $CONFIG_JSON \\\n  --port 8000 \\\n  --cfg_p_size 2 \\\n  --seq_p_size 2\n```\n\n**Key torchrun Arguments:**\n\n| Argument | Purpose |\n|----------|---------|\n| `--nproc_per_node` | Number of GPU workers per node |\n| `--master_addr` | Coordinator address (usually `localhost`) |\n| `--master_port` | Coordinator port for distributed init |\n\n**Parallel Configuration:**\n\n| Argument | Purpose | See Also |\n|----------|---------|----------|\n| `--cfg_p_size` | CFG parallelism degree | [Distributed and Parallel Inference](#6.5) |\n| `--seq_p_size` | Sequence parallelism degree | [Distributed and Parallel Inference](#6.5) |\n| `--seq_p_attn_type` | Sequence parallel type (`ulysses`, `ring`) | [Distributed and Parallel Inference](#6.5) |\n\n**Sources:** [lightx2v/server/services/inference/worker.py:27-36](), [lightx2v/pipeline.py:392-397]()\n\n---\n\n## LoRA Directory Serving\n\nThe server can serve LoRA weights from a configured directory, enabling per-request customization without model reloading.\n\n### Directory Structure\n\n```\nlora_dir/\n anime_style.safetensors\n photorealistic.safetensors\n sketch_style.safetensors\n watercolor.safetensors\n```\n\n### LoRA Path Resolution\n\n```mermaid\nflowchart LR\n    Request[\"Request\u003cbr/\u003elora_name='anime_style.safetensors'\"]\n    \n    Worker[\"TorchrunInferenceWorker\"]\n    \n    LoRADir[\"lora_dir Path\u003cbr/\u003e/data/loras\"]\n    \n    Resolve[\"Resolve Path\u003cbr/\u003elora_dir / lora_name\"]\n    \n    CheckExists{\"File\u003cbr/\u003eexists?\"}\n    \n    Apply[\"Apply LoRA\u003cbr/\u003e_update_lora(path, strength)\"]\n    \n    Skip[\"Skip LoRA\u003cbr/\u003eLog warning\"]\n    \n    Request --\u003e Worker\n    Worker --\u003e LoRADir\n    LoRADir --\u003e Resolve\n    Resolve --\u003e CheckExists\n    CheckExists --\u003e|Yes| Apply\n    CheckExists --\u003e|No| Skip\n```\n\n**Implementation:**\n\n```python\ndef _lora_path(self, lora_name: str) -\u003e str:\n    if not self.lora_dir:\n        return None\n    lora_file = self.lora_dir / lora_name\n    if lora_file.exists():\n        return str(lora_file)\n    return None\n```\n\n**Sources:** [lightx2v/server/services/inference/worker.py:159-165](), [lightx2v/server/services/inference/worker.py:38-45]()\n\n### Server Initialization with LoRA Directory\n\n```bash\npython -m lightx2v.server \\\n  --model_path /path/to/model \\\n  --model_cls qwen_image \\\n  --lora_dir /data/loras \\\n  --port 8000\n```\n\nCommand line arguments are parsed in [lightx2v/server/__main__.py:6-25]():\n- `--lora_dir`: Sets the LoRA directory path\n- Passed to worker via `args.lora_dir`\n- Worker validates directory existence on init\n\n**Sources:** [lightx2v/server/__main__.py:11](), [lightx2v/server/services/inference/worker.py:38-45]()\n\n---\n\n## Worker Loop Architecture\n\nNon-master workers (rank \u003e 0) run a continuous loop waiting for task broadcasts from the master:\n\n```mermaid\nstateDiagram-v2\n    [*] --\u003e Initialize\n    Initialize --\u003e WaitForTask: Start worker_loop()\n    \n    WaitForTask --\u003e ReceiveBroadcast: broadcast_task_data()\n    \n    ReceiveBroadcast --\u003e CheckStopSignal\n    \n    CheckStopSignal --\u003e Exit: Signal is 1 (stop)\n    CheckStopSignal --\u003e ProcessTask: Signal is 0 (task)\n    \n    ProcessTask --\u003e RunInference: process_request(task_data)\n    RunInference --\u003e Barrier: Synchronize workers\n    Barrier --\u003e WaitForTask: Continue loop\n    \n    Exit --\u003e [*]\n    \n    note right of WaitForTask\n        Workers block on broadcast\n        waiting for master to send\n        next task or stop signal\n    end note\n    \n    note right of ProcessTask\n        Each worker processes\n        same task data with\n        different distributed role\n    end note\n```\n\n**Worker Loop Implementation:**\n\n```python\nasync def worker_loop(self):\n    while True:\n        task_data = self.dist_manager.broadcast_task_data()  # Blocks until broadcast\n        if task_data is None:\n            logger.info(f\"Rank {self.rank} received stop signal\")\n            break\n        await self.process_request(task_data)\n```\n\n**Sources:** [lightx2v/server/services/inference/worker.py:167-191]()\n\n**Error Handling:**\n- Connection errors (peer closed/reset) trigger graceful shutdown\n- Task errors are caught, logged, and workers continue\n- Barrier failures trigger worker exit to prevent deadlock\n\n---\n\n## File Processing and Upload Handling\n\nThe server handles three types of input file references: URLs, base64-encoded data, and local paths.\n\n### File Processing Pipeline\n\n```mermaid\nflowchart TB\n    Input[\"Input Field\u003cbr/\u003eimage_path / audio_path\"]\n    \n    CheckType{\"Input Type?\"}\n    \n    URL[\"HTTP/HTTPS URL\"]\n    Base64[\"Base64 Data\"]\n    LocalPath[\"Local File Path\"]\n    \n    Download[\"FileService.download_image/audio()\u003cbr/\u003eHTTP GET with retries\"]\n    \n    DecodeBase64[\"Decode base64\u003cbr/\u003eValidate format\"]\n    \n    SaveToCache[\"Save to cache_dir/\u003cbr/\u003eUUID filename\"]\n    \n    UseDirectly[\"Use path directly\"]\n    \n    UpdateTaskData[\"Update task_data\u003cbr/\u003ewith resolved path\"]\n    \n    Input --\u003e CheckType\n    \n    CheckType --\u003e|\"startswith('http')\"| URL\n    CheckType --\u003e|\"is_base64_*\"| Base64\n    CheckType --\u003e|Otherwise| LocalPath\n    \n    URL --\u003e Download\n    Base64 --\u003e DecodeBase64\n    LocalPath --\u003e UseDirectly\n    \n    Download --\u003e SaveToCache\n    DecodeBase64 --\u003e SaveToCache\n    \n    SaveToCache --\u003e UpdateTaskData\n    UseDirectly --\u003e UpdateTaskData\n```\n\n**Processing Methods:**\n\n| Input Type | Detection | Processing Method | Output |\n|------------|-----------|-------------------|--------|\n| URL | `path.startswith(\"http\")` | `FileService.download_image/audio()` | Cached local path |\n| Base64 | `is_base64_image/audio(data)` | `save_base64_image/audio(data, dir)` | Saved local path |\n| Local Path | Default | No processing | Original path |\n\n**Sources:** [lightx2v/server/services/generation/base.py:33-57]()\n\n---\n\n## Summary\n\nThe LightX2V HTTP server provides production-ready deployment with the following key features:\n\n| Feature | Implementation | Benefit |\n|---------|----------------|---------|\n| **Distributed Inference** | `TorchrunInferenceWorker` + `DistributedManager` | Multi-GPU utilization with single API endpoint |\n| **Dynamic LoRA** | `switch_lora()` with directory serving | Per-request customization without reloading |\n| **Task Broadcasting** | Pickle serialization with chunked transfer | Reliable large payload distribution |\n| **File Handling** | URL download + base64 decoding | Flexible input format support |\n| **Error Recovery** | Barrier synchronization + graceful shutdown | Robust multi-worker coordination |\n| **Configuration** | Environment variables + CLI args | Flexible deployment configuration |\n\nThe architecture enables scaling from single-GPU development environments to multi-GPU production deployments using the same codebase and API interface.\n\n**Sources:** [lightx2v/server/services/inference/worker.py:1-194](), [lightx2v/server/services/distributed_utils.py:1-157](), [lightx2v/server/services/generation/base.py:1-148](), [lightx2v/server/config.py:1-66]()"])</script><script>self.__next_f.push([1,"25:T52a0,"])</script><script>self.__next_f.push([1,"# ComfyUI Integration\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [app/README.md](app/README.md)\n- [app/gradio_demo.py](app/gradio_demo.py)\n- [app/run_gradio.sh](app/run_gradio.sh)\n- [app/run_gradio_win.bat](app/run_gradio_win.bat)\n- [docs/EN/source/deploy_guides/deploy_gradio.md](docs/EN/source/deploy_guides/deploy_gradio.md)\n- [docs/EN/source/deploy_guides/deploy_local_windows.md](docs/EN/source/deploy_guides/deploy_local_windows.md)\n- [docs/ZH_CN/source/deploy_guides/deploy_gradio.md](docs/ZH_CN/source/deploy_guides/deploy_gradio.md)\n- [docs/ZH_CN/source/deploy_guides/deploy_local_windows.md](docs/ZH_CN/source/deploy_guides/deploy_local_windows.md)\n- [lightx2v/common/offload/manager.py](lightx2v/common/offload/manager.py)\n- [scripts/win/run_wan_i2v.bat](scripts/win/run_wan_i2v.bat)\n- [scripts/win/run_wan_t2v.bat](scripts/win/run_wan_t2v.bat)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the ComfyUI integration for LightX2V, which provides a node-based graphical workflow interface for video and image generation tasks. ComfyUI integration wraps the core [LightX2VPipeline](#3.2) functionality in a visual node editor, enabling users to construct complex generation workflows through drag-and-drop node connections. This interface is particularly suited for advanced users who require fine-grained control over generation parameters and multi-stage workflows.\n\nFor simpler web-based interfaces, see [Gradio Web Interface](#3.1). For programmatic access, see [Python API (LightX2VPipeline)](#3.2). For production deployments, see [HTTP Server and Production Deployment](#3.4).\n\n---\n\n## ComfyUI Architecture Overview\n\nComfyUI is a node-based workflow system where each node represents a discrete operation (e.g., text encoding, image generation, VAE decoding). LightX2V's ComfyUI integration exposes its capabilities as custom nodes that can be connected in workflows.\n\n### Integration with LightX2V Core\n\n```mermaid\ngraph TB\n    subgraph \"ComfyUI Frontend Layer\"\n        Browser[\"Web Browser\u003cbr/\u003eNode Editor UI\"]\n        ComfyUIServer[\"ComfyUI Server\u003cbr/\u003eFlask/WebSocket\"]\n    end\n    \n    subgraph \"LightX2V Custom Nodes\"\n        LoaderNode[\"Model Loader Node\u003cbr/\u003eInitializes Pipeline\"]\n        ConfigNode[\"Config Node\u003cbr/\u003eSets parameters\"]\n        PromptNode[\"Prompt Node\u003cbr/\u003eText input\"]\n        GenerateNode[\"Generate Node\u003cbr/\u003eExecutes inference\"]\n        SaveNode[\"Save Node\u003cbr/\u003eOutput to disk\"]\n    end\n    \n    subgraph \"LightX2V Core\"\n        Pipeline[\"LightX2VPipeline\"]\n        Runner[\"Model Runners\u003cbr/\u003eRUNNER_REGISTER\"]\n        Components[\"Components\u003cbr/\u003eVAE, Encoders, Schedulers\"]\n    end\n    \n    subgraph \"Hardware\"\n        GPU[\"GPU Backend\u003cbr/\u003eCUDA/MLU/NPU/ROCm\"]\n    end\n    \n    Browser --\u003e ComfyUIServer\n    ComfyUIServer --\u003e LoaderNode\n    ComfyUIServer --\u003e ConfigNode\n    ComfyUIServer --\u003e PromptNode\n    ComfyUIServer --\u003e GenerateNode\n    ComfyUIServer --\u003e SaveNode\n    \n    LoaderNode --\u003e Pipeline\n    ConfigNode --\u003e Pipeline\n    PromptNode --\u003e GenerateNode\n    GenerateNode --\u003e Pipeline\n    \n    Pipeline --\u003e Runner\n    Runner --\u003e Components\n    Components --\u003e GPU\n    \n    SaveNode --\u003e GenerateNode\n    \n    style Pipeline fill:#e1f5ff,stroke:#333,stroke-width:3px\n    style ComfyUIServer fill:#ffe1e1,stroke:#333,stroke-width:2px\n    style GenerateNode fill:#e1ffe1,stroke:#333,stroke-width:2px\n```\n\n**Diagram: ComfyUI Integration Architecture**\n\nThe integration follows a three-layer architecture:\n\n1. **Frontend Layer**: Web-based node editor where users visually construct workflows by connecting nodes\n2. **Custom Nodes Layer**: LightX2V-specific nodes that expose model loading, configuration, and generation capabilities\n3. **Core Layer**: The underlying `LightX2VPipeline` that handles actual inference execution\n\nSources: [README.md:234-248]()\n\n---\n\n## Node-Based Workflow System\n\n### Core Node Types\n\nComfyUI workflows for LightX2V typically consist of these node categories:\n\n| Node Category | Purpose | Example Operations |\n|--------------|---------|-------------------|\n| **Model Loader** | Initialize `LightX2VPipeline` with model path and configuration | Load Wan2.2, HunyuanVideo-1.5, Qwen-Image models |\n| **Configuration** | Set generation parameters (resolution, steps, guidance) | `create_generator()`, `enable_offload()`, `enable_quantization()` |\n| **Input Nodes** | Provide prompts, images, audio for conditioning | Text prompts, reference images, audio files |\n| **Generation Nodes** | Execute inference via `pipeline.generate()` | T2V, I2V, T2I, I2I generation |\n| **Post-Processing** | Apply frame interpolation, upscaling, format conversion | RIFE interpolation, video encoding |\n| **Output Nodes** | Save results to disk or stream to preview | Save MP4, preview frames |\n\n### Workflow Execution Flow\n\n```mermaid\nsequenceDiagram\n    participant User as User Browser\n    participant Server as ComfyUI Server\n    participant Loader as Model Loader Node\n    participant Config as Config Node\n    participant Gen as Generate Node\n    participant Pipeline as LightX2VPipeline\n    participant Save as Save Node\n    \n    User-\u003e\u003eServer: Queue workflow execution\n    Server-\u003e\u003eLoader: Execute model loading\n    Loader-\u003e\u003ePipeline: LightX2VPipeline(model_path, model_cls, task)\n    Pipeline--\u003e\u003eLoader: Initialized pipeline\n    \n    Server-\u003e\u003eConfig: Execute configuration\n    Config-\u003e\u003ePipeline: enable_offload(cpu_offload=True)\n    Config-\u003e\u003ePipeline: create_generator(attn_mode, infer_steps, ...)\n    Pipeline--\u003e\u003eConfig: Generator configured\n    \n    Server-\u003e\u003eGen: Execute generation\n    Gen-\u003e\u003ePipeline: generate(seed, prompt, image_path, ...)\n    \n    Pipeline-\u003e\u003ePipeline: Run inference loop\n    Note over Pipeline: See Three-Stage Pipeline (#4.2)\n    \n    Pipeline--\u003e\u003eGen: Generated video/image\n    \n    Server-\u003e\u003eSave: Execute save\n    Save-\u003e\u003eSave: Write to disk\n    Save--\u003e\u003eServer: File path\n    \n    Server--\u003e\u003eUser: Display result + preview\n```\n\n**Diagram: ComfyUI Workflow Execution Sequence**\n\nSources: [README.md:136-193]()\n\n---\n\n## Custom Node Implementation Pattern\n\nWhile the specific ComfyUI node implementation files are not included in this documentation, the integration follows standard ComfyUI node patterns:\n\n### Model Loader Node Structure\n\nThe model loader node wraps `LightX2VPipeline` initialization:\n\n```python\n# Conceptual structure (not actual code)\nclass LightX2VModelLoader:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"model_path\": (\"STRING\", {}),\n                \"model_cls\": ([\"wan2.1\", \"wan2.2_moe\", \"hunyuan_video_1.5\", \n                              \"qwen_image\", \"ltx2\"], {}),\n                \"task\": ([\"t2v\", \"i2v\", \"a2v\", \"t2i\", \"i2i\"], {}),\n            }\n        }\n    \n    RETURN_TYPES = (\"LIGHTX2V_PIPELINE\",)\n    \n    def load_model(self, model_path, model_cls, task):\n        # Wraps LightX2VPipeline initialization\n        pipeline = LightX2VPipeline(\n            model_path=model_path,\n            model_cls=model_cls,\n            task=task\n        )\n        return (pipeline,)\n```\n\nThis pattern exposes the same parameters as [LightX2VPipeline.__init__()](#3.2) but through ComfyUI's node interface.\n\n### Generation Node Structure\n\nThe generation node wraps `pipeline.generate()`:\n\n```python\n# Conceptual structure (not actual code)\nclass LightX2VGenerate:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"pipeline\": (\"LIGHTX2V_PIPELINE\",),\n                \"prompt\": (\"STRING\", {\"multiline\": True}),\n                \"seed\": (\"INT\", {\"default\": 42}),\n                \"save_path\": (\"STRING\", {}),\n            },\n            \"optional\": {\n                \"image_path\": (\"STRING\", {}),\n                \"audio_path\": (\"STRING\", {}),\n                \"negative_prompt\": (\"STRING\", {\"multiline\": True}),\n            }\n        }\n    \n    RETURN_TYPES = (\"STRING\",)  # Returns path to generated video\n    \n    def generate(self, pipeline, prompt, seed, save_path, **kwargs):\n        # Wraps pipeline.generate()\n        result_path = pipeline.generate(\n            seed=seed,\n            prompt=prompt,\n            save_result_path=save_path,\n            **kwargs\n        )\n        return (result_path,)\n```\n\nSources: [README.md:136-193]()\n\n---\n\n## Supported Model Workflows\n\n### Text-to-Video Workflow\n\nA typical T2V workflow connects nodes in this sequence:\n\n```mermaid\ngraph LR\n    ModelLoad[\"Model Loader\u003cbr/\u003emodel_cls=wan2.2_moe\u003cbr/\u003etask=t2v\"]\n    OffloadConfig[\"Offload Config\u003cbr/\u003eenable_offload()\u003cbr/\u003egranularity=block\"]\n    GenConfig[\"Generator Config\u003cbr/\u003ecreate_generator()\u003cbr/\u003einfer_steps=40\"]\n    PromptInput[\"Prompt Input\u003cbr/\u003etext string\"]\n    SeedInput[\"Seed Input\u003cbr/\u003einteger\"]\n    Generate[\"Generate Node\u003cbr/\u003epipeline.generate()\"]\n    SaveVideo[\"Save Video\u003cbr/\u003eoutput.mp4\"]\n    \n    ModelLoad --\u003e OffloadConfig\n    OffloadConfig --\u003e GenConfig\n    GenConfig --\u003e Generate\n    PromptInput --\u003e Generate\n    SeedInput --\u003e Generate\n    Generate --\u003e SaveVideo\n    \n    style Generate fill:#e1ffe1,stroke:#333,stroke-width:2px\n```\n\n**Diagram: Text-to-Video Node Workflow**\n\n### Image-to-Video Workflow\n\nI2V workflows add an image input node:\n\n```mermaid\ngraph LR\n    ModelLoad[\"Model Loader\u003cbr/\u003emodel_cls=wan2.2_moe\u003cbr/\u003etask=i2v\"]\n    ImageInput[\"Image Input\u003cbr/\u003ereference image\"]\n    PromptInput[\"Prompt Input\u003cbr/\u003etext description\"]\n    GenConfig[\"Generator Config\u003cbr/\u003eheight=720\u003cbr/\u003ewidth=1280\"]\n    Generate[\"Generate Node\u003cbr/\u003eimage_path + prompt\"]\n    SaveVideo[\"Save Video\u003cbr/\u003eoutput.mp4\"]\n    \n    ModelLoad --\u003e GenConfig\n    ImageInput --\u003e Generate\n    PromptInput --\u003e Generate\n    GenConfig --\u003e Generate\n    Generate --\u003e SaveVideo\n    \n    style Generate fill:#e1ffe1,stroke:#333,stroke-width:2px\n```\n\n**Diagram: Image-to-Video Node Workflow**\n\n### Audio-to-Video Workflow\n\nA2V workflows for talking head generation add audio input and person masking:\n\n```mermaid\ngraph LR\n    ModelLoad[\"Model Loader\u003cbr/\u003emodel_cls=wan2.2_audio\u003cbr/\u003etask=a2v\"]\n    AudioInput[\"Audio Input\u003cbr/\u003eaudio file path\"]\n    ImageInput[\"Image Input\u003cbr/\u003ereference face\"]\n    PersonMask[\"Person Mask\u003cbr/\u003eOptional masking\"]\n    GenConfig[\"Generator Config\u003cbr/\u003enum_frames=auto\"]\n    Generate[\"Generate Node\u003cbr/\u003eaudio + image\"]\n    SaveVideo[\"Save Video\u003cbr/\u003eoutput.mp4\"]\n    \n    ModelLoad --\u003e GenConfig\n    AudioInput --\u003e Generate\n    ImageInput --\u003e Generate\n    PersonMask --\u003e Generate\n    GenConfig --\u003e Generate\n    Generate --\u003e SaveVideo\n    \n    style Generate fill:#e1ffe1,stroke:#333,stroke-width:2px\n```\n\n**Diagram: Audio-to-Video Node Workflow**\n\nSources: [README.md:204-229]()\n\n---\n\n## Configuration and Optimization Nodes\n\n### Offloading Configuration\n\nComfyUI nodes expose the offloading system for memory management:\n\n| Parameter | Node Setting | Effect |\n|-----------|--------------|--------|\n| `cpu_offload` | Boolean toggle | Enable/disable CPU offloading |\n| `offload_granularity` | Dropdown: \"model\", \"block\", \"phase\" | Set offload granularity |\n| `text_encoder_offload` | Boolean toggle | Offload text encoders |\n| `vae_offload` | Boolean toggle | Offload VAE decoder |\n\nThese wrap the [enable_offload()](#6.3) method.\n\n### Quantization Configuration\n\nNodes expose quantization options:\n\n| Parameter | Node Setting | Mapped To |\n|-----------|--------------|-----------|\n| `quantize_transformer` | Boolean + format dropdown | `enable_quantization()` |\n| `quant_format` | \"int8\", \"fp8\", \"nvfp4\" | `quant_format` parameter |\n| `quantize_text_encoder` | Boolean | `quantize_text_encoder` parameter |\n\n### Attention Mode Selection\n\nA dropdown node exposes attention operators:\n\n- `sage_attn2` - SageAttention for 2x throughput\n- `flash_attn2` - Flash Attention 2\n- `flash_attn3` - Flash Attention 3 (Hopper)\n- `nbhd_attn` - Neighborhood attention\n- `vanilla` - Standard attention\n\nThis maps to the `attn_mode` parameter in [create_generator()](#3.2).\n\n### Parallel Processing Nodes\n\nNodes for distributed inference configuration:\n\n| Node | Configuration | Effect |\n|------|--------------|--------|\n| CFG Parallel Node | `cfg_p_group` size | Enable CFG parallelism across GPUs |\n| Sequence Parallel Node | `seq_p_size` | Enable Ulysses/Ring sequence parallelism |\n\nThese are typically set before workflow execution via environment variables or server launch configuration.\n\nSources: [README.md:250-268]()\n\n---\n\n## Deployment Methods\n\n### Local ComfyUI Deployment\n\nComfyUI runs as a standalone server that loads LightX2V custom nodes:\n\n```mermaid\ngraph TB\n    subgraph \"Installation\"\n        Clone[\"Clone ComfyUI\"]\n        InstallNodes[\"Install LightX2V custom nodes\u003cbr/\u003ein custom_nodes/\"]\n        InstallDeps[\"Install LightX2V dependencies\"]\n    end\n    \n    subgraph \"Execution\"\n        LaunchServer[\"Launch ComfyUI server\u003cbr/\u003epython main.py\"]\n        Browser[\"Access http://localhost:8188\"]\n        LoadWorkflow[\"Load/Create workflow\"]\n        Execute[\"Execute workflow\"]\n    end\n    \n    subgraph \"Backend\"\n        LightX2V[\"LightX2V Pipeline\"]\n        Models[\"Model Weights\u003cbr/\u003eon disk/HuggingFace\"]\n    end\n    \n    Clone --\u003e InstallNodes\n    InstallNodes --\u003e InstallDeps\n    InstallDeps --\u003e LaunchServer\n    LaunchServer --\u003e Browser\n    Browser --\u003e LoadWorkflow\n    LoadWorkflow --\u003e Execute\n    \n    Execute --\u003e LightX2V\n    LightX2V --\u003e Models\n    \n    style LaunchServer fill:#e1f5ff,stroke:#333,stroke-width:2px\n```\n\n**Diagram: ComfyUI Local Deployment Flow**\n\n### Docker Deployment\n\nThe recommended deployment uses Docker containers:\n\n```bash\n# Pull LightX2V Docker image with ComfyUI\ndocker pull lightx2v/lightx2v-comfyui:latest\n\n# Run container with GPU access\ndocker run --gpus all -p 8188:8188 -v /path/to/models:/models \\\n  lightx2v/lightx2v-comfyui:latest\n```\n\nThe Docker image includes:\n- ComfyUI server pre-installed\n- LightX2V custom nodes configured\n- All dependencies (Flash Attention, Sage Attention, quantization kernels)\n- Model path configuration\n\nSources: [README.md:234-248]()\n\n---\n\n## Model Organization for ComfyUI\n\nComfyUI expects models organized in a specific directory structure:\n\n```\nmodels/\n checkpoints/\n    Wan2.2-I2V-A14B/\n       DIT/\n       text_encoder/\n       image_encoder/\n       vae/\n    HunyuanVideo-1.5/\n    Qwen-Image/\n loras/\n    Wan2.1-Distill-Loras/\n    custom_loras/\n vae/\n     lightvae.safetensors\n     lighttaehy1_5.safetensors\n```\n\nThe Model Loader node uses `model_path` to point to these checkpoint directories, matching the structure expected by [LightX2VPipeline](#3.2).\n\nSources: [README.md:204-229]()\n\n---\n\n## Workflow Saving and Sharing\n\n### Workflow JSON Format\n\nComfyUI workflows are saved as JSON files containing:\n\n```json\n{\n  \"nodes\": [\n    {\n      \"id\": 1,\n      \"type\": \"LightX2VModelLoader\",\n      \"params\": {\n        \"model_path\": \"/models/checkpoints/Wan2.2-I2V-A14B\",\n        \"model_cls\": \"wan2.2_moe\",\n        \"task\": \"i2v\"\n      }\n    },\n    {\n      \"id\": 2,\n      \"type\": \"LightX2VGenerate\",\n      \"inputs\": {\n        \"pipeline\": [\"1\", 0],\n        \"prompt\": \"...\"\n      }\n    }\n  ],\n  \"edges\": [...]\n}\n```\n\nUsers can share workflows by distributing these JSON files.\n\n### Pre-built Workflow Templates\n\nCommon workflow templates include:\n\n| Template | Task | Nodes Included |\n|----------|------|----------------|\n| **basic_t2v.json** | Text-to-Video | Model Loader  Config  Prompt  Generate  Save |\n| **i2v_with_offload.json** | Image-to-Video with memory optimization | Loader  Offload Config  Image Input  Generate |\n| **distill_4step.json** | 4-step distilled inference | Distilled model loader  Simplified config  Generate |\n| **quantized_inference.json** | INT8/FP8 quantized inference | Loader  Quantization Config  Generate |\n| **multi_lora.json** | Dynamic LoRA switching | Loader  LoRA Config  Multiple Generate nodes |\n\n---\n\n## Advanced Features in ComfyUI\n\n### Dynamic LoRA Switching\n\nComfyUI nodes can expose the [switch_lora()](#6.6) functionality:\n\n```mermaid\ngraph LR\n    Pipeline[\"Initialized Pipeline\"]\n    LoRA1[\"LoRA Config Node 1\u003cbr/\u003estyle_lora.safetensors\"]\n    Gen1[\"Generate 1\u003cbr/\u003ewith LoRA 1\"]\n    LoRA2[\"LoRA Config Node 2\u003cbr/\u003equality_lora.safetensors\"]\n    Gen2[\"Generate 2\u003cbr/\u003ewith LoRA 2\"]\n    \n    Pipeline --\u003e LoRA1\n    LoRA1 --\u003e Gen1\n    Gen1 --\u003e LoRA2\n    LoRA2 --\u003e Gen2\n    \n    style LoRA1 fill:#ffe1e1,stroke:#333,stroke-width:2px\n    style LoRA2 fill:#ffe1e1,stroke:#333,stroke-width:2px\n```\n\n**Diagram: Dynamic LoRA Switching in Workflow**\n\nThis enables batch generation with different LoRAs without reloading the base model.\n\n### Caching and Streaming\n\nNodes can enable [feature caching](#6.4) for iterative workflows:\n\n| Cache Type | Node Configuration | Use Case |\n|-----------|-------------------|----------|\n| **TeaCache** | Enable via config node | Skip redundant transformer blocks |\n| **MagCache** | Enable via config node | Reuse features across similar prompts |\n| **VAE Cache** | `CACHE_T=2` parameter | Cache temporal VAE features |\n\n### Frame Interpolation Pipeline\n\nPost-processing nodes can apply RIFE frame interpolation:\n\n```mermaid\ngraph LR\n    Generate[\"Generate Node\u003cbr/\u003e81 frames @ 24fps\"]\n    RIFE[\"RIFE Interpolation\u003cbr/\u003e2x or 4x\"]\n    Upscale[\"Optional Upscale\"]\n    Encode[\"Video Encoder\u003cbr/\u003eH.264/H.265\"]\n    Save[\"Save Final Video\u003cbr/\u003e60fps output\"]\n    \n    Generate --\u003e RIFE\n    RIFE --\u003e Upscale\n    Upscale --\u003e Encode\n    Encode --\u003e Save\n    \n    style RIFE fill:#e1ffe1,stroke:#333,stroke-width:2px\n```\n\n**Diagram: Frame Interpolation Post-Processing Pipeline**\n\nSources: [README.md:266-281]()\n\n---\n\n## Performance Considerations\n\n### Memory Management in Workflows\n\nComfyUI workflows should configure offloading based on available VRAM:\n\n| VRAM | Recommended Configuration | Workflow Nodes |\n|------|--------------------------|----------------|\n| **80GB (H100)** | No offload, full precision | Standard config nodes |\n| **40GB (A100)** | Block offload or FP8 quantization | Offload config + optional quantization |\n| **24GB (RTX 4090)** | Block offload + INT8 quantization | Offload config + quantization config |\n| **16GB (RTX 4080)** | Phase offload + INT8 | Aggressive offload config |\n| **8GB (Consumer)** | Lazy load + INT8 | Maximum offload + quantization |\n\n### Workflow Execution Time\n\nTypical execution times for ComfyUI workflows:\n\n| Workflow Type | Hardware | Configuration | Time per Generation |\n|--------------|----------|---------------|---------------------|\n| T2V 480p 81 frames | 8x H100 | FP8 + CFG parallel | ~14 seconds (0.35s/step  40 steps) |\n| I2V 720p 81 frames | 1x RTX 4090 | INT8 + block offload | ~94 seconds (2.35s/step  40 steps) |\n| T2I 1024x1024 | 1x H100 | 4-step distilled | ~1-2 seconds |\n| A2V talking head | 1x RTX 4090 | Block offload | ~60 seconds per 5-second segment |\n\nThese times match the performance characteristics detailed in [Performance Optimization](#6).\n\nSources: [README.md:68-109]()\n\n---\n\n## Integration with Other Tools\n\n### Export to Python API\n\nComfyUI workflows can be converted to Python API calls by examining the workflow JSON and translating node configurations to `LightX2VPipeline` method calls. This enables users to prototype in ComfyUI then deploy via [Python API](#3.2) or [HTTP Server](#3.4).\n\n### Batch Processing\n\nComfyUI's queue system can process multiple prompts/inputs sequentially:\n\n```mermaid\ngraph TB\n    Queue[\"Workflow Queue\"]\n    Exec[\"Executor Thread\"]\n    Pipeline[\"Single LightX2VPipeline\u003cbr/\u003einstance\"]\n    \n    Input1[\"Input 1\"] --\u003e Queue\n    Input2[\"Input 2\"] --\u003e Queue\n    Input3[\"Input 3\"] --\u003e Queue\n    \n    Queue --\u003e Exec\n    Exec --\u003e Pipeline\n    Pipeline --\u003e Output1[\"Output 1\"]\n    Pipeline --\u003e Output2[\"Output 2\"]\n    Pipeline --\u003e Output3[\"Output 3\"]\n    \n    style Pipeline fill:#e1f5ff,stroke:#333,stroke-width:2px\n```\n\n**Diagram: Batch Processing in ComfyUI**\n\nThe pipeline is initialized once and reused across queue items, avoiding model reload overhead.\n\n---\n\n## Troubleshooting\n\n### Common Issues\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| \"Pipeline not found\" error | Custom nodes not installed | Verify LightX2V nodes in `ComfyUI/custom_nodes/` |\n| Out of memory during generation | Insufficient VRAM | Enable offload nodes, reduce resolution, or enable quantization |\n| Slow execution | Suboptimal attention mode | Switch to `sage_attn2` or `flash_attn2` in config node |\n| Model loading fails | Incorrect model path | Verify `model_path` points to correct checkpoint directory |\n| LoRA not applying | LoRA config not connected | Ensure LoRA node connects to generate node |\n\n### Debugging Workflows\n\nComfyUI provides execution logs that show:\n- Node execution order\n- Parameter values passed to each node\n- LightX2V pipeline initialization logs\n- Inference step progress (timesteps, blocks processed)\n- Memory usage metrics\n\nCheck the ComfyUI console output for detailed execution traces that map directly to the [Three-Stage Inference Pipeline](#4.2).\n\nSources: [README.md:234-248]()"])</script><script>self.__next_f.push([1,"26:T5617,"])</script><script>self.__next_f.push([1,"# Core Architecture\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [README_zh.md](README_zh.md)\n- [lightx2v/common/modules/weight_module.py](lightx2v/common/modules/weight_module.py)\n- [lightx2v/pipeline.py](lightx2v/pipeline.py)\n- [lightx2v/server/__main__.py](lightx2v/server/__main__.py)\n- [lightx2v/server/config.py](lightx2v/server/config.py)\n- [lightx2v/server/schema.py](lightx2v/server/schema.py)\n- [lightx2v/server/services/distributed_utils.py](lightx2v/server/services/distributed_utils.py)\n- [lightx2v/server/services/generation/base.py](lightx2v/server/services/generation/base.py)\n- [lightx2v/server/services/generation/image.py](lightx2v/server/services/generation/image.py)\n- [lightx2v/server/services/inference/worker.py](lightx2v/server/services/inference/worker.py)\n- [lightx2v_platform/README.md](lightx2v_platform/README.md)\n- [lightx2v_platform/README_zh.md](lightx2v_platform/README_zh.md)\n- [scripts/hunyuan_video_15/README.md](scripts/hunyuan_video_15/README.md)\n- [scripts/server/post_i2i_with_lora.py](scripts/server/post_i2i_with_lora.py)\n- [scripts/server/start_server_i2i_with_loradir.sh](scripts/server/start_server_i2i_with_loradir.sh)\n\n\u003c/details\u003e\n\n\n\nThis section provides an overview of LightX2V's core architectural components that power the inference pipeline. The system follows a modular design with clear separation of concerns across six primary components:\n\n1. **Runner System** - Orchestrates the inference workflow and manages model lifecycles (see [4.1](#4.1))\n2. **Inference Pipeline** - Three-stage processing (pre-inference, transformer, post-inference) (see [4.2](#4.2))\n3. **Scheduler System** - Manages diffusion sampling and timestep scheduling (see [4.3](#4.3))\n4. **Input Encoders** - Converts text, images, and audio into latent representations (see [4.4](#4.4))\n5. **VAE System** - Encodes/decodes between pixel space and latent space (see [4.5](#4.5))\n6. **InputInfo System** - Defines task-specific input structures and validation (see [4.6](#4.6))\n\n## System Architecture Overview\n\nLightX2V implements a three-tier architecture with clear separation between interface, orchestration, and model layers:\n\n**Diagram: Three-Tier System Architecture**\n\n```mermaid\ngraph TB\n    subgraph \"Interface_Layer[Interface Layer]\"\n        CLI[\"CLI: lightx2v.infer\"]\n        API[\"Python API: LightX2VPipeline\"]\n        Gradio[\"Gradio Web UI\"]\n    end\n    \n    subgraph \"Orchestration_Layer[Orchestration Layer]\"\n        Registry[\"RUNNER_REGISTER\"]\n        BaseRunner[\"BaseRunner\"]\n        DefaultRunner[\"DefaultRunner\"]\n        WanRunner[\"WanRunner\"]\n        SpecializedRunners[\"Specialized Runners:\u003cbr/\u003eWanAudioRunner\u003cbr/\u003eWan22MoeRunner\u003cbr/\u003eQwenImageRunner\"]\n    end\n    \n    subgraph \"Model_Layer[Model Layer]\"\n        WanModel[\"WanModel\"]\n        PreInfer[\"WanPreInfer\"]\n        TransInfer[\"WanTransformerInfer\"]\n        PostInfer[\"WanPostInfer\"]\n        Scheduler[\"WanScheduler\"]\n        Encoders[\"Input Encoders:\u003cbr/\u003eT5, CLIP, SekoAudio\"]\n        VAE[\"VAE:\u003cbr/\u003eWanVAE, Wan2_2_VAE\"]\n    end\n    \n    CLI --\u003e Registry\n    API --\u003e Registry\n    Gradio --\u003e Registry\n    \n    Registry --\u003e BaseRunner\n    BaseRunner --\u003e DefaultRunner\n    DefaultRunner --\u003e WanRunner\n    WanRunner --\u003e SpecializedRunners\n    \n    SpecializedRunners --\u003e WanModel\n    SpecializedRunners --\u003e Scheduler\n    SpecializedRunners --\u003e Encoders\n    SpecializedRunners --\u003e VAE\n    \n    WanModel --\u003e PreInfer\n    WanModel --\u003e TransInfer\n    WanModel --\u003e PostInfer\n    PreInfer --\u003e TransInfer\n    TransInfer --\u003e PostInfer\n    \n    Scheduler --\u003e PreInfer\n    Scheduler --\u003e TransInfer\n    Scheduler --\u003e PostInfer\n```\n\n**Sources:** [lightx2v/infer.py:30-146](), [lightx2v/pipeline.py:58-164](), [lightx2v/models/runners/default_runner.py:56-128](), [lightx2v/models/networks/wan/model.py:39-534]()\n\n### Component Responsibilities\n\n| Component | Location | Primary Responsibility | Details |\n|-----------|----------|----------------------|---------|\n| **Runners** | `lightx2v/models/runners/` | Orchestrate inference workflow, manage module lifecycles | See [4.1](#4.1) for runner hierarchy and registration |\n| **Inference Pipeline** | `lightx2v/models/networks/wan/infer/` | Execute three-stage processing (pre/trans/post) | See [4.2](#4.2) for pipeline details |\n| **Schedulers** | `lightx2v/models/schedulers/` | Manage diffusion timesteps and noise prediction | See [4.3](#4.3) for scheduler implementations |\n| **Input Encoders** | `lightx2v/models/input_encoders/` | Convert text, images, audio to latent representations | See [4.4](#4.4) for encoder architecture |\n| **VAE** | `lightx2v/models/video_encoders/` | Encode/decode between pixel and latent space | See [4.5](#4.5) for VAE structure |\n| **InputInfo** | `lightx2v/utils/input_info.py` | Define task-specific input structures | See [4.6](#4.6) for InputInfo system |\n\n**Sources:** [lightx2v/models/runners/default_runner.py:56-128](), [lightx2v/models/networks/wan/model.py:39-100](), [lightx2v/models/schedulers/wan/scheduler.py:10-50]()\n\n## Key Data Structures\n\nThe system uses several key data structures to pass information between components:\n\n### WanPreInferModuleOutput\n\nEncapsulates outputs from the pre-inference stage at [lightx2v/models/networks/wan/infer/module_io.py:8-32]():\n\n| Field | Type | Shape | Description |\n|-------|------|-------|-------------|\n| `x` | `Tensor` | `[seq_len, dim]` | Patched and flattened latent embeddings |\n| `freqs` | `Tensor` | `[1024, rope_dim]` | RoPE frequency embeddings for 3D positioning |\n| `context` | `Tensor` | `[context_len, dim]` | Combined text + CLIP image embeddings |\n| `embed` | `Tensor` | `[1, dim]` | Time embedding for conditioning |\n| `embed0` | `Tensor` | `[1, 6, dim]` | Modulation parameters (shift, scale, gate  2) |\n| `grid_sizes` | `GridOutput` | `[1, 3]` | Grid dimensions (T, H, W) for unpatchify |\n| `seq_lens` | `Tensor` | `[1]` | Sequence length for attention masking |\n| `adapter_args` | `Dict` | - | Task-specific data (audio features, motion vectors, etc.) |\n\n**Sources:** [lightx2v/models/networks/wan/infer/module_io.py:8-32](), [lightx2v/models/networks/wan/infer/pre_infer.py:9-136]()\n\n### InputInfo Hierarchy\n\nTask-specific input definitions at [lightx2v/utils/input_info.py]() (detailed in [4.6](#4.6)):\n\n| InputInfo Class | Task | Key Fields |\n|----------------|------|-----------|\n| `T2VInputInfo` | Text-to-Video | `prompt`, `negative_prompt`, `seed` |\n| `I2VInputInfo` | Image-to-Video | `image_path` + T2V fields |\n| `S2VInputInfo` | Speech-to-Video | `audio_path` + I2V fields |\n| `T2IInputInfo` | Text-to-Image | `prompt`, `aspect_ratio` |\n| `I2IInputInfo` | Image-to-Image | `image_path` + T2I fields |\n\n**Sources:** [lightx2v/utils/input_info.py:1-200]()\n\n### Scheduler State\n\nThe scheduler maintains state across timesteps at [lightx2v/models/schedulers/wan/scheduler.py:10-50]():\n\n| Attribute | Type | Shape | Purpose |\n|-----------|------|-------|---------|\n| `latents` | `Tensor` | `[C, T, H, W]` | Current denoised latents |\n| `timesteps` | `Tensor` | `[infer_steps]` | Noise levels (1000  0) |\n| `timestep_input` | `Tensor` | `[1]` | Current timestep |\n| `noise_pred` | `Tensor` | `[C, T, H, W]` | Model prediction |\n| `step_index` | `int` | scalar | Current step (0 to infer_steps-1) |\n| `infer_condition` | `bool` | scalar | CFG: conditional or unconditional |\n\n**Sources:** [lightx2v/models/schedulers/wan/scheduler.py:10-80]()\n\n## Registry System\n\nLightX2V uses a registry pattern for dynamic component selection throughout the codebase. The system defines multiple registries at [lightx2v/utils/registry_factory.py]() for different component types:\n\n**Diagram: Registry System Architecture**\n\n```mermaid\ngraph TB\n    subgraph \"Registry_Definitions\"\n        RUNNER[\"RUNNER_REGISTER\u003cbr/\u003eMaps model_cls  Runner\"]\n        ATTN[\"ATTN_WEIGHT_REGISTER\u003cbr/\u003eMaps attn_type  Attention\"]\n        MM[\"MM_WEIGHT_REGISTER\u003cbr/\u003eMaps quant_scheme  MMWeight\"]\n        RMS[\"RMS_WEIGHT_REGISTER\u003cbr/\u003eMaps norm_type  RMSNorm\"]\n        SPARSE_MASK[\"SPARSE_MASK_GENERATOR_REGISTER\u003cbr/\u003eMaps mask_type  MaskGenerator\"]\n        SPARSE_OP[\"SPARSE_OPERATOR_REGISTER\u003cbr/\u003eMaps op_type  SparseOperator\"]\n        PLATFORM[\"PLATFORM_DEVICE_REGISTER\u003cbr/\u003eMaps platform  DeviceImpl\"]\n    end\n    \n    subgraph \"Registration_Decorators\"\n        RunnerDec[\"@RUNNER_REGISTER('wan2.1')\u003cbr/\u003eclass WanRunner\"]\n        AttnDec[\"@ATTN_WEIGHT_REGISTER('flash_attn2')\u003cbr/\u003eclass FlashAttnWeight\"]\n        MMDec[\"@MM_WEIGHT_REGISTER('fp8-vllm')\u003cbr/\u003eclass MMWeightFP8vLLM\"]\n    end\n    \n    subgraph \"Component_Selection\"\n        ConfigKey[\"config['model_cls']\u003cbr/\u003econfig['self_attn_1_type']\u003cbr/\u003econfig['dit_quant_scheme']\"]\n        Lookup[\"registry_name[config_key]\"]\n        Instantiate[\"ComponentClass(**kwargs)\"]\n    end\n    \n    RUNNER --\u003e RunnerDec\n    ATTN --\u003e AttnDec\n    MM --\u003e MMDec\n    \n    RunnerDec --\u003e ConfigKey\n    AttnDec --\u003e ConfigKey\n    MMDec --\u003e ConfigKey\n    \n    ConfigKey --\u003e Lookup\n    Lookup --\u003e Instantiate\n```\n\n### Primary Registries\n\n| Registry | Purpose | Registration Location | Selection Point |\n|----------|---------|----------------------|-----------------|\n| `RUNNER_REGISTER` | Runner implementations | `@RUNNER_REGISTER(\"wan2.1\")` at [lightx2v/models/runners/wan/wan_runner.py:68]() | [lightx2v/infer.py:32]() |\n| `ATTN_WEIGHT_REGISTER` | Attention implementations | `@ATTN_WEIGHT_REGISTER(\"flash_attn2\")` at [lightx2v/common/ops/attn/flash_attn_weight.py]() | [lightx2v/models/networks/wan/weights/transformer_weights.py:160]() |\n| `MM_WEIGHT_REGISTER` | Matrix multiply variants | `@MM_WEIGHT_REGISTER(\"fp8-vllm\")` at [lightx2v/common/ops/mm/mm_weight.py:223]() | [lightx2v/models/networks/wan/weights/transformer_weights.py:180]() |\n| `SPARSE_MASK_GENERATOR_REGISTER` | Sparse mask generators | `@SPARSE_MASK_GENERATOR_REGISTER(\"svg\")` | Selected by `sparse_attn_mask_type` config |\n| `PLATFORM_DEVICE_REGISTER` | Hardware platform support | `@PLATFORM_DEVICE_REGISTER(\"cuda\")` at [lightx2v_platform/]() | [lightx2v/infer.py:137]() |\n\n**Registry Usage Pattern**\n\n```python\n# 1. Registration (at module load time)\n@RUNNER_REGISTER(\"wan2.1\")\nclass WanRunner(DefaultRunner):\n    ...\n\n# 2. Selection (at runtime)\ndef init_runner(config):\n    runner = RUNNER_REGISTER[config[\"model_cls\"]](config)\n    return runner\n```\n\n**Sources:** [lightx2v/utils/registry_factory.py](), [lightx2v/infer.py:30-34](), [lightx2v/models/runners/wan/wan_runner.py:68](), [lightx2v/models/networks/wan/weights/transformer_weights.py:143-200]()\n\n## Inference Execution Flow\n\nThe complete inference request flow from user input to generated output:\n\n## Weight Management System\n\nThe weight management system provides a unified interface for loading, quantizing, and offloading model weights. All linear operations access weights through the `MMWeight` hierarchy at [lightx2v/common/ops/mm/mm_weight.py:76-1050]().\n\n### MMWeight Class Hierarchy\n\n**Diagram: MMWeight Template Hierarchy and Dispatch**\n\n```mermaid\nclassDiagram\n    class MMWeightTemplate {\n        \u003c\u003cabstract\u003e\u003e\n        +weight_name: str\n        +bias_name: str\n        +create_cuda_buffer: bool\n        +lazy_load: bool\n        +lazy_load_file: SafeTensorFile\n        +weight: Tensor\n        +bias: Tensor\n        +load(weight_dict) void\n        +apply(input_tensor) Tensor\n        +to_cuda(non_blocking) void\n        +to_cpu(non_blocking) void\n    }\n    \n    class MMWeight {\n        +load(weight_dict) void\n        +apply(input_tensor) Tensor\n        \"torch.mm() or torch.addmm()\"\n    }\n    \n    class MMWeightQuant {\n        \u003c\u003cabstract\u003e\u003e\n        +weight_scale_name: str\n        +weight_scale: Tensor\n        +act_quant_func: Callable\n        +load_func: Callable\n        +load_from_disk() void\n        +clear() void\n    }\n    \n    class MMWeightFP8vLLM {\n        +load_func: load_fp8_perchannel_sym\n        +apply(input_tensor) Tensor\n        \"ops.scaled_fp8_quant()\u003cbr/\u003eops.cutlass_scaled_mm()\"\n    }\n    \n    class MMWeightINT8vLLM {\n        +load_func: load_int8_perchannel_sym\n        +apply(input_tensor) Tensor\n        \"quantize_activation_per_token_absmax()\u003cbr/\u003equant_int8_per_token_matmul()\"\n    }\n    \n    class MMWeightFP8Q8F {\n        +load_func: load_fp8_perchannel_sym\n        +apply(input_tensor) Tensor\n        \"fp8_linear(input, weight, weight_scale)\"\n    }\n    \n    class MMWeightINT8Q8F {\n        +load_func: load_int8_perchannel_sym\n        +apply(input_tensor) Tensor\n        \"q8_linear(input, weight, weight_scale)\"\n    }\n    \n    class MMWeightNVFP4 {\n        +load_func: load_quantized\n        +input_absmax: Tensor\n        +apply(input_tensor) Tensor\n        \"scaled_nvfp4_quant()\u003cbr/\u003ecutlass_scaled_nvfp4_mm()\"\n    }\n    \n    class MMWeightMXFP4 {\n        +load_func: load_mxfp4\n        +apply(input_tensor) Tensor\n        \"scaled_mxfp4_quant(input)\u003cbr/\u003ecutlass_scaled_mxfp4_mm()\"\n    }\n    \n    class MMWeightMXFP6 {\n        +load_func: load_mxfp6\n        +apply(input_tensor) Tensor\n        \"scaled_mxfp6_quant(input)\u003cbr/\u003ecutlass_scaled_mxfp6_mxfp8_mm()\"\n    }\n    \n    class MMWeightMXFP8 {\n        +load_func: load_mxfp8\n        +apply(input_tensor) Tensor\n        \"scaled_mxfp8_quant(input)\u003cbr/\u003ecutlass_scaled_mxfp8_mm()\"\n    }\n    \n    MMWeightTemplate \u003c|-- MMWeight\n    MMWeightTemplate \u003c|-- MMWeightQuant\n    MMWeightQuant \u003c|-- MMWeightFP8vLLM\n    MMWeightQuant \u003c|-- MMWeightINT8vLLM\n    MMWeightQuant \u003c|-- MMWeightFP8Q8F\n    MMWeightQuant \u003c|-- MMWeightINT8Q8F\n    MMWeightQuant \u003c|-- MMWeightNVFP4\n    MMWeightQuant \u003c|-- MMWeightMXFP4\n    MMWeightQuant \u003c|-- MMWeightMXFP6\n    MMWeightQuant \u003c|-- MMWeightMXFP8\n```\n\n**Weight Registration and Dispatch**\n\nWeights are registered in `MM_WEIGHT_REGISTER` at [lightx2v/utils/registry_factory.py]() and instantiated based on `config[\"dit_quant_scheme\"]`:\n\n| Quant Scheme | MM_WEIGHT_REGISTER Key | Class | Kernel Backend |\n|--------------|------------------------|-------|----------------|\n| `\"Default\"` | `\"Default\"` | `MMWeight` | PyTorch native `torch.mm()` |\n| `\"fp8-vllm\"` | `\"fp8-vllm\"` | `MMWeightFP8vLLM` | vLLM `scaled_fp8_quant()` + `cutlass_scaled_mm()` |\n| `\"int8-vllm\"` | `\"int8-vllm\"` | `MMWeightINT8vLLM` | vLLM `cutlass_scaled_mm()` with INT8 |\n| `\"fp8-q8f\"` | `\"fp8-q8f\"` | `MMWeightFP8Q8F` | q8_kernels `fp8_linear()` |\n| `\"int8-q8f\"` | `\"int8-q8f\"` | `MMWeightINT8Q8F` | q8_kernels `q8_linear()` |\n| `\"nvfp4\"` | `\"nvfp4\"` | `MMWeightNVFP4` | Custom CUTLASS kernel `cutlass_scaled_nvfp4_mm()` |\n| `\"mxfp4\"` | `\"mxfp4\"` | `MMWeightMXFP4` | Custom CUTLASS kernel `cutlass_scaled_mxfp4_mm()` |\n| `\"mxfp6-mxfp8\"` | `\"mxfp6-mxfp8\"` | `MMWeightMXFP6` | Custom CUTLASS kernel `cutlass_scaled_mxfp6_mxfp8_mm()` |\n| `\"mxfp8\"` | `\"mxfp8\"` | `MMWeightMXFP8` | Custom CUTLASS kernel `cutlass_scaled_mxfp8_mm()` |\n\n**Weight Access Pattern**\n\nAll transformer layers access weights through the unified interface at [lightx2v/models/networks/wan/infer/transformer_infer.py:164-166]():\n\n```python\n# Example from self-attention phase\nq = phase.self_attn_q.apply(norm1_out)  # MMWeight.apply()\nk = phase.self_attn_k.apply(norm1_out)\nv = phase.self_attn_v.apply(norm1_out)\n```\n\nThe `apply()` method transparently handles:\n1. **Quantization**: Dynamic or static weight quantization based on scheme\n2. **Offloading**: CPUGPU transfer if `cpu_offload=True`\n3. **Lazy Loading**: DiskRAM transfer if `lazy_load=True`\n4. **Dtype Conversion**: Automatic casting to `GET_DTYPE()`\n\n**Lazy Loading Implementation**\n\nFor memory-constrained environments, weights can be loaded on-demand at [lightx2v/common/ops/mm/mm_weight.py:231-245]():\n\n```python\ndef load_from_disk(self):\n    if not torch._dynamo.is_compiling():\n        self.weight = self.lazy_load_file.get_tensor(self.weight_name).pin_memory()\n        self.weight_scale = self.lazy_load_file.get_tensor(self.weight_scale_name).float().pin_memory()\n        if self.bias_name is not None:\n            self.bias = self.lazy_load_file.get_tensor(self.bias_name).to(self.infer_dtype).pin_memory()\n    \n    if self.weight_need_transpose:\n        self.weight = self.weight.t()\n```\n\nThis enables running 14B models on systems with limited RAM by only materializing the current block's weights.\n\n**Sources:** [lightx2v/common/ops/mm/mm_weight.py:76-1050](), [lightx2v/models/networks/wan/weights/transformer_weights.py:1-512](), [lightx2v/models/networks/wan/infer/transformer_infer.py:164-201]()\n\n## Configuration and Initialization\n\nThe system uses a hierarchical configuration system that merges settings from multiple sources, defined at [lightx2v/utils/set_config.py:13-87](). The final configuration is stored in a `LockableDict` that prevents accidental modifications during inference.\n\n### Key Configuration Categories\n\n| Category | Parameters | Purpose |\n|----------|------------|---------|\n| **Model Architecture** | `dim`, `num_heads`, `num_layers`, `patch_size`, `vae_stride` | Define model structure |\n| **Inference Settings** | `infer_steps`, `sample_guide_scale`, `enable_cfg` | Control generation quality/speed |\n| **Quantization** | `mm_config`, `dit_quantized`, `t5_quantized`, `clip_quantized` | Enable weight quantization |\n| **Offloading** | `cpu_offload`, `offload_granularity`, `lazy_load` | Memory optimization |\n| **Caching** | `feature_caching`, `teacache_thresh` | Computation optimization |\n| **Parallelism** | `parallel.cfg_p_size`, `parallel.seq_p_size` | Multi-GPU distribution |\n| **Task-Specific** | `task`, `target_video_length`, `target_height`, `target_width` | Task configuration |\n\n### Parallel Configuration Setup\n\nFor distributed inference, [lightx2v/utils/set_config.py:78-90]() initializes a 2D device mesh:\n\n```python\ndef set_parallel_config(config):\n    if config[\"parallel\"]:\n        cfg_p_size = config[\"parallel\"].get(\"cfg_p_size\", 1)\n        seq_p_size = config[\"parallel\"].get(\"seq_p_size\", 1)\n        assert cfg_p_size * seq_p_size == dist.get_world_size()\n        \n        config[\"device_mesh\"] = init_device_mesh(\"cuda\", (cfg_p_size, seq_p_size), \n                                                 mesh_dim_names=(\"cfg_p\", \"seq_p\"))\n```\n\nThis enables:\n- **CFG parallelism** (`cfg_p_size` \u003e 1): Distribute conditional/unconditional passes across GPUs\n- **Sequence parallelism** (`seq_p_size` \u003e 1): Split temporal dimension across GPUs\n\nThe device mesh is accessed during inference at [lightx2v/models/networks/wan/model.py:378-396]() to coordinate parallel execution.\n\n**Sources:** [lightx2v/utils/set_config.py:1-100](), [lightx2v/infer.py:30-106](), [lightx2v/models/networks/wan/model.py:54-57]()\n\n## Complete Inference Data Flow\n\nThe following diagram illustrates how data flows through all architectural components during a single inference request:\n\n```mermaid\ngraph TB\n    UserInput[\"User Input\u003cbr/\u003eprompt, image, audio, config\"]\n    \n    subgraph \"Configuration Phase\"\n        SetConfig[\"set_config(args)\u003cbr/\u003elightx2v/utils/set_config.py:36-75\"]\n        InitRunner[\"init_runner(config)\u003cbr/\u003elightx2v/infer.py:23-27\"]\n        LoadModules[\"runner.init_modules()\u003cbr/\u003elightx2v/models/runners/default_runner.py:35-57\"]\n    end\n    \n    subgraph \"Encoding Phase\"\n        RunTextEncoder[\"run_text_encoder(input_info)\u003cbr/\u003eReturns: {context, context_null}\"]\n        RunImageEncoder[\"run_image_encoder(first_frame)\u003cbr/\u003eReturns: clip_encoder_out\"]\n        RunAudioEncoder[\"Audio: read_audio_input(audio_path)\u003cbr/\u003eReturns: audio_segments\"]\n        RunVAEEncoder[\"run_vae_encoder(img)\u003cbr/\u003eReturns: vae_encoder_out\"]\n    end\n    \n    subgraph \"Scheduler Preparation\"\n        SchedulerPrepare[\"scheduler.prepare(seed, latent_shape, image_encoder_output)\u003cbr/\u003eInitializes latents, timesteps\"]\n    end\n    \n    subgraph \"Iterative Refinement Loop\"\n        StepPre[\"scheduler.step_pre(step_index)\u003cbr/\u003ePrepare timestep inputs\"]\n        ModelInfer[\"model.infer(inputs)\u003cbr/\u003eThree-stage pipeline\"]\n        StepPost[\"scheduler.step_post()\u003cbr/\u003eUpdate latents with solver\"]\n    end\n    \n    subgraph \"Decoding Phase\"\n        RunVAEDecoder[\"run_vae_decoder(latents)\u003cbr/\u003elightx2v/models/runners/default_runner.py:269-278\"]\n        ProcessImages[\"process_images_after_vae_decoder()\u003cbr/\u003eApply VFI if configured\"]\n        SaveVideo[\"save_to_video(images, output_path)\u003cbr/\u003elightx2v/utils/utils.py:167-293\"]\n    end\n    \n    UserInput --\u003e SetConfig\n    SetConfig --\u003e InitRunner\n    InitRunner --\u003e LoadModules\n    \n    LoadModules --\u003e RunTextEncoder\n    LoadModules --\u003e RunImageEncoder\n    LoadModules --\u003e RunAudioEncoder\n    LoadModules --\u003e RunVAEEncoder\n    \n    RunTextEncoder --\u003e SchedulerPrepare\n    RunImageEncoder --\u003e SchedulerPrepare\n    RunAudioEncoder --\u003e SchedulerPrepare\n    RunVAEEncoder --\u003e SchedulerPrepare\n    \n    SchedulerPrepare --\u003e StepPre\n    StepPre --\u003e ModelInfer\n    ModelInfer --\u003e StepPost\n    StepPost -.Loop.-\u003e StepPre\n    \n    StepPost --\u003e RunVAEDecoder\n    RunVAEDecoder --\u003e ProcessImages\n    ProcessImages --\u003e SaveVideo\n```\n\nThe complete pipeline execution is coordinated by [lightx2v/models/runners/default_runner.py:248-267]():\n\n```python\n@ProfilingContext4DebugL2(\"Run DiT\")\ndef run_main(self, total_steps=None):\n    self.init_run()\n    if self.config.get(\"compile\", False):\n        self.model.select_graph_for_compile(self.input_info)\n    for segment_idx in range(self.video_segment_num):\n        logger.info(f\" start segment {segment_idx + 1}/{self.video_segment_num}\")\n        self.check_stop()\n        # 1. Initialize segment (default: no-op, audio: prepare prev_latents)\n        self.init_run_segment(segment_idx)\n        # 2. Main inference loop\n        latents = self.run_segment(total_steps=total_steps)\n        # 3. VAE decoder\n        self.gen_video = self.run_vae_decoder(latents)\n        # 4. Post-processing (default: no-op, audio: concatenate segments)\n        self.end_run_segment(segment_idx)\n    gen_video_final = self.process_images_after_vae_decoder()\n    self.end_run()\n    return {\"video\": gen_video_final}\n```\n\n**Sources:** [lightx2v/models/runners/default_runner.py:123-335](), [lightx2v/infer.py:30-116]()"])</script><script>self.__next_f.push([1,"27:T5f1b,"])</script><script>self.__next_f.push([1,"# Runner System and Registry Pattern\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [configs/seko_talk/shot/rs2v/rs2v.json](configs/seko_talk/shot/rs2v/rs2v.json)\n- [configs/seko_talk/shot/stream/f2v.json](configs/seko_talk/shot/stream/f2v.json)\n- [configs/seko_talk/shot/stream/s2v.json](configs/seko_talk/shot/stream/s2v.json)\n- [lightx2v/common/modules/weight_module.py](lightx2v/common/modules/weight_module.py)\n- [lightx2v/models/schedulers/scheduler.py](lightx2v/models/schedulers/scheduler.py)\n- [lightx2v/models/schedulers/wan/audio/scheduler.py](lightx2v/models/schedulers/wan/audio/scheduler.py)\n- [lightx2v/pipeline.py](lightx2v/pipeline.py)\n- [lightx2v/server/__main__.py](lightx2v/server/__main__.py)\n- [lightx2v/server/config.py](lightx2v/server/config.py)\n- [lightx2v/server/schema.py](lightx2v/server/schema.py)\n- [lightx2v/server/services/distributed_utils.py](lightx2v/server/services/distributed_utils.py)\n- [lightx2v/server/services/generation/base.py](lightx2v/server/services/generation/base.py)\n- [lightx2v/server/services/generation/image.py](lightx2v/server/services/generation/image.py)\n- [lightx2v/server/services/inference/worker.py](lightx2v/server/services/inference/worker.py)\n- [lightx2v/shot_runner/rs2v_infer.py](lightx2v/shot_runner/rs2v_infer.py)\n- [lightx2v/shot_runner/shot_base.py](lightx2v/shot_runner/shot_base.py)\n- [lightx2v/shot_runner/stream_infer.py](lightx2v/shot_runner/stream_infer.py)\n- [lightx2v/utils/input_info.py](lightx2v/utils/input_info.py)\n- [lightx2v/utils/set_config.py](lightx2v/utils/set_config.py)\n- [scripts/server/post_i2i_with_lora.py](scripts/server/post_i2i_with_lora.py)\n- [scripts/server/start_server_i2i_with_loradir.sh](scripts/server/start_server_i2i_with_loradir.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document explains the **runner system** and **registry pattern** that form the backbone of LightX2V's architecture. Runners are the primary orchestration components that coordinate model loading, input encoding, inference execution, and output generation for specific model architectures. The registry pattern enables dynamic model selection and instantiation without coupling the codebase to specific implementations.\n\nFor details on individual runner implementations (WanRunner, QwenImageRunner, etc.), see [Model Runners and Tasks](#5). For the three-stage inference pipeline that runners execute, see [Three-Stage Inference Pipeline](#4.2). For weight management within runners, see [Weight Management System](#4.4).\n\n**Sources:** [lightx2v/infer.py:35-39](), [lightx2v/models/runners/default_runner.py:55-69]()\n\n---\n\n## Registry Pattern Overview\n\nLightX2V uses a **decorator-based factory pattern** to register runner classes. The `RUNNER_REGISTER` dictionary maps model class names (strings) to runner implementations, enabling runtime instantiation based on configuration without hardcoded dependencies.\n\n### Registration Mechanism\n\n```mermaid\ngraph TB\n    subgraph \"Registration Phase (Module Import)\"\n        ImportPhase[\"Import runner modules\u003cbr/\u003e(side effect registration)\"]\n        Decorator[\"@RUNNER_REGISTER('model_cls')\"]\n        Registry[\"RUNNER_REGISTER dict\u003cbr/\u003e{'wan2.1': WanRunner, ...}\"]\n        \n        ImportPhase --\u003e Decorator\n        Decorator --\u003e Registry\n    end\n    \n    subgraph \"Instantiation Phase (Runtime)\"\n        Config[\"config['model_cls'] = 'wan2.1'\"]\n        Lookup[\"RUNNER_REGISTER[config['model_cls']]\"]\n        Instance[\"WanRunner(config)\"]\n        \n        Config --\u003e Lookup\n        Registry --\u003e Lookup\n        Lookup --\u003e Instance\n    end\n```\n\n**Sources:** [lightx2v/utils/registry_factory.py](), [lightx2v/infer.py:9-25]()\n\n### Registration Example\n\nRunners register themselves using the `@RUNNER_REGISTER` decorator:\n\n| Model Class | Runner Class | Registration | File |\n|------------|--------------|--------------|------|\n| `wan2.1` | `WanRunner` | `@RUNNER_REGISTER(\"wan2.1\")` | [lightx2v/models/runners/wan/wan_runner.py:63-64]() |\n| `seko_talk` | `WanAudioRunner` | `@RUNNER_REGISTER(\"seko_talk\")` | [lightx2v/models/runners/wan/wan_audio_runner.py:276-277]() |\n| `wan2.2_moe` | `Wan22MoeRunner` | `@RUNNER_REGISTER(\"wan2.2_moe\")` | [lightx2v/models/runners/wan/wan_runner.py:581-582]() |\n| `z_image` | `ZImageRunner` | `@RUNNER_REGISTER(\"z_image\")` | [lightx2v/models/runners/z_image/z_image_runner.py:52-53]() |\n| `qwen_image` | `QwenImageRunner` | `@RUNNER_REGISTER(\"qwen_image\")` | [lightx2v/models/runners/qwen_image/qwen_image_runner.py]() |\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:63-64](), [lightx2v/models/runners/wan/wan_audio_runner.py:276-277](), [lightx2v/models/runners/z_image/z_image_runner.py:52-53]()\n\n### Instantiation Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant infer.py\n    participant RUNNER_REGISTER\n    participant RunnerClass\n    participant RunnerInstance\n    \n    User-\u003e\u003einfer.py: python -m lightx2v.infer\u003cbr/\u003e--model_cls wan2.1\n    infer.py-\u003e\u003einfer.py: config = set_config(args)\n    infer.py-\u003e\u003einfer.py: init_runner(config)\n    infer.py-\u003e\u003eRUNNER_REGISTER: RUNNER_REGISTER[config[\"model_cls\"]]\n    RUNNER_REGISTER--\u003e\u003einfer.py: WanRunner (class)\n    infer.py-\u003e\u003eRunnerClass: WanRunner(config)\n    RunnerClass-\u003e\u003eRunnerInstance: __init__(config)\n    RunnerInstance-\u003e\u003eRunnerInstance: init_modules()\n    RunnerInstance--\u003e\u003einfer.py: runner instance\n    infer.py-\u003e\u003eRunnerInstance: run_pipeline(input_info)\n```\n\n**Sources:** [lightx2v/infer.py:162-181](), [lightx2v/infer.py:35-39]()\n\n---\n\n## Runner Class Hierarchy\n\nThe runner system uses a three-level inheritance hierarchy: `BaseRunner`  `DefaultRunner`  specific implementations.\n\n```mermaid\ngraph TB\n    BaseRunner[\"BaseRunner\u003cbr/\u003e(base_runner.py)\"]\n    DefaultRunner[\"DefaultRunner\u003cbr/\u003e(default_runner.py)\"]\n    \n    WanRunner[\"WanRunner\u003cbr/\u003eT2V, I2V\"]\n    WanAudioRunner[\"WanAudioRunner\u003cbr/\u003eAudio-to-Video\"]\n    Wan22MoeRunner[\"Wan22MoeRunner\u003cbr/\u003eMoE High/Low Noise\"]\n    WanDistillRunner[\"WanDistillRunner\u003cbr/\u003e4-step Distilled\"]\n    \n    QwenImageRunner[\"QwenImageRunner\u003cbr/\u003eT2I, I2I\"]\n    ZImageRunner[\"ZImageRunner\u003cbr/\u003eFast T2I\"]\n    HunyuanVideo15Runner[\"HunyuanVideo15Runner\u003cbr/\u003eT2V, I2V\"]\n    LTX2Runner[\"LTX2Runner\u003cbr/\u003eLightweight T2V\"]\n    \n    BaseRunner --\u003e DefaultRunner\n    DefaultRunner --\u003e WanRunner\n    DefaultRunner --\u003e QwenImageRunner\n    DefaultRunner --\u003e ZImageRunner\n    DefaultRunner --\u003e HunyuanVideo15Runner\n    DefaultRunner --\u003e LTX2Runner\n    \n    WanRunner --\u003e WanAudioRunner\n    WanRunner --\u003e Wan22MoeRunner\n    WanRunner --\u003e WanDistillRunner\n```\n\n**Sources:** [lightx2v/models/runners/default_runner.py:55](), [lightx2v/models/runners/wan/wan_runner.py:63-64]()\n\n### Hierarchy Responsibilities\n\n| Class | File | Responsibilities |\n|-------|------|------------------|\n| `BaseRunner` | [lightx2v/models/runners/base_runner.py]() | Abstract base interface, stop/pause signals, video segment management |\n| `DefaultRunner` | [lightx2v/models/runners/default_runner.py:55-69]() | Complete inference pipeline, encoder orchestration, VAE processing, progress callbacks |\n| Specific Runners | [lightx2v/models/runners/wan/](), [lightx2v/models/runners/qwen_image/]() | Model-specific loading, architecture-specific preprocessing, task-specific encoding |\n\n**Sources:** [lightx2v/models/runners/default_runner.py:55](), [lightx2v/models/runners/wan/wan_runner.py:64]()\n\n---\n\n## Runner Lifecycle\n\nThe runner lifecycle consists of four phases: initialization, module loading, inference execution, and cleanup.\n\n```mermaid\nstateDiagram-v2\n    [*] --\u003e __init__\n    __init__ --\u003e init_modules\n    \n    state init_modules {\n        [*] --\u003e load_model\n        load_model --\u003e set_scheduler\n        set_scheduler --\u003e assign_encoder_methods\n        assign_encoder_methods --\u003e config_lock\n        config_lock --\u003e [*]\n    }\n    \n    init_modules --\u003e run_pipeline\n    \n    state run_pipeline {\n        [*] --\u003e run_input_encoder\n        run_input_encoder --\u003e run_main\n        \n        state run_main {\n            [*] --\u003e init_run\n            init_run --\u003e segment_loop\n            \n            state segment_loop {\n                init_run_segment --\u003e run_segment\n                run_segment --\u003e run_vae_decoder\n                run_vae_decoder --\u003e end_run_segment\n            }\n            \n            segment_loop --\u003e process_after_vae\n            process_after_vae --\u003e [*]\n        }\n        \n        run_main --\u003e [*]\n    }\n    \n    run_pipeline --\u003e end_run\n    end_run --\u003e [*]\n```\n\n**Sources:** [lightx2v/models/runners/default_runner.py:70-97](), [lightx2v/models/runners/default_runner.py:360-391](), [lightx2v/models/runners/default_runner.py:461-476]()\n\n### Phase 1: Initialization (`__init__`)\n\nConstructor sets up runner configuration and scheduler. Subclasses typically call `super().__init__(config)` and set model-specific parameters.\n\n```python\n# DefaultRunner.__init__\ndef __init__(self, config):\n    super().__init__(config)\n    self.has_prompt_enhancer = False\n    self.progress_callback = None\n    # Check prompt enhancer server availability\n    # Set initial device (CPU if offloading, else GPU)\n    self.init_scheduler()\n```\n\n**Sources:** [lightx2v/models/runners/default_runner.py:55-69]()\n\n### Phase 2: Module Loading (`init_modules`)\n\nLoads all model components and freezes configuration. This is called once after instantiation, before any inference.\n\n```mermaid\ngraph LR\n    A[\"init_modules()\"] --\u003e B[\"load_model()\"]\n    B --\u003e C[\"load_transformer()\"]\n    B --\u003e D[\"load_text_encoder()\"]\n    B --\u003e E[\"load_image_encoder()\"]\n    B --\u003e F[\"load_vae()\"]\n    \n    C --\u003e G[\"model.set_scheduler()\"]\n    \n    A --\u003e H[\"Assign run_input_encoder\u003cbr/\u003ebased on task\"]\n    H --\u003e I[\"config.lock()\"]\n    I --\u003e J[\"Optional: model.compile()\"]\n```\n\n**Sources:** [lightx2v/models/runners/default_runner.py:70-97](), [lightx2v/models/runners/default_runner.py:123-130]()\n\n**Key Methods:**\n\n| Method | Implementer | Purpose |\n|--------|-------------|---------|\n| `init_scheduler()` | Subclass | Create scheduler (WanScheduler, ZImageScheduler, etc.) |\n| `load_transformer()` | Subclass | Load main transformer model (must implement) |\n| `load_text_encoder()` | Subclass | Load T5/CLIP/Qwen2.5VL encoder |\n| `load_image_encoder()` | Subclass | Load CLIP image encoder (for I2V tasks) |\n| `load_vae()` | Subclass | Load VAE encoder/decoder |\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:72-79](), [lightx2v/models/runners/wan/wan_runner.py:119-157](), [lightx2v/models/runners/wan/wan_runner.py:214-220]()\n\n### Phase 3: Inference Execution (`run_pipeline`)\n\nMain inference entry point that orchestrates the complete generation process.\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Runner\n    participant Encoders\n    participant Scheduler\n    participant Model\n    participant VAE\n    \n    Client-\u003e\u003eRunner: run_pipeline(input_info)\n    \n    Note over Runner: Optional: Prompt Enhancement\n    Runner-\u003e\u003eRunner: post_prompt_enhancer()\n    \n    Runner-\u003e\u003eEncoders: run_input_encoder()\n    Encoders-\u003e\u003eEncoders: run_text_encoder()\n    Encoders-\u003e\u003eEncoders: run_image_encoder() (if I2V)\n    Encoders-\u003e\u003eEncoders: run_vae_encoder() (if I2V)\n    Encoders--\u003e\u003eRunner: encoder_outputs\n    \n    Runner-\u003e\u003eRunner: run_main()\n    Runner-\u003e\u003eScheduler: prepare(seed, latent_shape)\n    Scheduler--\u003e\u003eRunner: initial_latents\n    \n    loop For each segment\n        Runner-\u003e\u003eRunner: init_run_segment(idx)\n        Runner-\u003e\u003eRunner: run_segment(idx)\n        \n        loop Diffusion steps (4-50)\n            Runner-\u003e\u003eScheduler: step_pre(step_index)\n            Runner-\u003e\u003eModel: infer(inputs)\n            Model--\u003e\u003eScheduler: noise_prediction\n            Runner-\u003e\u003eScheduler: step_post()\n        end\n        \n        Runner-\u003e\u003eVAE: run_vae_decoder(latents)\n        VAE--\u003e\u003eRunner: video_frames\n        Runner-\u003e\u003eRunner: end_run_segment(idx)\n    end\n    \n    Runner-\u003e\u003eRunner: process_after_vae_decoder()\n    Runner-\u003e\u003eRunner: end_run()\n    Runner--\u003e\u003eClient: generated_video\n```\n\n**Sources:** [lightx2v/models/runners/default_runner.py:461-476](), [lightx2v/models/runners/default_runner.py:360-391](), [lightx2v/models/runners/default_runner.py:173-206]()\n\n### Phase 4: Cleanup (`end_run`)\n\nReleases resources after inference completes.\n\n```python\n# DefaultRunner.end_run\ndef end_run(self):\n    self.model.scheduler.clear()\n    if hasattr(self, \"inputs\"):\n        del self.inputs\n    self.input_info = None\n    # Clean up offload managers if lazy loading\n    # Save calibration data if enabled\n    torch.cuda.empty_cache()\n    gc.collect()\n```\n\n**Sources:** [lightx2v/models/runners/default_runner.py:217-242]()\n\n---\n\n## Key Runner Methods\n\nRunners implement these methods to customize behavior for specific model architectures.\n\n### Abstract Methods (Must Implement)\n\n| Method | Signature | Purpose | Example |\n|--------|-----------|---------|---------|\n| `load_transformer()` | `() -\u003e Model` | Load main transformer model | [lightx2v/models/runners/wan/wan_runner.py:72-79]() |\n| `init_scheduler()` | `() -\u003e None` | Initialize diffusion scheduler | [lightx2v/models/runners/wan/wan_runner.py:222-235]() |\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:72-79](), [lightx2v/models/runners/wan/wan_runner.py:222-235]()\n\n### Optional Override Methods\n\n| Method | Default Behavior | Override Purpose | Example |\n|--------|------------------|------------------|---------|\n| `load_text_encoder()` | Loads T5 encoder | Support different text encoders | [lightx2v/models/runners/z_image/z_image_runner.py:79-82]() |\n| `load_image_encoder()` | Loads CLIP encoder | Disable or customize image encoding | [lightx2v/models/runners/wan/wan_runner.py:81-117]() |\n| `load_vae()` | Loads WanVAE | Use architecture-specific VAE | [lightx2v/models/runners/qwen_image/qwen_image_runner.py]() |\n| `run_input_encoder()` | Task-based routing | Custom preprocessing logic | [lightx2v/models/runners/wan/wan_audio_runner.py:437-462]() |\n| `run_main()` | Standard diffusion loop | Custom inference patterns | [lightx2v/models/runners/wan/wan_audio_runner.py:661-733]() |\n\n**Sources:** [lightx2v/models/runners/default_runner.py:277-336](), [lightx2v/models/runners/wan/wan_audio_runner.py:437-462]()\n\n### Task-Specific Encoder Methods\n\n`run_input_encoder()` is assigned dynamically based on task during `init_modules()`:\n\n```python\n# DefaultRunner.init_modules\nif self.config[\"task\"] == \"i2v\":\n    self.run_input_encoder = self._run_input_encoder_local_i2v\nelif self.config[\"task\"] == \"t2v\":\n    self.run_input_encoder = self._run_input_encoder_local_t2v\nelif self.config[\"task\"] in [\"s2v\", \"rs2v\"]:\n    self.run_input_encoder = self._run_input_encoder_local_s2v\n```\n\n| Task | Method | Encoders Used |\n|------|--------|---------------|\n| `t2v` | `_run_input_encoder_local_t2v()` | Text only |\n| `i2v` | `_run_input_encoder_local_i2v()` | Text, Image (CLIP), VAE |\n| `s2v` | `_run_input_encoder_local_s2v()` | Text, Image (CLIP), VAE, Audio |\n| `flf2v` | `_run_input_encoder_local_flf2v()` | Text, First/Last Image, VAE |\n\n**Sources:** [lightx2v/models/runners/default_runner.py:78-89](), [lightx2v/models/runners/default_runner.py:277-309]()\n\n---\n\n## Specific Runner Implementations\n\n### WanRunner Family\n\nThe Wan family demonstrates three-level inheritance for progressive specialization.\n\n```mermaid\ngraph TB\n    DefaultRunner[\"DefaultRunner\u003cbr/\u003eBase inference pipeline\"]\n    \n    WanRunner[\"WanRunner\u003cbr/\u003e@RUNNER_REGISTER('wan2.1')\u003cbr/\u003e- T5 text encoder\u003cbr/\u003e- CLIP image encoder\u003cbr/\u003e- WanVAE\u003cbr/\u003e- WanScheduler\"]\n    \n    WanAudioRunner[\"WanAudioRunner\u003cbr/\u003e@RUNNER_REGISTER('seko_talk')\u003cbr/\u003e+ SekoAudioEncoder\u003cbr/\u003e+ AudioAdapter\u003cbr/\u003e+ Audio segmentation\"]\n    \n    Wan22MoeRunner[\"Wan22MoeRunner\u003cbr/\u003e@RUNNER_REGISTER('wan2.2_moe')\u003cbr/\u003e+ MultiModelStruct\u003cbr/\u003e+ High/low noise switching\"]\n    \n    WanDistillRunner[\"WanDistillRunner\u003cbr/\u003e@RUNNER_REGISTER('wan2.1_distill')\u003cbr/\u003e+ WanStepDistillScheduler\u003cbr/\u003e+ 4-step inference\"]\n    \n    DefaultRunner --\u003e WanRunner\n    WanRunner --\u003e WanAudioRunner\n    WanRunner --\u003e Wan22MoeRunner\n    WanRunner --\u003e WanDistillRunner\n```\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:63-64](), [lightx2v/models/runners/wan/wan_audio_runner.py:276-277](), [lightx2v/models/runners/wan/wan_runner.py:581-582]()\n\n**WanRunner** ([lightx2v/models/runners/wan/wan_runner.py:63-220]())\n- **Purpose:** Base for all Wan2.1/2.2 video models\n- **Key Methods:**\n  - `load_transformer()`: Instantiates `WanModel` with optional LoRA\n  - `load_text_encoder()`: T5-XXL encoder with quantization support\n  - `load_image_encoder()`: CLIP-ViT-Huge-14 for I2V conditioning\n  - `load_vae()`: WanVAE or Wan2_2_VAE depending on model version\n- **Configuration:** Supports quantization (INT8/FP8), CPU offloading, feature caching\n\n**WanAudioRunner** ([lightx2v/models/runners/wan/wan_audio_runner.py:276-808]())\n- **Purpose:** Audio-driven video generation (talking head synthesis)\n- **Additional Components:**\n  - `SekoAudioEncoderModel`: Extracts audio features\n  - `AudioAdapter`: Projects audio features to model space\n  - `AudioProcessor`: Segments long audio into overlapping chunks\n- **Key Methods:**\n  - `read_audio_input()`: Handles single/multi-person audio (with masks)\n  - `run_main()`: Supports streaming audio input via `VAController`\n  - `init_run_segment()`: Processes each audio segment independently\n\n**Wan22MoeRunner** ([lightx2v/models/runners/wan/wan_runner.py:581-627]())\n- **Purpose:** Mixture-of-Experts with high/low noise model switching\n- **Key Feature:** `MultiModelStruct` wraps two models, switches at `boundary_timestep`\n- **Model Loading:** Loads high_noise_model and low_noise_model separately\n- **Offloading:** Swaps models between CPU/GPU at boundary\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:63-627](), [lightx2v/models/runners/wan/wan_audio_runner.py:276-808]()\n\n### Image Generation Runners\n\n**QwenImageRunner** ([lightx2v/models/runners/qwen_image/qwen_image_runner.py]())\n- **Text Encoder:** Qwen2.5VL (multimodal, supports image conditioning)\n- **Transformer:** `QwenImageTransformerModel` (dual-stream attention)\n- **VAE:** `AutoencoderKL` from Diffusers\n- **Scheduler:** `QwenImageScheduler` (flow matching)\n- **Tasks:** T2I, I2I with aspect ratio control\n\n**ZImageRunner** ([lightx2v/models/runners/z_image/z_image_runner.py:52-254]())\n- **Text Encoder:** Qwen3-4B (lightweight)\n- **Transformer:** `ZImageTransformerModel` (Ada GPU optimized)\n- **VAE:** `AutoencoderKLZImageVAE` (Diffusers-based)\n- **Scheduler:** `ZImageScheduler` (turbo mode, 9-step inference)\n- **Tasks:** T2I, I2I with fast generation\n\n**Sources:** [lightx2v/models/runners/z_image/z_image_runner.py:52-254]()\n\n---\n\n## Dynamic LoRA Switching\n\nRunners support runtime LoRA switching without reloading models, enabling per-request customization in server deployments.\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Runner\n    participant Model\n    participant PreWeight\n    participant TransformerWeights\n    \n    Client-\u003e\u003eRunner: switch_lora(lora_path, strength=1.0)\n    Runner-\u003e\u003eRunner: Validate model loaded\n    \n    alt Empty lora_path (removal)\n        Runner-\u003e\u003eModel: _remove_lora()\n        Model-\u003e\u003ePreWeight: remove_lora()\n        Model-\u003e\u003eTransformerWeights: remove_lora()\n        PreWeight-\u003e\u003ePreWeight: Delete lora_down, lora_up\n        TransformerWeights-\u003e\u003eTransformerWeights: Delete LoRA tensors\n    else Non-empty lora_path (update)\n        Runner-\u003e\u003eModel: _update_lora(lora_path, strength)\n        Model-\u003e\u003eModel: Load safetensors\n        Model-\u003e\u003ePreWeight: update_lora(weight_dict, strength)\n        Model-\u003e\u003eTransformerWeights: update_lora(weight_dict, strength)\n        PreWeight-\u003e\u003ePreWeight: Copy new LoRA weights\n        TransformerWeights-\u003e\u003eTransformerWeights: Copy new LoRA weights\n    end\n    \n    Runner--\u003e\u003eClient: Success/Failure\n```\n\n**Sources:** [lightx2v/models/runners/default_runner.py:478-521]()\n\n### Implementation\n\nThe `switch_lora()` method enables dynamic updates:\n\n```python\n# DefaultRunner.switch_lora\ndef switch_lora(self, lora_path: str, strength: float = 1.0):\n    \"\"\"\n    Switch LoRA weights dynamically by calling weight modules' update_lora method.\n    If an empty lora_path is provided, it removes LoRA weights by calling weight\n    modules' remove_lora method.\n    \"\"\"\n    if not hasattr(self, \"model\") or self.model is None:\n        logger.error(\"Model not loaded. Please load model first.\")\n        return False\n    \n    if lora_path == \"\":\n        # Remove LoRA\n        self.model._remove_lora()\n    else:\n        # Update LoRA\n        self.model._update_lora(lora_path, strength)\n    \n    return True\n```\n\n**Weight Module Support:**\n\nEach `MMWeight` class implements:\n- `update_lora(weight_dict, strength)`: Copies new LoRA tensors\n- `remove_lora()`: Deletes LoRA tensors and sets `has_lora_branch = False`\n\n**Usage in Server:**\n```python\n# HTTP server per-request LoRA\nrunner.switch_lora(request.lora_path, request.lora_strength)\nresult = runner.run_pipeline(input_info)\n```\n\n**Sources:** [lightx2v/models/runners/default_runner.py:478-521](), [lightx2v/common/ops/mm/mm_weight.py:194-218]()\n\n---\n\n## Configuration and Device Management\n\n### Initialization Device Strategy\n\nRunners select initial device based on offloading configuration:\n\n```python\n# DefaultRunner.set_init_device\ndef set_init_device(self):\n    if self.config[\"cpu_offload\"]:\n        self.init_device = torch.device(\"cpu\")  # Load to CPU, transfer later\n    else:\n        self.init_device = torch.device(AI_DEVICE)  # Load directly to GPU\n```\n\n**Sources:** [lightx2v/models/runners/default_runner.py:99-103]()\n\n### Lazy Loading\n\nWhen `lazy_load=True` and `cpu_offload=True`, models are not loaded during `init_modules()`. Instead, they're loaded on-demand during inference:\n\n```python\n# DefaultRunner.init_modules (with lazy loading)\nif not self.config.get(\"lazy_load\", False) and not self.config.get(\"unload_modules\", False):\n    self.load_model()  # Normal loading\nelif self.config.get(\"lazy_load\", False):\n    assert self.config.get(\"cpu_offload\", False)  # Lazy requires offload\n```\n\n**Per-Component Lazy Loading:**\n```python\n# WanAudioRunner.run_image_encoder\nif self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n    self.image_encoder = self.load_image_encoder()  # Load on-demand\n\nclip_encoder_out = self.image_encoder.visual([first_frame])\n\nif self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n    del self.image_encoder  # Unload after use\n    torch.cuda.empty_cache()\n```\n\n**Sources:** [lightx2v/models/runners/default_runner.py:70-76](), [lightx2v/models/runners/wan/wan_audio_runner.py:409-415]()\n\n### Progress Callbacks\n\nRunners support progress tracking via callback functions:\n\n```python\n# DefaultRunner.run_segment\nif self.progress_callback:\n    current_step = segment_idx * infer_steps + step_index + 1\n    total_all_steps = self.video_segment_num * infer_steps\n    self.progress_callback((current_step / total_all_steps) * 100, 100)\n```\n\nUseful for Gradio UI and long-running inference:\n\n```python\n# Gradio integration\ndef progress_fn(current, total):\n    gr.Info(f\"Progress: {current}/{total}\")\n\nrunner.set_progress_callback(progress_fn)\nrunner.run_pipeline(input_info)\n```\n\n**Sources:** [lightx2v/models/runners/default_runner.py:170-171](), [lightx2v/models/runners/default_runner.py:197-200]()\n\n---\n\n## Summary\n\nThe runner system provides:\n\n1. **Registry Pattern:** Decorator-based factory enabling dynamic model selection\n2. **Inheritance Hierarchy:** Three-level structure (BaseRunner  DefaultRunner  Specific) for progressive specialization\n3. **Lifecycle Management:** Four phases (init  load  infer  cleanup) with clear separation\n4. **Extensibility:** Abstract methods for model-specific customization, override methods for optional behavior\n5. **Dynamic LoRA:** Runtime weight switching without model reloading\n6. **Resource Management:** Lazy loading, CPU offloading, progress callbacks\n\nThis architecture enables supporting 15+ model architectures (Wan2.1/2.2, Qwen-Image, Z-Image, HunyuanVideo, LTX-2, WorldPlay, etc.) with shared infrastructure while allowing model-specific optimizations.\n\n**Sources:** [lightx2v/models/runners/default_runner.py:55-535](), [lightx2v/models/runners/wan/wan_runner.py:63-627](), [lightx2v/infer.py:35-39]()"])</script><script>self.__next_f.push([1,"28:T5e2c,"])</script><script>self.__next_f.push([1,"# Three-Stage Inference Pipeline\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py](lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py)\n- [lightx2v/models/networks/qwen_image/infer/pre_infer.py](lightx2v/models/networks/qwen_image/infer/pre_infer.py)\n- [lightx2v/models/networks/qwen_image/infer/transformer_infer.py](lightx2v/models/networks/qwen_image/infer/transformer_infer.py)\n- [lightx2v/models/networks/qwen_image/model.py](lightx2v/models/networks/qwen_image/model.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/qwen_image/qwen_image_runner.py](lightx2v/models/runners/qwen_image/qwen_image_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/schedulers/qwen_image/scheduler.py](lightx2v/models/schedulers/qwen_image/scheduler.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/qwen_image/vae.py](lightx2v/models/video_encoders/hf/qwen_image/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the three-stage inference pipeline that forms the core computational architecture of the diffusion models in LightX2V. The pipeline consists of **pre-inference**, **transformer inference**, and **post-inference** stages that execute iteratively within each diffusion timestep.\n\nThis page focuses on the internal structure of a single inference pass through the model. For information about the overall runner system and model loading, see [Runner System and Model Registry](#4.1). For details about the scheduler and diffusion sampling process, see [Scheduler System and Diffusion Sampling](#4.3). For input encoder architectures, see [Input Encoder Architecture](#4.4).\n\n## Overview\n\nThe three-stage pipeline executes within each diffusion timestep to transform noisy latents toward the final denoised output. Each pass through the pipeline operates on the current latent state and produces a noise prediction that the scheduler uses to update the latents.\n\n```mermaid\ngraph LR\n    subgraph \"Single Timestep Iteration\"\n        Inputs[\"inputs\u003cbr/\u003e(text/image/audio encodings)\"]\n        Latents[\"scheduler.latents\u003cbr/\u003e(current noisy state)\"]\n        \n        PreInfer[\"Stage 1: Pre-Inference\u003cbr/\u003eWanPreInfer.infer()\"]\n        TransInfer[\"Stage 2: Transformer Inference\u003cbr/\u003eWanTransformerInfer.infer()\"]\n        PostInfer[\"Stage 3: Post-Inference\u003cbr/\u003eWanPostInfer.infer()\"]\n        \n        NoisePred[\"noise_pred\u003cbr/\u003e(predicted noise)\"]\n        \n        Inputs --\u003e PreInfer\n        Latents --\u003e PreInfer\n        PreInfer --\u003e|\"pre_infer_out\"| TransInfer\n        TransInfer --\u003e|\"x\"| PostInfer\n        PostInfer --\u003e NoisePred\n        \n        NoisePred -.-\u003e|\"scheduler.step_post()\"| Latents\n    end\n```\n\n**Diagram: Single Timestep Pipeline Flow**\n\nSources: [lightx2v/models/networks/wan/model.py:457-478](), [lightx2v/models/runners/default_runner.py:186-193]()\n\n## Stage 1: Pre-Inference\n\nThe pre-inference stage prepares the latent representation and conditioning signals for processing by the transformer. It is implemented in the `WanPreInfer` class.\n\n### Core Responsibilities\n\n1. **Patch Embedding**: Converts 4D latent tensors `[C, T, H, W]` into a sequence of patches\n2. **Time Embedding**: Encodes the current diffusion timestep as a sinusoidal embedding\n3. **Text Conditioning**: Processes text encoder outputs through projection layers\n4. **Image Conditioning**: For i2v/s2v tasks, integrates CLIP and VAE image features\n5. **Modulation Preparation**: Creates modulation signals for adaptive layer normalization\n\n```mermaid\ngraph TB\n    subgraph \"Pre-Inference Components\"\n        Latents[\"scheduler.latents\u003cbr/\u003e[C, T, H, W]\"]\n        Timestep[\"scheduler.timestep_input\"]\n        TextCtx[\"text_encoder_output['context']\"]\n        ImgEnc[\"image_encoder_output\"]\n        \n        PatchEmbed[\"patch_embedding.apply()\u003cbr/\u003eConverts to patches\"]\n        TimeEmbed[\"time_embedding layers\u003cbr/\u003esinusoidal_embedding_1d()\"]\n        TextProj[\"text_embedding layers\u003cbr/\u003eMLP projection\"]\n        ImgProj[\"proj layers\u003cbr/\u003eCLIP feature projection\"]\n        \n        PreInferOut[\"WanPreInferModuleOutput\"]\n        \n        Latents --\u003e PatchEmbed\n        ImgEnc --\u003e PatchEmbed\n        PatchEmbed --\u003e|\"flattened patches\"| PreInferOut\n        \n        Timestep --\u003e TimeEmbed\n        TimeEmbed --\u003e|\"embed, embed0\"| PreInferOut\n        \n        TextCtx --\u003e TextProj\n        TextProj --\u003e|\"context\"| PreInferOut\n        \n        ImgEnc --\u003e ImgProj\n        ImgProj --\u003e TextProj\n        \n        PreInferOut --\u003e|\"x, embed, embed0,\u003cbr/\u003econtext, grid_sizes\"| Stage2[\"To Transformer Stage\"]\n    end\n```\n\n**Diagram: Pre-Inference Data Flow**\n\nSources: [lightx2v/models/networks/wan/infer/pre_infer.py:9-139]()\n\n### Patch Embedding Process\n\nThe latent tensor is converted into a sequence of spatial-temporal patches:\n\n1. **Concatenation with Image Encoding** (for i2v/s2v tasks): The VAE-encoded reference image is concatenated with the current latents along the channel dimension\n2. **Patch Projection**: Applied via `patch_embedding.apply()` which reshapes `[B, C, T, H, W]` to patch representation\n3. **Flattening**: Patches are flattened from `[B, C, T', H', W']` to `[B, T'*H'*W', C]` sequence format\n\nSources: [lightx2v/models/networks/wan/infer/pre_infer.py:48-66]()\n\n### Time Embedding\n\nTimestep encoding follows a two-stage projection:\n\n```python\n# Sinusoidal embedding\nembed = sinusoidal_embedding_1d(freq_dim, timestep)\n# First projection + activation\nembed = time_embedding_0.apply(embed)\nembed = silu(embed)\n# Second projection\nembed = time_embedding_2.apply(embed)\nembed0 = silu(embed)\n# Final projection to 6 modulation channels\nembed0 = time_projection_1.apply(embed0).unflatten(1, (6, dim))\n```\n\nThe 6 channels correspond to modulation parameters: `[shift_msa, scale_msa, gate_msa, c_shift_msa, c_scale_msa, c_gate_msa]` used in transformer blocks.\n\nSources: [lightx2v/models/networks/wan/infer/pre_infer.py:68-95]()\n\n### Conditioning Integration\n\nFor **text conditioning**, the context passes through:\n```python\nout = text_embedding_0.apply(context)\nout = gelu(out, approximate=\"tanh\")\ncontext = text_embedding_2.apply(out)\n```\n\nFor **image conditioning** (i2v/flf2v/animate tasks), CLIP features pass through a 4-layer projection:\n```python\ncontext_clip = proj_0.apply(clip_fea)\ncontext_clip = proj_1.apply(context_clip)\ncontext_clip = gelu(context_clip)\ncontext_clip = proj_3.apply(context_clip)\ncontext_clip = proj_4.apply(context_clip)\ncontext = concat([context_clip, context], dim=0)\n```\n\nSources: [lightx2v/models/networks/wan/infer/pre_infer.py:98-123]()\n\n### Output Structure\n\nThe `WanPreInferModuleOutput` dataclass contains:\n- `x`: Flattened patch sequence `[seq_len, dim]`\n- `embed`: Time embedding for block processing\n- `embed0`: Modulation parameters `[6, dim]`\n- `context`: Combined text and image conditioning `[context_len, dim]`\n- `grid_sizes`: Original spatial-temporal dimensions for unpatchifying\n\nSources: [lightx2v/models/networks/wan/infer/module_io.py](), [lightx2v/models/networks/wan/infer/pre_infer.py:131-138]()\n\n## Stage 2: Transformer Inference\n\nThe transformer inference stage processes the patch sequence through multiple transformer blocks. Each block contains three computational phases executed sequentially.\n\n### Block Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Single Transformer Block\"\n        Input[\"x (from previous block)\"]\n        \n        subgraph \"Phase 0: Self-Attention\"\n            Modulation0[\"modulation + embed0\u003cbr/\u003e 6 parameters\"]\n            Norm1[\"norm1 + modulate\"]\n            QKV[\"q, k, v projections\"]\n            RoPE[\"RoPE (Rotary Position Embedding)\"]\n            SelfAttn[\"self_attn_1.apply()\u003cbr/\u003e(flash_attn2/3/sage/radial)\"]\n            OutProj0[\"self_attn_o projection\"]\n        end\n        \n        subgraph \"Phase 1: Cross-Attention\"\n            AddGate[\"x += y * gate_msa\"]\n            Norm3[\"norm3\"]\n            CrossQKV[\"cross q, k, v projections\"]\n            CrossAttn1[\"cross_attn_1.apply()\u003cbr/\u003e(text attention)\"]\n            CrossAttn2[\"cross_attn_2.apply()\u003cbr/\u003e(image attention, if i2v)\"]\n            OutProj1[\"cross_attn_o projection\"]\n        end\n        \n        subgraph \"Phase 2: FFN\"\n            AddCross[\"x += attn_out\"]\n            Norm2[\"norm2 + modulate\"]\n            FFN0[\"ffn_0 projection\"]\n            GELU[\"gelu activation\"]\n            FFN2[\"ffn_2 projection\"]\n            AddGate2[\"x += y * c_gate_msa\"]\n        end\n        \n        Output[\"x (to next block)\"]\n        \n        Input --\u003e Modulation0\n        Modulation0 --\u003e Norm1\n        Norm1 --\u003e QKV\n        QKV --\u003e RoPE\n        RoPE --\u003e SelfAttn\n        SelfAttn --\u003e OutProj0\n        OutProj0 --\u003e AddGate\n        \n        AddGate --\u003e Norm3\n        Norm3 --\u003e CrossQKV\n        CrossQKV --\u003e CrossAttn1\n        CrossQKV --\u003e CrossAttn2\n        CrossAttn1 --\u003e OutProj1\n        CrossAttn2 --\u003e OutProj1\n        OutProj1 --\u003e AddCross\n        \n        AddCross --\u003e Norm2\n        Norm2 --\u003e FFN0\n        FFN0 --\u003e GELU\n        GELU --\u003e FFN2\n        FFN2 --\u003e AddGate2\n        \n        AddGate2 --\u003e Output\n    end\n```\n\n**Diagram: Transformer Block Structure with Three Phases**\n\nSources: [lightx2v/models/networks/wan/infer/transformer_infer.py:131-154]()\n\n### Main Inference Loop\n\nThe transformer processes all blocks sequentially:\n\n```python\ndef infer_without_offload(self, blocks, x, pre_infer_out):\n    for block_idx in range(len(blocks)):\n        self.block_idx = block_idx\n        x = self.infer_block(blocks[block_idx], x, pre_infer_out)\n    return x\n```\n\nEach block execution involves three phases plus modulation preparation.\n\nSources: [lightx2v/models/networks/wan/infer/transformer_infer.py:125-129]()\n\n### Phase 0: Self-Attention\n\nThe self-attention phase processes spatial-temporal relationships:\n\n1. **Modulation Extraction**: Splits `embed0` into 6 modulation parameters\n2. **Normalization**: Applies layer norm and adaptive modulation via `modulate(norm1_out, scale_msa, shift_msa)`\n3. **QKV Projection**: Projects to query, key, value with RMSNorm on Q and K\n4. **Rotary Position Embedding**: Applies 3D RoPE to Q and K via `apply_rope_func(q, k, cos_sin)`\n5. **Attention Computation**: Executes using selected attention operator (Flash Attention 2/3, Sage Attention, etc.)\n6. **Output Projection**: Projects attention output back to model dimension\n\nSources: [lightx2v/models/networks/wan/infer/transformer_infer.py:156-253]()\n\n### RoPE Application\n\nThe rotary position embedding encodes 3D spatial-temporal positions:\n\n```python\n# cos_sin_cache contains pre-computed frequencies for (T, H, W) dimensions\nq, k = self.apply_rope_func(q, k, cos_sin)\n```\n\nDifferent implementations are available:\n- `flashinfer`: Uses flashinfer library for optimized RoPE\n- `torch`: Complex number-based torch implementation\n- `torch_naive`: Explicit cosine/sine rotation\n\nSources: [lightx2v/models/networks/wan/infer/transformer_infer.py:194](), [lightx2v/models/networks/wan/infer/utils.py:12-124]()\n\n### Phase 1: Cross-Attention\n\nCross-attention integrates conditioning signals:\n\n1. **Residual Addition**: `x = x + y_out * gate_msa` (gated residual from self-attention)\n2. **Normalization**: Applies `norm3` to x\n3. **Text Cross-Attention**: \n   - Q from normalized x\n   - K, V from text context\n   - Applies `cross_attn_1` operator\n4. **Image Cross-Attention** (i2v/flf2v/animate tasks only):\n   - Q from normalized x (same as text)\n   - K, V from CLIP image features (first 257 tokens of context)\n   - Applies `cross_attn_2` operator\n   - Adds to text attention output\n5. **Output Projection**: `cross_attn_o.apply(attn_out)`\n\nSources: [lightx2v/models/networks/wan/infer/transformer_infer.py:255-331]()\n\n### Phase 2: Feed-Forward Network\n\nThe FFN phase applies non-linear transformations:\n\n1. **Residual Addition**: `x = x + attn_out`\n2. **Normalization**: Applies `norm2` with adaptive modulation via `c_scale_msa` and `c_shift_msa`\n3. **Expansion**: `y = ffn_0.apply(norm2_out)` (typically 4x dimension expansion)\n4. **Activation**: `y = gelu(y, approximate=\"tanh\")`\n5. **Projection**: `y = ffn_2.apply(y)` (back to model dimension)\n6. **Gated Residual**: `x = x + y * c_gate_msa`\n\nSources: [lightx2v/models/networks/wan/infer/transformer_infer.py:333-376]()\n\n### Attention Operator Selection\n\nThe framework supports multiple attention implementations via `ATTN_WEIGHT_REGISTER`:\n- `flash_attn2`: Flash Attention 2 (general purpose)\n- `flash_attn3`: Flash Attention 3 (Hopper GPUs)\n- `sage_attn2`: Sage Attention 2 (recommended)\n- `radial_attn`: Radial Attention (spatial locality)\n- `nbhd_attn`: Neighborhood Attention (configurable windows)\n\nThe selected operator is applied uniformly across all blocks and phases.\n\nSources: [lightx2v/models/networks/wan/infer/transformer_infer.py:20-24]()\n\n## Stage 3: Post-Inference\n\nThe post-inference stage converts the processed patch sequence back to latent space format.\n\n### Final Modulation and Projection\n\nBefore unpatchifying, the output passes through final normalization and modulation:\n\n```python\ndef infer_non_blocks(self, weights, x, e):\n    modulation = weights.head_modulation.tensor  # [1, 2, dim]\n    e = (modulation + e.unsqueeze(1)).chunk(2, dim=1)\n    \n    x = weights.norm.apply(x)\n    x = x * (1 + e[1].squeeze()) + e[0].squeeze()\n    x = weights.head.apply(x)  # Final projection\n    return x\n```\n\nSources: [lightx2v/models/networks/wan/model.py:101-123]()\n\n### Unpatchifying\n\nThe `WanPostInfer.unpatchify()` method reconstructs the 4D latent tensor:\n\n1. **Reshape to Grid**: Converts `[seq_len, dim]` to `[T', H', W', patch_t, patch_h, patch_w, out_dim]`\n2. **Dimension Reordering**: Applies einsum `\"fhwpqrc-\u003ecfphqwr\"` to interleave patch dimensions\n3. **Final Reshape**: Produces `[out_dim, T, H, W]` latent tensor\n\n```python\ndef unpatchify(self, x, grid_sizes):\n    c = self.out_dim\n    x = x[:math.prod(grid_sizes)].view(*grid_sizes, *self.patch_size, c)\n    x = torch.einsum(\"fhwpqrc-\u003ecfphqwr\", x)\n    x = x.reshape(c, *[i * j for i, j in zip(grid_sizes, self.patch_size)])\n    return [x]\n```\n\nSources: [lightx2v/models/networks/wan/infer/post_infer.py:26-31]()\n\n### Output Format\n\nThe post-inference stage returns a list containing the noise prediction tensor in latent space format `[C, T, H, W]`, which the scheduler uses to update the latents.\n\nSources: [lightx2v/models/networks/wan/infer/post_infer.py:17-24]()\n\n## Scheduler Integration\n\nThe three-stage pipeline is orchestrated by the scheduler within a multi-step diffusion loop.\n\n### Per-Timestep Execution\n\n```mermaid\ngraph TB\n    subgraph \"Scheduler Control Flow\"\n        StepIndex[\"step_index = 0..infer_steps-1\"]\n        \n        StepPre[\"scheduler.step_pre(step_index)\u003cbr/\u003e- Set current timestep\u003cbr/\u003e- Update infer_condition flag\"]\n        \n        Infer[\"model.infer(inputs)\u003cbr/\u003eThree-Stage Pipeline\"]\n        \n        subgraph \"Three Stages\"\n            Pre[\"pre_infer.infer()\"]\n            Trans[\"transformer_infer.infer()\"]\n            Post[\"post_infer.infer()\"]\n            Pre --\u003e Trans --\u003e Post\n        end\n        \n        StepPost[\"scheduler.step_post()\u003cbr/\u003e- Apply predictor-corrector\u003cbr/\u003e- Update scheduler.latents\"]\n        \n        StepIndex --\u003e StepPre\n        StepPre --\u003e Infer\n        Infer --\u003e StepPost\n        StepPost -.-\u003e|\"next iteration\"| StepIndex\n    end\n    \n    FinalLatents[\"scheduler.latents\u003cbr/\u003e(denoised)\"]\n    StepPost -.-\u003e|\"after all steps\"| FinalLatents\n```\n\n**Diagram: Scheduler Integration with Three-Stage Pipeline**\n\nSources: [lightx2v/models/runners/default_runner.py:171-204](), [lightx2v/models/schedulers/wan/scheduler.py:376-406]()\n\n### Step Preparation and Updates\n\nThe scheduler manages the diffusion process:\n\n1. **`step_pre(step_index)`**: \n   - Sets `timestep_input` from pre-computed timestep schedule\n   - Configures `infer_condition` flag for CFG\n   - Updates internal state variables\n\n2. **`step_post()`**:\n   - Applies predictor-corrector update to latents\n   - Uses multi-step UniP-BH solver for higher-order updates\n   - Caches previous model outputs for multi-step methods\n\nSources: [lightx2v/models/schedulers/wan/scheduler.py:376-406]()\n\n### Classifier-Free Guidance\n\nWhen CFG is enabled, the three-stage pipeline executes twice per timestep:\n\n```python\ndef infer(self, inputs):\n    if self.config[\"enable_cfg\"]:\n        # Conditional pass\n        noise_pred_cond = self._infer_cond_uncond(inputs, infer_condition=True)\n        # Unconditional pass\n        noise_pred_uncond = self._infer_cond_uncond(inputs, infer_condition=False)\n        # Combine predictions\n        self.scheduler.noise_pred = (noise_pred_uncond + \n            self.scheduler.sample_guide_scale * (noise_pred_cond - noise_pred_uncond))\n    else:\n        self.scheduler.noise_pred = self._infer_cond_uncond(inputs, infer_condition=True)\n```\n\nSources: [lightx2v/models/networks/wan/model.py:424-448]()\n\n## Complete Data Flow\n\nThe following diagram shows the complete data flow through all three stages, including key tensor shapes and intermediate representations:\n\n```mermaid\ngraph TB\n    subgraph \"Input Preparation (Outside Pipeline)\"\n        TextEnc[\"Text Encoder\u003cbr/\u003e context [text_len, dim]\"]\n        ImgEncCLIP[\"Image Encoder (CLIP)\u003cbr/\u003e clip_out [257, dim]\"]\n        ImgEncVAE[\"VAE Encoder\u003cbr/\u003e vae_out [C, T, H/8, W/8]\"]\n        AudioEnc[\"Audio Encoder\u003cbr/\u003e audio_feat [audio_len, dim]\"]\n        \n        Inputs[\"inputs dict\"]\n        TextEnc --\u003e Inputs\n        ImgEncCLIP --\u003e Inputs\n        ImgEncVAE --\u003e Inputs\n        AudioEnc --\u003e Inputs\n    end\n    \n    SchedLatents[\"scheduler.latents\u003cbr/\u003e[C, T, H, W]\"]\n    SchedTime[\"scheduler.timestep_input\u003cbr/\u003e[1]\"]\n    \n    subgraph \"Stage 1: Pre-Inference\"\n        PatchEmbed1[\"patch_embedding\u003cbr/\u003e[C, T, H, W]  patches\"]\n        TimeEmbed1[\"time embeddings\u003cbr/\u003esinusoidal + MLP\"]\n        TextProj1[\"text projections\u003cbr/\u003eMLP layers\"]\n        ImgProj1[\"image projections\u003cbr/\u003e4-layer MLP\"]\n        \n        PreOut[\"WanPreInferModuleOutput\u003cbr/\u003ex: [seq_len, dim]\u003cbr/\u003eembed0: [6, dim]\u003cbr/\u003econtext: [ctx_len, dim]\u003cbr/\u003egrid_sizes: (T', H', W')\"]\n        \n        SchedLatents --\u003e PatchEmbed1\n        Inputs --\u003e PatchEmbed1\n        SchedTime --\u003e TimeEmbed1\n        Inputs --\u003e TextProj1\n        Inputs --\u003e ImgProj1\n        \n        PatchEmbed1 --\u003e PreOut\n        TimeEmbed1 --\u003e PreOut\n        TextProj1 --\u003e PreOut\n        ImgProj1 --\u003e PreOut\n    end\n    \n    subgraph \"Stage 2: Transformer Inference\"\n        Block1[\"Block 0\u003cbr/\u003e3 phases\"]\n        Block2[\"Block 1\u003cbr/\u003e3 phases\"]\n        BlockN[\"Block N-1\u003cbr/\u003e3 phases\"]\n        \n        Block1 --\u003e Block2\n        Block2 -.-\u003e BlockN\n        \n        PreOut --\u003e Block1\n        \n        TransOut[\"x: [seq_len, dim]\u003cbr/\u003e(processed)\"]\n        BlockN --\u003e TransOut\n    end\n    \n    subgraph \"Stage 3: Post-Inference\"\n        FinalMod[\"final modulation\u003cbr/\u003enorm + scale/shift\"]\n        HeadProj[\"head projection\u003cbr/\u003eto out_dim\"]\n        Unpatch[\"unpatchify\u003cbr/\u003ereshape + einsum\"]\n        \n        TransOut --\u003e FinalMod\n        FinalMod --\u003e HeadProj\n        HeadProj --\u003e Unpatch\n        \n        NoisePred[\"noise_pred\u003cbr/\u003e[C, T, H, W]\"]\n        Unpatch --\u003e NoisePred\n    end\n    \n    NoisePred --\u003e SchedUpdate[\"scheduler.step_post()\u003cbr/\u003eUpdates latents\"]\n    SchedUpdate --\u003e SchedLatents\n```\n\n**Diagram: Complete Three-Stage Data Flow with Tensor Shapes**\n\nSources: [lightx2v/models/networks/wan/model.py:457-478](), [lightx2v/models/networks/wan/infer/pre_infer.py:26-138](), [lightx2v/models/networks/wan/infer/transformer_infer.py:91-123](), [lightx2v/models/networks/wan/infer/post_infer.py:17-31]()\n\n## Configuration and Variants\n\n### Inference Class Selection\n\nThe pipeline supports different transformer inference implementations selected via `config[\"feature_caching\"]`:\n\n| Feature Caching Mode | Transformer Inference Class | Description |\n|---------------------|---------------------------|-------------|\n| `NoCaching` | `WanTransformerInfer` | Standard full computation |\n| `Tea` | `WanTransformerInferTeaCaching` | TeaCache feature reuse |\n| `TaylorSeer` | `WanTransformerInferTaylorCaching` | Taylor expansion caching |\n| `Ada` | `WanTransformerInferAdaCaching` | Adaptive caching |\n| `Mag` | `WanTransformerInferMagCaching` | MagCache strategy |\n| `Custom` | `WanTransformerInferCustomCaching` | Custom caching logic |\n\nAll variants follow the same three-stage interface but optimize transformer inference differently.\n\nSources: [lightx2v/models/networks/wan/model.py:100-123]()\n\n### CPU Offloading\n\nWhen `cpu_offload=True`, weights can be offloaded at different granularities:\n\n- **`model`**: Entire model moves to GPU at start, CPU at end\n- **`block`**: Two blocks rotate through GPU memory\n- **`phase`**: Individual phases (self-attn, cross-attn, FFN) rotate\n\nThe offloading manager handles weight transfers while maintaining the same three-stage interface.\n\nSources: [lightx2v/models/networks/wan/model.py:417-455](), [lightx2v/models/networks/wan/infer/offload/transformer_infer.py]()\n\n### Quantization\n\nThe pipeline supports quantized weights via `MM_WEIGHT_REGISTER`:\n\n- `int8-triton`, `int8-vllm`, `int8-sgl`, `int8-q8f`: INT8 quantization variants\n- `fp8-triton`, `fp8-vllm`, `fp8-sgl`, `fp8-q8f`: FP8 quantization variants\n- `nvfp4`, `mxfp4`, `mxfp6`, `mxfp8`: Low-precision formats\n- `gguf-*`: GGUF quantization formats\n\nQuantization is transparent to the three-stage pipeline structure.\n\nSources: [lightx2v/models/networks/wan/model.py:61-94](), [lightx2v/common/ops/mm/mm_weight.py]()\n\n### Sequence Parallelism\n\nWith `seq_parallel=True`, the sequence dimension is split across GPUs:\n\n```python\n# Pre-processing: split sequence across ranks\nif self.config[\"seq_parallel\"]:\n    pre_infer_out = self._seq_parallel_pre_process(pre_infer_out)\n\nx = self.transformer_infer.infer(self.transformer_weights, pre_infer_out)\n\n# Post-processing: gather sequence from all ranks\nif self.config[\"seq_parallel\"]:\n    x = self._seq_parallel_post_process(x)\n```\n\nThis enables processing longer sequences by distributing computation across multiple GPUs.\n\nSources: [lightx2v/models/networks/wan/model.py:464-470](), [lightx2v/models/networks/wan/model.py:481-512]()\n\n---\n\n**Summary**: The three-stage inference pipeline provides a clean separation of concerns: pre-inference prepares inputs and embeddings, transformer inference processes through attention and FFN layers, and post-inference reconstructs the latent representation. This architecture enables modular optimization strategies (caching, offloading, quantization) while maintaining a consistent interface across model variants."])</script><script>self.__next_f.push([1,"29:T6b20,"])</script><script>self.__next_f.push([1,"# Scheduler System and Diffusion Process\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [configs/seko_talk/shot/rs2v/rs2v.json](configs/seko_talk/shot/rs2v/rs2v.json)\n- [configs/seko_talk/shot/stream/f2v.json](configs/seko_talk/shot/stream/f2v.json)\n- [configs/seko_talk/shot/stream/s2v.json](configs/seko_talk/shot/stream/s2v.json)\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/schedulers/scheduler.py](lightx2v/models/schedulers/scheduler.py)\n- [lightx2v/models/schedulers/wan/audio/scheduler.py](lightx2v/models/schedulers/wan/audio/scheduler.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/shot_runner/rs2v_infer.py](lightx2v/shot_runner/rs2v_infer.py)\n- [lightx2v/shot_runner/shot_base.py](lightx2v/shot_runner/shot_base.py)\n- [lightx2v/shot_runner/stream_infer.py](lightx2v/shot_runner/stream_infer.py)\n- [lightx2v/utils/input_info.py](lightx2v/utils/input_info.py)\n- [lightx2v/utils/set_config.py](lightx2v/utils/set_config.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document explains the scheduler system responsible for managing the diffusion denoising process in LightX2V. Schedulers orchestrate timestep management, latent noise initialization, noise schedule computation, and integration of model predictions during iterative denoising. The scheduler acts as the state machine coordinating the diffusion loop between the runner and transformer model.\n\nFor information about the overall inference pipeline flow, see [Three-Stage Inference Pipeline](#4.2). For details on the runner orchestration layer, see [Runner System and Registry Pattern](#4.1).\n\n---\n\n## Scheduler Architecture Overview\n\nThe scheduler system implements a hierarchical design with base functionality in `BaseScheduler` and specialized implementations for different model architectures and optimization strategies.\n\n**Scheduler Class Hierarchy**\n\n```mermaid\nclassDiagram\n    class BaseScheduler {\n        +config: dict\n        +step_index: int\n        +infer_steps: int\n        +prepare()\n        +step_pre()\n        +step_post()\n        +clear()\n    }\n    \n    class WanScheduler {\n        +latents: Tensor\n        +timesteps: Tensor\n        +sigmas: Tensor\n        +noise_pred: Tensor\n        +sample_guide_scale: float\n        +num_train_timesteps: int\n        +prepare_latents()\n        +set_timesteps()\n        +multistep_uni_pc_bh_update()\n    }\n    \n    class EulerScheduler {\n        +audio_adapter: AudioAdapter\n        +prev_latents: Tensor\n        +prev_len: int\n        +set_audio_adapter()\n        +reset()\n        +unsqueeze_to_ndim()\n    }\n    \n    class WanSchedulerCaching {\n        +caching_records: List\n        +cached_model_outputs: Dict\n        +get_cached_output()\n        +save_cached_output()\n    }\n    \n    class WanSchedulerTaylorCaching {\n        +taylor_order: int\n        +compute_taylor_expansion()\n    }\n    \n    class WanScheduler4ChangingResolutionInterface {\n        +resolution_rate: List\n        +changing_resolution_index: int\n        +switch_resolution()\n    }\n    \n    BaseScheduler \u003c|-- WanScheduler\n    WanScheduler \u003c|-- EulerScheduler\n    WanScheduler \u003c|-- WanSchedulerCaching\n    WanSchedulerCaching \u003c|-- WanSchedulerTaylorCaching\n    WanScheduler \u003c|-- WanScheduler4ChangingResolutionInterface\n```\n\n**Sources:**\n- [lightx2v/models/schedulers/wan/scheduler.py:11-98]()\n- [lightx2v/models/schedulers/wan/audio/scheduler.py:13-16]()\n- [lightx2v/models/schedulers/wan/feature_caching/scheduler.py]()\n\n---\n\n## Core Scheduler Classes\n\n### BaseScheduler\n\nThe abstract base class defines the scheduler interface:\n\n| Method | Responsibility |\n|--------|---------------|\n| `prepare(seed, latent_shape, image_encoder_output)` | Initialize latents and timestep schedule |\n| `step_pre(step_index)` | Pre-process before model inference |\n| `step_post()` | Post-process to update latents after noise prediction |\n| `clear()` | Clean up scheduler state after inference |\n\n**Sources:** [lightx2v/models/schedulers/scheduler.py]()\n\n### WanScheduler\n\nThe primary scheduler for WAN model family implements the full diffusion denoising process.\n\n**Key Attributes:**\n\n```python\n# Timestep management\nself.timesteps: Tensor        # Current timestep values [T]\nself.sigmas: Tensor          # Noise schedule (t) values\nself.step_index: int         # Current denoising step index\n\n# State tensors\nself.latents: Tensor         # Current latent state [C, F, H, W]\nself.noise_pred: Tensor      # Model noise prediction output\nself.mask: Tensor           # Inpainting mask (for i2v tasks)\n\n# Configuration\nself.infer_steps: int       # Total denoising steps\nself.sample_shift: float    # Noise schedule shift parameter\nself.sample_guide_scale: float  # CFG guidance scale\n```\n\n**Noise Schedule Computation:**\n\nThe scheduler computes a shifted noise schedule that controls denoising strength:\n\n```\n(t) = shift * _base(t) / (1 + (shift - 1) * _base(t))\nt = (t) * num_train_timesteps\n```\n\nWhere `shift` (default 1.0) adjusts the schedule steepness. Higher shift values concentrate denoising power in early steps.\n\n**Sources:**\n- [lightx2v/models/schedulers/wan/scheduler.py:11-48]()\n- [lightx2v/models/schedulers/wan/scheduler.py:71-92]()\n\n### EulerScheduler\n\nSpecialized scheduler for audio-driven video generation with temporal conditioning support.\n\n**Audio-Specific Features:**\n\n1. **Audio Adapter Integration:** Stores reference to `AudioAdapter` for time embedding computation\n2. **Previous Frame Conditioning:** Maintains `prev_latents` and `prev_len` for temporal continuity across audio segments\n3. **Per-Token Timesteps:** Computes spatially-varying timesteps based on inpainting mask\n\n**Sources:**\n- [lightx2v/models/schedulers/wan/audio/scheduler.py:13-106]()\n- [lightx2v/models/schedulers/wan/audio/scheduler.py:26-58]()\n\n---\n\n## Diffusion Process Lifecycle\n\nThe diffusion process follows a three-phase lifecycle per denoising step, coordinated between the runner, scheduler, and model.\n\n**Denoising Loop Sequence**\n\n```mermaid\nsequenceDiagram\n    participant R as Runner\n    participant S as Scheduler\n    participant M as Model\n    \n    Note over R,M: Initialization Phase\n    R-\u003e\u003eS: prepare(seed, latent_shape, image_encoder_output)\n    S-\u003e\u003eS: prepare_latents() - Initialize noisy latents\n    S-\u003e\u003eS: set_timesteps() - Compute schedule\n    \n    Note over R,M: Iterative Denoising Loop\n    loop for step_index in range(infer_steps)\n        R-\u003e\u003eS: step_pre(step_index)\n        S-\u003e\u003eS: Update step_index\n        S-\u003e\u003eS: Compute timestep_input = timesteps[step_index]\n        S-\u003e\u003eS: Set infer_condition flag (for CFG)\n        \n        alt CFG Enabled and CFG Parallel\n            R-\u003e\u003eM: infer(inputs) - Conditional (rank 0)\n            R-\u003e\u003eM: infer(inputs) - Unconditional (rank 1)\n            M--\u003e\u003eR: noise_pred_cond, noise_pred_uncond\n            R-\u003e\u003eS: noise_pred = uncond + scale * (cond - uncond)\n        else CFG Enabled Sequential\n            R-\u003e\u003eM: infer(inputs) - Conditional\n            M--\u003e\u003eR: noise_pred_cond\n            R-\u003e\u003eM: infer(inputs) - Unconditional\n            M--\u003e\u003eR: noise_pred_uncond\n            R-\u003e\u003eS: noise_pred = uncond + scale * (cond - uncond)\n        else No CFG\n            R-\u003e\u003eM: infer(inputs)\n            M--\u003e\u003eR: noise_pred\n            S-\u003e\u003eS: Store noise_pred\n        end\n        \n        R-\u003e\u003eS: step_post()\n        S-\u003e\u003eS: Update latents using noise_pred\n        S-\u003e\u003eS: Apply solver (Euler/UniPC)\n    end\n    \n    Note over R,M: Finalization\n    R-\u003e\u003eS: latents - Final denoised latents\n    R-\u003e\u003eR: VAE decode(latents) -\u003e video\n```\n\n**Sources:**\n- [lightx2v/models/runners/default_runner.py:173-206]()\n- [lightx2v/models/networks/wan/model.py:160-192]()\n\n---\n\n## Timestep and Noise Schedule Management\n\n### Timestep Computation\n\nThe scheduler maps denoising steps to continuous timesteps using a noise schedule:\n\n**WanScheduler Schedule:**\n\n```python\n# Linear schedule in sigma space\nsigmas = np.linspace(sigma_max, sigma_min, infer_steps + 1)[:-1]\n\n# Apply shift transformation\nsigmas = shift * sigmas / (1 + (shift - 1) * sigmas)\n\n# Convert to timesteps\ntimesteps = sigmas * num_train_timesteps\n```\n\n**Key Parameters:**\n\n| Parameter | Default | Description |\n|-----------|---------|-------------|\n| `num_train_timesteps` | 1000 | Training timesteps (max t value) |\n| `sample_shift` | 1.0 | Schedule shift parameter |\n| `sigma_max` | 1.0 | Maximum noise level |\n| `sigma_min` | 0.0 | Minimum noise level |\n| `infer_steps` | 40-50 | Number of denoising iterations |\n\n**Timestep Usage Flow:**\n\n```mermaid\ngraph TB\n    A[\"set_timesteps()\u003cbr/\u003e(infer_steps, shift)\"] --\u003e B[\"Compute sigmas array\u003cbr/\u003e, , ..., \"]\n    B --\u003e C[\"Convert to timesteps\u003cbr/\u003et, t, ..., t\"]\n    \n    D[\"step_pre(step_index)\"] --\u003e E[\"timestep_input = timesteps[step_index]\"]\n    E --\u003e F[\"Model uses t as conditioning\"]\n    \n    F --\u003e G[\"Model predicts noise_pred(x_t, t)\"]\n    G --\u003e H[\"step_post()\"]\n    H --\u003e I[\"Update: x_{t-1} = solver(x_t, noise_pred, _t, _{t-1})\"]\n```\n\n**Sources:**\n- [lightx2v/models/schedulers/wan/scheduler.py:71-97]()\n- [lightx2v/models/schedulers/wan/scheduler.py:31-54]()\n\n### EulerScheduler Timestep Extensions\n\nFor audio tasks, `EulerScheduler` computes per-token timesteps to support spatially-varying denoising:\n\n```python\n# Compute per-latent-token timesteps based on mask\ntemp_ts = (mask[0][:, ::2, ::2] * timestep_input).flatten()\n\n# Pad to sequence length\nmax_seq_len = ((F - 1) // vae_stride[0] + 1) * per_latent_token_len\ntimestep_input = torch.cat([\n    temp_ts,\n    temp_ts.new_ones(max_seq_len - temp_ts.size(0)) * timestep_input\n])\n\n# Add reference frame timesteps (zero for no denoising)\ntimestep_input = torch.cat([timestep_input, torch.zeros(per_latent_token_len)])\n```\n\nThis enables masked denoising where previous frames remain fixed (t=0) while new frames are denoised (t\u003e0).\n\n**Sources:** [lightx2v/models/schedulers/wan/audio/scheduler.py:37-57]()\n\n---\n\n## Latent Preparation and Masking\n\n### Latent Initialization\n\nThe scheduler initializes noisy latents using a seeded random generator:\n\n```python\ndef prepare_latents(self, seed, latent_shape, dtype=torch.float32):\n    self.generator = torch.Generator(device=AI_DEVICE).manual_seed(seed)\n    self.latents = torch.randn(\n        latent_shape[0],  # Channels (typically 16)\n        latent_shape[1],  # Frames\n        latent_shape[2],  # Height\n        latent_shape[3],  # Width\n        dtype=dtype,\n        device=AI_DEVICE,\n        generator=self.generator,\n    )\n```\n\n**Latent Shape Conventions:**\n\n| Dimension | Index | Typical Value | Description |\n|-----------|-------|---------------|-------------|\n| Channels | 0 | 16 | VAE latent channels |\n| Frames | 1 | (video_length - 1) // 4 + 1 | Temporal dimension (4x downsampled) |\n| Height | 2 | height // 8 | Spatial height (8x downsampled) |\n| Width | 3 | width // 8 | Spatial width (8x downsampled) |\n\n**Sources:** [lightx2v/models/schedulers/wan/scheduler.py:56-69]()\n\n### Inpainting Masks for I2V Tasks\n\nFor image-to-video and audio-to-video tasks, the scheduler applies masks to preserve reference frames:\n\n**WAN 2.2 I2V Masking:**\n\n```python\nif self.config[\"model_cls\"] == \"wan2.2\" and self.config[\"task\"] in [\"i2v\", \"s2v\", \"rs2v\"]:\n    self.mask = masks_like(self.latents, zero=True)\n    self.latents = (1.0 - self.mask) * self.vae_encoder_out + self.mask * self.latents\n```\n\nWhere:\n- `mask = 0` for reference frames (preserve VAE encoded image)\n- `mask = 1` for generated frames (use random noise)\n\n**Audio Scheduler Masking:**\n\n```python\nif self.config[\"model_cls\"] == \"wan2.2_audio\":\n    self.mask = masks_like(self.latents, zero=True, prev_len=self.prev_len)\n    if self.prev_latents is not None:\n        self.latents = (1.0 - self.mask) * self.prev_latents + self.mask * self.latents\n```\n\nThis enables temporal continuity across audio segments by preserving previous frames from the last segment.\n\n**Mask Application Diagram:**\n\n```mermaid\ngraph LR\n    subgraph \"Latent Tensor [16, F, H, W]\"\n        L0[\"Frame 0\u003cbr/\u003e(Reference)\"]\n        L1[\"Frame 1\u003cbr/\u003e(Generate)\"]\n        L2[\"Frame 2\u003cbr/\u003e(Generate)\"]\n        Ln[\"Frame N\u003cbr/\u003e(Generate)\"]\n    end\n    \n    subgraph \"Mask Tensor [4, F, H, W]\"\n        M0[\"mask = 0\"]\n        M1[\"mask = 1\"]\n        M2[\"mask = 1\"]\n        Mn[\"mask = 1\"]\n    end\n    \n    subgraph \"Result\"\n        R0[\"VAE Encoded\"]\n        R1[\"Random Noise\"]\n        R2[\"Random Noise\"]\n        Rn[\"Random Noise\"]\n    end\n    \n    L0 --\u003e M0\n    L1 --\u003e M1\n    L2 --\u003e M2\n    Ln --\u003e Mn\n    \n    M0 --\u003e R0\n    M1 --\u003e R1\n    M2 --\u003e R2\n    Mn --\u003e Rn\n```\n\n**Sources:**\n- [lightx2v/models/schedulers/wan/scheduler.py:67-69]()\n- [lightx2v/models/schedulers/wan/audio/scheduler.py:71-73]()\n\n---\n\n## CFG Implementation\n\nClassifier-Free Guidance (CFG) improves generation quality by combining conditional and unconditional predictions.\n\n### CFG Formula\n\n```\nnoise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n```\n\n### Sequential CFG\n\nStandard CFG runs two forward passes per denoising step:\n\n```python\n# Model infer() method with sequential CFG\nif self.config[\"enable_cfg\"]:\n    # Conditional pass\n    noise_pred_cond = self._infer_cond_uncond(inputs, infer_condition=True)\n    \n    # Unconditional pass\n    noise_pred_uncond = self._infer_cond_uncond(inputs, infer_condition=False)\n    \n    # Combine predictions\n    self.scheduler.noise_pred = (\n        noise_pred_uncond + \n        self.scheduler.sample_guide_scale * (noise_pred_cond - noise_pred_uncond)\n    )\nelse:\n    # No CFG - single conditional pass\n    self.scheduler.noise_pred = self._infer_cond_uncond(inputs, infer_condition=True)\n```\n\n**Sources:** [lightx2v/models/networks/wan/model.py:168-192]()\n\n### CFG Parallel Mode\n\nFor multi-GPU setups, CFG parallel mode runs conditional and unconditional passes simultaneously on different GPUs, doubling throughput:\n\n```python\nif self.config[\"cfg_parallel\"]:\n    cfg_p_group = self.config[\"device_mesh\"].get_group(mesh_dim=\"cfg_p\")\n    cfg_p_rank = dist.get_rank(cfg_p_group)\n    \n    if cfg_p_rank == 0:\n        noise_pred = self._infer_cond_uncond(inputs, infer_condition=True)\n    else:\n        noise_pred = self._infer_cond_uncond(inputs, infer_condition=False)\n    \n    # Gather from both ranks\n    noise_pred_list = [torch.zeros_like(noise_pred) for _ in range(2)]\n    dist.all_gather(noise_pred_list, noise_pred, group=cfg_p_group)\n    noise_pred_cond = noise_pred_list[0]\n    noise_pred_uncond = noise_pred_list[1]\n    \n    self.scheduler.noise_pred = (\n        noise_pred_uncond + \n        self.scheduler.sample_guide_scale * (noise_pred_cond - noise_pred_uncond)\n    )\n```\n\n**CFG Configuration:**\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `enable_cfg` | bool | Enable CFG (default: True for most tasks) |\n| `sample_guide_scale` | float | Guidance strength (typical: 4.0-7.0) |\n| `cfg_parallel` | bool | Use parallel CFG across 2 GPUs |\n| `cfg_p_size` | int | Number of GPUs for CFG parallelism (must be 2) |\n\n**Sources:** [lightx2v/models/networks/wan/model.py:169-189]()\n\n### Dynamic CFG Scaling\n\nSome schedulers support dynamic CFG scales that vary per timestep:\n\n```python\nif self.enable_dynamic_cfg:\n    current_cfg_scale = self.compute_dynamic_cfg_scale(self.step_index)\n    self.sample_guide_scale = current_cfg_scale\n```\n\n**Sources:** [lightx2v/models/schedulers/wan/scheduler.py]()\n\n---\n\n## Solver Algorithms\n\nSchedulers implement different numerical solvers to update latents from noise predictions.\n\n### Euler Solver\n\nThe `EulerScheduler` uses a first-order Euler method:\n\n```python\ndef step_post(self):\n    model_output = self.noise_pred.to(torch.float32)\n    sample = self.latents.to(torch.float32)\n    \n    sigma = self.unsqueeze_to_ndim(self.sigmas[self.step_index], sample.ndim)\n    sigma_next = self.unsqueeze_to_ndim(self.sigmas[self.step_index + 1], sample.ndim)\n    \n    # Euler update: x_{t-1} = x_t + (_{t-1} - _t) * noise_pred\n    x_t_next = sample + (sigma_next - sigma) * model_output\n    self.latents = x_t_next\n```\n\n**Sources:** [lightx2v/models/schedulers/wan/audio/scheduler.py:87-93]()\n\n### UniPC Solver\n\nThe `WanScheduler` implements a higher-order UniPC (Unified Predictor-Corrector) solver for better quality with fewer steps:\n\n```python\ndef multistep_uni_pc_bh_update(self, model_output, sample):\n    \"\"\"\n    Multi-step UniPC-BH (Boundary Handling) solver\n    Uses stored previous outputs for higher-order interpolation\n    \"\"\"\n    # Store current output\n    self.model_outputs[self.lower_order_nums] = model_output\n    \n    # Use solver_order previous outputs for polynomial interpolation\n    if self.lower_order_nums \u003c self.solver_order - 1:\n        # Low-order update (not enough history)\n        return self.multistep_uni_pc_bh_update_low_order(...)\n    else:\n        # High-order update (full history available)\n        return self.multistep_uni_pc_bh_update_high_order(...)\n```\n\nUniPC maintains `self.model_outputs` history buffer and uses higher-order extrapolation for smoother denoising trajectories.\n\n**Sources:** [lightx2v/models/schedulers/wan/scheduler.py:125-250]()\n\n### Solver Comparison\n\n| Solver | Order | Steps Required | Quality | Speed |\n|--------|-------|---------------|---------|-------|\n| Euler | 1st | 40-50 | Good | Fast |\n| UniPC | 2nd | 30-40 | Better | Medium |\n| UniPC-BH | 2nd | 25-35 | Best | Medium |\n\n---\n\n## Audio-Specific Scheduler Extensions\n\nThe `EulerScheduler` extends the base scheduler with audio-driven video generation support.\n\n### Audio Adapter Integration\n\n```python\ndef set_audio_adapter(self, audio_adapter):\n    \"\"\"Store reference to AudioAdapter for time embedding\"\"\"\n    self.audio_adapter = audio_adapter\n\ndef step_pre(self, step_index):\n    super().step_pre(step_index)\n    \n    # Compute audio time embeddings\n    if self.audio_adapter.cpu_offload:\n        self.audio_adapter.time_embedding.to(AI_DEVICE)\n    \n    self.audio_adapter_t_emb = self.audio_adapter.time_embedding(\n        self.timestep_input\n    ).unflatten(1, (3, -1))\n    \n    if self.audio_adapter.cpu_offload:\n        self.audio_adapter.time_embedding.to(\"cpu\")\n```\n\nThe time embedding tensor is used by the audio adapter cross-attention layers to condition on the current timestep.\n\n**Sources:** [lightx2v/models/schedulers/wan/audio/scheduler.py:26-35]()\n\n### Segment Reset for Continuity\n\nWhen processing multiple audio segments sequentially, the scheduler resets latents while preserving previous frames:\n\n```python\ndef reset(self, seed, latent_shape, image_encoder_output=None):\n    \"\"\"Reset for next audio segment, preserving temporal continuity\"\"\"\n    if self.config[\"model_cls\"] == \"wan2.2_audio\":\n        self.prev_latents = image_encoder_output[\"prev_latents\"]\n        self.prev_len = image_encoder_output[\"prev_len\"]\n    \n    # Re-initialize latents with new seed but apply mask\n    self.prepare_latents(seed, latent_shape, dtype=torch.float32)\n```\n\nThis enables smooth transitions between 5-second audio segments in long-form video generation.\n\n**Sources:** [lightx2v/models/schedulers/wan/audio/scheduler.py:97-101]()\n\n---\n\n## Integration with Model Inference\n\nThe scheduler interfaces with other system components through well-defined protocols.\n\n**Component Integration Diagram:**\n\n```mermaid\ngraph TB\n    subgraph \"Runner Layer\"\n        R[DefaultRunner]\n        AR[WanAudioRunner]\n    end\n    \n    subgraph \"Scheduler Layer\"\n        WS[WanScheduler]\n        ES[EulerScheduler]\n        WSC[WanSchedulerCaching]\n    end\n    \n    subgraph \"Model Layer\"\n        WM[WanModel]\n        WAM[WanAudioModel]\n        PI[WanPreInfer]\n        TI[WanTransformerInfer]\n        POI[WanPostInfer]\n    end\n    \n    R --\u003e|\"init_scheduler()\u003cbr/\u003eset scheduler\"| WS\n    AR --\u003e|\"init_scheduler()\u003cbr/\u003eset scheduler\"| ES\n    \n    R --\u003e|\"run_segment()\u003cbr/\u003estep_pre/post\"| WS\n    AR --\u003e|\"init_run_segment()\u003cbr/\u003ereset()\"| ES\n    \n    WS --\u003e|\"set_scheduler()\"| WM\n    ES --\u003e|\"set_scheduler()\u003cbr/\u003eset_audio_adapter()\"| WAM\n    \n    WM --\u003e|\"accesses scheduler state\"| PI\n    WM --\u003e|\"stores noise_pred\"| WS\n    WAM --\u003e|\"accesses timestep_input\u003cbr/\u003eaudio_adapter_t_emb\"| PI\n    \n    PI --\u003e|\"self.scheduler.latents\u003cbr/\u003eself.scheduler.timestep_input\"| WS\n    PI --\u003e|\"self.scheduler.audio_adapter_t_emb\"| ES\n```\n\n### Scheduler State Access Patterns\n\nThe model accesses scheduler state during inference:\n\n```python\n# In WanPreInfer.infer()\nx = self.scheduler.latents          # Current latent state\nt = self.scheduler.timestep_input   # Current timestep\n\n# In WanAudioPreInfer.infer()\naudio_t_emb = self.scheduler.audio_adapter_t_emb  # Audio time conditioning\n```\n\nThe model writes predictions back to the scheduler:\n\n```python\n# In WanModel.infer()\nself.scheduler.noise_pred = noise_prediction  # Store for step_post()\n```\n\n**Sources:**\n- [lightx2v/models/networks/wan/infer/pre_infer.py:97-99]()\n- [lightx2v/models/networks/wan/infer/audio/pre_infer.py:58]()\n- [lightx2v/models/networks/wan/model.py:189-192]()\n\n### Runner Coordination\n\nThe runner coordinates the complete denoising loop:\n\n```python\ndef run_segment(self, segment_idx=0):\n    infer_steps = self.model.scheduler.infer_steps\n    \n    for step_index in range(infer_steps):\n        # Pre-processing\n        self.model.scheduler.step_pre(step_index=step_index)\n        \n        # Model inference\n        self.model.infer(self.inputs)\n        \n        # Post-processing\n        self.model.scheduler.step_post()\n    \n    return self.model.scheduler.latents\n```\n\n**Sources:** [lightx2v/models/runners/default_runner.py:173-206]()\n\n### Initialization Protocol\n\nSchedulers are initialized and bound to models in a specific sequence:\n\n```python\n# 1. Runner creates scheduler\nrunner.init_scheduler()  # Creates WanScheduler or EulerScheduler\n\n# 2. Runner creates model\nmodel = runner.load_transformer()\n\n# 3. Bind scheduler to model\nmodel.set_scheduler(runner.scheduler)\n\n# 4. Before inference: prepare latents and timesteps\nrunner.scheduler.prepare(seed, latent_shape, image_encoder_output)\n```\n\n**Sources:**\n- [lightx2v/models/runners/default_runner.py:68]()\n- [lightx2v/models/runners/default_runner.py:77]()\n- [lightx2v/models/runners/default_runner.py:349]()\n\n---\n\n## Feature Caching Schedulers\n\nCaching schedulers optimize inference by reusing computations from previous steps.\n\n### WanSchedulerCaching\n\nImplements block-level feature caching (TeaCache, MagCache, AdaCache):\n\n```python\nclass WanSchedulerCaching(WanScheduler):\n    def __init__(self, config):\n        super().__init__(config)\n        self.caching_records = [True] * self.config[\"infer_steps\"]\n        self.cached_model_outputs = {}\n    \n    def get_cached_output(self, block_idx, step_index):\n        \"\"\"Retrieve cached block output if available\"\"\"\n        key = f\"block_{block_idx}_step_{step_index}\"\n        return self.cached_model_outputs.get(key, None)\n    \n    def save_cached_output(self, block_idx, step_index, output):\n        \"\"\"Store block output for future reuse\"\"\"\n        key = f\"block_{block_idx}_step_{step_index}\"\n        self.cached_model_outputs[key] = output\n```\n\nThe transformer infer checks the scheduler for cached outputs:\n\n```python\ndef infer_block(self, block, x, pre_infer_out):\n    # Check if block output is cached\n    cached = self.scheduler.get_cached_output(self.block_idx, self.scheduler.step_index)\n    if cached is not None:\n        return cached\n    \n    # Compute block normally\n    x = self.infer_self_attn(...)\n    x = self.infer_cross_attn(...)\n    x = self.infer_ffn(...)\n    \n    # Save for future reuse\n    self.scheduler.save_cached_output(self.block_idx, self.scheduler.step_index, x)\n    return x\n```\n\n**Caching Performance:**\n\n| Caching Method | Speedup | Quality Impact | Config Parameter |\n|----------------|---------|----------------|------------------|\n| None | 1.0x | - | `feature_caching: \"NoCaching\"` |\n| TeaCache | ~1.3x | Minimal | `feature_caching: \"Tea\"` |\n| AdaCache | ~1.4x | Minimal | `feature_caching: \"Ada\"` |\n| MagCache | ~1.5x | Minor | `feature_caching: \"Mag\"` |\n\n**Sources:** [lightx2v/models/schedulers/wan/feature_caching/scheduler.py]()\n\n---\n\n## Configuration Reference\n\n### Common Scheduler Parameters\n\n```python\nconfig = {\n    # Base parameters\n    \"infer_steps\": 40,              # Number of denoising iterations\n    \"sample_shift\": 1.0,            # Noise schedule shift\n    \"sample_guide_scale\": 4.5,      # CFG guidance strength\n    \n    # CFG configuration\n    \"enable_cfg\": True,             # Enable classifier-free guidance\n    \"cfg_parallel\": False,          # Parallel CFG across 2 GPUs\n    \n    # Solver selection\n    \"feature_caching\": \"NoCaching\", # [\"NoCaching\", \"Tea\", \"Ada\", \"Mag\"]\n    \n    # Audio-specific (EulerScheduler)\n    \"model_cls\": \"wan2.2_audio\",\n    \"target_video_length\": 81,      # Video length in frames\n}\n```\n\n### Scheduler Selection Logic\n\n```python\ndef init_scheduler(self):\n    \"\"\"Runner method to instantiate appropriate scheduler\"\"\"\n    if self.config[\"feature_caching\"] == \"NoCaching\":\n        scheduler_class = WanScheduler\n    elif self.config[\"feature_caching\"] == \"TaylorSeer\":\n        scheduler_class = WanSchedulerTaylorCaching\n    elif self.config.feature_caching in [\"Tea\", \"Ada\", \"Custom\", ...]:\n        scheduler_class = WanSchedulerCaching\n    else:\n        raise NotImplementedError(...)\n    \n    if self.config.get(\"changing_resolution\", False):\n        self.scheduler = WanScheduler4ChangingResolutionInterface(scheduler_class, ...)\n    else:\n        self.scheduler = scheduler_class(self.config)\n```\n\n**Sources:**\n- [lightx2v/models/runners/wan/wan_runner.py:222-235]()\n- [lightx2v/models/runners/wan/wan_audio_runner.py:287-289]()\n\n---\n\nThis document covers the scheduler system's architecture, diffusion algorithms, and integration patterns. For details on how noise predictions are generated by the transformer, see [Transformer Block Architecture](#7.1). For information on VAE decoding of final latents, see [VAE System and Video Processing](#4.6)."])</script><script>self.__next_f.push([1,"2a:T9e65,"])</script><script>self.__next_f.push([1,"# Weight Management System\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/common/modules/weight_module.py](lightx2v/common/modules/weight_module.py)\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/pipeline.py](lightx2v/pipeline.py)\n- [lightx2v/server/__main__.py](lightx2v/server/__main__.py)\n- [lightx2v/server/config.py](lightx2v/server/config.py)\n- [lightx2v/server/schema.py](lightx2v/server/schema.py)\n- [lightx2v/server/services/distributed_utils.py](lightx2v/server/services/distributed_utils.py)\n- [lightx2v/server/services/generation/base.py](lightx2v/server/services/generation/base.py)\n- [lightx2v/server/services/generation/image.py](lightx2v/server/services/generation/image.py)\n- [lightx2v/server/services/inference/worker.py](lightx2v/server/services/inference/worker.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n- [scripts/server/post_i2i_with_lora.py](scripts/server/post_i2i_with_lora.py)\n- [scripts/server/start_server_i2i_with_loradir.sh](scripts/server/start_server_i2i_with_loradir.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe Weight Management System provides infrastructure for loading, organizing, and managing neural network weights across the LightX2V framework. This system handles:\n\n- **Weight Loading**: SafeTensors, GGUF, and traditional PyTorch formats\n- **LoRA Management**: Dynamic registration, switching, and removal of LoRA adapters at runtime\n- **Quantization**: INT8, FP8, NVFP4, and MxFP variants with weight scaling\n- **Memory Management**: CPU offloading, buffer creation, and lazy loading\n- **State Management**: Serialization and deserialization of weight states\n\nFor model-specific weight organization, see [4.1](#4.1) (Runner System). For quantization techniques, see [6.1](#6.1) (Quantization System). For CPU offloading strategies, see [6.3](#6.3) (Memory Management and CPU Offloading).\n\n**Sources**: [lightx2v/common/ops/mm/mm_weight.py:99-250](), [lightx2v/common/modules/weight_module.py:1-173]()\n\n---\n\n## Architecture Overview\n\nThe weight management system uses a hierarchical template pattern with three primary layers:\n\n```mermaid\ngraph TB\n    subgraph \"Weight Organization Layer\"\n        WeightModule[\"WeightModule\u003cbr/\u003eBase container\"]\n        WeightModuleList[\"WeightModuleList\u003cbr/\u003eSequential collection\"]\n    end\n    \n    subgraph \"Weight Template Layer\"\n        MMWeightTemplate[\"MMWeightTemplate\u003cbr/\u003eAbstract base\"]\n        MMWeight[\"MMWeight\u003cbr/\u003eStandard weights\"]\n        MMWeightQuant[\"MMWeightQuantTemplate\u003cbr/\u003eQuantized base\"]\n    end\n    \n    subgraph \"Quantization Variants\"\n        MMWeightINT8[\"MMWeightINT8\u003cbr/\u003eint8 per-channel\"]\n        MMWeightFP8[\"MMWeightFP8\u003cbr/\u003efp8 E4M3/E5M2\"]\n        MMWeightNVFP4[\"MMWeightNVFP4\u003cbr/\u003e4-bit QAT\"]\n        MMWeightMxFP[\"MMWeightMxFP4/6/8\u003cbr/\u003eMicrosoft formats\"]\n    end\n    \n    subgraph \"Model Weight Structures\"\n        TransformerWeights[\"WanTransformerWeights\u003cbr/\u003enum_layers blocks\"]\n        PreWeights[\"WanPreWeights\u003cbr/\u003epatch embedding\"]\n        PostWeights[\"WanPostWeights\u003cbr/\u003eunpatchify\"]\n    end\n    \n    subgraph \"Component Types\"\n        Conv3DWeight[\"Conv3DWeight\u003cbr/\u003e3D convolutions\"]\n        AttentionWeight[\"AttentionWeight\u003cbr/\u003eQ/K/V/O projections\"]\n        TensorParam[\"TensorParameter\u003cbr/\u003eModulation/embeddings\"]\n        LNWeight[\"LayerNormWeight\u003cbr/\u003eNormalization\"]\n    end\n    \n    WeightModule --\u003e WeightModuleList\n    WeightModule --\u003e MMWeightTemplate\n    \n    MMWeightTemplate --\u003e MMWeight\n    MMWeightTemplate --\u003e MMWeightQuant\n    \n    MMWeightQuant --\u003e MMWeightINT8\n    MMWeightQuant --\u003e MMWeightFP8\n    MMWeightQuant --\u003e MMWeightNVFP4\n    MMWeightQuant --\u003e MMWeightMxFP\n    \n    TransformerWeights --\u003e WeightModuleList\n    PreWeights --\u003e Conv3DWeight\n    PreWeights --\u003e MMWeight\n    TransformerWeights --\u003e AttentionWeight\n    TransformerWeights --\u003e TensorParam\n    \n    style MMWeightTemplate fill:#f9f9f9\n    style WeightModule fill:#f9f9f9\n    style TransformerWeights fill:#e8f4f8\n```\n\n**Key Design Principles:**\n\n1. **Template Pattern**: `MMWeightTemplate` defines abstract interface for weight operations\n2. **Lazy Loading**: Weights can be streamed from disk on-demand to minimize memory footprint\n3. **Buffer Management**: Separate CUDA and CPU buffers enable asynchronous offloading\n4. **LoRA Branch**: Optional parallel path for adapter weights with dynamic strength control\n\n**Sources**: [lightx2v/common/ops/mm/mm_weight.py:99-329](), [lightx2v/common/modules/weight_module.py:1-173](), [lightx2v/models/networks/wan/weights/transformer_weights.py:1-218]()\n\n---\n\n## Core Components\n\n### WeightModule Hierarchy\n\n`WeightModule` provides a container for organizing weights with lifecycle methods:\n\n```mermaid\ngraph LR\n    subgraph \"WeightModule Lifecycle\"\n        Init[\"__init__()\u003cbr/\u003eCreate structure\"]\n        Load[\"load(weight_dict)\u003cbr/\u003eLoad from dict\"]\n        RegDiff[\"register_diff(weight_dict)\u003cbr/\u003eAdd weight diffs\"]\n        RegLoRA[\"register_lora(weight_dict, strength)\u003cbr/\u003eAdd LoRA\"]\n        UpdateLoRA[\"update_lora(weight_dict, strength)\u003cbr/\u003eSwap LoRA\"]\n        RemoveLoRA[\"remove_lora()\u003cbr/\u003eClear LoRA\"]\n        StateDict[\"state_dict(destination)\u003cbr/\u003eSerialize\"]\n        LoadState[\"load_state_dict(destination, block_idx)\u003cbr/\u003eDeserialize\"]\n        ToCUDA[\"to_cuda()\u003cbr/\u003eMove to GPU\"]\n        ToCPU[\"to_cpu()\u003cbr/\u003eMove to CPU\"]\n    end\n    \n    Init --\u003e Load\n    Load --\u003e RegDiff\n    RegDiff --\u003e RegLoRA\n    RegLoRA --\u003e UpdateLoRA\n    UpdateLoRA --\u003e RemoveLoRA\n    RegLoRA --\u003e StateDict\n    StateDict --\u003e LoadState\n    Load --\u003e ToCUDA\n    ToCUDA --\u003e ToCPU\n```\n\n**Key Methods** (from [lightx2v/common/modules/weight_module.py:1-173]()):\n\n| Method | Purpose | Parameters |\n|--------|---------|------------|\n| `load(weight_dict)` | Load weights from dictionary | `weight_dict`: Tensor dictionary |\n| `register_lora(weight_dict, strength)` | Register LoRA adapters | `weight_dict`: LoRA tensors, `strength`: Scale factor |\n| `update_lora(weight_dict, strength)` | In-place LoRA update | Same as register |\n| `remove_lora()` | Delete LoRA tensors | None |\n| `state_dict(destination)` | Serialize to dictionary | `destination`: Optional dict |\n| `to_cuda(non_blocking)` | Move to GPU | `non_blocking`: Async flag |\n| `to_cpu(non_blocking)` | Move to CPU | `non_blocking`: Async flag |\n\n**Sources**: [lightx2v/common/modules/weight_module.py:1-173]()\n\n---\n\n### MMWeight Template System\n\n`MMWeightTemplate` defines the abstract interface for matrix multiplication weights:\n\n```mermaid\nclassDiagram\n    class MMWeightTemplate {\n        +str weight_name\n        +str bias_name\n        +bool create_cuda_buffer\n        +bool create_cpu_buffer\n        +bool lazy_load\n        +str lazy_load_file\n        +bool has_lora_branch\n        +bool has_diff\n        \n        +_get_base_attrs_mapping()\n        +_get_lora_attr_mapping()\n        +_get_actual_weight() Tensor\n        +_get_actual_bias() Tensor\n        +apply_lora(input_tensor) Tensor\n        +register_diff(weight_dict)\n        +register_lora(weight_dict, strength)\n        +update_lora(weight_dict, strength)\n        +remove_lora()\n        +state_dict() dict\n        +load_state_dict(destination, block_idx)\n        +to_cuda(non_blocking)\n        +to_cpu(non_blocking)\n        +load(weight_dict)*\n        +apply(input_tensor)*\n    }\n    \n    class MMWeight {\n        +Tensor weight\n        +Tensor bias\n        +Tensor pin_weight\n        +Tensor pin_bias\n        +Tensor weight_cuda_buffer\n        +Tensor bias_cuda_buffer\n        \n        +load(weight_dict)\n        +apply(input_tensor) Tensor\n        +load_state_dict_from_disk(block_idx)\n    }\n    \n    class MMWeightQuantTemplate {\n        +str weight_scale_name\n        +bool weight_need_transpose\n        +Callable act_quant_func\n        +bool bias_force_fp32\n        +bool scale_force_fp32\n        \n        +load_quantized(weight_dict)\n        +post_process()\n        +load_fp8_perchannel_sym(weight_dict)\n        +load_int8_perchannel_sym(weight_dict)\n        +load_mxfp4(weight_dict)\n    }\n    \n    MMWeightTemplate \u003c|-- MMWeight\n    MMWeightTemplate \u003c|-- MMWeightQuantTemplate\n```\n\n**Attribute Mapping** (from [lightx2v/common/ops/mm/mm_weight.py:126-142]()):\n\nThe system uses string-based name mapping to locate tensors in state dicts:\n\n| Attribute | Template | Example |\n|-----------|----------|---------|\n| `weight_name` | `{prefix}.weight` | `blocks.0.compute_phases.0.self_attn_q.weight` |\n| `bias_name` | `{prefix}.bias` | `blocks.0.compute_phases.0.self_attn_q.bias` |\n| `lora_down_name` | `{prefix}.lora_down` | `lora.diffusion_model.blocks.0.self_attn_q.lora_down` |\n| `lora_up_name` | `{prefix}.lora_up` | `lora.diffusion_model.blocks.0.self_attn_q.lora_up` |\n| `lora_alpha_name` | `{prefix}.lora_alpha` | `lora.diffusion_model.blocks.0.self_attn_q.alpha` |\n| `weight_diff_name` | `{prefix}.weight_diff` | `diff.blocks.0.self_attn_q.weight_diff` |\n\n**Sources**: [lightx2v/common/ops/mm/mm_weight.py:99-329]()\n\n---\n\n## Weight Loading Mechanisms\n\n### SafeTensors and GGUF Formats\n\nThe system supports multiple weight formats with unified loading:\n\n```mermaid\ngraph TB\n    subgraph \"Loading Entry Points\"\n        LoadWeights[\"load_weights(path)\u003cbr/\u003elightx2v/utils/utils.py\"]\n        LoadSafetensors[\"load_safetensors(path)\u003cbr/\u003eFilter by keys\"]\n        LoadGGUF[\"load_gguf(path)\u003cbr/\u003eGGUF format\"]\n    end\n    \n    subgraph \"Format Handlers\"\n        SafetensorsFile[\"safetensors.safe_open()\"]\n        GGUFReader[\"gguf.GGUFReader()\"]\n        TorchLoad[\"torch.load()\"]\n    end\n    \n    subgraph \"Weight Creation Strategies\"\n        DefaultTensors[\"create_default_tensors()\u003cbr/\u003eDevice + pinned\"]\n        CUDABuffers[\"create_cuda_buffers()\u003cbr/\u003eGPU resident\"]\n        CPUBuffers[\"create_cpu_buffers()\u003cbr/\u003ePinned memory\"]\n    end\n    \n    subgraph \"MMWeight.load() Flow\"\n        CheckFlags{\"create_cuda_buffer?\u003cbr/\u003ecreate_cpu_buffer?\u003cbr/\u003elazy_load?\"}\n        LoadDefault[\"Load to device\u003cbr/\u003eself.weight = tensor\"]\n        LoadCUDA[\"Allocate CUDA buffer\u003cbr/\u003eself.weight_cuda_buffer\"]\n        LoadCPU[\"Allocate pinned buffer\u003cbr/\u003eself.pin_weight\"]\n        LoadLazy[\"Store file path\u003cbr/\u003eLoad on-demand\"]\n    end\n    \n    LoadWeights --\u003e SafetensorsFile\n    LoadWeights --\u003e GGUFReader\n    LoadWeights --\u003e TorchLoad\n    \n    SafetensorsFile --\u003e CheckFlags\n    \n    CheckFlags --\u003e|\"All False\"| LoadDefault\n    CheckFlags --\u003e|\"cuda=True\"| LoadCUDA\n    CheckFlags --\u003e|\"cpu=True\"| LoadCPU\n    CheckFlags --\u003e|\"lazy=True\"| LoadLazy\n    \n    LoadDefault --\u003e DefaultTensors\n    LoadCUDA --\u003e CUDABuffers\n    LoadCPU --\u003e CPUBuffers\n    \n    style CheckFlags fill:#f9f9f9\n```\n\n**Loading Strategies** (from [lightx2v/common/ops/mm/mm_weight.py:278-294]()):\n\n```python\n# Strategy 1: Default - Device + Pinned Memory\nif not self.create_cuda_buffer and not self.create_cpu_buffer and not self.lazy_load:\n    device_tensors, pin_tensors = create_default_tensors(self.base_attrs, weight_dict)\n    self.weight = device_tensors.get(\"weight\")          # On GPU\n    self.pin_weight = pin_tensors.get(\"weight\")        # On CPU (pinned)\n\n# Strategy 2: CUDA Buffer - For CPU Offloading\nelif self.create_cuda_buffer:\n    result = create_cuda_buffers(self.base_attrs, weight_dict, ...)\n    self.weight_cuda_buffer = result.get(\"weight\")      # Pre-allocated GPU buffer\n\n# Strategy 3: CPU Buffer - For Lazy Loading\nelif self.create_cpu_buffer:\n    result = create_cpu_buffers(self.base_attrs, self.lazy_load_file)\n    self.pin_weight = result.get(\"weight\")              # Pinned CPU buffer\n    self.weight = None                                   # Not in GPU yet\n```\n\n**Sources**: [lightx2v/common/ops/mm/mm_weight.py:278-327](), [lightx2v/utils/utils.py:369-451]()\n\n---\n\n### Lazy Loading from Disk\n\nLazy loading streams weights from disk only when needed:\n\n```mermaid\nsequenceDiagram\n    participant Runner\n    participant OffloadMgr as \"Offload Manager\"\n    participant MMWeight\n    participant Disk as \"SafeTensors File\"\n    participant CPU as \"Pinned Buffer\"\n    participant GPU as \"CUDA Buffer\"\n    \n    Note over Runner,GPU: Initialization Phase\n    Runner-\u003e\u003eMMWeight: Create with lazy_load=True\n    MMWeight-\u003e\u003eCPU: Allocate pin_weight buffer\n    MMWeight--\u003e\u003eRunner: Ready (no disk I/O yet)\n    \n    Note over Runner,GPU: First Inference Step\n    Runner-\u003e\u003eOffloadMgr: Load block 0\n    OffloadMgr-\u003e\u003eMMWeight: load_state_dict_from_disk(block_index=0)\n    MMWeight-\u003e\u003eDisk: safe_open(lazy_load_file)\n    Disk--\u003e\u003eMMWeight: weight tensor\n    MMWeight-\u003e\u003eCPU: pin_weight.copy_(weight_tensor)\n    MMWeight-\u003e\u003eGPU: weight_cuda_buffer.copy_(pin_weight)\n    MMWeight--\u003e\u003eOffloadMgr: Block loaded\n    \n    Note over Runner,GPU: Inference\n    Runner-\u003e\u003eGPU: Run computation with block 0\n    \n    Note over Runner,GPU: Block Switching\n    Runner-\u003e\u003eOffloadMgr: Load block 1\n    OffloadMgr-\u003e\u003eMMWeight: load_state_dict_from_disk(block_index=1)\n    MMWeight-\u003e\u003eDisk: safe_open(lazy_load_file)\n    MMWeight-\u003e\u003eCPU: pin_weight.copy_(new_weight)\n    MMWeight-\u003e\u003eGPU: weight_cuda_buffer.copy_(pin_weight)\n```\n\n**Implementation** (from [lightx2v/common/ops/mm/mm_weight.py:311-327]()):\n\n```python\ndef load_state_dict_from_disk(self, block_index, adapter_block_index=None):\n    # Resolve block-specific names (e.g., blocks.{block_index}.weight)\n    self.weight_name = resolve_block_name(self.weight_name, block_index, ...)\n    \n    # Determine which file contains this block\n    lazy_load_file_path = get_lazy_load_file_path(self.lazy_load_file, self.weight_name)\n    \n    # Stream from disk -\u003e CPU pinned buffer\n    with safe_open(lazy_load_file_path, framework=\"pt\", device=\"cpu\") as f:\n        weight_tensor = f.get_tensor(self.weight_name).t()\n        self.pin_weight = self.pin_weight.copy_(weight_tensor)\n        del weight_tensor\n```\n\n**Sources**: [lightx2v/common/ops/mm/mm_weight.py:311-327]()\n\n---\n\n## LoRA Management System\n\n### Registration and Dynamic Switching\n\nThe LoRA system allows runtime adapter switching without model reloading:\n\n```mermaid\ngraph TB\n    subgraph \"LoRA Weight Structure\"\n        LoRADown[\"lora_down\u003cbr/\u003eInput projection\"]\n        LoRAUp[\"lora_up\u003cbr/\u003eOutput projection\"]\n        LoRAAlpha[\"lora_alpha\u003cbr/\u003eScaling factor\"]\n        LoRAScale[\"lora_scale\u003cbr/\u003e= alpha / down.shape[0]\"]\n        LoRAStrength[\"lora_strength\u003cbr/\u003eRuntime multiplier\"]\n    end\n    \n    subgraph \"Application Formula\"\n        Formula[\"output = W @ x + strength * scale * (up @ (down @ x))\u003cbr/\u003ewhere:\u003cbr/\u003e  W = base weight\u003cbr/\u003e  down, up = LoRA matrices\u003cbr/\u003e  scale = alpha / rank\u003cbr/\u003e  strength = runtime parameter\"]\n    end\n    \n    subgraph \"Lifecycle Operations\"\n        Register[\"register_lora()\u003cbr/\u003e1. Load tensors\u003cbr/\u003e2. Compute scale\u003cbr/\u003e3. Set has_lora_branch=True\"]\n        Update[\"update_lora()\u003cbr/\u003e1. Copy new tensors\u003cbr/\u003e2. Update scale\u003cbr/\u003e3. Keep has_lora_branch=True\"]\n        Remove[\"remove_lora()\u003cbr/\u003e1. Delete tensors\u003cbr/\u003e2. Set has_lora_branch=False\"]\n        Switch[\"switch_lora()\u003cbr/\u003ePer-request adapter\"]\n    end\n    \n    LoRADown --\u003e Formula\n    LoRAUp --\u003e Formula\n    LoRAAlpha --\u003e LoRAScale\n    LoRAScale --\u003e Formula\n    LoRAStrength --\u003e Formula\n    \n    Register --\u003e Update\n    Update --\u003e Remove\n    Register --\u003e Switch\n    \n    style Formula fill:#f9f9f9\n```\n\n**Registration** (from [lightx2v/common/ops/mm/mm_weight.py:180-206]()):\n\n```python\ndef register_lora(self, weight_dict, lora_strength=1):\n    \"\"\"Register LoRA adapter from weight dictionary\"\"\"\n    if self.lora_down_name in weight_dict:\n        self.has_lora_branch = True\n        self.lora_down = weight_dict[self.lora_down_name]        # [rank, in_features]\n        self.lora_up = weight_dict[self.lora_up_name]            # [out_features, rank]\n        self.lora_strength = lora_strength\n        \n        # Compute scaling: alpha / rank\n        if self.lora_alpha_name in weight_dict:\n            self.lora_alpha = weight_dict[self.lora_alpha_name]\n            self.lora_scale = self.lora_alpha / self.lora_down.shape[0]\n        else:\n            self.lora_scale = torch.tensor(1.0, device=AI_DEVICE)\n\ndef update_lora(self, weight_dict, lora_strength=1):\n    \"\"\"Update existing LoRA in-place (zero-copy)\"\"\"\n    if self.lora_down_name in weight_dict:\n        self.lora_down.copy_(weight_dict[self.lora_down_name])  # In-place copy\n        self.lora_up.copy_(weight_dict[self.lora_up_name])\n        # ... update scale\n```\n\n**Application** (from [lightx2v/common/ops/mm/mm_weight.py:161-164]()):\n\n```python\ndef apply_lora(self, input_tensor):\n    \"\"\"Compute LoRA contribution: strength * scale * (up @ (down @ x))\"\"\"\n    h = torch.mm(input_tensor, self.lora_down.t())    # [batch, rank]\n    out = torch.mm(h, self.lora_up.t())               # [batch, out_features]\n    return self.lora_strength * self.lora_scale * out\n```\n\n**Sources**: [lightx2v/common/ops/mm/mm_weight.py:161-218]()\n\n---\n\n### Runner-Level LoRA Switching\n\nThe runner provides a high-level interface for LoRA management:\n\n```mermaid\nsequenceDiagram\n    participant Client as \"HTTP Client\"\n    participant Server as \"API Server\"\n    participant Worker as \"TorchrunWorker\"\n    participant Runner as \"DefaultRunner\"\n    participant Model as \"WanModel\"\n    participant PreWeight as \"pre_weight\"\n    participant TransWeight as \"transformer_weights\"\n    participant PostWeight as \"post_weight\"\n    participant MMWeight\n    \n    Note over Client,MMWeight: Dynamic LoRA Request\n    Client-\u003e\u003eServer: POST /v1/tasks/image/\u003cbr/\u003e{lora_name: \"style.safetensors\", lora_strength: 0.8}\n    Server-\u003e\u003eWorker: process_request(task_data)\n    Worker-\u003e\u003eWorker: switch_lora(lora_name, strength)\n    Worker-\u003e\u003eRunner: switch_lora(lora_path, strength)\n    \n    alt lora_path is empty\n        Runner-\u003e\u003eModel: _remove_lora()\n        Model-\u003e\u003ePreWeight: remove_lora()\n        PreWeight-\u003e\u003eMMWeight: remove_lora()\n        MMWeight-\u003e\u003eMMWeight: Delete lora_down, lora_up, etc.\n    else lora_path provided\n        Runner-\u003e\u003eModel: _update_lora(lora_path, strength)\n        Model-\u003e\u003eModel: Load LoRA from safetensors\n        Model-\u003e\u003ePreWeight: update_lora(weight_dict, strength)\n        PreWeight-\u003e\u003eMMWeight: update_lora(weight_dict, strength)\n        MMWeight-\u003e\u003eMMWeight: Copy new tensors in-place\n        Model-\u003e\u003eTransWeight: update_lora(weight_dict, strength)\n        Model-\u003e\u003ePostWeight: update_lora(weight_dict, strength)\n    end\n    \n    Runner--\u003e\u003eWorker: Success\n    Worker--\u003e\u003eServer: LoRA switched\n    Server--\u003e\u003eClient: Ready for inference\n```\n\n**Runner Implementation** (from [lightx2v/models/runners/default_runner.py:478-521]()):\n\n```python\ndef switch_lora(self, lora_path: str, strength: float = 1.0):\n    \"\"\"Switch LoRA weights dynamically at runtime\"\"\"\n    if not hasattr(self.model, \"_update_lora\"):\n        logger.error(\"Model does not support LoRA switching\")\n        return False\n    \n    try:\n        if lora_path == \"\":\n            # Remove LoRA\n            if hasattr(self.model, \"_remove_lora\"):\n                logger.info(\"Removing LoRA weights\")\n                self.model._remove_lora()\n                return True\n        else:\n            # Load and update LoRA\n            logger.info(f\"Switching LoRA to: {lora_path} with strength={strength}\")\n            self.model._update_lora(lora_path, strength)\n            return True\n    except Exception as e:\n        logger.error(f\"Failed to switch LoRA: {e}\")\n        return False\n```\n\n**Model-Level Propagation** (from [lightx2v/models/networks/base_model.py:175-196]()):\n\n```python\ndef _update_lora(self, lora_path, lora_strength=1.0):\n    \"\"\"Update LoRA across all weight modules\"\"\"\n    weight_dict = load_lora_weights(lora_path)\n    \n    # Propagate to all weight modules\n    if hasattr(self, 'pre_weight'):\n        self.pre_weight.update_lora(weight_dict, lora_strength)\n    if hasattr(self, 'transformer_weights'):\n        self.transformer_weights.update_lora(weight_dict, lora_strength)\n    if hasattr(self, 'post_weight'):\n        self.post_weight.update_lora(weight_dict, lora_strength)\n```\n\n**Sources**: [lightx2v/models/runners/default_runner.py:478-521](), [lightx2v/server/services/inference/worker.py:74-80](), [scripts/server/post_i2i_with_lora.py:1-28]()\n\n---\n\n## Quantization Support\n\n### Weight Quantization Hierarchy\n\nThe system supports multiple quantization formats through specialized subclasses:\n\n```mermaid\ngraph TB\n    subgraph \"Quantization Formats\"\n        INT8[\"MMWeightINT8\u003cbr/\u003ePer-channel symmetric\u003cbr/\u003eweight_scale: [out_channels]\u003cbr/\u003eKernels: triton, vllm, torchao, q8f\"]\n        FP8[\"MMWeightFP8\u003cbr/\u003eE4M3/E5M2 formats\u003cbr/\u003eweight_scale: [out_channels]\u003cbr/\u003eKernels: triton, sgl, torchao\"]\n        NVFP4[\"MMWeightNVFP4\u003cbr/\u003e4-bit QAT\u003cbr/\u003eweight_scale: [out_channels]\u003cbr/\u003eKernels: lightx2v_kernel\"]\n        MxFP4[\"MMWeightMxFP4\u003cbr/\u003eBlock-scaled FP4\u003cbr/\u003escale_bits: [blocks]\u003cbr/\u003eKernels: cutlass\"]\n        MxFP6[\"MMWeightMxFP6\u003cbr/\u003eBlock-scaled FP6\u003cbr/\u003escale_bits: [blocks]\u003cbr/\u003eKernels: cutlass\"]\n        MxFP8[\"MMWeightMxFP8\u003cbr/\u003eBlock-scaled FP8\u003cbr/\u003escale_bits: [blocks]\u003cbr/\u003eKernels: cutlass\"]\n    end\n    \n    subgraph \"Quantization Operations\"\n        WeightQuant[\"Weight Quantization\u003cbr/\u003eOffline or auto_quant\"]\n        ActQuant[\"Activation Quantization\u003cbr/\u003eRuntime per-token\"]\n        GEMMQuant[\"Quantized GEMM\u003cbr/\u003eMixed-precision multiply\"]\n    end\n    \n    subgraph \"Auto-Quantization\"\n        AutoQuant[\"weight_auto_quant=True\u003cbr/\u003eQuantize at load time\"]\n        Quantizer[\"FloatQuantizer/IntegerQuantizer\u003cbr/\u003ePer-channel analysis\"]\n        RealQuant[\"real_quant_tensor()\u003cbr/\u003eCompute scale + quantize\"]\n    end\n    \n    INT8 --\u003e WeightQuant\n    FP8 --\u003e WeightQuant\n    NVFP4 --\u003e WeightQuant\n    \n    WeightQuant --\u003e ActQuant\n    ActQuant --\u003e GEMMQuant\n    \n    AutoQuant --\u003e Quantizer\n    Quantizer --\u003e RealQuant\n    RealQuant --\u003e WeightQuant\n    \n    style WeightQuant fill:#f9f9f9\n    style ActQuant fill:#f9f9f9\n    style GEMMQuant fill:#f9f9f9\n```\n\n**Quantization Formats** (from [lightx2v/common/ops/mm/mm_weight.py:426-492]()):\n\n| Format | Weight Dtype | Scale Dtype | Granularity | Kernel Options |\n|--------|-------------|-------------|-------------|----------------|\n| INT8 | `torch.int8` | `torch.float32` | Per-channel | triton, vllm, torchao, q8f, tmo, npu |\n| FP8 | `torch.float8_e4m3fn` | `torch.float32` | Per-channel | triton, vllm, sgl, torchao |\n| NVFP4 | Custom 4-bit | `torch.float32` | Per-channel | lightx2v_kernel (QAT) |\n| MxFP4 | Packed bits | `torch.uint8` | Per-block | cutlass_scaled_mxfp4_mm |\n| MxFP6 | Packed bits | `torch.uint8` | Per-block | cutlass_scaled_mxfp6_mxfp8_mm |\n| MxFP8 | Packed bits | `torch.uint8` | Per-block | cutlass_scaled_mxfp8_mm |\n\n**INT8 Example** (from [lightx2v/common/ops/mm/mm_weight.py:852-889]()):\n\n```python\nclass MMWeightINT8(MMWeightQuantTemplate):\n    def apply(self, input_tensor):\n        # Quantize activation per-token\n        if self.config.get(\"act_auto_quant\", False):\n            input_q, input_scale = self.act_quant_func(input_tensor)\n        else:\n            input_q = input_tensor\n            input_scale = None\n        \n        # Quantized GEMM: Y = X_q @ W_q with scaling\n        if self.config.get(\"quant_scheme\") == \"int8-triton\":\n            output = int8_gemm_triton(input_q, self.weight, input_scale, self.weight_scale)\n        elif self.config.get(\"quant_scheme\") == \"int8-vllm\":\n            output = ops.cutlass_scaled_mm(input_q, self.weight, input_scale, self.weight_scale)\n        elif self.config.get(\"quant_scheme\") == \"int8-torchao\":\n            output = torchao_int8_gemm(input_q, self.weight, self.weight_scale)\n        \n        # Add bias if present\n        if hasattr(self, \"bias\") and self.bias is not None:\n            output += self.bias\n        \n        return output\n```\n\n**Sources**: [lightx2v/common/ops/mm/mm_weight.py:426-889](), [lightx2v/common/ops/mm/triton_kernels.py:1-500]()\n\n---\n\n## Offloading and Buffer Management\n\n### Block-Level CPU Offloading\n\nThe system enables CPU offloading at block granularity to reduce GPU memory:\n\n```mermaid\ngraph TB\n    subgraph \"Buffer Allocation Strategy\"\n        MainBlocks[\"Main Block Storage\u003cbr/\u003eblocks[0..num_layers-1]\u003cbr/\u003eCreated with create_cuda_buffer=False\"]\n        CUDABuffers[\"CUDA Buffers\u003cbr/\u003eoffload_block_cuda_buffers[0..1]\u003cbr/\u003eCreated with create_cuda_buffer=True\"]\n        CPUBuffers[\"CPU Buffers\u003cbr/\u003eoffload_block_cpu_buffers[0..1]\u003cbr/\u003eCreated with create_cpu_buffer=True\"]\n    end\n    \n    subgraph \"Offload Manager Operations\"\n        LoadBlock[\"load_block_to_cuda(block_idx)\u003cbr/\u003e1. Load to CPU buffer from disk (if lazy)\u003cbr/\u003e2. Copy CPU buffer -\u003e CUDA buffer\u003cbr/\u003e3. Swap block pointer\"]\n        OffloadBlock[\"offload_block_to_cpu(block_idx)\u003cbr/\u003e1. Copy CUDA buffer -\u003e CPU buffer\u003cbr/\u003e2. Swap block pointer\"]\n        Prefetch[\"prefetch_next_block(block_idx)\u003cbr/\u003eAsync disk -\u003e CPU in background\"]\n    end\n    \n    subgraph \"Block Lifecycle\"\n        Disk[\"Disk\u003cbr/\u003eSafeTensors file\u003cbr/\u003eAll 30+ blocks\"]\n        CPU[\"CPU Pinned Memory\u003cbr/\u003eoffload_block_cpu_buffers\u003cbr/\u003e2 blocks resident\"]\n        GPU[\"GPU Memory\u003cbr/\u003eoffload_block_cuda_buffers\u003cbr/\u003e2 blocks resident\"]\n        Compute[\"Inference\u003cbr/\u003eActive block computation\"]\n    end\n    \n    MainBlocks -.-\u003e|\"Pointer swap\"| CUDABuffers\n    MainBlocks -.-\u003e|\"Pointer swap\"| CPUBuffers\n    \n    CUDABuffers --\u003e LoadBlock\n    CPUBuffers --\u003e LoadBlock\n    \n    LoadBlock --\u003e OffloadBlock\n    OffloadBlock --\u003e Prefetch\n    \n    Disk --\u003e|\"Lazy load\"| CPU\n    CPU --\u003e|\"Copy\"| GPU\n    GPU --\u003e Compute\n    Compute --\u003e|\"Offload\"| CPU\n    \n    style LoadBlock fill:#e8f4f8\n    style Compute fill:#e8f4f8\n```\n\n**Buffer Creation** (from [lightx2v/models/networks/wan/weights/transformer_weights.py:55-97]()):\n\n```python\ndef register_offload_buffers(self, config, lazy_load_path, lora_path):\n    if config[\"cpu_offload\"] and config[\"offload_granularity\"] == \"block\":\n        # Allocate 2 CUDA buffers (pre-allocated on GPU)\n        self.offload_blocks_num = 2\n        self.offload_block_cuda_buffers = WeightModuleList([\n            WanTransformerAttentionBlock(\n                block_index=i,\n                create_cuda_buffer=True,  # Allocate GPU memory\n                create_cpu_buffer=False,\n                lazy_load=self.lazy_load,\n                lazy_load_path=lazy_load_path,\n                ...\n            )\n            for i in range(self.offload_blocks_num)\n        ])\n        \n        # Allocate 2 CPU buffers (pinned memory)\n        if self.lazy_load:\n            self.offload_block_cpu_buffers = WeightModuleList([\n                WanTransformerAttentionBlock(\n                    block_index=i,\n                    create_cuda_buffer=False,\n                    create_cpu_buffer=True,  # Allocate pinned CPU memory\n                    lazy_load=self.lazy_load,\n                    lazy_load_path=lazy_load_path,\n                    ...\n                )\n                for i in range(self.offload_blocks_num)\n            ])\n```\n\n**Offload Manager** (from [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:1-200]()):\n\n```python\nclass WeightAsyncStreamManager:\n    def __init__(self, transformer_weights, config):\n        self.blocks = transformer_weights.blocks\n        self.cuda_buffers = transformer_weights.offload_block_cuda_buffers\n        self.cpu_buffers = transformer_weights.offload_block_cpu_buffers if lazy_load else None\n        self.current_buffers = [0, 1]  # Which buffers are active\n        \n    def load_block_to_cuda(self, block_idx, stream_idx):\n        \"\"\"Load block from CPU/disk to GPU\"\"\"\n        buffer_idx = stream_idx % 2\n        \n        # Lazy load: Disk -\u003e CPU buffer\n        if self.cpu_buffers:\n            self.cpu_buffers[buffer_idx].load_state_dict_from_disk(block_idx)\n        \n        # Copy: CPU buffer -\u003e CUDA buffer\n        self.cuda_buffers[buffer_idx].load_state_dict(\n            self.blocks[block_idx], block_idx\n        )\n        \n        # Swap pointer: blocks[block_idx] now points to cuda_buffers[buffer_idx]\n        self.current_buffers[stream_idx] = buffer_idx\n```\n\n**Sources**: [lightx2v/models/networks/wan/weights/transformer_weights.py:55-132](), [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:1-350]()\n\n---\n\n## State Dict Operations\n\n### Serialization and Deserialization\n\nState dict operations enable saving and loading model states:\n\n```mermaid\ngraph LR\n    subgraph \"Serialization Path\"\n        WeightModule[\"WeightModule\u003cbr/\u003estate_dict()\"]\n        CollectParams[\"Collect Parameters\u003cbr/\u003e_parameters\"]\n        CollectModules[\"Collect Submodules\u003cbr/\u003e_modules\"]\n        MMWeightState[\"MMWeight.state_dict()\u003cbr/\u003eweight, bias, LoRA\"]\n        Destination[\"destination dict\u003cbr/\u003e{name: tensor}\"]\n    end\n    \n    subgraph \"Deserialization Path\"\n        SourceDict[\"source dict\u003cbr/\u003e{name: tensor}\"]\n        LoadStateDictMod[\"WeightModule\u003cbr/\u003eload_state_dict()\"]\n        LoadStateDictWeight[\"MMWeight\u003cbr/\u003eload_state_dict()\"]\n        ResolveNames[\"resolve_block_name()\u003cbr/\u003eblocks.{idx}.weight\"]\n        CopyTensors[\"copy_() tensors\u003cbr/\u003eIn-place update\"]\n    end\n    \n    subgraph \"State Dict Structure\"\n        Structure[\"blocks.0.compute_phases.0.self_attn_q.weight\u003cbr/\u003eblocks.0.compute_phases.0.self_attn_q.bias\u003cbr/\u003eblocks.0.compute_phases.0.self_attn_q.lora_down\u003cbr/\u003eblocks.0.compute_phases.0.self_attn_q.lora_up\u003cbr/\u003eblocks.0.compute_phases.0.self_attn_k.weight\u003cbr/\u003e...\"]\n    end\n    \n    WeightModule --\u003e CollectParams\n    WeightModule --\u003e CollectModules\n    CollectModules --\u003e MMWeightState\n    MMWeightState --\u003e Destination\n    Destination --\u003e Structure\n    \n    SourceDict --\u003e LoadStateDictMod\n    LoadStateDictMod --\u003e LoadStateDictWeight\n    LoadStateDictWeight --\u003e ResolveNames\n    ResolveNames --\u003e CopyTensors\n    \n    style Structure fill:#f9f9f9\n```\n\n**State Dict Implementation** (from [lightx2v/common/modules/weight_module.py:62-88]()):\n\n```python\ndef state_dict(self, destination=None):\n    \"\"\"Serialize weight module to dictionary\"\"\"\n    if destination is None:\n        destination = {}\n    \n    # Collect parameters (tensors, modulation, etc.)\n    for _, param in self._parameters.items():\n        if param is not None:\n            param.state_dict(destination)\n    \n    # Recursively collect submodules\n    for _, module in self._modules.items():\n        if module is not None:\n            module.state_dict(destination)\n    \n    return destination\n\ndef load_state_dict(self, source_dict, block_index=None, adapter_block_index=None):\n    \"\"\"Deserialize from dictionary\"\"\"\n    # Recursively load submodules\n    for _, module in self._modules.items():\n        if hasattr(module, \"load_state_dict\"):\n            module.load_state_dict(source_dict, block_index, adapter_block_index)\n    \n    # Load parameters\n    for _, parameter in self._parameters.items():\n        if hasattr(parameter, \"load_state_dict\"):\n            parameter.load_state_dict(source_dict, block_index, adapter_block_index)\n```\n\n**Name Resolution** (from [lightx2v/common/ops/utils.py:1-100]()):\n\n```python\ndef resolve_block_name(name, block_index, adapter_block_index=None, is_post_adapter=False):\n    \"\"\"Resolve template names to actual block-specific names\n    \n    Example:\n        Input: \"blocks.{}.compute_phases.0.self_attn_q.weight\", block_index=5\n        Output: \"blocks.5.compute_phases.0.self_attn_q.weight\"\n    \"\"\"\n    if \"{}\" in name:\n        if is_post_adapter and adapter_block_index is not None:\n            return name.replace(\"{}\", str(adapter_block_index))\n        else:\n            return name.replace(\"{}\", str(block_index))\n    return name\n```\n\n**Sources**: [lightx2v/common/modules/weight_module.py:62-106](), [lightx2v/common/ops/mm/mm_weight.py:220-236](), [lightx2v/common/ops/utils.py:1-500]()\n\n---\n\n## Integration with Model Lifecycle\n\n### Model Initialization and Weight Loading\n\nThe weight management system integrates with the model initialization pipeline:\n\n```mermaid\nsequenceDiagram\n    participant Config\n    participant Runner as \"WanRunner\"\n    participant Model as \"WanModel\"\n    participant PreWeight as \"WanPreWeights\"\n    participant TransWeight as \"WanTransformerWeights\"\n    participant Block as \"WanTransformerAttentionBlock\"\n    participant MMWeight\n    participant Loader as \"load_weights()\"\n    \n    Note over Config,Loader: Initialization Phase\n    Config-\u003e\u003eRunner: Create runner with config\n    Runner-\u003e\u003eModel: __init__(model_path, config, device)\n    \n    Model-\u003e\u003eModel: _should_init_empty_model()\n    alt LoRA configs and not lazy_load\n        Model-\u003e\u003eModel: Initialize with empty tensors\n        Note over Model: Skip loading, LoRA will merge later\n    else Normal loading\n        Model-\u003e\u003eModel: _init_weights()\n        Model-\u003e\u003ePreWeight: __init__(config)\n        Model-\u003e\u003eTransWeight: __init__(config, lazy_load_path)\n        \n        TransWeight-\u003e\u003eBlock: Create blocks[0..num_layers-1]\n        Block-\u003e\u003eMMWeight: Create self_attn_q, k, v, o, etc.\n        \n        alt Not lazy_load\n            Model-\u003e\u003eLoader: load_weights(model_path)\n            Loader--\u003e\u003eModel: weight_dict\n            Model-\u003e\u003ePreWeight: load(weight_dict)\n            PreWeight-\u003e\u003eMMWeight: load(weight_dict)\n            MMWeight-\u003e\u003eMMWeight: self.weight = weight_dict[weight_name]\n            Model-\u003e\u003eTransWeight: load(weight_dict)\n        else lazy_load\n            Model-\u003e\u003eMMWeight: Create with lazy_load_file=model_path\n            Note over MMWeight: Weights loaded on-demand later\n        end\n    end\n    \n    Note over Config,Loader: LoRA Application (if configured)\n    alt lora_dynamic_apply\n        Model-\u003e\u003eModel: Store lora_path for runtime loading\n    else Static LoRA merge\n        Model-\u003e\u003eLoader: load_lora_weights(lora_path)\n        Loader--\u003e\u003eModel: lora_weight_dict\n        Model-\u003e\u003ePreWeight: register_lora(lora_weight_dict, strength)\n        PreWeight-\u003e\u003eMMWeight: register_lora(lora_weight_dict, strength)\n        MMWeight-\u003e\u003eMMWeight: self.lora_down = lora_weight_dict[name]\n        Model-\u003e\u003eTransWeight: register_lora(lora_weight_dict, strength)\n    end\n```\n\n**Initialization Code** (from [lightx2v/models/networks/base_model.py:30-120]()):\n\n```python\nclass BaseTransformerModel:\n    def __init__(self, model_path, config, device, model_type=\"wan2.1\", lora_path=None, lora_strength=1.0):\n        self.config = config\n        self.lazy_load = config.get(\"lazy_load\", False)\n        \n        # Determine if we should skip initial weight loading\n        if self._should_init_empty_model():\n            # LoRA will be merged statically, skip base weights\n            self._init_weights()  # Create empty weight structure\n            return\n        \n        # Initialize weight modules\n        self._init_weights()\n        \n        # Load base weights\n        if not self.lazy_load:\n            weight_dict = load_weights(model_path, cpu_offload=self.cpu_offload)\n            self.pre_weight.load(weight_dict)\n            self.transformer_weights.load(weight_dict)\n            self.post_weight.load(weight_dict)\n        \n        # Apply LoRA if configured\n        if lora_path and not config.get(\"lora_dynamic_apply\", False):\n            lora_weight_dict = load_weights(lora_path)\n            self.pre_weight.register_lora(lora_weight_dict, lora_strength)\n            self.transformer_weights.register_lora(lora_weight_dict, lora_strength)\n```\n\n**Sources**: [lightx2v/models/networks/base_model.py:30-120](), [lightx2v/models/networks/wan/model.py:37-54](), [lightx2v/models/runners/wan/wan_runner.py:72-79]()\n\n---\n\n## Usage Examples\n\n### Example 1: Basic Weight Loading\n\n```python\n# Configuration\nconfig = {\n    \"model_path\": \"/path/to/model\",\n    \"num_layers\": 30,\n    \"dim\": 4096,\n    \"lazy_load\": False,\n    \"cpu_offload\": False,\n}\n\n# Create transformer weights\ntransformer_weights = WanTransformerWeights(config, lazy_load_path=None)\n\n# Load from safetensors\nweight_dict = load_weights(config[\"model_path\"])\ntransformer_weights.load(weight_dict)\n\n# Access specific block\nblock_0 = transformer_weights.blocks[0]\nself_attn_q = block_0.compute_phases[0].self_attn_q\n\n# Forward pass\noutput = self_attn_q.apply(input_tensor)  # Uses base weight only\n```\n\n**Sources**: [lightx2v/models/networks/wan/weights/transformer_weights.py:11-53]()\n\n---\n\n### Example 2: Dynamic LoRA Switching\n\n```python\n# Initial setup without LoRA\nrunner = WanRunner(config)\nrunner.init_modules()\n\n# Generate with base model\nresult1 = runner.run_pipeline(input_info)\n\n# Switch to LoRA adapter\nsuccess = runner.switch_lora(\n    lora_path=\"/path/to/lora/style_anime.safetensors\",\n    strength=0.8\n)\n\n# Generate with LoRA\nresult2 = runner.run_pipeline(input_info)  # Now uses LoRA\n\n# Remove LoRA\nrunner.switch_lora(lora_path=\"\", strength=1.0)\n\n# Back to base model\nresult3 = runner.run_pipeline(input_info)\n```\n\n**Sources**: [lightx2v/models/runners/default_runner.py:478-521](), [scripts/server/post_i2i_with_lora.py:14-28]()\n\n---\n\n### Example 3: Lazy Loading with CPU Offloading\n\n```python\n# Configuration for memory-constrained environments\nconfig = {\n    \"model_path\": \"/path/to/model\",\n    \"lazy_load\": True,              # Stream from disk\n    \"cpu_offload\": True,            # Enable CPU offloading\n    \"offload_granularity\": \"block\", # Block-level offloading\n    \"num_layers\": 30,\n}\n\n# Create weights with lazy loading\nlazy_load_path = config[\"model_path\"]\ntransformer_weights = WanTransformerWeights(config, lazy_load_path=lazy_load_path)\n\n# Weights are NOT loaded yet - only buffers allocated\n\n# Create offload manager\nfrom lightx2v.models.networks.wan.infer.offload.transformer_infer import WeightAsyncStreamManager\noffload_manager = WeightAsyncStreamManager(transformer_weights, config)\n\n# During inference, load blocks on-demand\nfor block_idx in range(30):\n    # Stream: Disk -\u003e CPU buffer -\u003e GPU buffer\n    offload_manager.load_block_to_cuda(block_idx, stream_idx=block_idx)\n    \n    # Run inference with block\n    output = transformer_weights.blocks[block_idx].forward(input)\n    \n    # Offload when done\n    offload_manager.offload_block_to_cpu(block_idx)\n```\n\n**Sources**: [lightx2v/models/networks/wan/weights/transformer_weights.py:55-132](), [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:1-350]()\n\n---\n\n### Example 4: Quantized Weight Loading\n\n```python\n# Configuration for INT8 quantization\nconfig = {\n    \"model_path\": \"/path/to/model\",\n    \"dit_quantized\": True,\n    \"dit_quant_scheme\": \"int8-triton\",  # Use Triton kernels\n    \"act_auto_quant\": True,             # Quantize activations at runtime\n}\n\n# Create quantized weight module\nmm_weight = MMWeightINT8(\n    weight_name=\"blocks.0.compute_phases.0.self_attn_q.weight\",\n    bias_name=\"blocks.0.compute_phases.0.self_attn_q.bias\",\n)\nmm_weight.set_config(config)\n\n# Load quantized weights (INT8 + FP32 scales)\nweight_dict = {\n    \"blocks.0.compute_phases.0.self_attn_q.weight\": torch.randint(-128, 127, (4096, 4096), dtype=torch.int8),\n    \"blocks.0.compute_phases.0.self_attn_q.weight_scale\": torch.randn(4096, dtype=torch.float32),\n    \"blocks.0.compute_phases.0.self_attn_q.bias\": torch.randn(4096, dtype=torch.bfloat16),\n}\nmm_weight.load(weight_dict)\n\n# Forward pass uses quantized GEMM\noutput = mm_weight.apply(input_tensor)\n# Internally: quantize input -\u003e int8_gemm_triton -\u003e dequantize output\n```\n\n**Sources**: [lightx2v/common/ops/mm/mm_weight.py:852-889]()"])</script><script>self.__next_f.push([1,"2b:T6c58,"])</script><script>self.__next_f.push([1,"# Input Encoder Architecture\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [app/utils/image_page.py](app/utils/image_page.py)\n- [configs/z_image/z_image_turbo_t2i_offload.json](configs/z_image/z_image_turbo_t2i_offload.json)\n- [examples/z-image-turbo/z_image_turbo.py](examples/z-image-turbo/z_image_turbo.py)\n- [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py](lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py)\n- [lightx2v/models/input_encoders/hf/z_image/qwen3_model.py](lightx2v/models/input_encoders/hf/z_image/qwen3_model.py)\n- [lightx2v/models/networks/base_model.py](lightx2v/models/networks/base_model.py)\n- [lightx2v/models/networks/qwen_image/infer/pre_infer.py](lightx2v/models/networks/qwen_image/infer/pre_infer.py)\n- [lightx2v/models/networks/qwen_image/infer/transformer_infer.py](lightx2v/models/networks/qwen_image/infer/transformer_infer.py)\n- [lightx2v/models/networks/qwen_image/model.py](lightx2v/models/networks/qwen_image/model.py)\n- [lightx2v/models/networks/z_image/model.py](lightx2v/models/networks/z_image/model.py)\n- [lightx2v/models/networks/z_image/weights/post_weights.py](lightx2v/models/networks/z_image/weights/post_weights.py)\n- [lightx2v/models/networks/z_image/weights/transformer_weights.py](lightx2v/models/networks/z_image/weights/transformer_weights.py)\n- [lightx2v/models/runners/qwen_image/qwen_image_runner.py](lightx2v/models/runners/qwen_image/qwen_image_runner.py)\n- [lightx2v/models/runners/z_image/z_image_runner.py](lightx2v/models/runners/z_image/z_image_runner.py)\n- [lightx2v/models/schedulers/qwen_image/scheduler.py](lightx2v/models/schedulers/qwen_image/scheduler.py)\n- [lightx2v/models/video_encoders/hf/qwen_image/vae.py](lightx2v/models/video_encoders/hf/qwen_image/vae.py)\n- [lightx2v/models/video_encoders/hf/z_image/vae.py](lightx2v/models/video_encoders/hf/z_image/vae.py)\n- [scripts/z_image/z_image_turbo_t2i.sh](scripts/z_image/z_image_turbo_t2i.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the input encoder architecture in LightX2V, covering how text, image, and audio inputs are processed before entering the diffusion transformer. The encoders transform raw inputs into high-dimensional embeddings that condition the video generation process.\n\nFor information about the VAE encoder/decoder system, see [VAE System and Video Encoding](#4.5). For the complete inference pipeline that uses these encoders, see [Three-Stage Inference Pipeline](#4.2). For model-specific encoding implementations, see [Model Variants and Tasks](#5).\n\n---\n\n## Text Encoder System\n\n### Supported Text Encoders\n\nLightX2V supports multiple text encoder architectures depending on the model family:\n\n| Encoder | Model Family | Output Dimension | Context Length | Purpose |\n|---------|-------------|------------------|----------------|---------|\n| **T5-XXL** | WAN 2.1/2.2 | 4096 | 512 tokens | Primary text conditioning |\n| **ByT5** | HunyuanVideo 1.5 | Variable | Variable | Multilingual support |\n| **Qwen2.5-VL** | HunyuanVideo 1.5 | 7B LLM | Variable | Vision-language understanding |\n| **XLM-RoBERTa CLIP** | WAN (image tasks) | 1280 | 77 tokens | Visual-text alignment |\n\n**Text Encoder Loading Architecture**\n\n```mermaid\ngraph TB\n    Runner[\"WanRunner / HunyuanVideo15Runner\"]\n    LoadMethod[\"load_text_encoder()\"]\n    \n    subgraph \"Configuration Detection\"\n        CheckQuant[\"Check t5_quantized config\"]\n        CheckOffload[\"Check t5_cpu_offload config\"]\n        FindCkpt[\"Find checkpoint path\"]\n    end\n    \n    subgraph \"T5 Encoder Components\"\n        T5Model[\"T5EncoderModel\"]\n        Tokenizer[\"HuggingfaceTokenizer\"]\n        Embedding[\"Token Embeddings\"]\n        Blocks[\"T5EncoderBlocks (24 layers)\"]\n        FinalNorm[\"Final LayerNorm\"]\n    end\n    \n    subgraph \"Quantization Options\"\n        INT8[\"INT8: VllmQuantLinearInt8\u003cbr/\u003eTritonQuantLinearInt8\u003cbr/\u003eTorchaoQuantLinearInt8\"]\n        FP8[\"FP8: VllmQuantLinearFp8\u003cbr/\u003eTritonQuantLinearFp8\"]\n        NPU[\"NPU: NpuQuantLinearInt8\"]\n    end\n    \n    Runner --\u003e LoadMethod\n    LoadMethod --\u003e CheckQuant\n    LoadMethod --\u003e CheckOffload\n    LoadMethod --\u003e FindCkpt\n    \n    FindCkpt --\u003e T5Model\n    T5Model --\u003e Tokenizer\n    T5Model --\u003e Embedding\n    T5Model --\u003e Blocks\n    Blocks --\u003e FinalNorm\n    \n    CheckQuant --\u003e|\"Enabled\"| INT8\n    CheckQuant --\u003e|\"Enabled\"| FP8\n    CheckQuant --\u003e|\"Enabled\"| NPU\n    \n    style T5Model fill:#f9f9f9\n    style Blocks fill:#f9f9f9\n```\n\n**Sources:**\n- [lightx2v/models/runners/wan/wan_runner.py:98-136]()\n- [lightx2v/models/input_encoders/hf/wan/t5/model.py:44-49]()\n\n### T5EncoderModel Implementation\n\nThe `T5EncoderModel` class implements Google's T5 (Text-to-Text Transfer Transformer) encoder for text conditioning.\n\n**Key Architecture Components:**\n\n```mermaid\ngraph LR\n    Input[\"Text String\"]\n    Tokenizer[\"HuggingfaceTokenizer\u003cbr/\u003eumt5-xxl vocabulary\"]\n    Embedding[\"T5Embedding\u003cbr/\u003eLearned token embeddings\"]\n    \n    subgraph \"T5EncoderBlocks (24 layers)\"\n        SelfAttn[\"Self-Attention\u003cbr/\u003eMultiheadAttention\"]\n        FFN[\"Feed-Forward Network\u003cbr/\u003e2-layer MLP with GELU\"]\n        Norm[\"RMSNorm\u003cbr/\u003ePre-normalization\"]\n    end\n    \n    FinalNorm[\"Final RMSNorm\"]\n    Output[\"Context Embeddings\u003cbr/\u003e[1, 512, 4096]\"]\n    \n    Input --\u003e Tokenizer\n    Tokenizer --\u003e Embedding\n    Embedding --\u003e Norm\n    Norm --\u003e SelfAttn\n    SelfAttn --\u003e FFN\n    FFN --\u003e FinalNorm\n    FinalNorm --\u003e Output\n    \n    style Output fill:#f9f9f9\n```\n\n**Encoding Process:**\n\n1. **Tokenization**: Text is tokenized using the ByT5/UMT5 tokenizer, supporting multilingual input\n2. **Embedding Lookup**: Token IDs are converted to 4096-dimensional embeddings\n3. **Transformer Layers**: 24 encoder blocks apply self-attention and feed-forward transformations\n4. **Normalization**: Final RMSNorm produces stable output embeddings\n5. **Padding**: Sequences are padded to `text_len` (default 512) with zeros\n\n**Code Implementation:**\n\nThe T5 encoder's forward pass is defined in [lightx2v/models/input_encoders/hf/wan/t5/model.py:448-492]():\n\n```python\n# Key method signature\ndef infer(self, prompts: List[str]) -\u003e torch.Tensor:\n    \"\"\"\n    Args:\n        prompts: List of text strings to encode\n    Returns:\n        Encoded embeddings of shape [batch_size, seq_len, 4096]\n    \"\"\"\n```\n\n**Memory Optimization Features:**\n\n| Feature | Configuration | Memory Savings | Implementation |\n|---------|--------------|----------------|----------------|\n| **Quantization** | `t5_quantized=True` | ~4x (INT8/FP8) | QuantLinear layers |\n| **CPU Offload** | `t5_cpu_offload=True` | GPU memory  CPU | Async transfers |\n| **Lazy Loading** | `t5_lazy_load=True` | Reduced peak memory | Load on demand |\n| **Block-level Offload** | `offload_granularity=\"block\"` | Gradual memory usage | 2-block rotation |\n\n**Sources:**\n- [lightx2v/models/input_encoders/hf/wan/t5/model.py:338-492]()\n- [lightx2v/models/runners/wan/wan_runner.py:98-136]()\n- [lightx2v/utils/set_config.py:14-34]()\n\n### ByT5 Encoder for Multilingual Support\n\nHunyuanVideo 1.5 uses ByT5 (Byte-level T5) for multilingual text conditioning with special token support.\n\n**ByT5 Unique Features:**\n\n```mermaid\ngraph TB\n    Input[\"Multilingual Text\"]\n    ByteTokenizer[\"Byte-level Tokenizer\u003cbr/\u003eNo vocabulary limit\"]\n    \n    subgraph \"Special Token System\"\n        ColorTokens[\"Color Tokens\u003cbr/\u003e\u0026lt;color-0\u0026gt; to \u0026lt;color-N\u0026gt;\"]\n        FontTokens[\"Font Tokens\u003cbr/\u003e\u0026lt;font-0\u0026gt; to \u0026lt;font-N\u0026gt;\u003cbr/\u003e\u0026lt;zh-font-X\u0026gt;, \u0026lt;en-font-Y\u0026gt;\"]\n        SpecialEmbed[\"Specialized Embeddings\"]\n    end\n    \n    T5Enc[\"T5 Encoder Layers\"]\n    Output[\"Contextual Embeddings\"]\n    \n    Input --\u003e ByteTokenizer\n    ByteTokenizer --\u003e ColorTokens\n    ByteTokenizer --\u003e FontTokens\n    ColorTokens --\u003e SpecialEmbed\n    FontTokens --\u003e SpecialEmbed\n    SpecialEmbed --\u003e T5Enc\n    T5Enc --\u003e Output\n    \n    style SpecialEmbed fill:#f9f9f9\n```\n\nThe `add_special_token()` function extends the vocabulary with color and font tokens for fine-grained text rendering control in text-to-image generation.\n\n**Sources:**\n- [lightx2v/models/input_encoders/hf/hunyuan15/byt5/model.py:21-61]()\n- [lightx2v/models/input_encoders/hf/hunyuan15/byt5/model.py:63-102]()\n\n### Text Encoder Integration in Runners\n\nThe text encoder is invoked during the pre-inference stage via `run_text_encoder()`:\n\n**WanRunner Text Encoding Flow:**\n\n```mermaid\nsequenceDiagram\n    participant Runner as WanRunner\n    participant Encoder as T5EncoderModel\n    participant Output as Text Embeddings\n    \n    Runner-\u003e\u003eRunner: Check lazy_load/unload_modules\n    alt Lazy Loading\n        Runner-\u003e\u003eRunner: load_text_encoder()\n    end\n    \n    Runner-\u003e\u003eRunner: Select prompt\u003cbr/\u003e(enhanced or original)\n    Runner-\u003e\u003eEncoder: infer([prompt])\n    Encoder-\u003e\u003eEncoder: Tokenize text\n    Encoder-\u003e\u003eEncoder: Run T5 layers\n    Encoder-\u003e\u003eOutput: Return embeddings [1, 512, 4096]\n    \n    Runner-\u003e\u003eRunner: Pad to text_len with zeros\n    \n    alt CFG Parallel Mode\n        Runner-\u003e\u003eEncoder: Encode negative prompt separately\n        Note over Runner: Rank 0: positive\u003cbr/\u003eRank 1: negative\n    else Standard CFG\n        Runner-\u003e\u003eEncoder: Encode negative prompt\n        Output-\u003e\u003eRunner: context \u0026 context_null\n    end\n    \n    alt Lazy Loading\n        Runner-\u003e\u003eRunner: Delete encoder, free GPU\n    end\n```\n\n**Sources:**\n- [lightx2v/models/runners/wan/wan_runner.py:216-260]()\n- [lightx2v/models/runners/default_runner.py:275-295]()\n\n---\n\n## Image Encoder System\n\n### CLIP Visual Encoder\n\nThe image encoder uses OpenAI's CLIP (XLM-RoBERTa CLIP ViT-H/14) to extract visual features from input images for image-to-video (i2v) and related tasks.\n\n**CLIP Architecture:**\n\n```mermaid\ngraph TB\n    InputImg[\"Input Image\u003cbr/\u003e(after resize)\"]\n    \n    subgraph \"Vision Transformer\"\n        PatchEmbed[\"Patch Embedding\u003cbr/\u003e1414 patches  1280 dim\"]\n        PosEmbed[\"Positional Embedding\u003cbr/\u003epos_interpolate for dynamic size\"]\n        \n        subgraph \"Transformer Blocks (31 layers)\"\n            Block1[\"Block 0-30\u003cbr/\u003eSelf-Attention + FFN\"]\n            Block31[\"Block 31 (optional)\u003cbr/\u003euse_31_block config\"]\n        end\n        \n        Norm[\"Layer Normalization\"]\n    end\n    \n    Output[\"Visual Features\u003cbr/\u003e[257, 1280]\"]\n    \n    InputImg --\u003e PatchEmbed\n    PatchEmbed --\u003e PosEmbed\n    PosEmbed --\u003e Block1\n    Block1 --\u003e Block31\n    Block31 --\u003e Norm\n    Norm --\u003e Output\n    \n    style Output fill:#f9f9f9\n```\n\n**Output Format:**\n- Shape: `[257, 1280]` where 257 = 1 CLS token + 256 patch tokens (1616 grid)\n- Dtype: Configurable (default `torch.float16`)\n- Used for: Cross-attention conditioning in i2v/animate/s2v tasks\n\n**CLIP Loading and Quantization:**\n\nThe CLIP model supports the same quantization schemes as T5:\n\n```python\n# From wan_runner.py:60-95\ndef load_image_encoder(self):\n    clip_quantized = self.config.get(\"clip_quantized\", False)\n    if clip_quantized:\n        clip_quant_scheme = self.config.get(\"clip_quant_scheme\")\n        # Supports: int8-*, fp8-*, etc.\n        clip_model_name = f\"models_clip_...-{quant_scheme}.pth\"\n```\n\n**Sources:**\n- [lightx2v/models/input_encoders/hf/wan/xlm_roberta/model.py:1-32]()\n- [lightx2v/models/runners/wan/wan_runner.py:60-95]()\n\n### Image Encoder Invocation\n\n**Encoding Process in WanRunner:**\n\n```mermaid\ngraph LR\n    ReadImg[\"read_image_input()\u003cbr/\u003eLoad \u0026 preprocess image\"]\n    Resize[\"resize_image()\u003cbr/\u003eAdaptive bucket sizing\"]\n    RunCLIP[\"run_image_encoder()\u003cbr/\u003eExtract CLIP features\"]\n    \n    subgraph \"Image Preprocessing\"\n        ToTensor[\"to_tensor()  normalize\u003cbr/\u003e[-1, 1] range\"]\n        AdaptiveBucket[\"Match closest bucket\u003cbr/\u003e480p/540p/720p\"]\n        Interpolate[\"Bicubic interpolation\u003cbr/\u003eto target size\"]\n    end\n    \n    Output[\"clip_encoder_out\u003cbr/\u003e[257, 1280]\"]\n    \n    ReadImg --\u003e ToTensor\n    ToTensor --\u003e AdaptiveBucket\n    AdaptiveBucket --\u003e Interpolate\n    Interpolate --\u003e Resize\n    Resize --\u003e RunCLIP\n    RunCLIP --\u003e Output\n    \n    style Output fill:#f9f9f9\n```\n\nThe `run_image_encoder()` method handles lazy loading and cleanup:\n\n```python\n# From wan_runner.py:262-279\n@ProfilingContext4DebugL1(\"Run Image Encoder\")\ndef run_image_encoder(self, first_frame, last_frame=None):\n    if self.config.get(\"lazy_load\", False):\n        self.image_encoder = self.load_image_encoder()\n    \n    if last_frame is None:\n        clip_encoder_out = self.image_encoder.visual([first_frame])\n    else:\n        # For flf2v (first-last-frame-to-video)\n        clip_encoder_out = self.image_encoder.visual([first_frame, last_frame])\n    \n    if self.config.get(\"lazy_load\", False):\n        del self.image_encoder\n        torch.cuda.empty_cache()\n```\n\n**Sources:**\n- [lightx2v/models/runners/wan/wan_runner.py:262-279]()\n- [lightx2v/models/runners/default_runner.py:241-273]()\n\n---\n\n## Audio Encoder System\n\nThe audio encoder system is specialized for speech-to-video (s2v) tasks, converting audio waveforms into conditioning features.\n\n### Audio Encoding Pipeline\n\n**Complete Audio Processing Architecture:**\n\n```mermaid\ngraph TB\n    AudioFile[\"Audio File(s)\u003cbr/\u003e.wav, .mp3\"]\n    \n    subgraph \"Audio Loading\"\n        LoadAudio[\"AudioProcessor.load_audio()\u003cbr/\u003eResample to 16kHz\"]\n        MultiPerson[\"load_multi_person_audio()\u003cbr/\u003eHandle multiple speakers\"]\n    end\n    \n    subgraph \"Audio Segmentation\"\n        Segment[\"segment_audio()\u003cbr/\u003eSplit into temporal chunks\"]\n        FrameAlign[\"Frame-aligned segments\u003cbr/\u003e81 frames per segment\"]\n    end\n    \n    subgraph \"Audio Encoding\"\n        HubertEnc[\"SekoAudioEncoderModel\u003cbr/\u003eHubert-based encoder\"]\n        Features[\"Audio Features\u003cbr/\u003e[N, T, 1024]\"]\n    end\n    \n    subgraph \"Audio Adaptation\"\n        AudioAdapter[\"AudioAdapter\u003cbr/\u003eforward_audio_proj()\"]\n        Projection[\"Multi-layer projection\u003cbr/\u003e1024  321024\"]\n        Output[\"Adapted Features\u003cbr/\u003e[N, latent_len, 128, 1024]\"]\n    end\n    \n    AudioFile --\u003e LoadAudio\n    LoadAudio --\u003e MultiPerson\n    MultiPerson --\u003e Segment\n    Segment --\u003e FrameAlign\n    FrameAlign --\u003e HubertEnc\n    HubertEnc --\u003e Features\n    Features --\u003e AudioAdapter\n    AudioAdapter --\u003e Projection\n    Projection --\u003e Output\n    \n    style Output fill:#f9f9f9\n```\n\n**Sources:**\n- [lightx2v/models/runners/wan/wan_audio_runner.py:175-253]()\n- [lightx2v/models/runners/wan/wan_audio_runner.py:292-329]()\n\n### SekoAudioEncoderModel\n\nThe `SekoAudioEncoderModel` wraps a pre-trained Hubert model (TencentGameMate-chinese-hubert-large) to extract audio features.\n\n**Key Characteristics:**\n\n| Property | Value | Description |\n|----------|-------|-------------|\n| **Input** | Raw audio waveform @ 16kHz | Mono channel audio |\n| **Output** | `[T, 1024]` features | T = temporal frames |\n| **Base Model** | Hubert Large | Self-supervised speech model |\n| **Temporal Granularity** | ~50Hz | One feature per 20ms |\n\n**Audio Encoding Process:**\n\n```python\n# From wan_audio_runner.py:569-580\nif (self.config.get(\"lazy_load\", False) or \n    self.config.get(\"unload_modules\", False)) and \n    not hasattr(self, \"audio_encoder\"):\n    self.audio_encoder = self.load_audio_encoder()\n\nfeatures_list = []\nfor i in range(self.segment.audio_array.shape[0]):\n    feat = self.audio_encoder.infer(self.segment.audio_array[i])\n    feat = self.audio_adapter.forward_audio_proj(\n        feat, self.model.scheduler.latents.shape[1])\n    features_list.append(feat.squeeze(0))\naudio_features = torch.stack(features_list, dim=0)\n```\n\n**Sources:**\n- [lightx2v/models/runners/wan/wan_audio_runner.py:749-753]()\n- [lightx2v/models/runners/wan/wan_audio_runner.py:569-580]()\n\n### AudioAdapter Architecture\n\nThe `AudioAdapter` projects Hubert features into the transformer's conditioning space with specialized temporal and frequency modeling.\n\n**AudioAdapter Structure:**\n\n```mermaid\ngraph TB\n    Input[\"Audio Features\u003cbr/\u003e[T, 1024]\"]\n    \n    subgraph \"Projection Layers\"\n        TimeFreq[\"Time-Frequency Projection\u003cbr/\u003e1024  256\"]\n        LayerNorm[\"Layer Normalization\"]\n    end\n    \n    subgraph \"Transformer Refinement\"\n        ProjTransformer[\"Projection Transformer\u003cbr/\u003e4 layers of self-attention\"]\n        CrossAttn[\"Optional Cross-Attention\u003cbr/\u003ewith base model layers\"]\n    end\n    \n    subgraph \"Final Projection\"\n        MLPLayers[\"Multi-layer MLP\u003cbr/\u003e1024  1024  32768\"]\n        Reshape[\"Reshape to\u003cbr/\u003e[latent_T, 128, 1024]\"]\n    end\n    \n    Output[\"Adapted Audio Features\u003cbr/\u003e[N, latent_T, 128, 1024]\"]\n    \n    Input --\u003e TimeFreq\n    TimeFreq --\u003e LayerNorm\n    LayerNorm --\u003e ProjTransformer\n    ProjTransformer --\u003e CrossAttn\n    CrossAttn --\u003e MLPLayers\n    MLPLayers --\u003e Reshape\n    Reshape --\u003e Output\n    \n    style Output fill:#f9f9f9\n```\n\n**Configuration Parameters:**\n\n```python\n# From wan_audio_runner.py:761-779\naudio_adapter = AudioAdapter(\n    attention_head_dim=self.config[\"dim\"] // self.config[\"num_heads\"],\n    num_attention_heads=self.config[\"num_heads\"],\n    base_num_layers=self.config[\"num_layers\"],\n    interval=1,\n    audio_feature_dim=1024,        # Hubert output dim\n    time_freq_dim=256,              # Intermediate projection\n    projection_transformer_layers=4, # Refinement layers\n    mlp_dims=(1024, 1024, 32*1024), # Final projection\n    quantized=self.config.get(\"adapter_quantized\", False),\n    quant_scheme=self.config.get(\"adapter_quant_scheme\", None),\n    cpu_offload=audio_adapter_offload,\n)\n```\n\n**Sources:**\n- [lightx2v/models/runners/wan/wan_audio_runner.py:755-779]()\n- [lightx2v/models/networks/wan/audio_model.py:1-17]()\n\n### Multi-Person Audio Processing\n\nFor multi-speaker audio-to-video generation, the system handles multiple audio streams with spatial masking:\n\n**Multi-Person Audio Flow:**\n\n```mermaid\ngraph TB\n    ConfigFile[\"config.json\u003cbr/\u003etalk_objects array\"]\n    \n    subgraph \"Per-Person Processing\"\n        Audio1[\"Person 1 Audio\u003cbr/\u003eaudio1.wav\"]\n        Audio2[\"Person 2 Audio\u003cbr/\u003eaudio2.wav\"]\n        AudioN[\"Person N Audio\u003cbr/\u003eaudioN.wav\"]\n        \n        Mask1[\"Person 1 Mask\u003cbr/\u003emask1.png\"]\n        Mask2[\"Person 2 Mask\u003cbr/\u003emask2.png\"]\n        MaskN[\"Person N Mask\u003cbr/\u003emaskN.png\"]\n    end\n    \n    subgraph \"Encoding Stage\"\n        LoadMulti[\"load_multi_person_audio()\u003cbr/\u003ePad to max length\"]\n        EncodeAll[\"Encode all audio streams\u003cbr/\u003eN  [T, 1024]\"]\n        ProcessMasks[\"process_single_mask()\u003cbr/\u003eResize \u0026 downsample\"]\n    end\n    \n    Output[\"Combined Output\u003cbr/\u003eaudio_features: [N, latent_T, 128, 1024]\u003cbr/\u003emask_latents: [N, 1, H/16, W/16]\"]\n    \n    ConfigFile --\u003e Audio1\n    ConfigFile --\u003e Audio2\n    ConfigFile --\u003e AudioN\n    ConfigFile --\u003e Mask1\n    ConfigFile --\u003e Mask2\n    ConfigFile --\u003e MaskN\n    \n    Audio1 --\u003e LoadMulti\n    Audio2 --\u003e LoadMulti\n    AudioN --\u003e LoadMulti\n    \n    LoadMulti --\u003e EncodeAll\n    \n    Mask1 --\u003e ProcessMasks\n    Mask2 --\u003e ProcessMasks\n    MaskN --\u003e ProcessMasks\n    \n    EncodeAll --\u003e Output\n    ProcessMasks --\u003e Output\n    \n    style Output fill:#f9f9f9\n```\n\nThe mask latents are used in the diffusion model to spatially separate different speakers' lip movements.\n\n**Sources:**\n- [lightx2v/models/runners/wan/wan_audio_runner.py:331-348]()\n- [lightx2v/models/runners/wan/wan_audio_runner.py:350-372]()\n- [lightx2v/models/runners/wan/wan_audio_runner.py:188-204]()\n\n---\n\n## Encoder Memory Management\n\nAll input encoders support advanced memory optimization techniques to enable inference on consumer hardware.\n\n### Quantization Support\n\n**Quantization Scheme Comparison:**\n\n```mermaid\ngraph TB\n    Original[\"Original FP16/BF16 Weights\"]\n    \n    subgraph \"Integer Quantization\"\n        INT8[\"INT8 Quantization\u003cbr/\u003e8-bit integers\"]\n        INT8Variants[\"Backends:\u003cbr/\u003e int8-vllm\u003cbr/\u003e int8-triton\u003cbr/\u003e int8-sgl\u003cbr/\u003e int8-q8f\u003cbr/\u003e int8-torchao\u003cbr/\u003e int8-npu\"]\n    end\n    \n    subgraph \"Floating Point Quantization\"\n        FP8[\"FP8 Quantization\u003cbr/\u003eE4M3 format\"]\n        FP8Variants[\"Backends:\u003cbr/\u003e fp8-vllm\u003cbr/\u003e fp8-triton\u003cbr/\u003e fp8-sgl\u003cbr/\u003e fp8-q8f\u003cbr/\u003e fp8-torchao\"]\n    end\n    \n    subgraph \"Advanced Formats\"\n        NVFP4[\"NVFP4\u003cbr/\u003e4-bit NVIDIA format\"]\n        MXFP[\"MXFP4/6/8\u003cbr/\u003eMixed-precision formats\"]\n    end\n    \n    Memory[\"Memory Savings\u003cbr/\u003eINT8: 2\u003cbr/\u003eFP8: 2\u003cbr/\u003eNVFP4: 4\u003cbr/\u003eMXFP4: 4\"]\n    \n    Original --\u003e INT8\n    Original --\u003e FP8\n    Original --\u003e NVFP4\n    Original --\u003e MXFP\n    \n    INT8 --\u003e INT8Variants\n    FP8 --\u003e FP8Variants\n    NVFP4 --\u003e Memory\n    MXFP --\u003e Memory\n    INT8Variants --\u003e Memory\n    FP8Variants --\u003e Memory\n```\n\nAll quantization schemes are unified through the `MM_WEIGHT_REGISTER` and `QuantLinear` abstractions.\n\n**Sources:**\n- [lightx2v/models/input_encoders/hf/q_linear.py:1-33]()\n- [lightx2v/models/input_encoders/hf/q_linear.py:37-95]()\n- [lightx2v/models/input_encoders/hf/q_linear.py:97-223]()\n- [lightx2v/common/ops/mm/mm_weight.py:149-319]()\n\n### CPU Offloading\n\nEncoders can be offloaded to CPU memory between encoding stages:\n\n**Offloading Strategy:**\n\n```mermaid\nsequenceDiagram\n    participant GPU as GPU Memory\n    participant CPU as CPU Memory (Pinned)\n    participant Runner as Runner Process\n    \n    Note over GPU,CPU: Initial State: Encoder on CPU\n    \n    Runner-\u003e\u003eCPU: Check lazy_load/unload_modules config\n    \n    alt Lazy Loading Enabled\n        Runner-\u003e\u003eGPU: load_text_encoder()\n        CPU-\u003e\u003eGPU: Transfer encoder weights (async)\n        Note over GPU: Encoder ready on GPU\n    end\n    \n    Runner-\u003e\u003eGPU: Run text encoding\n    GPU-\u003e\u003eGPU: Process input  embeddings\n    GPU-\u003e\u003eRunner: Return embeddings\n    \n    alt Lazy Loading Enabled\n        Runner-\u003e\u003eCPU: Delete encoder from GPU\n        GPU-\u003e\u003eGPU: torch.cuda.empty_cache()\n        Note over GPU: GPU memory freed\n    end\n    \n    Note over CPU: Encoder remains on CPU for next use\n```\n\n**Offloading Configuration:**\n\n| Config Key | Effect | Use Case |\n|-----------|--------|----------|\n| `t5_cpu_offload` | T5 lives on CPU | 8GB VRAM systems |\n| `clip_cpu_offload` | CLIP lives on CPU | Low memory mode |\n| `audio_encoder_cpu_offload` | Audio encoder on CPU | Multi-person s2v |\n| `audio_adapter_cpu_offload` | Audio adapter on CPU | Further memory reduction |\n| `lazy_load` | Load encoders on-demand | Minimal peak memory |\n| `unload_modules` | Unload after each use | Sequential processing |\n\n**Sources:**\n- [lightx2v/models/runners/wan/wan_runner.py:98-136]()\n- [lightx2v/models/runners/wan/wan_runner.py:216-260]()\n- [lightx2v/models/runners/wan/wan_audio_runner.py:749-779]()\n\n### Lazy Loading from Disk\n\nThe `lazy_load` mode enables loading encoder weights directly from disk to GPU on-demand:\n\n```python\n# From wan_runner.py:223-258\n@ProfilingContext4DebugL1(\"Run Text Encoder\")\ndef run_text_encoder(self, input_info):\n    # Load encoder only when needed\n    if self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n        self.text_encoders = self.load_text_encoder()\n    \n    # Encode text\n    context = self.text_encoders[0].infer([prompt])\n    \n    # Immediately free GPU memory\n    if self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n        del self.text_encoders[0]\n        torch_device_module.empty_cache()\n        gc.collect()\n```\n\nThis pattern is repeated across all encoder types.\n\n**Sources:**\n- [lightx2v/models/runners/wan/wan_runner.py:222-260]()\n- [lightx2v/models/runners/default_runner.py:120-127]()\n\n---\n\n## Encoder Output Integration\n\n### Pre-Inference Stage Integration\n\nAll encoder outputs are collected and passed to the `WanPreInfer` module:\n\n**Encoder Output Flow:**\n\n```mermaid\ngraph TB\n    subgraph \"Encoder Execution\"\n        TextEnc[\"run_text_encoder()\u003cbr/\u003e context, context_null\"]\n        ImageEnc[\"run_image_encoder()\u003cbr/\u003e clip_encoder_out\"]\n        VAEEnc[\"run_vae_encoder()\u003cbr/\u003e vae_encoder_out\"]\n        AudioEnc[\"run_audio_encoder()\u003cbr/\u003e audio_features\"]\n    end\n    \n    subgraph \"Output Dictionary\"\n        TextOut[\"text_encoder_output:\u003cbr/\u003e context: [1, 512, 4096]\u003cbr/\u003e context_null: [1, 512, 4096]\"]\n        ImageOut[\"image_encoder_output:\u003cbr/\u003e clip_encoder_out: [257, 1280]\u003cbr/\u003e vae_encoder_out: [16, T, H, W]\"]\n        AudioOut[\"audio_encoder_output:\u003cbr/\u003e audio_features: [N, T, 128, 1024]\"]\n    end\n    \n    PreInfer[\"WanPreInfer.infer()\u003cbr/\u003eProcesses all encoder outputs\"]\n    \n    subgraph \"Pre-Inference Processing\"\n        PatchEmbed[\"Patch embedding of latents\"]\n        TimeEmbed[\"Time step embedding\u003cbr/\u003esinusoidal_embedding_1d()\"]\n        TextEmbed[\"Text embedding projection\u003cbr/\u003etext_embedding_0/2\"]\n        CLIPProj[\"CLIP projection\u003cbr/\u003eproj_0/1/3\"]\n    end\n    \n    TransformerInput[\"Transformer Input\u003cbr/\u003e x: patched latents\u003cbr/\u003e context: text features\u003cbr/\u003e embed0: timestep + modulation\"]\n    \n    TextEnc --\u003e TextOut\n    ImageEnc --\u003e ImageOut\n    VAEEnc --\u003e ImageOut\n    AudioEnc --\u003e AudioOut\n    \n    TextOut --\u003e PreInfer\n    ImageOut --\u003e PreInfer\n    AudioOut --\u003e PreInfer\n    \n    PreInfer --\u003e PatchEmbed\n    PreInfer --\u003e TimeEmbed\n    PreInfer --\u003e TextEmbed\n    PreInfer --\u003e CLIPProj\n    \n    PatchEmbed --\u003e TransformerInput\n    TimeEmbed --\u003e TransformerInput\n    TextEmbed --\u003e TransformerInput\n    CLIPProj --\u003e TransformerInput\n    \n    style TransformerInput fill:#f9f9f9\n```\n\n**Sources:**\n- [lightx2v/models/networks/wan/infer/pre_infer.py:9-142]()\n- [lightx2v/models/runners/default_runner.py:275-295]()\n- [lightx2v/models/runners/wan/wan_audio_runner.py:440-465]()\n\n### Conditioning Tensor Shapes\n\n**Summary of Encoder Output Shapes:**\n\n| Encoder | Output Name | Shape | Dtype | Purpose |\n|---------|------------|-------|-------|---------|\n| **T5** | `context` | `[1, 512, 4096]` | bf16 | Text conditioning |\n| **T5** | `context_null` | `[1, 512, 4096]` | bf16 | Negative prompt (CFG) |\n| **CLIP Visual** | `clip_encoder_out` | `[257, 1280]` | fp16 | Image conditioning |\n| **VAE Encoder** | `vae_encoder_out` | `[16, T, H, W]` | bf16/fp16 | First frame latent |\n| **Audio Hubert** | `audio_features` | `[N, T, 128, 1024]` | bf16 | Audio conditioning |\n| **Audio Mask** | `person_mask_latens` | `[N, 1, H/16, W/16]` | int8 | Spatial masks |\n\nWhere:\n- `T` = temporal dimension (latent frames)\n- `H, W` = spatial latent dimensions\n- `N` = number of audio streams (speakers)\n\n**Sources:**\n- [lightx2v/models/runners/wan/wan_runner.py:437-445]()\n- [lightx2v/models/runners/wan/wan_audio_runner.py:456-465]()\n- [lightx2v/models/networks/wan/audio_model.py:75-92]()\n\n---\n\n## Platform-Specific Optimizations\n\nInput encoders support platform-specific quantization kernels:\n\n**Platform Support Matrix:**\n\n```mermaid\ngraph TB\n    EncoderWeights[\"Encoder Linear Layers\"]\n    \n    subgraph \"NVIDIA CUDA\"\n        VllmInt8[\"VllmQuantLinearInt8\u003cbr/\u003eVllmQuantLinearFp8\"]\n        TritonInt8[\"TritonQuantLinearInt8\u003cbr/\u003eTritonQuantLinearFp8\"]\n        Q8F[\"Q8FQuantLinearInt8\u003cbr/\u003eQ8FQuantLinearFp8\"]\n        TorchAO[\"TorchaoQuantLinearInt8\u003cbr/\u003eTorchaoQuantLinearFp8\"]\n    end\n    \n    subgraph \"Cambricon MLU\"\n        MluInt8[\"MluQuantLinearInt8\u003cbr/\u003ePlatform-optimized kernels\"]\n    end\n    \n    subgraph \"Ascend NPU\"\n        NpuInt8[\"NpuQuantLinearInt8\u003cbr/\u003eCANN-accelerated\"]\n    end\n    \n    subgraph \"AMD ROCm\"\n        ROCmVllm[\"VLLM ROCm backend\u003cbr/\u003eHip kernels\"]\n    end\n    \n    EncoderWeights --\u003e VllmInt8\n    EncoderWeights --\u003e TritonInt8\n    EncoderWeights --\u003e Q8F\n    EncoderWeights --\u003e TorchAO\n    EncoderWeights --\u003e MluInt8\n    EncoderWeights --\u003e NpuInt8\n    EncoderWeights --\u003e ROCmVllm\n```\n\nThe platform abstraction allows the same encoder code to run on different hardware with optimized kernels.\n\n**Sources:**\n- [lightx2v/models/input_encoders/hf/q_linear.py:1-33]()\n- [lightx2v_platform/ops/mm/cambricon_mlu/q_linear.py]() (referenced in imports)\n- [lightx2v_platform/ops/mm/ascend_npu/npu_q_linear.py]() (referenced in imports)\n- [lightx2v/models/input_encoders/hf/wan/t5/model.py:18-42]()"])</script><script>self.__next_f.push([1,"2c:T63bc,"])</script><script>self.__next_f.push([1,"# VAE System and Video Processing\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py](lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py)\n- [lightx2v/models/networks/qwen_image/infer/pre_infer.py](lightx2v/models/networks/qwen_image/infer/pre_infer.py)\n- [lightx2v/models/networks/qwen_image/infer/transformer_infer.py](lightx2v/models/networks/qwen_image/infer/transformer_infer.py)\n- [lightx2v/models/networks/qwen_image/model.py](lightx2v/models/networks/qwen_image/model.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/qwen_image/qwen_image_runner.py](lightx2v/models/runners/qwen_image/qwen_image_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/schedulers/qwen_image/scheduler.py](lightx2v/models/schedulers/qwen_image/scheduler.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/qwen_image/vae.py](lightx2v/models/video_encoders/hf/qwen_image/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n\n\u003c/details\u003e\n\n\n\nThis document describes the Variational Autoencoder (VAE) system used for encoding videos into latent representations and decoding latents back to pixel space. The VAE handles spatial and temporal compression, causal convolution with feature caching, tiled processing for large resolutions, and distributed inference across multiple GPUs.\n\nFor information about the DiT transformer models that process latent representations, see [Transformer Block Architecture](#7.1). For scheduler integration during diffusion, see [Scheduler System and Diffusion Process](#4.3).\n\n## Purpose and Architecture Overview\n\nThe VAE serves as the bridge between pixel space and latent space in the diffusion pipeline. It compresses videos from `(B, C=3, T, H, W)` pixel tensors to `(B, C=16, T', H', W')` latent tensors with spatial compression ratio 8x and temporal compression determined by the model configuration. The architecture uses 3D causal convolutions to maintain temporal causality, enabling streaming video processing.\n\nTwo primary implementations exist:\n- **WanVAE**: Base architecture with 16-channel latents, spatial compression 88, configurable temporal compression\n- **Wan2_2_VAE**: Enhanced variant with 48-channel latents, spatial patchification (22), and improved residual connections\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:882-994](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py:885-913]()\n\n```mermaid\ngraph TB\n    subgraph \"Pixel Space\"\n        Video[\"Video Tensor\u003cbr/\u003e(B, 3, T, H, W)\"]\n    end\n    \n    subgraph \"VAE Encoder Path\"\n        Patchify[\"patchify (Wan2.2 only)\u003cbr/\u003e22 spatial patch\"]\n        Conv1Enc[\"conv1: CausalConv3d\u003cbr/\u003e3 or 12  dim\"]\n        DownBlocks[\"Downsampling Blocks\u003cbr/\u003eResidualBlock + Resample\"]\n        MiddleEnc[\"Middle Blocks\u003cbr/\u003eResidualBlock + AttentionBlock\"]\n        HeadEnc[\"Head: CausalConv3d\u003cbr/\u003e z_dim*2\"]\n        Conv1Post[\"conv1: CausalConv3d\u003cbr/\u003e z_dim*2\"]\n    end\n    \n    subgraph \"Latent Space\"\n        Latent[\"Latent Tensor\u003cbr/\u003e(B, 16 or 48, T', H/8, W/8)\u003cbr/\u003ewith mean/std scaling\"]\n    end\n    \n    subgraph \"VAE Decoder Path\"\n        Conv2Dec[\"conv2: CausalConv3d\u003cbr/\u003ez_dim  z_dim\"]\n        Conv1Dec[\"conv1: CausalConv3d\u003cbr/\u003ez_dim  dim\"]\n        MiddleDec[\"Middle Blocks\u003cbr/\u003eResidualBlock + AttentionBlock\"]\n        UpBlocks[\"Upsampling Blocks\u003cbr/\u003eResidualBlock + Resample\"]\n        HeadDec[\"Head: CausalConv3d\u003cbr/\u003e 3 or 12\"]\n        Unpatchify[\"unpatchify (Wan2.2 only)\u003cbr/\u003e22 patch  pixels\"]\n    end\n    \n    Video --\u003e Patchify\n    Patchify --\u003e Conv1Enc\n    Conv1Enc --\u003e DownBlocks\n    DownBlocks --\u003e MiddleEnc\n    MiddleEnc --\u003e HeadEnc\n    HeadEnc --\u003e Conv1Post\n    Conv1Post --\u003e Latent\n    \n    Latent --\u003e Conv2Dec\n    Conv2Dec --\u003e Conv1Dec\n    Conv1Dec --\u003e MiddleDec\n    MiddleDec --\u003e UpBlocks\n    UpBlocks --\u003e HeadDec\n    HeadDec --\u003e Unpatchify\n    Unpatchify --\u003e Video\n    \n    style Latent fill:#f0f0f0,stroke:#333,stroke-width:2px\n```\n\n## Core Architecture Components\n\n### CausalConv3d - Temporal Causality\n\n`CausalConv3d` extends `nn.Conv3d` to implement causal padding along the temporal dimension, ensuring that each frame only depends on current and past frames, never future frames. This enables streaming video generation.\n\n**Key Features:**\n- Asymmetric temporal padding: `(2*padding[0], 0)` instead of symmetric padding\n- Cache-based computation: accepts `cache_x` parameter containing previous frame activations\n- Dynamic padding adjustment when cache is present\n\nThe implementation modifies padding to `(pad_h, pad_h, pad_w, pad_w, 2*pad_t, 0)`, applying full temporal padding on the left (past) and zero on the right (future).\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:24-49](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py:22-47]()\n\n```mermaid\ngraph LR\n    subgraph \"Standard Conv3d Padding\"\n        PastStd[\"Past frames\u003cbr/\u003epad_t\"]\n        CurrentStd[\"Current frame\"]\n        FutureStd[\"Future frames\u003cbr/\u003epad_t\"]\n    end\n    \n    subgraph \"CausalConv3d Padding\"\n        PastCausal[\"Past frames\u003cbr/\u003e2*pad_t\"]\n        CurrentCausal[\"Current frame\"]\n        FutureCausal[\"Future frames\u003cbr/\u003e0 (no padding)\"]\n    end\n    \n    subgraph \"With Cache\"\n        Cache[\"cache_x\u003cbr/\u003elast CACHE_T frames\"]\n        Input[\"new input frames\"]\n        Concat[\"torch.cat along T dim\"]\n        AdjustedPad[\"padding[4] -= cache.shape[2]\"]\n    end\n    \n    PastStd --\u003e CurrentStd\n    CurrentStd --\u003e FutureStd\n    \n    PastCausal --\u003e CurrentCausal\n    CurrentCausal -.no dependency.-\u003e FutureCausal\n    \n    Cache --\u003e Concat\n    Input --\u003e Concat\n    Concat --\u003e AdjustedPad\n    \n    style CurrentCausal fill:#f0f0f0,stroke:#333,stroke-width:2px\n```\n\n### Encoder3d and Decoder3d Structure\n\nThe encoder and decoder follow a symmetric U-Net-like architecture with downsampling and upsampling stages. Both use `ResidualBlock` for feature transformation and optional `AttentionBlock` at specific scales.\n\n**Encoder3d Configuration (WanVAE):**\n- `dim=96`, `z_dim=16`, `dim_mult=[1, 2, 4, 4]`, `temperal_downsample=[False, True, True]`\n- Stages: 96  192  384  384 channels\n- Temporal compression: 1x  2x  4x across stages\n- Output: `z_dim*2=32` channels (split into mean and log_var)\n\n**Decoder3d Configuration (WanVAE):**\n- Mirrors encoder dimensions in reverse: 384  384  192  96\n- Temporal upsampling: 4x  2x  1x (matches encoder)\n- `num_res_blocks+1` per stage (3 residual blocks vs 2 in encoder)\n\n**Wan2_2_VAE Differences:**\n- Encoder: `dim=160`, 12 input channels (after 22 patchify), `z_dim=16`\n- Decoder: `dec_dim=256`, 12 output channels (before unpatchify)\n- Uses `Down_ResidualBlock` and `Up_ResidualBlock` with average pooling shortcuts\n- Temporal downsampling: `[True, True, True]` for 8x total temporal compression\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:282-329](), [lightx2v/models/video_encoders/hf/wan/vae.py:386-436](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py:478-586](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py:589-703]()\n\n### ResidualBlock and AttentionBlock\n\n**ResidualBlock:**\n```\nRMS_norm  SiLU  CausalConv3d(333)  RMS_norm  SiLU  Dropout  CausalConv3d(333)\n+ shortcut (CausalConv3d(111) if dim changes)\n```\n\n**AttentionBlock:**\n- Operates per-frame (rearranges `(B, C, T, H, W)` to `(B*T, C, H, W)`)\n- Single-head self-attention: `to_qkv(3C)`  attention  `proj(C)`\n- Uses `F.scaled_dot_product_attention` for computation\n- Output added to identity (residual connection)\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:202-241](), [lightx2v/models/video_encoders/hf/wan/vae.py:243-279]()\n\n## Feature Caching System\n\nThe VAE implements a frame-level caching mechanism using `CACHE_T=2` to store the last 2 frames' activations at each CausalConv3d layer. This enables temporal continuity during streaming decode and handles edge cases where input chunks have fewer than 2 frames.\n\n### Cache Management\n\nThe cache system uses two key components:\n1. **feat_cache**: List of cached tensors, one per CausalConv3d layer\n2. **feat_idx**: List containing current index, incremented after each layer\n\n**Cache Lifecycle:**\n```python\n# Initialization\nself.clear_decode_cache()\n# Creates: self._feat_map = [None] * conv_count\n\n# During forward pass\ncache_x = x[:, :, -CACHE_T:, :, :].clone()  # Save last 2 frames\nif feat_cache[idx] is not None:\n    # Concatenate previous cache with current input\n    cache_x = torch.cat([feat_cache[idx][:, :, -1:], cache_x], dim=2)\nx = conv_layer(x, feat_cache[idx])  # Use cache as padding\nfeat_cache[idx] = cache_x  # Update cache\n```\n\n**Edge Case Handling:**\n- If `cache_x.shape[2] \u003c 2` and cache exists: prepends last frame from previous cache\n- If `cache_x.shape[2] \u003c 2` and cache is `\"Rep\"` (first frame): prepends zeros\n- For `Resample` layers with `upsample3d`: stores 2 frames, handles temporal interpolation\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:220-240](), [lightx2v/models/video_encoders/hf/wan/vae.py:438-491](), [lightx2v/models/video_encoders/hf/wan/vae.py:816-825]()\n\n```mermaid\nsequenceDiagram\n    participant Decode as decode() method\n    participant Cache as _feat_map cache\n    participant Conv as CausalConv3d\n    participant Layer as Current layer\n    \n    Decode-\u003e\u003eCache: clear_decode_cache()\n    Cache--\u003e\u003eDecode: _feat_map = [None] * N\n    \n    loop For each frame chunk\n        Decode-\u003e\u003eLayer: Process frame i\n        Layer-\u003e\u003eCache: Read feat_cache[idx]\n        \n        alt Cache is None (first chunk)\n            Layer-\u003e\u003eConv: conv(x, cache=None)\n        else Cache exists\n            Layer-\u003e\u003eLayer: Concatenate cache with input\n            Layer-\u003e\u003eConv: conv(x, cache=cache_x)\n        end\n        \n        Conv--\u003e\u003eLayer: Output\n        Layer-\u003e\u003eCache: Store last CACHE_T frames\n        Layer-\u003e\u003eLayer: feat_idx[0] += 1\n    end\n    \n    Decode-\u003e\u003eCache: clear_decode_cache()\n```\n\n## Tiled Encoding and Decoding\n\nFor high-resolution videos exceeding GPU memory, the VAE supports spatial tiling with overlap blending to prevent seam artifacts.\n\n### Tiling Configuration\n\n**Parameters (WanVAE):**\n- `tile_sample_min_height = 256` pixels\n- `tile_sample_min_width = 256` pixels  \n- `tile_sample_stride_height = 192` pixels\n- `tile_sample_stride_width = 192` pixels\n- Overlap: 64 pixels (256 - 192) in each dimension\n\n**Latent Space Tiles:**\n- `tile_latent_min_height = 256 / 8 = 32` latent units\n- `tile_latent_min_width = 256 / 8 = 32` latent units\n- `tile_latent_stride_height = 192 / 8 = 24` latent units\n- `tile_latent_stride_width = 192 / 8 = 24` latent units\n- `blend_height = blend_width = 8` latent units\n\n### Tiling Algorithm\n\n**Tiled Encode Process:**\n1. Split input video into overlapping tiles with stride\n2. For each tile, encode separately (clearing cache between tiles)\n3. Process temporal dimension in chunks of 4 frames (after first frame)\n4. Blend overlapping regions using linear interpolation\n5. Crop to final latent dimensions\n\n**Tiled Decode Process:**\n1. Split latent into overlapping tiles\n2. Decode each tile frame-by-frame (preserving temporal cache)\n3. Blend overlapping regions in pixel space\n4. Concatenate tiles to reconstruct full video\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:563-624](), [lightx2v/models/video_encoders/hf/wan/vae.py:626-677]()\n\n```mermaid\ngraph TB\n    subgraph \"Tiling Process\"\n        FullVideo[\"Full Video\u003cbr/\u003eHW (e.g. 10241024)\"]\n        \n        Tile1[\"Tile (0,0)\u003cbr/\u003e256256 pixels\"]\n        Tile2[\"Tile (0,192)\u003cbr/\u003e256256 pixels\u003cbr/\u003eOverlap: 64px left\"]\n        Tile3[\"Tile (192,0)\u003cbr/\u003e256256 pixels\u003cbr/\u003eOverlap: 64px top\"]\n        Tile4[\"Tile (192,192)\u003cbr/\u003e256256 pixels\u003cbr/\u003eOverlap: 64px top+left\"]\n        \n        FullVideo --\u003e Tile1\n        FullVideo --\u003e Tile2\n        FullVideo --\u003e Tile3\n        FullVideo --\u003e Tile4\n    end\n    \n    subgraph \"Per-Tile Encode\"\n        TileEncode[\"Encoder3d\u003cbr/\u003e(clear cache per tile)\"]\n        TileLatent[\"Latent 3232\"]\n    end\n    \n    subgraph \"Blending\"\n        BlendV[\"blend_v()\u003cbr/\u003eLinear blend in H\"]\n        BlendH[\"blend_h()\u003cbr/\u003eLinear blend in W\"]\n        BlendFormula[\"b[y,:] = a[-blend+y,:] * (1-y/blend)\u003cbr/\u003e+ b[y,:] * (y/blend)\"]\n    end\n    \n    subgraph \"Result\"\n        ResultLatent[\"Combined Latent\u003cbr/\u003eH/8  W/8\"]\n    end\n    \n    Tile1 --\u003e TileEncode\n    Tile2 --\u003e TileEncode\n    Tile3 --\u003e TileEncode\n    Tile4 --\u003e TileEncode\n    \n    TileEncode --\u003e TileLatent\n    TileLatent --\u003e BlendV\n    BlendV --\u003e BlendH\n    BlendH --\u003e BlendFormula\n    BlendFormula --\u003e ResultLatent\n    \n    style BlendFormula fill:#f0f0f0,stroke:#333,stroke-width:2px\n```\n\n### Blend Functions\n\n**blend_v (vertical blending):**\n```python\ndef blend_v(a, b, blend_extent):\n    blend_extent = min(a.shape[-2], b.shape[-2], blend_extent)\n    for y in range(blend_extent):\n        b[:, :, :, y, :] = a[:, :, :, -blend_extent + y, :] * (1 - y / blend_extent) \\\n                         + b[:, :, :, y, :] * (y / blend_extent)\n    return b\n```\n\n**blend_h (horizontal blending):**\n- Analogous to `blend_v`, operates on width dimension (`shape[-1]`)\n- Linear interpolation from tile boundary: weight transitions from 1.0 to 0.0\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:551-561]()\n\n## Distributed Processing and 2D Grid\n\nThe VAE supports distributed encoding/decoding across multiple GPUs by splitting the latent space into a 2D grid. This is primarily used during encoding when world_size \u003e 1.\n\n### Grid Calculation\n\nThe system maintains a `grid_table` mapping `(latent_height, latent_width, world_size)` to `(grid_h, grid_w)`:\n\n**Example Grid Table Entries:**\n```python\n(60, 104, 2): (1, 2)    # Split horizontally for 2 GPUs\n(60, 104, 4): (2, 2)    # 22 grid for 4 GPUs\n(60, 104, 8): (2, 4)    # 24 grid for 8 GPUs\n(120, 120, 4): (2, 2)   # Square latent  square grid\n```\n\n**Grid Selection Algorithm (`_calculate_2d_grid`):**\n1. Check if `(latent_h, latent_w, world_size)` exists in cache\n2. If not, iterate through all possible `(h, w)` where `h*w = world_size`\n3. Filter by divisibility: `latent_h % h == 0 and latent_w % w == 0`\n4. Select grid minimizing `aspect_diff = abs(latent_h/h - latent_w/w)` (closest to square chunks)\n5. Cache result for future use\n\n### encode_dist_2d Method\n\n**Distributed 2D Encoding Flow:**\n1. Calculate grid dimensions using `_calculate_2d_grid(latent_h, latent_w, world_size)`\n2. Compute chunk sizes: `h_per_chunk = latent_h // grid_h`, `w_per_chunk = latent_w // grid_w`\n3. Each rank processes its assigned chunk `[rank_h*h_per_chunk:(rank_h+1)*h_per_chunk, ...]`\n4. Return local encoded latent with boundary information\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:995-1015](), [lightx2v/models/video_encoders/hf/wan/vae.py:1071-1112]()\n\n```mermaid\ngraph TB\n    subgraph \"Input Video (Rank-specific slice)\"\n        VideoSlice[\"Video chunk\u003cbr/\u003e(B, 3, T, H_chunk, W_chunk)\"]\n    end\n    \n    subgraph \"Grid Calculation\"\n        GridTable[\"grid_table lookup\u003cbr/\u003e(H, W, world_size)  (grid_h, grid_w)\"]\n        FallbackCalc[\"_calculate_2d_grid()\u003cbr/\u003eMinimize aspect_diff\"]\n    end\n    \n    subgraph \"Rank Assignment\"\n        RankCalc[\"rank_h = rank // grid_w\u003cbr/\u003erank_w = rank % grid_w\"]\n        ChunkCalc[\"h_start = rank_h * h_per_chunk\u003cbr/\u003ew_start = rank_w * w_per_chunk\"]\n    end\n    \n    subgraph \"Distributed Encoding\"\n        Rank0[\"Rank 0\u003cbr/\u003eEncodes [0:h_chunk, 0:w_chunk]\"]\n        Rank1[\"Rank 1\u003cbr/\u003eEncodes [0:h_chunk, w_chunk:2*w_chunk]\"]\n        RankN[\"Rank N\u003cbr/\u003eEncodes corresponding chunk\"]\n    end\n    \n    subgraph \"Output Latents\"\n        LatentChunk[\"Local latent chunk\u003cbr/\u003e(B, 16, T', H_chunk/8, W_chunk/8)\"]\n    end\n    \n    VideoSlice --\u003e GridTable\n    GridTable --\u003e RankCalc\n    GridTable -.cache miss.-\u003e FallbackCalc\n    FallbackCalc --\u003e RankCalc\n    \n    RankCalc --\u003e ChunkCalc\n    ChunkCalc --\u003e Rank0\n    ChunkCalc --\u003e Rank1\n    ChunkCalc --\u003e RankN\n    \n    Rank0 --\u003e LatentChunk\n    Rank1 --\u003e LatentChunk\n    RankN --\u003e LatentChunk\n    \n    style GridTable fill:#f0f0f0,stroke:#333,stroke-width:2px\n```\n\n## Streaming Decode\n\nThe VAE supports progressive video decoding via `decode_stream()`, which yields decoded frames one at a time as a generator. This enables memory-efficient processing and progressive rendering.\n\n**Streaming Decode Implementation:**\n```python\ndef decode_stream(self, z, scale):\n    self.clear_decode_cache()\n    # Denormalize latents\n    if isinstance(scale[0], torch.Tensor):\n        z = z / scale[1].view(1, self.z_dim, 1, 1, 1) + scale[0].view(1, self.z_dim, 1, 1, 1)\n    else:\n        z = z / scale[1] + scale[0]\n    \n    iter_ = z.shape[2]  # Number of frames\n    x = self.conv2(z)\n    \n    for i in range(iter_):\n        self._conv_idx = [0]\n        out = self.decoder(\n            x[:, :, i : i + 1, :, :],\n            feat_cache=self._feat_map,\n            feat_idx=self._conv_idx,\n        )\n        yield out  # Generator yields one frame at a time\n```\n\n**Key Properties:**\n- Maintains temporal cache across frames for causality\n- Memory footprint: only one frame in GPU memory at a time (plus cache)\n- Used in real-time generation scenarios\n\n**Alternative Caching Methods:**\n- `cached_decode()`: Standard caching, returns all frames concatenated\n- `cached_decode_withflag()`: Supports `is_first_clip`/`is_last_clip` flags for chunk-based processing, enabling cache persistence across multiple decode calls\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:740-757](), [lightx2v/models/video_encoders/hf/wan/vae.py:759-798]()\n\n## Latent Space Normalization\n\nThe VAE uses per-channel normalization with precomputed statistics to standardize the latent distribution. This improves training stability and generation quality.\n\n### WanVAE Normalization (16 channels)\n\n**Mean and Std (dtype=torch.float16):**\n```python\nmean = [-0.7571, -0.7089, -0.9113, 0.1075, -0.1745, 0.9653, -0.1517, 1.5508,\n        0.4134, -0.0715, 0.5517, -0.3632, -0.1922, -0.9497, 0.2503, -0.2921]\nstd = [2.8184, 1.4541, 2.3275, 2.6558, 1.2196, 1.7708, 2.6052, 2.0743,\n       3.2687, 2.1526, 2.8652, 1.5579, 1.6382, 1.1253, 2.8251, 1.9160]\n```\n\n**Normalization (encode):**\n```python\nscale = [mean_tensor, 1.0 / std_tensor]\nmu = (mu - scale[0].view(1, z_dim, 1, 1, 1)) * scale[1].view(1, z_dim, 1, 1, 1)\n```\n\n**Denormalization (decode):**\n```python\nz = z / scale[1].view(1, z_dim, 1, 1, 1) + scale[0].view(1, z_dim, 1, 1, 1)\n```\n\n### Wan2_2_VAE Normalization (48 channels)\n\nUses 48 mean/std values following the same normalization pattern. Values are stored as tensors on device for efficient computation.\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:909-947](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py:905-946]()\n\n## Memory Optimization Features\n\n### Channels Last 3D Format\n\nWhen `USE_CHANNELS_LAST_3D=1` environment variable is set, all Conv3d weights are converted to `torch.channels_last_3d` memory format during initialization. This eliminates NCHWNHWC format conversion overhead in cuDNN, providing ~9% speedup.\n\n**Conversion Process:**\n```python\ndef convert_to_channels_last_3d(module):\n    for child in module.children():\n        if isinstance(child, nn.Conv3d):\n            child.weight.data = child.weight.data.to(memory_format=torch.channels_last_3d)\n        else:\n            convert_to_channels_last_3d(child)\n```\n\nApplied during VAE initialization: [lightx2v/models/video_encoders/hf/wan/vae.py:875-877]()\n\n### Pruning Support (LightVAE)\n\nWanVAE supports channel pruning via the `pruning_rate` parameter:\n- `pruning_rate=0.0`: Full model (default)\n- `pruning_rate=0.75`: LightVAE variant with 75% channels pruned\n- Dimensions computed as: `dims = [int(d * (1 - pruning_rate)) for d in base_dims]`\n\nThis reduces memory footprint and computation while maintaining acceptable quality for certain use cases.\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:283-294](), [lightx2v/models/video_encoders/hf/wan/vae.py:904-907]()\n\n### CPU Offloading\n\nThe VAE supports CPU offloading when initialized with `cpu_offload=True`. Weights are loaded with `load_weights(..., cpu_offload=True)` which keeps weights on CPU and transfers to GPU only during forward passes.\n\nFor Wan2_2_VAE, the decoder supports `offload_cache=True` which offloads feature cache to CPU after each layer to minimize GPU memory:\n```python\nif offload_cache:\n    for _idx in range(idx, feat_idx[0]):\n        if isinstance(feat_cache[_idx], torch.Tensor):\n            feat_cache[_idx] = feat_cache[_idx].cpu()\n```\n\nThis enables running large VAE models on consumer GPUs with limited VRAM.\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae_2_2.py:664-670](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py:793-810]()\n\n## WanVAE vs Wan2_2_VAE Comparison\n\n| Feature | WanVAE | Wan2_2_VAE |\n|---------|--------|------------|\n| **Latent Channels** | 16 | 48 |\n| **Encoder Input** | 3 channels (RGB) | 12 channels (after 22 patchify) |\n| **Decoder Output** | 3 channels (RGB) | 12 channels (before unpatchify) |\n| **Encoder Dim** | 96 | 160 |\n| **Decoder Dim** | 96 | 256 (asymmetric) |\n| **Temporal Downsample** | `[False, True, True]` (configurable) | `[True, True, True]` (8x total) |\n| **Residual Structure** | Standard ResidualBlock | Down_ResidualBlock / Up_ResidualBlock with AvgDown3D / DupUp3D shortcuts |\n| **Patchify/Unpatchify** | None | 22 spatial patches |\n| **Upsampling Blocks** | `num_res_blocks + 1` (3 per stage) | `num_res_blocks + 1` (3 per stage) |\n\n**Key Architectural Difference:**\n\nWan2_2_VAE uses `AvgDown3D` and `DupUp3D` for shortcut connections in residual blocks, which average or duplicate features during downsampling/upsampling. This provides better gradient flow and preserves information better than identity shortcuts.\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:502-543](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py:714-755]()\n\n## Integration with Runner System\n\nThe VAE is instantiated by model runners during initialization and used in the post-inference stage to decode latents to videos.\n\n**Initialization in WanRunner:**\n```python\n# lightx2v/runner/wan_runner.py (typical usage)\nfrom lightx2v.models.video_encoders.hf.wan.vae import WanVAE\n\nself.vae = WanVAE(\n    z_dim=16,\n    vae_path=config['vae_path'],\n    dtype=dtype,\n    device=device,\n    parallel=False,\n    use_tiling=True,  # Enable tiled decode for large videos\n    cpu_offload=config.get('cpu_offload_vae', False),\n    use_2d_split=True,  # Enable 2D grid for distributed\n    use_cache_vae=True,  # Enable feature caching\n)\n```\n\n**Decode Call:**\n```python\n# After diffusion loop completes\nsamples = self.vae.decode(latents, scale=self.vae.scale)\n# Or for streaming:\n# for frame in self.vae.decode_stream(latents, scale=self.vae.scale):\n#     process_frame(frame)\n```\n\nThe decoded output is then converted from `[-1, 1]` range to `[0, 1]` and saved as video frames.\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:882-993]()\n\n## Performance Characteristics\n\n| Operation | Configuration | Approximate Time |\n|-----------|--------------|------------------|\n| **Encode 1024102449f** | Full GPU, no tiling | ~2.0s on H100 |\n| **Encode 1024102449f** | Tiled (256256) | ~3.5s on H100 |\n| **Decode 12812849f** | Streaming, cached | ~0.8s on H100 |\n| **Decode 12812849f** | Tiled (3232), 2D split (4 GPUs) | ~0.3s per rank |\n| **Encode with LightVAE** | `pruning_rate=0.75` | ~1.5s (25% faster) |\n\n**Memory Footprint:**\n- WanVAE full model: ~850MB parameters\n- Wan2_2_VAE full model: ~1.2GB parameters  \n- LightVAE (pruning_rate=0.75): ~220MB parameters\n- Feature cache per frame (CACHE_T=2): ~50MB for 10241024 resolution\n\n**Optimization Impact:**\n- Channels Last 3D: ~9% speedup on NVIDIA GPUs\n- Tiling overhead: ~40-75% slower due to redundant computation in overlaps\n- 2D distributed: Near-linear scaling up to 8 GPUs\n- Streaming decode: Enables unlimited length videos with constant memory\n\nSources: Based on architectural analysis and typical performance patterns for 3D ConvNets"])</script><script>self.__next_f.push([1,"2d:T6574,"])</script><script>self.__next_f.push([1,"# Model Runners and Tasks\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [README_zh.md](README_zh.md)\n- [lightx2v_platform/README.md](lightx2v_platform/README.md)\n- [lightx2v_platform/README_zh.md](lightx2v_platform/README_zh.md)\n- [scripts/hunyuan_video_15/README.md](scripts/hunyuan_video_15/README.md)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis page provides comprehensive documentation of the **runner system** in LightX2V, which serves as the task-specific orchestration layer between the high-level pipeline API and the underlying model architectures. Each runner encapsulates the logic for a specific model family and generation task (e.g., text-to-video, audio-to-video, text-to-image).\n\n**Related pages:**\n- For the core pipeline and configuration system, see [Core Architecture](#4)\n- For runner implementation details like base classes and inheritance patterns, see [Runner System and Registry Pattern](#4.1)\n- For specific optimization features used by runners, see [Performance Optimization](#6)\n- For the three-stage inference pipeline all runners follow, see [Three-Stage Inference Pipeline](#4.2)\n\n---\n\n## Runner Registry and Discovery Pattern\n\nThe LightX2V framework uses a registry pattern (`RUNNER_REGISTER`) to dynamically map model classes and tasks to their corresponding runner implementations. This enables flexible model selection at runtime without hardcoded dependencies.\n\n```mermaid\ngraph TB\n    UserInput[\"User Input\u003cbr/\u003e(model_cls, task)\"]\n    Registry[\"RUNNER_REGISTER\u003cbr/\u003eRegistry Pattern\"]\n    \n    subgraph \"Registry Lookup\"\n        Key[\"lookup_key =\u003cbr/\u003ef'{model_cls}_{task}'\"]\n        Match[\"Match to Runner Class\"]\n    end\n    \n    subgraph \"Runner Classes\"\n        WanAudioRunner[\"WanAudioRunner\u003cbr/\u003ewan2.1_s2v, wan2.2_moe_s2v\"]\n        WanRunner[\"WanRunner\u003cbr/\u003ewan2.1_t2v, wan2.1_i2v\"]\n        WanDistillRunner[\"WanDistillRunner\u003cbr/\u003ewan2.1_distill_t2v\"]\n        QwenImageRunner[\"QwenImageRunner\u003cbr/\u003eqwen-image_t2i\"]\n        ZImageRunner[\"ZImageRunner\u003cbr/\u003ez-image-turbo_t2i\"]\n        HunyuanRunner[\"HunyuanVideo15Runner\u003cbr/\u003ehunyuan_video_1.5_t2v\"]\n        LTX2Runner[\"LTX2Runner\u003cbr/\u003eltx2_t2av\"]\n    end\n    \n    UserInput --\u003e Registry\n    Registry --\u003e Key\n    Key --\u003e Match\n    \n    Match --\u003e WanAudioRunner\n    Match --\u003e WanRunner\n    Match --\u003e WanDistillRunner\n    Match --\u003e QwenImageRunner\n    Match --\u003e ZImageRunner\n    Match --\u003e HunyuanRunner\n    Match --\u003e LTX2Runner\n```\n\n**Registry mechanism:**\n- Each runner class registers itself with `RUNNER_REGISTER` using a unique key\n- The key format is typically `\"{model_cls}_{task}\"` (e.g., `\"wan2.2_moe_s2v\"`)\n- The `LightX2VPipeline` queries the registry with user-provided `model_cls` and `task` parameters\n- The registry returns the appropriate runner class constructor\n\n**Sources:** README.md:19-20, README.md:139-197\n\n---\n\n## Task Type Overview\n\nLightX2V supports seven primary generation task types, each handled by specialized runners:\n\n| Task Code | Full Name | Input Modalities | Output Modality | Example Runners |\n|-----------|-----------|------------------|-----------------|-----------------|\n| `t2v` | Text-to-Video | Text prompt | Video | WanRunner, HunyuanVideo15Runner |\n| `i2v` | Image-to-Video | Text prompt + Image | Video | WanRunner, HunyuanVideo15Runner |\n| `s2v` | Speech/Audio-to-Video | Text prompt + Audio + (optional) Image | Video | WanAudioRunner, Wan22AudioRunner |\n| `t2i` | Text-to-Image | Text prompt | Image | QwenImageRunner, ZImageRunner |\n| `i2i` | Image-to-Image (Edit) | Text prompt + Image | Image | QwenImageRunner, ZImageRunner |\n| `t2av` | Text-to-Audio+Video | Text prompt | Audio + Video | LTX2Runner |\n| `i2av` | Image-to-Audio+Video | Text prompt + Image | Audio + Video | LTX2Runner |\n\n```mermaid\ngraph LR\n    subgraph \"Input Types\"\n        Text[\"Text Prompt\u003cbr/\u003e(required for all)\"]\n        Image[\"Condition Image\u003cbr/\u003e(I2V, I2I, I2AV)\"]\n        Audio[\"Audio File\u003cbr/\u003e(S2V only)\"]\n    end\n    \n    subgraph \"Task Types\"\n        T2V[\"T2V\u003cbr/\u003eText  Video\"]\n        I2V[\"I2V\u003cbr/\u003eImage+Text  Video\"]\n        S2V[\"S2V\u003cbr/\u003eAudio+Text  Video\u003cbr/\u003e(highest importance)\"]\n        T2I[\"T2I\u003cbr/\u003eText  Image\"]\n        I2I[\"I2I\u003cbr/\u003eImage+Text  Image\"]\n        T2AV[\"T2AV\u003cbr/\u003eText  Audio+Video\"]\n        I2AV[\"I2AV\u003cbr/\u003eImage+Text  Audio+Video\"]\n    end\n    \n    subgraph \"Output Types\"\n        VideoOut[\"Video Output\u003cbr/\u003e(MP4, frames)\"]\n        ImageOut[\"Image Output\u003cbr/\u003e(PNG, JPEG)\"]\n        AudioVideoOut[\"Audio+Video Output\u003cbr/\u003e(combined)\"]\n    end\n    \n    Text --\u003e T2V\n    Text --\u003e T2I\n    Text --\u003e T2AV\n    \n    Text --\u003e I2V\n    Image --\u003e I2V\n    \n    Text --\u003e S2V\n    Audio --\u003e S2V\n    Image -.optional.-\u003e S2V\n    \n    Text --\u003e I2I\n    Image --\u003e I2I\n    \n    Text --\u003e I2AV\n    Image --\u003e I2AV\n    \n    T2V --\u003e VideoOut\n    I2V --\u003e VideoOut\n    S2V --\u003e VideoOut\n    T2I --\u003e ImageOut\n    I2I --\u003e ImageOut\n    T2AV --\u003e AudioVideoOut\n    I2AV --\u003e AudioVideoOut\n```\n\n**Task characteristics:**\n- **S2V (Speech-to-Video)** is the highest importance task with score 222.45, unique to WAN models\n- **T2V/I2V** are the most widely supported tasks across multiple model families\n- **T2AV/I2AV** are unique to LTX2 model, generating synchronized audio and video\n\n**Sources:** README.md:19, README_zh.md:19\n\n---\n\n## Runner Lifecycle and Methods\n\nAll runners follow a standardized three-phase lifecycle that aligns with the inference pipeline architecture:\n\n```mermaid\ngraph TB\n    subgraph \"1. Initialization Phase\"\n        InitModules[\"init_modules()\u003cbr/\u003eLoad model components\"]\n        LoadTransformer[\"Load Transformer\u003cbr/\u003e(DiT/MMDiT)\"]\n        LoadEncoders[\"Load Encoders\u003cbr/\u003e(Text, Image, Audio)\"]\n        LoadVAE[\"Load VAE\u003cbr/\u003e(Encoder/Decoder)\"]\n        LoadScheduler[\"Load Scheduler\u003cbr/\u003e(Euler, Distill)\"]\n        ApplyOpt[\"Apply Optimizations\u003cbr/\u003e(Quant, Offload, LoRA)\"]\n    end\n    \n    subgraph \"2. Pipeline Execution Phase\"\n        RunPipeline[\"run_pipeline()\u003cbr/\u003eMain generation loop\"]\n        RunInputEncoder[\"run_input_encoder()\u003cbr/\u003eEncode inputs\"]\n        RunDiT[\"run_dit()\u003cbr/\u003eDiffusion loop\"]\n        RunVAEDecoder[\"run_vae_decoder()\u003cbr/\u003eDecode latents\"]\n    end\n    \n    subgraph \"3. Sub-methods\"\n        PreInfer[\"pre_infer()\u003cbr/\u003ePrepare latents\"]\n        TransformerInfer[\"transformer_infer()\u003cbr/\u003ePredict noise\"]\n        PostInfer[\"post_infer()\u003cbr/\u003eUpdate latents\"]\n    end\n    \n    InitModules --\u003e LoadTransformer\n    InitModules --\u003e LoadEncoders\n    InitModules --\u003e LoadVAE\n    InitModules --\u003e LoadScheduler\n    LoadScheduler --\u003e ApplyOpt\n    \n    ApplyOpt --\u003e RunPipeline\n    RunPipeline --\u003e RunInputEncoder\n    RunInputEncoder --\u003e RunDiT\n    RunDiT --\u003e RunVAEDecoder\n    \n    RunDiT --\u003e PreInfer\n    PreInfer --\u003e TransformerInfer\n    TransformerInfer --\u003e PostInfer\n    PostInfer -.loop.-\u003e PreInfer\n```\n\n**Key methods in runner classes:**\n\n| Method | Phase | Purpose |\n|--------|-------|---------|\n| `init_modules()` | Initialization | Load all model components (transformer, encoders, VAE, scheduler) and apply optimizations |\n| `run_pipeline()` | Execution | Orchestrate the complete generation pipeline from input to output |\n| `run_input_encoder()` | Pre-inference | Encode text/image/audio inputs into embeddings |\n| `run_dit()` | Inference | Execute the diffusion denoising loop |\n| `run_vae_decoder()` | Post-inference | Decode latent representations to pixel space |\n| `pre_infer()` | Per-step | Prepare inputs for transformer (scheduler pre-processing) |\n| `transformer_infer()` | Per-step | Forward pass through transformer to predict noise |\n| `post_infer()` | Per-step | Update latents using scheduler (scheduler post-processing) |\n\n**Sources:** README.md:139-197\n\n---\n\n## Runner Hierarchy and Inheritance\n\nThe runner system uses a multi-level inheritance hierarchy to maximize code reuse while enabling task-specific customization:\n\n```mermaid\ngraph TB\n    subgraph \"Abstract Base\"\n        BaseRunner[\"BaseRunner\u003cbr/\u003eAbstract interface\u003cbr/\u003edefines required methods\"]\n    end\n    \n    subgraph \"Common Implementation\"\n        DefaultRunner[\"DefaultRunner\u003cbr/\u003eCommon pipeline logic\u003cbr/\u003einput encoding, VAE decode\"]\n    end\n    \n    subgraph \"Model Family Base Classes\"\n        WanRunner[\"WanRunner\u003cbr/\u003eWAN-specific features\u003cbr/\u003esample_shift, rope, etc\"]\n        QwenBase[\"QwenImageRunner base\u003cbr/\u003eQwen-specific features\"]\n    end\n    \n    subgraph \"Task Specialized - WAN Family\"\n        WanAudioRunner[\"WanAudioRunner\u003cbr/\u003eAudio-driven S2V\u003cbr/\u003eaudio segmentation\"]\n        Wan22AudioRunner[\"Wan22AudioRunner\u003cbr/\u003eWAN 2.2 S2V\u003cbr/\u003edual-model MoE\"]\n        WanDistillRunner[\"WanDistillRunner\u003cbr/\u003e4-step inference\u003cbr/\u003eno-CFG\"]\n        Wan22MoeDistillRunner[\"Wan22MoeDistillRunner\u003cbr/\u003eMoE distillation\u003cbr/\u003ehigh/low noise models\"]\n        WanCausVidRunner[\"WanCausVidRunner\u003cbr/\u003eAutoregressive video\u003cbr/\u003eKV cache\"]\n    end\n    \n    subgraph \"Task Specialized - Other Families\"\n        QwenImageRunner[\"QwenImageRunner\u003cbr/\u003eT2I/I2I\u003cbr/\u003elayered generation\"]\n        ZImageRunner[\"ZImageRunner\u003cbr/\u003eT2I/I2I\u003cbr/\u003eTurbo inference\"]\n        HunyuanVideo15Runner[\"HunyuanVideo15Runner\u003cbr/\u003eT2V/I2V\u003cbr/\u003e720p support\"]\n        LTX2Runner[\"LTX2Runner\u003cbr/\u003eT2AV/I2AV\u003cbr/\u003emulti-stage pipeline\"]\n        LongCatImageRunner[\"LongCatImageRunner\u003cbr/\u003eT2I/I2I\u003cbr/\u003ehigh-res support\"]\n    end\n    \n    BaseRunner --\u003e DefaultRunner\n    \n    DefaultRunner --\u003e WanRunner\n    DefaultRunner --\u003e QwenBase\n    DefaultRunner --\u003e ZImageRunner\n    DefaultRunner --\u003e HunyuanVideo15Runner\n    DefaultRunner --\u003e LTX2Runner\n    DefaultRunner --\u003e LongCatImageRunner\n    \n    WanRunner --\u003e WanAudioRunner\n    WanRunner --\u003e WanDistillRunner\n    WanRunner --\u003e WanCausVidRunner\n    \n    WanAudioRunner --\u003e Wan22AudioRunner\n    \n    WanDistillRunner --\u003e Wan22MoeDistillRunner\n    \n    QwenBase --\u003e QwenImageRunner\n```\n\n**Inheritance layers:**\n1. **BaseRunner**: Defines abstract interface with required methods like `init_modules()`, `run_pipeline()`\n2. **DefaultRunner**: Implements common pipeline logic shared across all model families (input encoding, VAE decoding, basic scheduling)\n3. **Model Family Base**: Adds model-family-specific features (e.g., WanRunner adds WAN-specific timestep calculation, RoPE embeddings)\n4. **Task Specialized**: Implements specific generation tasks (e.g., WanAudioRunner adds audio processing and segmentation)\n\n**Specialization pattern:**\n- Runners inherit only what they need, overriding specific methods for customization\n- Most complex specialization is in WAN audio/video domain due to unique S2V capabilities\n- Distillation runners form a separate branch with accelerated inference logic\n\n**Sources:** README.md:139-197\n\n---\n\n## Model Family to Runner Mapping\n\nThe following diagram and table show the complete mapping of model families, their variants, and corresponding runner classes:\n\n```mermaid\ngraph TB\n    subgraph \"WAN Model Family\"\n        WAN21[\"wan2.1\u003cbr/\u003e14B parameters\u003cbr/\u003e480p/720p\"]\n        WAN22[\"wan2.2_moe\u003cbr/\u003e14B MoE\u003cbr/\u003edual-model architecture\"]\n        WANCausVid[\"wan2.1_causvid\u003cbr/\u003eAutoregressive\u003cbr/\u003eKV cache\"]\n    end\n    \n    subgraph \"WAN Runners\"\n        WanRunner_R[\"WanRunner\u003cbr/\u003eT2V, I2V\"]\n        WanAudioRunner_R[\"WanAudioRunner\u003cbr/\u003eS2V (base)\"]\n        Wan22AudioRunner_R[\"Wan22AudioRunner\u003cbr/\u003eS2V (2.2 MoE)\"]\n        WanDistillRunner_R[\"WanDistillRunner\u003cbr/\u003e4-step distill\"]\n        Wan22MoeDistillRunner_R[\"Wan22MoeDistillRunner\u003cbr/\u003e4-step MoE distill\"]\n        WanCausVidRunner_R[\"WanCausVidRunner\u003cbr/\u003eAutoregressive\"]\n    end\n    \n    subgraph \"Qwen Model Family\"\n        QwenImage[\"qwen-image\u003cbr/\u003eT2I generation\"]\n        QwenImageEdit[\"qwen-image-edit\u003cbr/\u003eImage editing\"]\n        QwenImage2512[\"qwen-image-2512\u003cbr/\u003eLatest T2I\"]\n    end\n    \n    subgraph \"Qwen Runners\"\n        QwenImageRunner_R[\"QwenImageRunner\u003cbr/\u003eT2I, I2I\"]\n    end\n    \n    subgraph \"Other Model Families\"\n        ZImage[\"z-image-turbo\u003cbr/\u003eFast T2I\"]\n        HunyuanVideo[\"hunyuan_video_1.5\u003cbr/\u003eT2V/I2V 720p\"]\n        LTX2[\"ltx2\u003cbr/\u003eAudio+Video generation\"]\n        LongCat[\"longcat_image\u003cbr/\u003eHigh-res T2I\"]\n    end\n    \n    subgraph \"Other Runners\"\n        ZImageRunner_R[\"ZImageRunner\u003cbr/\u003eT2I, I2I\"]\n        HunyuanVideo15Runner_R[\"HunyuanVideo15Runner\u003cbr/\u003eT2V, I2V\"]\n        LTX2Runner_R[\"LTX2Runner\u003cbr/\u003eT2AV, I2AV\"]\n        LongCatImageRunner_R[\"LongCatImageRunner\u003cbr/\u003eT2I, I2I\"]\n    end\n    \n    WAN21 --\u003e WanRunner_R\n    WAN21 --\u003e WanAudioRunner_R\n    WAN21 --\u003e WanDistillRunner_R\n    WAN21 --\u003e WanCausVidRunner_R\n    \n    WAN22 --\u003e Wan22AudioRunner_R\n    WAN22 --\u003e Wan22MoeDistillRunner_R\n    \n    WANCausVid --\u003e WanCausVidRunner_R\n    \n    QwenImage --\u003e QwenImageRunner_R\n    QwenImageEdit --\u003e QwenImageRunner_R\n    QwenImage2512 --\u003e QwenImageRunner_R\n    \n    ZImage --\u003e ZImageRunner_R\n    HunyuanVideo --\u003e HunyuanVideo15Runner_R\n    LTX2 --\u003e LTX2Runner_R\n    LongCat --\u003e LongCatImageRunner_R\n```\n\n**Complete runner mapping table:**\n\n| Runner Class | Model Classes | Supported Tasks | Key Features |\n|--------------|---------------|-----------------|--------------|\n| `WanRunner` | `wan2.1` | `t2v`, `i2v` | Base WAN inference, sample_shift, RoPE |\n| `WanAudioRunner` | `wan2.1` | `s2v` | Audio segmentation, temporal consistency, multi-person |\n| `Wan22AudioRunner` | `wan2.2_moe` | `s2v` | Dual-model MoE, high/low noise separation |\n| `WanDistillRunner` | `wan2.1_distill` | `t2v`, `i2v` | 4-step inference, no CFG, ~25x speedup |\n| `Wan22MoeDistillRunner` | `wan2.2_moe_distill` | `t2v`, `i2v` | MoE 4-step inference, boundary_step_index |\n| `WanCausVidRunner` | `wan2.1_causvid` | `t2v` | Autoregressive generation, KV cache |\n| `QwenImageRunner` | `qwen-image`, `qwen-image-edit` | `t2i`, `i2i` | Qwen VL model, layered generation |\n| `ZImageRunner` | `z-image-turbo` | `t2i`, `i2i` | Fast inference, Qwen3 encoder |\n| `HunyuanVideo15Runner` | `hunyuan_video_1.5` | `t2v`, `i2v` | 720p support, TeaCache/MagCache |\n| `LTX2Runner` | `ltx2` | `t2av`, `i2av` | Audio+video synthesis, multi-stage |\n| `LongCatImageRunner` | `longcat_image` | `t2i`, `i2i` | High-resolution image generation |\n\n**Sources:** README.md:207-234, scripts/hunyuan_video_15/README.md:1-104\n\n---\n\n## WAN Model Family Architecture\n\nThe WAN (Wan-AI) model family represents the most complex and feature-rich runner implementations in LightX2V, particularly for audio-driven video generation:\n\n```mermaid\ngraph TB\n    subgraph \"WAN 2.1 Architecture\"\n        WAN21Base[\"Base Model\u003cbr/\u003e14B parameters\u003cbr/\u003eDiT architecture\"]\n        WAN21T2V[\"T2V/I2V\u003cbr/\u003e40-50 steps\"]\n        WAN21S2V[\"S2V\u003cbr/\u003eAudio conditioning\"]\n        WAN21Distill[\"4-step Distillation\u003cbr/\u003eWanStepDistillScheduler\"]\n        WAN21CausVid[\"CausVid\u003cbr/\u003eAutoregressive KV cache\"]\n    end\n    \n    subgraph \"WAN 2.2 MoE Architecture\"\n        WAN22Base[\"MoE Model\u003cbr/\u003e14B parameters\u003cbr/\u003eDual-model system\"]\n        WAN22High[\"High-noise Model\u003cbr/\u003esteps 0 to boundary\"]\n        WAN22Low[\"Low-noise Model\u003cbr/\u003esteps boundary to N\"]\n        WAN22S2V[\"S2V with MoE\u003cbr/\u003eWan22AudioRunner\"]\n        WAN22Distill[\"4-step MoE Distill\u003cbr/\u003eWan22MoeDistillRunner\"]\n    end\n    \n    subgraph \"Key Features\"\n        SampleShift[\"sample_shift\u003cbr/\u003eTimestep calculation\"]\n        RoPE[\"RoPE embeddings\u003cbr/\u003ePositional encoding\"]\n        AudioAdapter[\"AudioAdapter\u003cbr/\u003eAudio cross-attention\"]\n        PrevLatents[\"prev_latents\u003cbr/\u003eTemporal consistency\"]\n        MultiPerson[\"Multi-person support\u003cbr/\u003eSpatial masks\"]\n    end\n    \n    WAN21Base --\u003e WAN21T2V\n    WAN21Base --\u003e WAN21S2V\n    WAN21Base --\u003e WAN21Distill\n    WAN21Base --\u003e WAN21CausVid\n    \n    WAN22Base --\u003e WAN22High\n    WAN22Base --\u003e WAN22Low\n    WAN22High --\u003e WAN22S2V\n    WAN22Low --\u003e WAN22S2V\n    WAN22Base --\u003e WAN22Distill\n    \n    WAN21T2V -.uses.-\u003e SampleShift\n    WAN21T2V -.uses.-\u003e RoPE\n    WAN21S2V -.uses.-\u003e AudioAdapter\n    WAN21S2V -.uses.-\u003e PrevLatents\n    WAN21S2V -.uses.-\u003e MultiPerson\n    WAN21CausVid -.uses.-\u003e RoPE\n```\n\n**WAN architecture variants:**\n\n1. **wan2.1**: Single 14B parameter DiT-based model\n   - Supports T2V, I2V, S2V tasks\n   - Standard 40-50 step inference with CFG\n   - `sample_shift` parameter controls timestep distribution\n\n2. **wan2.2_moe**: Dual-model Mixture-of-Experts architecture\n   - Two separate models: high-noise and low-noise experts\n   - `boundary_step_index` determines transition point between models\n   - Enhanced quality through specialized noise handling\n   - Uses `Wan22AudioRunner` for S2V tasks\n\n3. **wan2.1_distill**: 4-step distilled variants\n   - Compressed from 40-50 steps to just 4 steps\n   - No CFG required (guidance_scale=1.0)\n   - ~25x speedup compared to standard inference\n   - `WanStepDistillScheduler` for accelerated sampling\n\n4. **wan2.1_causvid**: Autoregressive video generation\n   - KV cache for autoregressive attention\n   - `kv_start` and `kv_end` parameters control cache range\n   - Enables long-form video generation\n\n**Sources:** README.md:139-197, README.md:212\n\n---\n\n## Qwen and Z-Image Model Families\n\nThe Qwen and Z-Image model families focus on high-quality image generation and editing:\n\n```mermaid\ngraph TB\n    subgraph \"Qwen-Image Models\"\n        QwenBase[\"Qwen2.5_VLForConditionalGeneration\u003cbr/\u003eVision-language base\"]\n        QwenT2I[\"Qwen-Image\u003cbr/\u003eText-to-Image\"]\n        QwenEdit[\"Qwen-Image-Edit\u003cbr/\u003eImage editing\"]\n        Qwen2512[\"Qwen-Image-2512\u003cbr/\u003eLatest generation\"]\n    end\n    \n    subgraph \"Z-Image Models\"\n        ZBase[\"Z-Image-Turbo\u003cbr/\u003eFast inference\"]\n        Qwen3Encoder[\"Qwen3 Text Encoder\u003cbr/\u003eEnhanced text understanding\"]\n    end\n    \n    subgraph \"Shared Features\"\n        LayeredGen[\"Layered Generation\u003cbr/\u003eProgressive refinement\"]\n        CFGDistill[\"CFG Distillation\u003cbr/\u003e8-step accelerated\"]\n        FP8Support[\"FP8 Quantization\u003cbr/\u003eMemory efficient\"]\n    end\n    \n    QwenBase --\u003e QwenT2I\n    QwenBase --\u003e QwenEdit\n    QwenBase --\u003e Qwen2512\n    \n    ZBase --\u003e Qwen3Encoder\n    \n    QwenT2I -.uses.-\u003e LayeredGen\n    QwenEdit -.uses.-\u003e LayeredGen\n    QwenT2I -.supports.-\u003e CFGDistill\n    QwenEdit -.supports.-\u003e CFGDistill\n    QwenEdit -.supports.-\u003e FP8Support\n```\n\n**Qwen-Image characteristics:**\n- Based on `Qwen2.5_VLForConditionalGeneration` vision-language model\n- Supports both T2I (text-to-image) and I2I (image-to-image editing)\n- Uses layered generation approach for progressive refinement\n- `QwenImageRunner` handles all Qwen-Image variants\n- CFG/step-distilled LoRAs available for 8-step inference\n\n**Z-Image characteristics:**\n- Designed for fast T2I/I2I inference (\"Turbo\" variants)\n- Uses Qwen3 text encoder for enhanced text understanding\n- `ZImageRunner` implements optimizations for speed\n- Supports distillation for accelerated generation\n\n**Sources:** README.md:213-216, README.md:49, README_zh.md:49\n\n---\n\n## Video Generation Models (HunyuanVideo, LTX2)\n\nLightX2V supports additional video generation model families beyond WAN:\n\n```mermaid\ngraph TB\n    subgraph \"HunyuanVideo 1.5\"\n        HyBase[\"HunyuanVideo-1.5\u003cbr/\u003eTencent model\"]\n        Hy720p[\"720p T2V/I2V\u003cbr/\u003etransformer_model_name='720p_t2v'\"]\n        HyDistill[\"4-step Distillation\u003cbr/\u003e~25x speedup\"]\n        HyLightVAE[\"LightTAE\u003cbr/\u003eFast VAE decode\"]\n        HyFP8[\"FP8 Quantization\u003cbr/\u003ePer-tensor quant\"]\n    end\n    \n    subgraph \"LTX-2\"\n        LTXBase[\"LTX-2\u003cbr/\u003eLightricks model\"]\n        LTXAudioVideo[\"Audio+Video Generation\u003cbr/\u003eSynchronized output\"]\n        LTXMultiStage[\"Multi-stage Pipeline\u003cbr/\u003eUpsampling support\"]\n        LTXBlockOffload[\"Block-level Offload\u003cbr/\u003eMemory efficient\"]\n    end\n    \n    subgraph \"Optimization Features\"\n        TeaCache[\"TeaCache\u003cbr/\u003eFeature caching\"]\n        MagCache[\"MagCache\u003cbr/\u003eMagnitude caching\"]\n        CFGParallel[\"CFG Parallelism\u003cbr/\u003eMulti-GPU\"]\n        UlyssesParallel[\"Ulysses Parallelism\u003cbr/\u003eSequence split\"]\n    end\n    \n    HyBase --\u003e Hy720p\n    HyBase --\u003e HyDistill\n    HyBase --\u003e HyLightVAE\n    HyBase --\u003e HyFP8\n    \n    LTXBase --\u003e LTXAudioVideo\n    LTXBase --\u003e LTXMultiStage\n    LTXBase --\u003e LTXBlockOffload\n    \n    Hy720p -.uses.-\u003e TeaCache\n    Hy720p -.uses.-\u003e MagCache\n    Hy720p -.uses.-\u003e CFGParallel\n    Hy720p -.uses.-\u003e UlyssesParallel\n    \n    LTXAudioVideo -.uses.-\u003e CFGParallel\n    LTXAudioVideo -.uses.-\u003e LTXBlockOffload\n```\n\n**HunyuanVideo-1.5:**\n- Tencent's video generation model supporting T2V and I2V\n- 720p resolution support via `transformer_model_name=\"720p_t2v\"`\n- `HunyuanVideo15Runner` implements the inference pipeline\n- 4-step distilled models available for ~25x speedup\n- LightTAE (lightweight temporal autoencoder) for faster VAE decoding\n- Supports TeaCache and MagCache for feature reuse\n- Can run on 24GB RTX 4090 with offloading\n\n**LTX-2:**\n- Lightricks model for synchronized audio and video generation\n- Unique T2AV (text-to-audio+video) and I2AV capabilities\n- Multi-stage pipeline with upsampling support\n- Block-level CPU offloading for memory efficiency\n- FP8 per-tensor quantization support\n- Uses `LTX2Runner` with specialized audio-video orchestration\n\n**Sources:** README.md:47, README.md:69, README_zh.md:47, README_zh.md:69, scripts/hunyuan_video_15/README.md:1-104\n\n---\n\n## Runner Usage Patterns\n\nThe following code snippets illustrate typical usage patterns for different runners:\n\n**Pattern 1: WAN Model I2V Generation**\n```python\nfrom lightx2v import LightX2VPipeline\n\n# Initialize with model_cls and task\npipe = LightX2VPipeline(\n    model_path=\"/path/to/Wan2.2-I2V-A14B\",\n    model_cls=\"wan2.2_moe\",  # Selects Wan22AudioRunner for S2V or base for I2V\n    task=\"i2v\",\n)\n\n# Configure generation parameters\npipe.create_generator(\n    attn_mode=\"sage_attn2\",\n    infer_steps=40,\n    height=480,\n    width=832,\n    num_frames=81,\n    guidance_scale=[3.5, 3.5],  # For wan2.2_moe (high, low noise)\n    sample_shift=5.0,\n)\n\n# Generate video\npipe.generate(\n    seed=42,\n    image_path=\"/path/to/image.jpg\",\n    prompt=\"...\",\n    negative_prompt=\"...\",\n    save_result_path=\"/path/to/output.mp4\",\n)\n```\n\n**Pattern 2: HunyuanVideo T2V Generation**\n```python\n# Initialize HunyuanVideo runner\npipe = LightX2VPipeline(\n    model_path=\"/path/to/hunyuanvideo-1.5/\",\n    model_cls=\"hunyuan_video_1.5\",\n    transformer_model_name=\"720p_t2v\",  # Specifies 720p variant\n    task=\"t2v\",\n)\n\n# Enable lightweight VAE\npipe.enable_lightvae(\n    use_tae=True,\n    tae_path=\"/path/to/lighttaehy1_5.safetensors\",\n)\n\n# Configure for 720p generation\npipe.create_generator(\n    attn_mode=\"sage_attn2\",\n    infer_steps=50,\n    num_frames=121,\n    guidance_scale=6.0,\n    sample_shift=9.0,\n    aspect_ratio=\"16:9\",\n    fps=24,\n)\n```\n\n**Pattern 3: Distilled Model (4-step inference)**\n```python\n# Initialize distilled WAN model\npipe = LightX2VPipeline(\n    model_path=\"/path/to/Wan2.1-I2V-14B-Distill\",\n    model_cls=\"wan2.1_distill\",  # Selects WanDistillRunner\n    task=\"i2v\",\n)\n\n# Configure for 4-step inference\npipe.create_generator(\n    attn_mode=\"sage_attn2\",\n    infer_steps=4,  # Only 4 steps needed\n    height=480,\n    width=832,\n    num_frames=81,\n    guidance_scale=1.0,  # No CFG for distilled models\n    sample_shift=5.0,\n)\n```\n\n**Pattern 4: LTX2 Audio+Video Generation**\n```python\n# Initialize LTX2 runner\npipe = LightX2VPipeline(\n    model_path=\"/path/to/LTX-2\",\n    model_cls=\"ltx2\",\n    task=\"t2av\",  # Text-to-audio+video\n)\n\n# Configure multi-stage pipeline\npipe.create_generator(\n    attn_mode=\"sage_attn2\",\n    infer_steps=50,\n    # LTX2-specific parameters for audio+video\n)\n\n# Generate synchronized audio and video\npipe.generate(\n    seed=42,\n    prompt=\"...\",\n    save_result_path=\"/path/to/output.mp4\",  # Contains audio track\n)\n```\n\n**Sources:** README.md:139-197, scripts/hunyuan_video_15/README.md:34-95\n\n---\n\n## Summary: Complete Runner-Task Matrix\n\nThe following table provides a comprehensive reference of all runners, their model classes, supported tasks, and key distinguishing features:\n\n| Runner Class | Model Classes | Tasks | Resolution | Key Features | Performance Notes |\n|--------------|---------------|-------|------------|--------------|-------------------|\n| `WanRunner` | `wan2.1` | `t2v`, `i2v` | 480p, 720p | sample_shift, RoPE, CFG | 40-50 steps standard |\n| `WanAudioRunner` | `wan2.1` | `s2v` | 480p, 720p | Audio segmentation, prev_latents, multi-person | Highest importance (222.45) |\n| `Wan22AudioRunner` | `wan2.2_moe` | `s2v` | 480p, 720p | Dual-model MoE, high/low noise separation | boundary_step_index switching |\n| `WanDistillRunner` | `wan2.1_distill` | `t2v`, `i2v` | 480p, 720p | 4-step inference, no CFG | ~25x speedup |\n| `Wan22MoeDistillRunner` | `wan2.2_moe_distill` | `t2v`, `i2v` | 480p, 720p | MoE 4-step, model switching | ~25x speedup with MoE |\n| `WanCausVidRunner` | `wan2.1_causvid` | `t2v` | Variable | Autoregressive, KV cache | Long-form video generation |\n| `QwenImageRunner` | `qwen-image`, `qwen-image-edit`, `qwen-image-2512` | `t2i`, `i2i` | Variable | Qwen VL model, layered generation | 8-step distill available |\n| `ZImageRunner` | `z-image-turbo` | `t2i`, `i2i` | Variable | Qwen3 encoder, turbo inference | Optimized for speed |\n| `HunyuanVideo15Runner` | `hunyuan_video_1.5` | `t2v`, `i2v` | 720p | TeaCache, MagCache, LightTAE | 4-step distill available |\n| `LTX2Runner` | `ltx2` | `t2av`, `i2av` | Variable | Audio+video sync, multi-stage | Unique audio generation |\n| `LongCatImageRunner` | `longcat_image` | `t2i`, `i2i` | High-res | High-resolution support | Large image generation |\n\n**Selection guidelines:**\n- For **audio-driven video**: Use `WanAudioRunner` (wan2.1) or `Wan22AudioRunner` (wan2.2_moe)\n- For **fastest video generation**: Use `WanDistillRunner` or `Wan22MoeDistillRunner` (4 steps)\n- For **highest quality video**: Use `WanRunner` (wan2.1) with 40-50 steps\n- For **image editing**: Use `QwenImageRunner` with qwen-image-edit model\n- For **fast image generation**: Use `ZImageRunner` with z-image-turbo\n- For **720p video**: Use `HunyuanVideo15Runner` or `WanRunner` (720p config)\n- For **audio+video synthesis**: Use `LTX2Runner` (ltx2 model only)\n\n**Sources:** README.md:139-197, README.md:207-234, README_zh.md:207-234"])</script><script>self.__next_f.push([1,"2e:T6cfd,"])</script><script>self.__next_f.push([1,"# WAN Model Family Overview\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [README_zh.md](README_zh.md)\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n- [lightx2v_platform/README.md](lightx2v_platform/README.md)\n- [lightx2v_platform/README_zh.md](lightx2v_platform/README_zh.md)\n- [scripts/hunyuan_video_15/README.md](scripts/hunyuan_video_15/README.md)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis page documents the **WAN (Wan) model family**, which implements Diffusion Transformer (DiT) architectures for video generation. The family includes Wan 2.1 and Wan 2.2, both using 14B parameter transformers with Adaptive Layer Normalization (AdaLN) modulation for conditional generation.\n\n**Core Architecture:** The WAN family uses a DiT backbone where each transformer block applies modulation via 6 learned parameters (shift, scale, gate) for both attention and feed-forward sub-layers. This AdaLN-Zero mechanism enables timestep and text/image conditioning without architectural modifications.\n\n**Shared Components Across Variants:**\n- `WanModel` base class implementing DiT with 42 transformer blocks\n- `WanTransformerWeights` managing per-block weight storage\n- `WanScheduler` for Euler-based diffusion sampling\n- Three-stage inference: `WanPreInfer`  `WanTransformerInfer`  `WanPostInfer`\n- Modulation system extracting 6 parameters per block from time/text embeddings\n\n**Related Pages:**\n- For audio-driven video generation using WAN models, see [Audio-Driven Video Generation (S2V)](#5.2)\n- For distilled variants with 4-step inference, see [Distilled Models and Step Reduction](#5.4)\n- For Mixture-of-Experts architecture (Wan 2.2 MoE), see [Mixture-of-Experts Models](#5.5)\n- For standard T2V/I2V workflows, see [Text-to-Video and Image-to-Video](#5.3)\n- For autoregressive variants, see [Autoregressive Models (CausVid)](#5.8)\n\n---\n\n## Diffusion Transformer (DiT) Architecture\n\nThe WAN family implements a Diffusion Transformer architecture where conditioning is applied through Adaptive Layer Normalization (AdaLN) modulation rather than cross-attention for timestep information.\n\n### DiT Block Structure with AdaLN Modulation\n\n**AdaLN Modulation Diagram**\n\n```mermaid\ngraph TB\n    TimeEmbed[\"timestep_input\u003cbr/\u003escheduler.timestep_input\"]\n    TextEmbed[\"text_embeddings\u003cbr/\u003econtext\"]\n    \n    TimeProj[\"time_embedding MLP\u003cbr/\u003epre_weight.time_embedding_0/2\"]\n    TextProj[\"text_embedding projection\u003cbr/\u003epre_weight.text_embedding_0/2\"]\n    \n    Modulation[\"modulation.tensor\u003cbr/\u003eblocks[i].compute_phases[0].modulation\"]\n    \n    AdaLN[\"AdaLN: (modulation + embed0).chunk(6)\"]\n    \n    ShiftMSA[\"shift_msa\"]\n    ScaleMSA[\"scale_msa\"]\n    GateMSA[\"gate_msa\"]\n    ShiftFFN[\"c_shift_msa\"]\n    ScaleFFN[\"c_scale_msa\"]\n    GateFFN[\"c_gate_msa\"]\n    \n    Norm1[\"LayerNorm\u003cbr/\u003eblocks[i].compute_phases[0].norm1\"]\n    SelfAttn[\"Self-Attention\u003cbr/\u003eblocks[i].compute_phases[0].self_attn_*\"]\n    Norm2[\"LayerNorm\u003cbr/\u003eblocks[i].compute_phases[2].norm2\"]\n    FFN[\"Feed-Forward\u003cbr/\u003eblocks[i].compute_phases[2].mlp\"]\n    \n    TimeEmbed --\u003e TimeProj\n    TextEmbed --\u003e TextProj\n    TimeProj --\u003e Modulation\n    TextProj --\u003e Modulation\n    \n    Modulation --\u003e AdaLN\n    \n    AdaLN --\u003e ShiftMSA\n    AdaLN --\u003e ScaleMSA\n    AdaLN --\u003e GateMSA\n    AdaLN --\u003e ShiftFFN\n    AdaLN --\u003e ScaleFFN\n    AdaLN --\u003e GateFFN\n    \n    ShiftMSA -.modulates.-\u003e Norm1\n    ScaleMSA -.modulates.-\u003e Norm1\n    GateMSA -.gates.-\u003e SelfAttn\n    \n    ShiftFFN -.modulates.-\u003e Norm2\n    ScaleFFN -.modulates.-\u003e Norm2\n    GateFFN -.gates.-\u003e FFN\n```\n\n**Key Code Entities:**\n- Modulation extraction: [lightx2v/models/networks/wan/infer/transformer_infer.py:155-167]()\n- Modulation application: [lightx2v/models/networks/wan/infer/transformer_infer.py:169-185]()\n- Time embedding: [lightx2v/models/networks/wan/infer/pre_infer.py:85-95]()\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:155-185](), [lightx2v/models/networks/wan/infer/pre_infer.py:85-95](), [lightx2v/models/networks/wan/weights/pre_weights.py:43-65]()\n\n### System Components Diagram\n\n**WanModel Class Hierarchy and Dependencies**\n\n```mermaid\ngraph TB\n    WanModel[\"WanModel\u003cbr/\u003emodels/networks/wan/model.py:33\"]\n    \n    PreWeightClass[\"pre_weight_class = WanPreWeights\u003cbr/\u003emodel.py:34\"]\n    TransWeightClass[\"transformer_weight_class = WanTransformerWeights\u003cbr/\u003emodel.py:35\"]\n    \n    PreInferClass[\"pre_infer_class = WanPreInfer\u003cbr/\u003emodel.py:57\"]\n    TransInferClass[\"transformer_infer_class = WanTransformerInfer\u003cbr/\u003emodel.py:61\"]\n    PostInferClass[\"post_infer_class = WanPostInfer\u003cbr/\u003emodel.py:58\"]\n    \n    PreWeight[\"self.pre_weight\u003cbr/\u003eWanPreWeights instance\"]\n    TransWeight[\"self.transformer_weights\u003cbr/\u003eWanTransformerWeights instance\"]\n    \n    PreInfer[\"self.pre_infer\u003cbr/\u003eWanPreInfer instance\"]\n    TransInfer[\"self.transformer_infer\u003cbr/\u003eWanTransformerInfer instance\"]\n    PostInfer[\"self.post_infer\u003cbr/\u003eWanPostInfer instance\"]\n    \n    Scheduler[\"self.scheduler\u003cbr/\u003eWanScheduler/variants\"]\n    \n    WanModel --\u003e PreWeightClass\n    WanModel --\u003e TransWeightClass\n    WanModel --\u003e PreInferClass\n    WanModel --\u003e TransInferClass\n    WanModel --\u003e PostInferClass\n    \n    PreWeightClass -.instantiates.-\u003e PreWeight\n    TransWeightClass -.instantiates.-\u003e TransWeight\n    \n    PreInferClass -.instantiates.-\u003e PreInfer\n    TransInferClass -.instantiates.-\u003e TransInfer\n    PostInferClass -.instantiates.-\u003e PostInfer\n    \n    PreInfer -.uses.-\u003e PreWeight\n    TransInfer -.uses.-\u003e TransWeight\n    \n    PreInfer --\u003e Scheduler\n    TransInfer --\u003e Scheduler\n    PostInfer --\u003e Scheduler\n```\n\n**Sources:** [lightx2v/models/networks/wan/model.py:33-84](), [lightx2v/models/runners/wan/wan_runner.py:64-79](), [lightx2v/models/networks/wan/weights/transformer_weights.py:11-42]()\n\n---\n\n## Model Variants\n\n### Wan 2.1\n\nWan 2.1 is the base model variant with a unified 14B transformer architecture.\n\n**Key Characteristics:**\n- **Model Class:** `wan2.1` \n- **Runner Implementation:** `WanRunner` registered at `@RUNNER_REGISTER(\"wan2.1\")`\n- **Architecture:** Single unified transformer with 42 blocks (configurable via `num_layers`)\n- **VAE:** `WanVAE` using 4D causal convolutions\n- **Guidance Scale:** Scalar value (e.g., 5.0)\n- **Model Path Structure:** Single model directory with weights\n\n**Code Registration:**\n\n```\n@RUNNER_REGISTER(\"wan2.1\")\nclass WanRunner(DefaultRunner):\n    def load_transformer(self):\n        return WanModel(model_path=..., config=..., device=...)\n```\n\n### Wan 2.2\n\nWan 2.2 introduces architectural improvements for image-to-video tasks, particularly in handling reference images.\n\n**Key Characteristics:**\n- **Model Class:** `wan2.2` or `wan2.2_moe`\n- **Runner Implementation:** `Wan22MoeRunner` for MoE variant\n- **Architecture:** Dual-model structure (high/low noise) for MoE variant\n- **VAE:** `Wan2_2_VAE` with optimized patchify operations\n- **Guidance Scale:** Dual-component `[3.5, 3.5]` for text and image guidance\n- **Enhanced I2V:** Improved handling of reference image embeddings\n\n**Wan 2.2 Specific Features:**\n\n| Feature | Wan 2.1 | Wan 2.2 |\n|---------|---------|---------|\n| VAE Class | `WanVAE` | `Wan2_2_VAE` |\n| Guidance Scale Type | Scalar | List of 2 values |\n| I2V VAE Encoding | Direct encode | Special mask handling |\n| Reference Image Processing | Standard | Enhanced with `embed0` |\n| Model Structure | Single | Single or MoE |\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:63-79](), [README.md:208](), [lightx2v/infer.py:54-59]()\n\n---\n\n## Resolution and Task Support\n\nBoth Wan 2.1 and Wan 2.2 support multiple resolution tiers through adaptive bucket configurations.\n\n### Resolution Tiers\n\n```mermaid\ngraph LR\n    subgraph \"480P Tier\"\n        P480_169[\"480x832\u003cbr/\u003e16:9 aspect\"]\n        P480_916[\"832x480\u003cbr/\u003e9:16 aspect\"]\n        P480_11[\"480x480\u003cbr/\u003e1:1 aspect\"]\n    end\n    \n    subgraph \"544P Tier\"\n        P544_169[\"544x960\u003cbr/\u003e16:9 aspect\"]\n        P544_916[\"960x544\u003cbr/\u003e9:16 aspect\"]\n        P544_11[\"576x576\u003cbr/\u003e1:1 aspect\"]\n    end\n    \n    subgraph \"720P Tier\"\n        P720_169[\"720x1280\u003cbr/\u003e16:9 aspect\"]\n        P720_916[\"1280x720\u003cbr/\u003e9:16 aspect\"]\n        P720_11[\"960x960\u003cbr/\u003e1:1 aspect\"]\n    end\n    \n    AdaptiveResize[\"resize_image()\u003cbr/\u003eBucket selection\"] --\u003e P480_169\n    AdaptiveResize --\u003e P480_916\n    AdaptiveResize --\u003e P480_11\n    AdaptiveResize --\u003e P544_169\n    AdaptiveResize --\u003e P544_916\n    AdaptiveResize --\u003e P544_11\n    AdaptiveResize --\u003e P720_169\n    AdaptiveResize --\u003e P720_916\n    AdaptiveResize --\u003e P720_11\n```\n\n**Bucket Configuration (from code):**\n\nThe resolution selection uses aspect ratio-based bucketing defined in `resize_image()`:\n\n```python\nbucket_config = {\n    0.667: [[480, 832], [544, 960], [720, 1280]],  # 16:9 landscape\n    1.500: [[832, 480], [960, 544], [1280, 720]],  # 9:16 portrait  \n    1.000: [[480, 480], [576, 576], [704, 704], [960, 960]]  # 1:1 square\n}\n```\n\n### Task Support Matrix\n\n| Task | Wan 2.1 | Wan 2.2 | Input Requirements |\n|------|---------|---------|-------------------|\n| **Text-to-Video (T2V)** |  |  | Text prompt only |\n| **Image-to-Video (I2V)** |  |  | Image + text prompt |\n| **First-Last Frame (FLF2V)** |  |  | Two images + prompt |\n| **Video Editing (VACE)** |  |  | Source video + mask |\n| **Speech-to-Video (S2V)** | Via `WanAudioRunner` | Via `Wan22AudioRunner` | Audio + reference image |\n\n**Sources:** [lightx2v/models/runners/default_runner.py:25-53](), [lightx2v/models/runners/wan/wan_audio_runner.py:36-119]()\n\n---\n\n## Core Components and Code Mapping\n\n### Model Core Components\n\nThe WAN model is composed of several interconnected classes that manage weights, inference, and scheduling.\n\n```mermaid\ngraph TB\n    subgraph \"WanModel Initialization\"\n        Init[\"WanModel.__init__()\u003cbr/\u003emodel.py:43-101\"]\n        InitWeights[\"_init_weights()\u003cbr/\u003eLoad safetensors\"]\n        InitInfer[\"_init_infer()\u003cbr/\u003eCreate infer objects\"]\n    end\n    \n    subgraph \"Weight Classes\"\n        PreW[\"WanPreWeights\u003cbr/\u003epre_weights.py:10-75\"]\n        TransW[\"WanTransformerWeights\u003cbr/\u003etransformer_weights.py:11-153\"]\n        BlockW[\"WanTransformerAttentionBlock\u003cbr/\u003e3 phases each block\"]\n    end\n    \n    subgraph \"Infer Classes\"\n        PreI[\"WanPreInfer\u003cbr/\u003epre_infer.py:9-129\"]\n        TransI[\"WanTransformerInfer\u003cbr/\u003etransformer_infer.py:17-296\"]\n        PostI[\"WanPostInfer\u003cbr/\u003epost_infer.py:8-32\"]\n    end\n    \n    Init --\u003e InitWeights\n    Init --\u003e InitInfer\n    \n    InitWeights --\u003e PreW\n    InitWeights --\u003e TransW\n    TransW --\u003e BlockW\n    \n    InitInfer --\u003e PreI\n    InitInfer --\u003e TransI\n    InitInfer --\u003e PostI\n    \n    PreW -.used by.-\u003e PreI\n    TransW -.used by.-\u003e TransI\n```\n\n### Weight Structure and Storage\n\n**WanPreWeights Weight Modules**\n\n| Module Name | Type | Purpose | File Location |\n|-------------|------|---------|---------------|\n| `patch_embedding` | Conv3D | Patchify input latents (stride=[1,2,2]) | `pre_weights.py:14-21` |\n| `text_embedding_0` | Linear | Project text embeddings (40963072) | `pre_weights.py:43-48` |\n| `text_embedding_2` | Linear | Second text projection layer | `pre_weights.py:49-54` |\n| `time_embedding_0` | Linear | Timestep MLP first layer | `pre_weights.py:56-61` |\n| `time_embedding_2` | Linear | Timestep MLP second layer (freq_dim*6) | `pre_weights.py:62-67` |\n| `img_emb.proj.0` | Linear | Image encoder projection (I2V) | `pre_weights.py:75-82` |\n| `img_emb.proj.4` | Linear | Second image projection | `pre_weights.py:83-90` |\n\n**WanTransformerWeights Structure:**\n- `blocks`: `WeightModuleList` containing 42 `WanTransformerAttentionBlock` instances\n- `norm`: Final `LayerNorm` before output projection\n- `head`: Output `Linear` layer projecting to latent dimension\n- `head_modulation`: Modulation tensor for final layer\n\n**WanTransformerAttentionBlock Phase Structure** (per block):\n\nEach block contains 3 compute phases stored in `compute_phases` list:\n\n```python\n# Phase 0: Self-Attention\ncompute_phases[0].modulation          # Modulation parameters\ncompute_phases[0].norm1               # LayerNorm\ncompute_phases[0].self_attn_q/k/v     # Q, K, V projections\ncompute_phases[0].self_attn_norm_q/k  # RMSNorm for Q, K\ncompute_phases[0].self_attn_1         # Attention operator\ncompute_phases[0].self_attn_o         # Output projection\n\n# Phase 1: Cross-Attention (text/image conditioning)\ncompute_phases[1].norm3               # LayerNorm\ncompute_phases[1].cross_attn_q        # Query projection\ncompute_phases[1].cross_attn_k/v      # Key, Value for text\ncompute_phases[1].cross_attn_k_img/v_img  # Key, Value for image\ncompute_phases[1].cross_attn_norm_q/k     # RMSNorm\ncompute_phases[1].cross_attn_1        # Text attention\ncompute_phases[1].cross_attn_2        # Image attention\ncompute_phases[1].cross_attn_o        # Output projection\n\n# Phase 2: Feed-Forward Network\ncompute_phases[2].norm2               # LayerNorm\ncompute_phases[2].mlp.up_proj         # Expansion projection\ncompute_phases[2].mlp.down_proj       # Reduction projection\n```\n\n**Sources:** [lightx2v/models/networks/wan/weights/pre_weights.py:6-90](), [lightx2v/models/networks/wan/weights/transformer_weights.py:11-153](), [lightx2v/models/networks/wan/weights/transformer_weights.py:159-334]()\n\n---\n\n## Inference Pipeline Flow\n\nThe WAN model processes inputs through three coordinated inference stages.\n\n```mermaid\nsequenceDiagram\n    participant S as WanScheduler\n    participant M as WanModel\n    participant Pre as WanPreInfer\n    participant Trans as WanTransformerInfer\n    participant Post as WanPostInfer\n    participant W as WanTransformerWeights\n    \n    Note over S: prepare(seed, latent_shape)\n    S-\u003e\u003eS: Initialize latents (noise)\n    S-\u003e\u003eS: Compute timestep schedule\n    \n    loop Each Diffusion Step\n        S-\u003e\u003eS: step_pre(step_index)\n        S-\u003e\u003eM: infer(inputs)\n        \n        M-\u003e\u003ePre: infer(weights, inputs)\n        Note over Pre: Patch embedding\u003cbr/\u003eTime/text embeddings\u003cbr/\u003ePosition encoding\n        Pre--\u003e\u003eM: pre_infer_out\n        \n        M-\u003e\u003eTrans: infer(weights, pre_infer_out)\n        Note over Trans: Loop through 42 blocks\u003cbr/\u003eSelf-attention + RoPE\u003cbr/\u003eCross-attention\u003cbr/\u003eFeed-forward\n        Trans-\u003e\u003eW: Access block weights\n        W--\u003e\u003eTrans: Weight tensors\n        Trans--\u003e\u003eM: transformer_out\n        \n        M-\u003e\u003ePost: infer(transformer_out, pre_infer_out)\n        Note over Post: Unpatchify\u003cbr/\u003eReshape to latent\n        Post--\u003e\u003eM: noise_pred\n        \n        M--\u003e\u003eS: noise_pred\n        S-\u003e\u003eS: step_post()\n        Note over S: Denoise step\u003cbr/\u003eUpdate latents\n    end\n    \n    Note over S: Final latents ready\n```\n\n### Inference Phases Detail\n\n**Phase 1: Pre-Inference** \n\nThe `WanPreInfer.infer()` method prepares inputs for the transformer:\n\n```python\n# Key operations in pre_infer.py:69-129\ndef infer(self, weights, inputs):\n    # 1. Extract and patchify latents\n    x = self.scheduler.latents  # [16, T, H, W]\n    x = weights.patch_embedding.apply(x.unsqueeze(0))  # Conv3D patchify\n    \n    # 2. Compute timestep embeddings\n    t_emb = sinusoidal_embedding_1d(self.scheduler.timestep_input, self.freq_dim)\n    t_emb = weights.time_embedding_0.apply(t_emb)\n    t_emb = F.silu(t_emb)\n    embed0 = weights.time_embedding_2.apply(t_emb)  #  [1, freq_dim*6]\n    \n    # 3. Project text embeddings\n    context = inputs[\"text_encoder_output\"][\"context\"]\n    embed = weights.text_embedding_0.apply(context)\n    embed = weights.text_embedding_2.apply(F.silu(embed))\n    \n    # 4. Combine for modulation: embed0 = embed0 + embed\n    \n    # 5. Compute RoPE position encodings\n    cos_sin = self.prepare_cos_sin(grid_sizes, self.freqs)\n    \n    return WanPreInferModuleOutput(x, embed, embed0, context, cos_sin, ...)\n```\n\n**Phase 2: Transformer Inference with Modulation**\n\nThe `WanTransformerInfer.infer_block()` method processes each transformer block:\n\n```python\n# Key operations in transformer_infer.py:130-153\ndef infer_block(self, block, x, pre_infer_out):\n    # 1. Extract 6 modulation parameters\n    shift_msa, scale_msa, gate_msa, c_shift_msa, c_scale_msa, c_gate_msa = \\\n        self.pre_process(block.compute_phases[0].modulation, pre_infer_out.embed0)\n    \n    # 2. Self-attention with AdaLN modulation\n    y_out = self.infer_self_attn(\n        block.compute_phases[0], x, shift_msa, scale_msa\n    )\n    # Inside infer_self_attn:\n    #   norm1_out = phase.norm1.apply(x)\n    #   norm1_out = modulate(norm1_out, scale_msa, shift_msa)  # AdaLN\n    #   q/k/v = ... apply projections and RoPE ...\n    #   attn_out = attention(q, k, v)\n    #   y = phase.self_attn_o.apply(attn_out)\n    \n    # 3. Cross-attention with text/image\n    x, attn_out = self.infer_cross_attn(\n        block.compute_phases[1], x, context, y_out, gate_msa\n    )\n    # Gated residual: x = x + y_out * gate_msa\n    \n    # 4. Feed-forward with AdaLN modulation\n    y = self.infer_ffn(\n        block.compute_phases[2], x, attn_out, c_shift_msa, c_scale_msa\n    )\n    # Inside infer_ffn:\n    #   norm2_out = phase.norm2.apply(x)\n    #   norm2_out = modulate(norm2_out, c_scale_msa, c_shift_msa)\n    #   ffn_out = phase.mlp.apply(norm2_out)\n    \n    # 5. Final gated residual\n    x = self.post_process(x, y, c_gate_msa, pre_infer_out)\n    # x = x + y * c_gate_msa\n    \n    return x\n```\n\n**Modulation Function:**\n\nThe core modulation operation implements AdaLN:\n\n```python\n# transformer_infer.py:13-14\ndef modulate(x, scale, shift):\n    return x * (1 + scale.squeeze()) + shift.squeeze()\n```\n\n**Phase 3: Post-Inference** \n\nThe `WanPostInfer.infer()` method unpatchifies the output:\n\n```python\n# post_infer.py:18-31\ndef infer(self, x, pre_infer_out):\n    x = self.unpatchify(x, pre_infer_out.grid_sizes.tuple)\n    return [u.float() for u in x]\n\ndef unpatchify(self, x, grid_sizes):\n    c = self.out_dim\n    x = x[:math.prod(grid_sizes)].view(*grid_sizes, *self.patch_size, c)\n    x = torch.einsum(\"fhwpqrc-\u003ecfphqwr\", x)  # Rearrange patches\n    x = x.reshape(c, *[i * j for i, j in zip(grid_sizes, self.patch_size)])\n    return [x]\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/pre_infer.py:69-129](), [lightx2v/models/networks/wan/infer/transformer_infer.py:130-329](), [lightx2v/models/networks/wan/infer/post_infer.py:18-31](), [lightx2v/models/networks/wan/infer/transformer_infer.py:13-14]()\n\n---\n\n## Runner Architecture\n\nRunners orchestrate the complete generation pipeline from user input to final video output.\n\n### Runner Class Hierarchy\n\n```mermaid\ngraph TB\n    BaseRunner[\"BaseRunner\u003cbr/\u003eAbstract base\"]\n    DefaultRunner[\"DefaultRunner\u003cbr/\u003ebase_runner.py\u003cbr/\u003eCommon pipeline logic\"]\n    WanRunner[\"WanRunner\u003cbr/\u003ewan_runner.py:64\u003cbr/\u003eWan 2.1 implementation\"]\n    Wan22MoeRunner[\"Wan22MoeRunner\u003cbr/\u003eWan 2.2 MoE variant\"]\n    \n    WanAudioRunner[\"WanAudioRunner\u003cbr/\u003ewan_audio_runner.py:277\u003cbr/\u003eS2V with audio\"]\n    WanDistillRunner[\"WanDistillRunner\u003cbr/\u003e4-step distilled\"]\n    WanVaceRunner[\"WanVaceRunner\u003cbr/\u003eVideo editing\"]\n    WanSFRunner[\"WanSFRunner\u003cbr/\u003eSelf-forcing AR\"]\n    WanAnimateRunner[\"WanAnimateRunner\u003cbr/\u003eAnimation task\"]\n    \n    BaseRunner --\u003e DefaultRunner\n    DefaultRunner --\u003e WanRunner\n    WanRunner --\u003e Wan22MoeRunner\n    WanRunner --\u003e WanAudioRunner\n    WanRunner --\u003e WanDistillRunner\n    WanRunner --\u003e WanVaceRunner\n    WanRunner --\u003e WanSFRunner\n    WanRunner --\u003e WanAnimateRunner\n```\n\n### Runner Responsibilities\n\n**WanRunner** ([wan_runner.py:64-523]())\n\n| Method | Purpose | Key Operations |\n|--------|---------|----------------|\n| `__init__()` | Initialize configuration | Set VAE class, model name, scheduler |\n| `load_transformer()` | Load main model | Instantiate `WanModel` with LoRA support |\n| `load_text_encoder()` | Load T5 encoder | Initialize `T5EncoderModel` with quantization |\n| `load_image_encoder()` | Load CLIP encoder | Initialize `CLIPModel` for I2V tasks |\n| `load_vae()` | Load VAE codec | Choose `WanVAE` or `Wan2_2_VAE` |\n| `init_scheduler()` | Create scheduler | Instantiate `WanScheduler` or variants |\n| `run_text_encoder()` | Encode prompts | Process positive/negative prompts |\n| `run_image_encoder()` | Encode images | Extract CLIP features for conditioning |\n| `run_vae_encoder()` | Encode reference images | Convert images to latent space |\n| `run_vae_decoder()` | Decode latents | Convert final latents to video frames |\n\n**DefaultRunner** ([default_runner.py:56-524]())\n\nProvides the core pipeline structure inherited by all runners:\n\n```python\ndef run_pipeline(self, input_info):\n    # 1. Encode inputs\n    self.inputs = self.run_input_encoder()\n    \n    # 2. Run diffusion\n    gen_video_final = self.run_main()\n    \n    # 3. Post-process and save\n    return gen_video_final\n\ndef run_main(self):\n    self.init_run()  # Prepare latents\n    for segment_idx in range(self.video_segment_num):\n        latents = self.run_segment(segment_idx)  # Diffusion loop\n        self.gen_video = self.run_vae_decoder(latents)  # Decode\n    self.end_run()  # Cleanup\n    return self.gen_video_final\n```\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:64-523](), [lightx2v/models/runners/default_runner.py:56-524]()\n\n---\n\n## Relationship to Variant Models\n\nThe base WAN architecture serves as foundation for multiple specialized variants.\n\n### Variant Dependency Graph\n\n```mermaid\ngraph TB\n    WanModel[\"WanModel\u003cbr/\u003eBase transformer\u003cbr/\u003emodel.py:39\"]\n    WanRunner[\"WanRunner\u003cbr/\u003eBase runner\u003cbr/\u003ewan_runner.py:64\"]\n    \n    subgraph \"Optimization Variants\"\n        WanDistill[\"WanDistillModel\u003cbr/\u003e4-step inference\u003cbr/\u003eNo CFG required\"]\n        LoRAModel[\"WanLoraWrapper\u003cbr/\u003eDynamic LoRA\u003cbr/\u003elora_adapter.py\"]\n        QuantModel[\"Quantized Variants\u003cbr/\u003eFP8, INT8, NVFP4\"]\n    end\n    \n    subgraph \"Task-Specific Variants\"\n        AudioModel[\"WanAudioModel\u003cbr/\u003eaudio_model.py:19\u003cbr/\u003eAudio conditioning\"]\n        VaceModel[\"VACE Models\u003cbr/\u003eVideo editing\"]\n        ARModel[\"WanCausVidModel\u003cbr/\u003eAutoregressive KV cache\"]\n        MoEModel[\"Wan2.2 MoE\u003cbr/\u003eDual model structure\"]\n    end\n    \n    WanModel --\u003e WanDistill\n    WanModel --\u003e LoRAModel\n    WanModel --\u003e QuantModel\n    \n    WanModel --\u003e AudioModel\n    WanModel --\u003e VaceModel\n    WanModel --\u003e ARModel\n    WanModel --\u003e MoEModel\n    \n    WanRunner -.provides baseline.-\u003e WanDistill\n    WanRunner -.provides baseline.-\u003e AudioModel\n    WanRunner -.provides baseline.-\u003e VaceModel\n    WanRunner -.provides baseline.-\u003e ARModel\n```\n\n### Variant Overview\n\n| Variant | Base Class | Key Differences | Related Page |\n|---------|------------|-----------------|--------------|\n| **Distilled** | `WanModel` | Modified scheduler, 4 steps, no CFG | [5.4](#5.4) |\n| **Audio (S2V)** | `WanModel`  `WanAudioModel` | Additional audio adapter weights, segmented processing | [5.2](#5.2) |\n| **MoE (2.2)** | `WanModel`  2 | Dual models for high/low noise, boundary switching | [5.5](#5.5) |\n| **VACE** | `WanModel` | Video editing masks, source video encoding | Specialized |\n| **CausVid** | `WanModel` | KV cache, incremental inference | [5.8](#5.8) |\n| **LoRA** | `WanModel` | Dynamic weight updates via `WanLoraWrapper` | Runtime |\n| **Quantized** | `WanModel` | INT8/FP8/NVFP4 weights, specialized kernels | Optimization |\n\n**Key Inheritance Patterns:**\n\n1. **WanAudioModel** extends `WanModel`:\n   - Adds `WanAudioTransformerWeights` with audio adapter\n   - Overrides `_init_infer_class()` to use `WanAudioTransformerInfer`\n   - Loads separate audio adapter checkpoint\n\n2. **WanDistillModel** modifies:\n   - Uses distillation-specific scheduler\n   - Changes inference steps (4 instead of 40)\n   - Disables CFG by default\n\n3. **Wan2.2 MoE** structure:\n   - `MultiDistillModelStruct` container with high/low noise models\n   - `boundary_step_index` determines model switching\n   - Supports model-level offloading\n\n**Sources:** [lightx2v/models/networks/wan/audio_model.py:19-27](), [lightx2v/models/networks/wan/lora_adapter.py:1-60](), [lightx2v/infer.py:46-68]()\n\n---\n\n## Configuration and Model Loading\n\n### Model Path Structure\n\nWan models follow a specific directory structure:\n\n```\nmodel_path/\n config.json                          # Model architecture config\n diffusion_model.safetensors          # Main weights (single file)\n or: diffusion_model_split/           # Chunked weights\n    non_block.safetensors\n    block_0.safetensors\n    block_1.safetensors\n    ...\n google/umt5-xxl/                     # T5 text encoder\n models_clip_*.pth                    # CLIP image encoder  \n models_t5_*.pth                      # T5 weights\n Wan2.1_VAE.pth / Wan2.2_VAE.pth     # VAE codec\n audio_adapter_model.safetensors      # (Audio variants only)\n```\n\n### Configuration Loading Process\n\nThe configuration is loaded in multiple stages ([set_config.py:37-128]()): \n\n1. **Base defaults** from `get_default_config()`\n2. **Command-line arguments** via `argparse`\n3. **JSON config file** specified by `--config_json`\n4. **Model-specific config** from `model_path/config.json`\n5. **Quantized config** from quantized checkpoint directory if applicable\n\n**Key Configuration Parameters:**\n\n```python\n{\n    \"model_cls\": \"wan2.1\" or \"wan2.2\" or \"wan2.2_moe\",\n    \"task\": \"t2v\" or \"i2v\",\n    \"num_layers\": 42,  # Transformer blocks\n    \"dim\": 3072,       # Hidden dimension\n    \"num_heads\": 24,   # Attention heads\n    \"vae_stride\": [4, 8, 8],  # Temporal, H, W compression\n    \"patch_size\": [1, 2, 2],  # Spatial patch size\n    \"target_video_length\": 81,  # Frames\n    \"infer_steps\": 40,  # Diffusion steps\n    \"sample_guide_scale\": 5.0,  # CFG strength\n    \"sample_shift\": 5.0,  # Timestep shift\n}\n```\n\n**Sources:** [lightx2v/utils/set_config.py:37-128](), [lightx2v/models/networks/wan/model.py:171-201]()\n\n---\n\n## Summary\n\nThe WAN model family provides the core video generation capabilities of LightX2V:\n\n- **Wan 2.1**: Unified 14B transformer with scalar guidance\n- **Wan 2.2**: Enhanced I2V with dual guidance and improved VAE\n- **Architecture**: DiT-based with three-stage inference (Pre  Transformer  Post)\n- **Resolutions**: 480P, 544P, 720P with aspect ratio bucketing\n- **Tasks**: T2V, I2V, FLF2V, VACE, S2V (with variants)\n- **Extensibility**: Base classes extended for distillation, audio, MoE, and autoregressive variants\n\nThe modular design allows specialized variants to inherit common functionality while customizing specific components (weights, inference, schedulers) for their use cases."])</script><script>self.__next_f.push([1,"2f:T73fe,"])</script><script>self.__next_f.push([1,"# WanAudioRunner - Audio-to-Video Generation\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\n`WanAudioRunner` implements audio-driven video generation (Audio-to-Video, A2V) for the Wan model family, specializing in **talking head generation** and **speech-to-video (S2V)** tasks. This runner processes audio input (speech or other sounds) alongside a reference image to generate temporally-coherent video sequences where visual content is synchronized with audio features.\n\nKey capabilities:\n- **Audio segmentation** with configurable overlap for long-duration generation\n- **Multi-person support** with per-person audio tracks and spatial masks\n- **Streaming inference** via `VAController` for real-time or continuous generation\n- **Frame-level synchronization** at 16 FPS audio-to-video alignment\n\nFor basic text-to-video and image-to-video without audio conditioning, see [WanRunner](#5.3). For distilled 4-step models, see [WanDistillRunner](#5.4). For the underlying Wan model architecture, see [WAN Model Family Overview](#5.1).\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:1-40](), [lightx2v/infer.py:78]()\n\n---\n\n## Architecture Overview\n\nComponent dependencies and class hierarchy for audio-to-video generation:\n\n```mermaid\ngraph TB\n    WanAudioRunner[\"WanAudioRunner\u003cbr/\u003ewan_audio_runner.py:277\u003cbr/\u003e@RUNNER_REGISTER('seko_talk')\"]\n    WanRunner[\"WanRunner\u003cbr/\u003ewan_runner.py:64\"]\n    DefaultRunner[\"DefaultRunner\u003cbr/\u003edefault_runner.py:55\"]\n    \n    WanAudioModel[\"WanAudioModel\u003cbr/\u003eaudio_model.py:19\"]\n    WanModel[\"WanModel\u003cbr/\u003emodel.py:33\"]\n    \n    AudioProcessor[\"AudioProcessor\u003cbr/\u003ewan_audio_runner.py:174\"]\n    SekoAudioEncoderModel[\"SekoAudioEncoderModel\u003cbr/\u003eaudio_encoder.py:8\"]\n    AudioAdapter[\"AudioAdapter\u003cbr/\u003eaudio_adapter.py:179\"]\n    FramePreprocessorTorchVersion[\"FramePreprocessorTorchVersion\u003cbr/\u003ewan_audio_runner.py:131\"]\n    \n    EulerScheduler[\"EulerScheduler\u003cbr/\u003escheduler.py:13\"]\n    Wan22VAE[\"Wan2_2_VAE\u003cbr/\u003evae_2_2.py:16\"]\n    VAController[\"VAController\u003cbr/\u003eva_controller.py\"]\n    \n    AudioSegment[\"@dataclass AudioSegment\u003cbr/\u003ewan_audio_runner.py:123\"]\n    \n    WanAudioRunner --\u003e WanRunner\n    WanRunner --\u003e DefaultRunner\n    WanAudioModel --\u003e WanModel\n    \n    WanAudioRunner --\u003e AudioProcessor\n    WanAudioRunner --\u003e SekoAudioEncoderModel\n    WanAudioRunner --\u003e AudioAdapter\n    WanAudioRunner --\u003e FramePreprocessorTorchVersion\n    WanAudioRunner --\u003e WanAudioModel\n    WanAudioRunner --\u003e EulerScheduler\n    WanAudioRunner --\u003e Wan22VAE\n    WanAudioRunner --\u003e VAController\n    \n    AudioProcessor --\u003e AudioSegment\n    SekoAudioEncoderModel --\u003e AudioAdapter\n    AudioAdapter --\u003e WanAudioModel\n```\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:276-289](), [lightx2v/models/networks/wan/audio_model.py:19-26](), [lightx2v/models/input_encoders/hf/seko_audio/audio_encoder.py:8-17]()\n\n---\n\n## Key Components\n\n### WanAudioRunner Class\n\nRegistered as `\"seko_talk\"` in `RUNNER_REGISTER` [lightx2v/models/runners/wan/wan_audio_runner.py:276]().\n\n**Constructor** [lightx2v/models/runners/wan/wan_audio_runner.py:278-285]():\n```python\ndef __init__(self, config):\n    super().__init__(config)\n    self.task = config.get(\"task\", \"i2v\")  # typically \"s2v\" or \"rs2v\"\n    self.prev_frame_length = config.get(\"prev_frame_length\", 5)\n    self.video_duration = config.get(\"video_duration\", 5)\n    self.frame_preprocessor = FramePreprocessorTorchVersion()\n```\n\n**Instance Attributes:**\n\n| Attribute | Type | Initialized In | Purpose |\n|-----------|------|----------------|---------|\n| `task` | str | `__init__` | Task type: `\"s2v\"` or `\"rs2v\"` |\n| `prev_frame_length` | int | `__init__` | Overlap frames between segments (default: 5) |\n| `video_duration` | float | `__init__` | Target duration in seconds |\n| `_audio_processor` | AudioProcessor | `read_audio_input()` | Audio loading and segmentation |\n| `audio_encoder` | SekoAudioEncoderModel | `load_audio_encoder()` | Audio feature extraction |\n| `audio_adapter` | AudioAdapter | `load_audio_adapter()` | Audio-to-video cross-attention |\n| `frame_preprocessor` | FramePreprocessorTorchVersion | `__init__` | Noise/mask for previous frames |\n| `scheduler` | EulerScheduler | `init_scheduler()` | Audio-specific scheduler |\n\n**Key Method Signatures:**\n\n| Method | Line | Purpose |\n|--------|------|---------|\n| `read_audio_input(audio_path)` | 291-328 | Load/segment audio, process masks |\n| `init_run_segment(segment_idx, audio_array)` | 553-582 | Encode audio, prepare prev_latents |\n| `end_run_segment(segment_idx, valid_duration)` | 589-626 | VAE decode, accumulate output |\n| `run_main()` | 661-732 | Main loop with streaming support |\n| `prepare_prev_latents(prev_video, prev_frame_length)` | 464-516 | Encode overlap frames |\n| `load_audio_encoder()` | 751-755 | Load SekoAudioEncoderModel |\n| `load_audio_adapter()` | 757-781 | Load AudioAdapter with weights |\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:276-789]()\n\n---\n\n### AudioProcessor Class\n\nDefined at [lightx2v/models/runners/wan/wan_audio_runner.py:174-251]().\n\n```mermaid\nclassDiagram\n    class AudioProcessor {\n        +int audio_sr\n        +int target_fps\n        +int audio_frame_rate\n        +__init__(audio_sr, target_fps)\n        +load_audio(audio_path) Tensor\n        +load_multi_person_audio(audio_paths) Tensor\n        +segment_audio(audio_array, expected_frames, max_num_frames, prev_frame_length) List\n        +get_audio_range(start_frame, end_frame) Tuple\n        +init_segments_idx(total_frame, clip_frame, overlap_frame) List\n    }\n    \n    class AudioSegment {\n        \u0026lt;\u0026lt;dataclass\u0026gt;\u0026gt;\n        +Tensor audio_array\n        +int start_frame\n        +int end_frame\n    }\n    \n    AudioProcessor --\u003e AudioSegment : creates\n```\n\n**Constructor** [lightx2v/models/runners/wan/wan_audio_runner.py:177-180]():\n```python\ndef __init__(self, audio_sr: int = 16000, target_fps: int = 16):\n    self.audio_sr = audio_sr\n    self.target_fps = target_fps\n    self.audio_frame_rate = audio_sr // target_fps  # 1000 samples/frame\n```\n\n**Method Details:**\n\n| Method | Lines | Returns | Description |\n|--------|-------|---------|-------------|\n| `load_audio(audio_path)` | 182-185 | `Tensor[T]` | Load single audio file, resample to 16kHz |\n| `load_multi_person_audio(audio_paths)` | 187-203 | `Tensor[N,T]` | Load multiple audio files, pad to max length |\n| `get_audio_range(start_frame, end_frame)` | 205-207 | `Tuple[int,int]` | Convert frame indices to sample indices |\n| `segment_audio(...)` | 209-235 | `List[AudioSegment]` | Split audio into overlapping segments |\n| `init_segments_idx(...)` | 237-251 | `List[Tuple]` | Calculate segment boundaries with overlap |\n\n**Segmentation Algorithm** [lightx2v/models/runners/wan/wan_audio_runner.py:237-251]():\n- Iterate with stride: `clip_frame - overlap_frame`\n- Ensure last segment has minimum `min_frame` length\n- Align segment length: `(end - start - 1) % 4 == 0` for VAE stride compatibility\n- Pad last segment if audio is shorter than required\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:174-251]()\n\n---\n\n### SekoAudioEncoderModel\n\nWraps a HuggingFace pretrained audio encoder (Chinese Hubert Large) for feature extraction.\n\n| Property | Default Value | Description |\n|----------|---------------|-------------|\n| `model_path` | `{model_path}/TencentGameMate-chinese-hubert-large` | Path to pretrained model |\n| `audio_sr` | 16000 | Audio sample rate |\n| `cpu_offload` | False | Whether to offload to CPU between inferences |\n\n**Inference Flow:**\n1. Extract features using `AutoFeatureExtractor` [lightx2v/models/input_encoders/hf/seko_audio/audio_encoder.py:34]()\n2. Encode with `AutoModel` (HuBERT) to get `last_hidden_state` [lightx2v/models/input_encoders/hf/seko_audio/audio_encoder.py:37]()\n3. Return audio features with shape `(1, T, 1024)` where T is temporal dimension\n\n**Sources:** [lightx2v/models/input_encoders/hf/seko_audio/audio_encoder.py:8-40]()\n\n---\n\n### AudioAdapter\n\nProjects audio features to model dimension and applies perceiver-based cross-attention.\n\n**Architecture:**\n\n```mermaid\ngraph LR\n    AudioFeatures[\"Audio Features\u003cbr/\u003e(1, T, 1024)\"]\n    TimeEmbed[\"Time Embedding\u003cbr/\u003esinusoidal(timestep)\"]\n    AudioProj[\"AudioProjection\u003cbr/\u003eMLP: 1024102432768\"]\n    Reshape[\"Reshape\u003cbr/\u003e(B, T, 128, 1024)\"]\n    PerceiverAttn[\"Perceiver Cross-Attention\u003cbr/\u003en_query_tokens per frame\"]\n    Output[\"Audio Context\u003cbr/\u003e(B, T, n_query, 1024)\"]\n    \n    AudioFeatures --\u003e AudioProj\n    TimeEmbed --\u003e AudioProj\n    AudioProj --\u003e Reshape\n    Reshape --\u003e PerceiverAttn\n    PerceiverAttn --\u003e Output\n```\n\n**Key Methods:**\n- **`forward_audio_proj(audio_feat, latent_length)`** [lightx2v/models/input_encoders/hf/seko_audio/audio_adapter.py:250-262](): Projects audio features using MLP and interpolates to match latent sequence length.\n- **`calculate_n_query_tokens(n_query_frame, n_tokens_per_frame)`** [lightx2v/models/input_encoders/hf/seko_audio/audio_adapter.py:68-74](): Computes number of query tokens per frame based on spatial dimensions.\n\n**Multi-Person Support:**\nWhen `person_mask_latens` is provided, the adapter:\n1. Aligns audio features with spatial masks using `align_hidden_states_and_mask()` [lightx2v/models/input_encoders/hf/seko_audio/audio_adapter.py:76-105]()\n2. Applies separate cross-attention for each person\n3. Aggregates outputs using mask-weighted sum\n\n**Sources:** [lightx2v/models/input_encoders/hf/seko_audio/audio_adapter.py:179-315]()\n\n---\n\n### EulerScheduler\n\nSpecialized scheduler for audio-conditioned generation, inheriting from `WanScheduler`.\n\n**Key Features:**\n- **Time embedding integration**: Computes sinusoidal embeddings for timesteps and passes them to `AudioAdapter` [lightx2v/models/schedulers/wan/audio/scheduler.py:29-35]()\n- **Mask-aware timesteps**: For Wan2.2 audio, applies per-token timesteps based on mask values [lightx2v/models/schedulers/wan/audio/scheduler.py:44-57]()\n- **Euler method**: One-step update: `x_{t+1} = x_t + (_{t+1} - _t) * model_output` [lightx2v/models/schedulers/wan/audio/scheduler.py:88-95]()\n\n**Sources:** [lightx2v/models/schedulers/wan/audio/scheduler.py:13-105]()\n\n---\n\n## Audio Segmentation with Overlap\n\nLong audio is split into overlapping segments for temporal coherence.\n\n### Segmentation Parameters\n\n| Parameter | Default | Description |\n|-----------|---------|-------------|\n| `target_video_length` | 81 | Max frames per segment |\n| `prev_frame_length` | 5 | Overlap frames |\n| `audio_frame_rate` | 1000 | Samples per frame (16000 Hz / 16 fps) |\n\n### Overlap Strategy Diagram\n\n```mermaid\ngraph LR\n    Seg1[\"Segment 1\u003cbr/\u003eframes 0-80\"]\n    Seg2[\"Segment 2\u003cbr/\u003eframes 76-156\"]\n    Seg3[\"Segment 3\u003cbr/\u003eframes 152+\"]\n    \n    Seg1 --\u003e|\"prev_video\u003cbr/\u003eframes 76-80\"| Seg2\n    Seg2 --\u003e|\"prev_video\u003cbr/\u003eframes 152-156\"| Seg3\n```\n\n### Implementation\n\n**Segment Index Calculation** [lightx2v/models/runners/wan/wan_audio_runner.py:237-251]():\n```python\ndef init_segments_idx(self, total_frame, clip_frame=81, overlap_frame=5):\n    start_end_list = []\n    for start in range(0, total_frame, clip_frame - overlap_frame):\n        end = min(start + clip_frame, total_frame)\n        # Align to VAE stride: (length - 1) divisible by 4\n        if ((end - start) - 1) % 4 != 0:\n            end = start + (((end - start) - 1) // 4) * 4 + 1\n        start_end_list.append((start, end))\n```\n\n**Previous Latent Preparation** [lightx2v/models/runners/wan/wan_audio_runner.py:464-516]():\n1. Extract last `prev_frame_length` frames from `prev_video`\n2. Apply noise/masking via `frame_preprocessor` (except Wan2.2 or f2v mode)\n3. Encode with VAE to get `prev_latents`\n4. Create `prev_mask`: 1 for conditioned frames, 0 for new frames\n\n**Frame Accumulation** [lightx2v/models/runners/wan/wan_audio_runner.py:589-623]():\n- Extract `useful_length = segment.end_frame - segment.start_frame`\n- Store in `gen_video_final[start:end]` or stream via `VAController`\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:237-251](), [lightx2v/models/runners/wan/wan_audio_runner.py:464-516](), [lightx2v/models/runners/wan/wan_audio_runner.py:589-623]()\n\n---\n\n## Multi-Person Support\n\nThe runner supports generating videos with multiple speakers, each with their own audio track and spatial mask.\n\n### Input Format\n\n**Directory Structure:**\n```\naudio_input_dir/\n config.json          # Configuration file\n person1.wav          # Audio track for person 1\n person1_mask.png     # Spatial mask for person 1\n person2.wav          # Audio track for person 2\n person2_mask.png     # Spatial mask for person 2\n```\n\n**config.json Format:**\n```json\n{\n  \"talk_objects\": [\n    {\"audio\": \"person1.wav\", \"mask\": \"person1_mask.png\"},\n    {\"audio\": \"person2.wav\", \"mask\": \"person2_mask.png\"}\n  ]\n}\n```\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:330-345]()\n\n### Processing Flow\n\n```mermaid\nsequenceDiagram\n    participant Runner as WanAudioRunner\n    participant AudioProc as AudioProcessor\n    participant Encoder as SekoAudioEncoder\n    participant Adapter as AudioAdapter\n    participant Model as WanAudioModel\n    \n    Runner-\u003e\u003eRunner: read_audio_input(audio_dir)\n    Runner-\u003e\u003eRunner: Load config.json, parse talk_objects\n    \n    loop For each person\n        Runner-\u003e\u003eAudioProc: load_audio(person_audio)\n        AudioProc--\u003e\u003eRunner: audio_array_i\n        Runner-\u003e\u003eRunner: process_single_mask(person_mask)\n        Note over Runner: Resize mask to (H/16, W/16)\u003cbr/\u003eConvert to binary int8\n    end\n    \n    Runner-\u003e\u003eAudioProc: Pad all audio arrays to max length\n    AudioProc--\u003e\u003eRunner: audio_arrays (N, T)\n    Runner-\u003e\u003eRunner: Concatenate mask_latents (N, 1, H/16, W/16)\n    \n    loop For each segment\n        loop For each person i\n            Runner-\u003e\u003eEncoder: infer(audio_arrays[i])\n            Encoder--\u003e\u003eRunner: audio_features_i (1, T, 1024)\n            Runner-\u003e\u003eAdapter: forward_audio_proj(audio_features_i)\n            Adapter--\u003e\u003eRunner: projected_features_i\n        end\n        \n        Runner-\u003e\u003eModel: infer(inputs + mask_latents)\n        Note over Model: AudioAdapter applies\u003cbr/\u003emask-weighted attention\u003cbr/\u003eper person\n        Model--\u003e\u003eRunner: latents\n    end\n```\n\n**Key Implementation:**\n\n1. **Audio loading** [lightx2v/models/runners/wan/wan_audio_runner.py:187-203](): All audio tracks are padded to the maximum length across all persons.\n\n2. **Mask processing** [lightx2v/models/runners/wan/wan_audio_runner.py:347-369]():\n   - Load mask image and convert to tensor\n   - Resize to match latent dimensions (`H/16, W/16`)\n   - Threshold to binary mask: `(mask \u003e 0).to(torch.int8)`\n\n3. **Audio encoding** [lightx2v/models/runners/wan/wan_audio_runner.py:569-575]():\n   - Each person's audio is encoded separately\n   - Features are projected and stacked: `torch.stack(features_list, dim=0)`  shape `(N, T, 128, 1024)`\n\n4. **Masked attention** [lightx2v/models/input_encoders/hf/seko_audio/audio_adapter.py:76-105]():\n   - Audio features are aligned with spatial positions using masks\n   - Cross-attention is computed per-person with mask-weighted aggregation\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:330-369](), [lightx2v/models/input_encoders/hf/seko_audio/audio_adapter.py:76-105]()\n\n---\n\n## Inference Pipeline Flow\n\nEnd-to-end method call sequence:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant WanAudioRunner\n    participant AudioProcessor\n    participant SekoAudioEncoderModel\n    participant AudioAdapter\n    participant WanAudioModel\n    participant EulerScheduler\n    participant Wan22VAE\n    \n    User-\u003e\u003eWanAudioRunner: run_pipeline(input_info)\n    \n    WanAudioRunner-\u003e\u003eWanAudioRunner: run_input_encoder()\u003cbr/\u003ewan_audio_runner.py:438\n    WanAudioRunner-\u003e\u003eWanAudioRunner: read_image_input()\u003cbr/\u003ewan_audio_runner.py:371\n    WanAudioRunner-\u003e\u003eWanAudioRunner: run_image_encoder()\u003cbr/\u003ewan_audio_runner.py:408\n    WanAudioRunner-\u003e\u003eWanAudioRunner: run_vae_encoder()\u003cbr/\u003ewan_audio_runner.py:424\n    WanAudioRunner-\u003e\u003eWanAudioRunner: read_audio_input()\u003cbr/\u003ewan_audio_runner.py:291\n    WanAudioRunner-\u003e\u003eAudioProcessor: segment_audio()\u003cbr/\u003ewan_audio_runner.py:209\n    AudioProcessor--\u003e\u003eWanAudioRunner: List[AudioSegment]\n    WanAudioRunner-\u003e\u003eWanAudioRunner: run_text_encoder()\u003cbr/\u003ewan_runner.py:243\n    \n    WanAudioRunner-\u003e\u003eWanAudioRunner: init_run()\u003cbr/\u003ewan_audio_runner.py:533\n    WanAudioRunner-\u003e\u003eEulerScheduler: prepare()\u003cbr/\u003escheduler.py:31\n    \n    loop For each segment\n        WanAudioRunner-\u003e\u003eWanAudioRunner: init_run_segment()\u003cbr/\u003ewan_audio_runner.py:553\n        \n        loop For each person\n            WanAudioRunner-\u003e\u003eSekoAudioEncoderModel: infer()\u003cbr/\u003eaudio_encoder.py:28\n            SekoAudioEncoderModel--\u003e\u003eWanAudioRunner: audio_features\n            WanAudioRunner-\u003e\u003eAudioAdapter: forward_audio_proj()\u003cbr/\u003eaudio_adapter.py:250\n            AudioAdapter--\u003e\u003eWanAudioRunner: projected_features\n        end\n        \n        WanAudioRunner-\u003e\u003eWanAudioRunner: prepare_prev_latents()\u003cbr/\u003ewan_audio_runner.py:464\n        WanAudioRunner-\u003e\u003eEulerScheduler: reset()\u003cbr/\u003escheduler.py:98\n        \n        WanAudioRunner-\u003e\u003eWanAudioRunner: run_segment()\u003cbr/\u003edefault_runner.py:173\n        \n        loop For each diffusion step\n            WanAudioRunner-\u003e\u003eEulerScheduler: step_pre()\u003cbr/\u003escheduler.py:63\n            WanAudioRunner-\u003e\u003eWanAudioModel: infer()\u003cbr/\u003emodel.py:160\n            WanAudioRunner-\u003e\u003eEulerScheduler: step_post()\u003cbr/\u003escheduler.py:72\n        end\n        \n        WanAudioRunner-\u003e\u003eWan22VAE: decode()\u003cbr/\u003evae_2_2.py:280\n        Wan22VAE--\u003e\u003eWanAudioRunner: gen_video\n        \n        WanAudioRunner-\u003e\u003eWanAudioRunner: end_run_segment()\u003cbr/\u003ewan_audio_runner.py:589\n    end\n    \n    WanAudioRunner--\u003e\u003eUser: video + audio\n```\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:461-732](), [lightx2v/models/runners/default_runner.py:173-206](), [lightx2v/models/schedulers/wan/audio/scheduler.py:31-101]()\n\n---\n\n## Configuration Parameters\n\n### Core Configuration\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `model_cls` | str | `\"wan2.2_audio\"` or `\"seko_talk\"` | Model class identifier |\n| `task` | str | `\"s2v\"` or `\"rs2v\"` | Task type (speech-to-video, reference-speech-to-video) |\n| `target_video_length` | int | 81 | Maximum frames per segment |\n| `infer_steps` | int | 40-50 | Number of diffusion steps |\n| `sample_guide_scale` | float | 4.0 | CFG scale |\n| `sample_shift` | float | 1.0 | Noise schedule shift |\n\n### Audio Configuration\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `audio_sr` | int | 16000 | Audio sample rate (Hz) |\n| `target_fps` | int | 16 | Video frame rate |\n| `prev_frame_length` | int | 5 | Overlap frames between segments |\n| `video_duration` | float | 5.0 | Target video duration (seconds) |\n| `audio_encoder_path` | str | `{model_path}/TencentGameMate-chinese-hubert-large` | Path to audio encoder |\n| `adapter_model_path` | str | `{model_path}/audio_adapter_model.safetensors` | Path to audio adapter weights |\n\n### Image Processing\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `resize_mode` | str | `\"adaptive\"` | Resize strategy: `\"adaptive\"`, `\"fixed_shape\"`, `\"fixed_min_side\"` |\n| `bucket_shape` | dict | See below | Aspect ratio buckets for adaptive resize |\n| `fixed_area` | str | `\"480p\"` or `\"720p\"` | Resolution for fixed area modes |\n\n**Default Bucket Configuration:**\n```python\n{\n    0.667: [[480, 832], [544, 960], [720, 1280]],  # Portrait\n    1.500: [[832, 480], [960, 544], [1280, 720]],  # Landscape\n    1.000: [[480, 480], [576, 576], [704, 704], [960, 960]]  # Square\n}\n```\n\n### Optimization Options\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `use_stream_vae` | bool | False | Enable streaming VAE decode |\n| `f2v_process` | bool | False | First-frame-to-video mode (no noise on prev frames) |\n| `audio_encoder_cpu_offload` | bool | False | Offload audio encoder to CPU |\n| `audio_adapter_cpu_offload` | bool | False | Offload audio adapter to CPU |\n| `adapter_quantized` | bool | False | Use quantized adapter |\n| `adapter_quant_scheme` | str | None | Quantization scheme: `\"fp8\"`, `\"int8\"`, `\"mxfp4\"` |\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:278-284](), [lightx2v/utils/set_config.py:15-34]()\n\n---\n\n## Streaming Inference with VAController\n\n`VAController` enables streaming audio input and progressive video output.\n\n### Streaming Architecture\n\n```mermaid\ngraph LR\n    VAReader[\"VAReader\u003cbr/\u003eva_controller.py\"]\n    next_control[\"next_control()\"]\n    get_audio[\"get_audio_segment()\"]\n    run_main[\"run_main()\u003cbr/\u003ewan_audio_runner.py:661\"]\n    init_run_segment[\"init_run_segment()\"]\n    run_segment[\"run_segment()\"]\n    end_run_segment_stream[\"end_run_segment_stream()\u003cbr/\u003ewan_audio_runner.py:633\"]\n    pub_livestream[\"pub_livestream()\"]\n    VARecorder[\"VARecorder\u003cbr/\u003eva_controller.py\"]\n    \n    VAReader --\u003e get_audio\n    get_audio --\u003e run_main\n    run_main --\u003e next_control\n    next_control --\u003e init_run_segment\n    init_run_segment --\u003e run_segment\n    run_segment --\u003e end_run_segment_stream\n    end_run_segment_stream --\u003e pub_livestream\n    pub_livestream --\u003e VARecorder\n```\n\n### run_main() Streaming Loop\n\n[lightx2v/models/runners/wan/wan_audio_runner.py:661-732]()\n\n```python\ndef run_main(self):\n    self.va_controller = VAController(self)\n    if self.va_controller.reader is None:\n        return super().run_main()  # Fixed segments mode\n    \n    self.va_controller.start()\n    self.init_run()\n    \n    segment_idx = 0\n    while True:\n        control = self.va_controller.next_control()\n        \n        if control.action == \"blank_to_voice\":\n            self.prev_video = control.data\n        elif control.action == \"switch_image\":\n            self.input_info.image_path = control.data\n            self.inputs = self.run_input_encoder()\n            self.prev_video = None\n        elif control.action == \"wait\":\n            time.sleep(0.01)\n            continue\n        \n        audio_array, valid_duration = self.va_controller.reader.get_audio_segment()\n        if audio_array is None:\n            continue\n        \n        self.init_run_segment(segment_idx, audio_array)\n        latents = self.run_segment(segment_idx)\n        \n        if self.config.get(\"use_stream_vae\", False):\n            self.end_run_segment_stream(latents, valid_duration)\n        else:\n            self.gen_video = self.run_vae_decoder(latents)\n            self.end_run_segment(segment_idx, valid_duration)\n        \n        segment_idx += 1\n```\n\n**Control Actions:**\n- `\"blank_to_voice\"`: Initialize `prev_video` for voice activation\n- `\"switch_image\"`: Change reference image mid-stream\n- `\"wait\"`: No audio available, continue polling\n\n**Pause Support:** `self.check_stop()` checks `self.pause_signal` for graceful interruption\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:661-732](), [lightx2v/deploy/common/va_controller.py]()\n\n---\n\n## Model-Specific Variants\n\nThe runner supports two primary model classes:\n\n### Wan2.2 Audio\n\n**Configuration:** `model_cls = \"wan2.2_audio\"`\n\n**Features:**\n- Uses `prev_latents` and `prev_mask` for temporal conditioning [lightx2v/models/schedulers/wan/audio/scheduler.py:22-24]()\n- Mask-aware timestep embedding: each token gets its own timestep value [lightx2v/models/schedulers/wan/audio/scheduler.py:38-57]()\n- No noise/masking applied to previous frames [lightx2v/models/runners/wan/wan_audio_runner.py:491-498]()\n- Scheduler reset uses `prev_latents` directly [lightx2v/models/schedulers/wan/audio/scheduler.py:98-101]()\n\n### Seko Talk (Wan2.1 variant)\n\n**Configuration:** `model_cls = \"seko_talk\"` (registry name)\n\n**Features:**\n- Applies noise and masking to previous frames via `FramePreprocessorTorchVersion` [lightx2v/models/runners/wan/wan_audio_runner.py:474-476]()\n- Concatenates `[latents, prev_mask, prev_latents]` during pre-infer [lightx2v/models/networks/wan/infer/audio/pre_infer.py:62-63]()\n- Standard Euler scheduler without per-token timesteps\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:474-498](), [lightx2v/models/schedulers/wan/audio/scheduler.py:22-24]()\n\n---\n\n## Integration with WanAudioModel\n\nThe `WanAudioModel` extends `WanModel` with audio-specific inference classes:\n\n| Component | Class | Purpose |\n|-----------|-------|---------|\n| `pre_infer_class` | `WanAudioPreInfer` | Processes audio features, previous latents, and reference images |\n| `transformer_infer_class` | `WanAudioTransformerInfer` | Adds post-adapter phase for audio cross-attention |\n| `post_infer_class` | `WanAudioPostInfer` | Unpatchifies valid tokens (excluding reference frames) |\n| `transformer_weight_class` | `WanAudioTransformerWeights` | Includes audio adapter weights |\n\n**Audio Cross-Attention in Transformer Blocks:**\n\nEach transformer block has 4 phases (vs. 3 in standard WanModel):\n1. **Self-Attention Phase:** Spatial-temporal attention within video tokens\n2. **Cross-Attention Phase:** Text and image cross-attention\n3. **FFN Phase:** Feed-forward network\n4. **Post-Adapter Phase:** Audio cross-attention using perceiver pattern [lightx2v/models/networks/wan/infer/audio/transformer_infer.py:29-73]()\n\nThe audio adapter computes attention between video hidden states (queries) and audio features (keys/values), with optional mask-based spatial alignment for multi-person scenarios.\n\n**Sources:** [lightx2v/models/networks/wan/audio_model.py:55-59](), [lightx2v/models/networks/wan/infer/audio/transformer_infer.py:18-73]()\n\n---\n\n## Usage Examples\n\n### Single-Person S2V\n\n```python\nfrom lightx2v.models.runners.wan.wan_audio_runner import WanAudioRunner\nfrom lightx2v.utils.input_info import S2VInputInfo\n\nconfig = {\n    \"model_cls\": \"seko_talk\",\n    \"task\": \"s2v\",\n    \"model_path\": \"/path/to/model\",\n    \"target_video_length\": 81,\n    \"infer_steps\": 40,\n    \"audio_sr\": 16000,\n    \"target_fps\": 16,\n    \"prev_frame_length\": 5,\n    \"resize_mode\": \"adaptive\",\n}\n\nrunner = WanAudioRunner(config)\nrunner.init_modules()\n\ninput_info = S2VInputInfo(\n    prompt=\"A person speaking\",\n    image_path=\"reference.jpg\",\n    audio_path=\"speech.wav\",\n    save_result_path=\"output.mp4\",\n    seed=42,\n)\n\nrunner.run_pipeline(input_info)\n```\n\n### Multi-Person S2V\n\n**audio_dir/config.json:**\n```json\n{\n  \"talk_objects\": [\n    {\"audio\": \"person1.wav\", \"mask\": \"person1_mask.png\"},\n    {\"audio\": \"person2.wav\", \"mask\": \"person2_mask.png\"}\n  ]\n}\n```\n\n**Code:**\n```python\ninput_info = S2VInputInfo(\n    prompt=\"Two people talking\",\n    image_path=\"group.jpg\",\n    audio_path=\"audio_dir\",  # Directory with config.json\n    save_result_path=\"output.mp4\",\n    seed=42,\n)\n\nrunner.run_pipeline(input_info)\n```\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:330-345](), [lightx2v/infer.py:78-156]()\n\n---\n\n## Performance Considerations\n\n### Memory Usage\n\n| Component | Memory Impact | Optimization |\n|-----------|---------------|--------------|\n| Audio Encoder | ~1-2 GB | Enable `audio_encoder_cpu_offload` |\n| Audio Adapter | ~0.5-1 GB | Enable `adapter_quantized` with `adapter_quant_scheme=\"fp8\"` |\n| Previous Latents | ~100 MB per segment | Minimize `prev_frame_length` |\n| Multi-Person Masks | ~5 MB per person | Use int8 masks |\n\n### Computation Time\n\nTypical inference times on RTX 4090 (single GPU):\n- **5-second video (80 frames, 1 person):** ~120-180 seconds with 40 steps\n- **Per-segment overhead:** ~2-3 seconds for audio encoding + previous latent encoding\n- **Streaming latency:** ~1-2 seconds per segment (with `use_stream_vae=True`)\n\n**Optimization Strategies:**\n1. Use distilled models (4 steps) for ~10x speedup - see [WanDistillRunner](#5.4)\n2. Enable `use_stream_vae=True` for progressive output\n3. Reduce `infer_steps` to 30-35 with minimal quality loss\n4. Use quantized audio adapter: `adapter_quant_scheme=\"fp8\"`\n5. Enable model quantization: `dit_quantized=True` with `dit_quant_scheme=\"fp8-vllm\"`\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:751-788](), System diagrams"])</script><script>self.__next_f.push([1,"30:T8e4c,"])</script><script>self.__next_f.push([1,"# WanRunner - Text-to-Video and Image-to-Video\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis page documents the `WanRunner` class for standard Text-to-Video (T2V) and Image-to-Video (I2V) generation workflows. `WanRunner` extends `DefaultRunner` [lightx2v/models/runners/wan/wan_runner.py:63-66]() and serves as the base implementation for the WAN model family (wan2.1, wan2.2).\n\n**Key Capabilities:**\n- **T2V**: Generates videos from text prompts using T5 text encoder\n- **I2V**: Conditions generation on reference image via CLIP image encoder + VAE latent conditioning\n- **FLF2V**: First-Last-Frame-to-Video interpolation with dual frame conditioning\n\n**Model Variants:**\n- `wan2.1`: Standard 14B parameter model [lightx2v/models/runners/wan/wan_runner.py:63]()\n- `wan2.2`: Enhanced variant with improved conditioning [lightx2v/models/networks/wan/model.py:33-54]()\n- `wan2.2_moe`: Mixture-of-Experts with high/low noise model switching [lightx2v/models/runners/wan/wan_runner.py:581-626]()\n\n**Related Pages:**\n- Page 5.2: `WanAudioRunner` for audio-driven video (S2V) - extends `WanRunner` with audio encoding\n- Page 5.1: WAN model family overview and DiT architecture\n- Page 5.4: `WanDistillRunner` for 4-step distilled inference\n- Page 4.2: Three-stage inference pipeline (Pre-Infer, Transformer-Infer, Post-Infer)\n\nThis page focuses on the **non-audio video generation tasks** handled by the base `WanRunner` class.\n\nSources: [lightx2v/models/runners/wan/wan_runner.py:63-66](), [lightx2v/models/runners/default_runner.py:55-98]()\n\n---\n\n## Task Definitions and Input Formats\n\n### Input Info Dataclasses\n\nLightX2V defines task-specific input structures that encapsulate all parameters needed for generation. These dataclasses are defined in [lightx2v/utils/input_info.py]() and are used throughout the inference pipeline.\n\n**T2VInputInfo Fields:**\n- `seed`: Random seed for reproducibility\n- `prompt`: Text description of desired video\n- `prompt_enhanced`: Enhanced prompt (if prompt enhancer service is used)\n- `negative_prompt`: Qualities to avoid in generation\n- `latent_shape`: Shape of latent tensor `[C, T, H, W]` (computed during initialization)\n- `target_shape`: Output video resolution `[H, W]` (can be overridden via CLI)\n- `save_result_path`: Output file path (if None, video is not saved)\n- `return_result_tensor`: Whether to return tensor instead of saving (used by ComfyUI)\n\n**I2VInputInfo Additional Fields:**\n- Inherits all T2V fields\n- `image_path`: Path to conditioning image (can be PIL.Image, file path, or URL)\n- `original_size`: Original image dimensions `(width, height)` tuple\n\n**Flf2vInputInfo Additional Fields:**\n- Inherits all I2V fields\n- `last_frame_path`: Path to last frame for interpolation\n\n### Task Selection and Pipeline Routing\n\nTasks are specified via the `--task` CLI argument [lightx2v/infer.py:78]():\n```bash\n--task {t2v, i2v, flf2v, vace, animate, s2v, rs2v, t2i, i2i, t2av, i2av}\n```\n\n`DefaultRunner.init_modules()` routes tasks to appropriate encoder methods [lightx2v/models/runners/default_runner.py:78-93]():\n\n```python\nif self.config[\"task\"] == \"i2v\":\n    self.run_input_encoder = self._run_input_encoder_local_i2v\nelif self.config[\"task\"] == \"flf2v\":\n    self.run_input_encoder = self._run_input_encoder_local_flf2v\nelif self.config[\"task\"] == \"t2v\":\n    self.run_input_encoder = self._run_input_encoder_local_t2v\nelif self.config[\"task\"] == \"vace\":\n    self.run_input_encoder = self._run_input_encoder_local_vace\nelif self.config[\"task\"] == \"animate\":\n    self.run_input_encoder = self._run_input_encoder_local_animate\nelif self.config[\"task\"] in [\"s2v\", \"rs2v\"]:\n    self.run_input_encoder = self._run_input_encoder_local_s2v\n```\n\nThese methods are called by `run_pipeline()` [lightx2v/models/runners/default_runner.py:462-476]() to prepare encoder outputs before entering the diffusion loop.\n\nSources: [lightx2v/utils/input_info.py](), [lightx2v/infer.py:78](), [lightx2v/models/runners/default_runner.py:70-93]()\n\n---\n\n## T2V Pipeline Flow\n\n### T2V Encoder Pipeline\n\n```mermaid\ngraph TB\n    Input[\"T2VInputInfo\u003cbr/\u003e(prompt, seed, target_shape)\"]\n    CalcLatent[\"Calculate Latent Shape\u003cbr/\u003eget_latent_shape_with_target_hw()\"]\n    T5[\"T5 Text Encoder\u003cbr/\u003erun_text_encoder()\"]\n    Output[\"Encoder Output\u003cbr/\u003e{text_encoder_output,\u003cbr/\u003eimage_encoder_output: None}\"]\n    \n    Input --\u003e CalcLatent\n    CalcLatent --\u003e T5\n    T5 --\u003e Output\n    \n    subgraph \"Text Encoding\"\n        T5_Load[\"Load T5 Model\u003cbr/\u003e(google/umt5-xxl)\"]\n        T5_Tokenize[\"Tokenize Prompt\"]\n        T5_Encode[\"Encode to Context\u003cbr/\u003e[1, 512, 4096]\"]\n        \n        T5_Load --\u003e T5_Tokenize\n        T5_Tokenize --\u003e T5_Encode\n    end\n    \n    T5 -.-\u003e T5_Load\n```\n\n**T2V Encoder Implementation** [lightx2v/models/runners/default_runner.py:290-299]():\n\nThe T2V pipeline is the simplest, requiring only text encoding:\n\n```python\ndef _run_input_encoder_local_t2v(self):\n    self.input_info.latent_shape = self.get_latent_shape_with_target_hw()\n    text_encoder_output = self.run_text_encoder(self.input_info)\n    return {\n        \"text_encoder_output\": text_encoder_output,\n        \"image_encoder_output\": None,\n    }\n```\n\n**Steps:**\n1. **Latent Shape Calculation**: `get_latent_shape_with_target_hw()` computes output dimensions from `target_height` and `target_width` config\n2. **Text Encoding**: `run_text_encoder()` processes prompt through T5EncoderModel\n3. **No Image Encoding**: Returns `None` for `image_encoder_output`\n\n**Text Encoder Output** [lightx2v/models/runners/wan/wan_runner.py:243-281]():\n\nThe T5 text encoder (UMT5-XXL) processes prompts through the `T5EncoderModel` class:\n\n1. **Tokenizer Path**: `{model_path}/google/umt5-xxl` [lightx2v/models/runners/wan/wan_runner.py:126]()\n2. **Model Loading**: Supports quantized (INT8/FP8) or BF16 checkpoints [lightx2v/models/runners/wan/wan_runner.py:128-141]()\n   - Default checkpoint: `models_t5_umt5-xxl-enc-bf16.pth`\n   - Quantized variants: `models_t5_umt5-xxl-enc-{int8|fp8}.pth`\n3. **Token Length**: Fixed at 512 tokens via `text_len` config [lightx2v/models/runners/wan/wan_runner.py:143]()\n4. **Padding Strategy**: Shorter prompts zero-padded to 512 tokens [lightx2v/models/runners/wan/wan_runner.py:257-265]()\n\n**Output Structure:**\n- `context`: Positive prompt embeddings `[1, 512, 4096]` (batch, sequence_length, hidden_dim)\n- `context_null`: Negative prompt embeddings (only if CFG enabled) [lightx2v/models/runners/wan/wan_runner.py:266-274]()\n\n**CFG Parallel Mode**: When `cfg_parallel=True`, GPU rank 0 computes `context` while GPU rank 1 computes `context_null` simultaneously for 2x throughput during cross-attention [lightx2v/models/runners/wan/wan_runner.py:252-262]().\n\nSources: [lightx2v/models/runners/default_runner.py:289-297](), [lightx2v/models/runners/wan/wan_runner.py:119-281]()\n\n---\n\n## I2V Pipeline Flow\n\n### I2V Encoder Pipeline\n\n```mermaid\ngraph TB\n    Input[\"I2VInputInfo\u003cbr/\u003e(prompt, image_path, seed)\"]\n    ReadImg[\"Read and Resize Image\u003cbr/\u003eread_image_input()\"]\n    \n    subgraph \"Parallel Encoding\"\n        CLIP[\"CLIP Image Encoder\u003cbr/\u003erun_image_encoder()\"]\n        VAE[\"VAE Encoder\u003cbr/\u003erun_vae_encoder()\"]\n        T5[\"T5 Text Encoder\u003cbr/\u003erun_text_encoder()\"]\n    end\n    \n    Combine[\"Combine Encoder Outputs\u003cbr/\u003eget_encoder_output_i2v()\"]\n    Output[\"Encoder Output\u003cbr/\u003e{text_encoder_output,\u003cbr/\u003eimage_encoder_output}\"]\n    \n    Input --\u003e ReadImg\n    ReadImg --\u003e CLIP\n    ReadImg --\u003e VAE\n    ReadImg --\u003e T5\n    \n    CLIP --\u003e Combine\n    VAE --\u003e Combine\n    T5 --\u003e Combine\n    Combine --\u003e Output\n    \n    subgraph \"Image Preprocessing\"\n        Resize[\"Adaptive Resize\u003cbr/\u003eresize_image()\"]\n        Normalize[\"Normalize [-1, 1]\u003cbr/\u003eto_tensor().sub(0.5).div(0.5)\"]\n        Latent[\"Calculate Latent Shape\u003cbr/\u003eBased on VAE stride\"]\n        \n        Resize --\u003e Normalize\n        Normalize --\u003e Latent\n    end\n    \n    ReadImg -.-\u003e Resize\n```\n\n**I2V Encoder Implementation** [lightx2v/models/runners/default_runner.py:279-288]():\n\nThe I2V pipeline adds visual conditioning through three encoders:\n\n```python\ndef _run_input_encoder_local_i2v(self):\n    img, img_ori = self.read_image_input(self.input_info.image_path)\n    clip_encoder_out = self.run_image_encoder(img)\n    vae_encode_out, latent_shape = self.run_vae_encoder(img_ori if self.vae_encoder_need_img_original else img)\n    self.input_info.latent_shape = latent_shape\n    text_encoder_output = self.run_text_encoder(self.input_info)\n    return self.get_encoder_output_i2v(clip_encoder_out, vae_encode_out, text_encoder_output, img)\n```\n\n**Steps:**\n1. **Image Preprocessing**: `read_image_input()` loads image, applies adaptive resize, normalizes to `[-1, 1]`\n2. **CLIP Encoding**: `run_image_encoder()` extracts semantic features `[257, 1280]` via CLIPModel\n3. **VAE Encoding**: `run_vae_encoder()` encodes first frame to latents with mask\n4. **T5 Encoding**: `run_text_encoder()` same as T2V\n5. **Combine**: Returns dictionary with all encoder outputs\n\nSources: [lightx2v/models/runners/default_runner.py:276-284](), [lightx2v/models/runners/default_runner.py:241-273]()\n\n---\n\n## Image Conditioning Mechanisms\n\n### VAE-Based Frame Initialization\n\nThe VAE encoding process for I2V creates a masked latent tensor that conditions the first frame while leaving subsequent frames to be generated:\n\n```mermaid\ngraph LR\n    subgraph \"Input Preparation\"\n        FirstFrame[\"First Frame\u003cbr/\u003e[1, 3, H, W]\"]\n        Interpolate[\"Interpolate to\u003cbr/\u003eTarget Size\"]\n        ZeroFrames[\"Zero Frames\u003cbr/\u003e[3, T-1, H, W]\"]\n    end\n    \n    subgraph \"Masking\"\n        Mask[\"Mask Tensor\u003cbr/\u003e[1, T, lat_h, lat_w]\"]\n        FirstMask[\"mask[:, 0] = 1\u003cbr/\u003e(condition)\"]\n        RestMask[\"mask[:, 1:] = 0\u003cbr/\u003e(generate)\"]\n    end\n    \n    subgraph \"VAE Encoding\"\n        Concat[\"Concatenate\u003cbr/\u003e[C, T, H, W]\"]\n        Encode[\"VAE Encode\u003cbr/\u003e3D Causal Conv\"]\n        AddMask[\"Concatenate Mask\u003cbr/\u003eto Latents\"]\n        Output[\"VAE Output\u003cbr/\u003e[C+4, T_lat, lat_h, lat_w]\"]\n    end\n    \n    FirstFrame --\u003e Interpolate\n    Interpolate --\u003e Concat\n    ZeroFrames --\u003e Concat\n    \n    Mask --\u003e FirstMask\n    FirstMask --\u003e RestMask\n    RestMask --\u003e AddMask\n    \n    Concat --\u003e Encode\n    Encode --\u003e AddMask\n    AddMask --\u003e Output\n```\n\n**VAE Encoding Implementation** [lightx2v/models/runners/wan/wan_runner.py:414-461]():\n\nThe VAE encoding creates a structured conditioning signal:\n\n1. **Resize First Frame**: Interpolates input image to match calculated latent dimensions\n2. **Create Sequence**: Concatenates first frame with zeros for remaining `target_video_length - 1` frames\n3. **Build Mask**: Creates binary mask where:\n   - `msk[:, 0] = 1`: First frame is known (condition on this)\n   - `msk[:, 1:] = 0`: Remaining frames to be generated\n4. **Mask Rearrangement**: Reshapes mask to match 3D causal structure with 4-frame grouping [lightx2v/models/runners/wan/wan_runner.py:429-431]()\n5. **VAE Encoding**: Passes through causal 3D VAE encoder\n6. **Concatenate Mask**: Appends mask channels to encoded latents\n\nThe resulting `vae_encoder_out` tensor has shape `[C+4, T_lat, lat_h, lat_w]` where the first 4 channels are mask information.\n\n### CLIP Visual Features\n\n**CLIP Image Encoding** [lightx2v/models/runners/wan/wan_runner.py:289-300]():\n\nThe CLIP image encoder (`CLIPModel`) provides semantic visual features used in cross-attention during I2V generation:\n\n**\"CLIP Image Encoder Architecture\"**\n\n```mermaid\ngraph LR\n    Image[\"Input Image\u003cbr/\u003e[1, 3, H, W]\"]\n    CLIPModel[\"CLIPModel instance\u003cbr/\u003eXLM-RoBERTa ViT-H/14\"]\n    VisualMethod[\"visual() method\u003cbr/\u003e31 transformer blocks\"]\n    Features[\"CLIP Features\u003cbr/\u003e[257, 1280]\"]\n    CrossAttn[\"Cross-Attention\u003cbr/\u003ein WAN transformer\"]\n    \n    Image --\u003e CLIPModel\n    CLIPModel --\u003e VisualMethod\n    VisualMethod --\u003e Features\n    Features --\u003e CrossAttn\n    \n    subgraph \"Configuration\"\n        CheckpointPath[\"models_clip_open-clip-\u003cbr/\u003exlm-roberta-large-vit-huge-14.pth\"]\n        QuantConfig[\"clip_quantized config\u003cbr/\u003eINT8/FP8 variants\"]\n        OffloadConfig[\"clip_cpu_offload config\u003cbr/\u003eCPU memory offload\"]\n    end\n```\n\n**Implementation Details:**\n- **Architecture**: XLM-RoBERTa Vision Transformer Huge/14 (ViT-H/14)\n- **Checkpoint Name**: `models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth` [lightx2v/models/runners/wan/wan_runner.py:102-103]()\n- **Output Shape**: `[257, 1280]` - 257 patch tokens (1 CLS + 1616 patches) with 1280-dimensional embeddings\n- **Block Count**: 31 transformer blocks configurable via `use_31_block` [lightx2v/models/runners/wan/wan_runner.py:113]()\n- **Quantization**: INT8/FP8 quantized checkpoints available [lightx2v/models/runners/wan/wan_runner.py:91-97]()\n- **Memory Optimization**: CPU offloading via `clip_cpu_offload` config flag [lightx2v/models/runners/wan/wan_runner.py:85-89]()\n\nThe CLIP features are concatenated with text encoder output and used in cross-attention layers [lightx2v/models/networks/wan/infer/transformer_infer.py:250-259](), providing semantic guidance complementary to VAE's pixel-level conditioning.\n\nSources: [lightx2v/models/runners/wan/wan_runner.py:81-117](), [lightx2v/models/runners/wan/wan_runner.py:289-300]()\n\n### Dual Conditioning Strategy\n\nThe I2V pipeline uses two complementary conditioning signals:\n\n| Encoder | Purpose | Shape | Usage |\n|---------|---------|-------|-------|\n| VAE | Pixel-level first frame | `[C+4, T_lat, lat_h, lat_w]` | Direct latent initialization |\n| CLIP | Semantic content | `[257, 1280]` | Cross-attention conditioning |\n\nThis dual approach ensures both spatial coherence (VAE) and semantic consistency (CLIP) in the generated video.\n\nSources: [lightx2v/models/runners/wan/wan_runner.py:365-461](), [lightx2v/models/runners/wan/wan_runner.py:86-122]()\n\n---\n\n## First-Last-Frame-to-Video (FLF2V)\n\n### FLF2V Pipeline\n\nFLF2V extends I2V by conditioning on both the first and last frames, enabling interpolation between two keyframes:\n\n```mermaid\ngraph TB\n    Input[\"Flf2vInputInfo\u003cbr/\u003e(image_path, last_frame_path)\"]\n    \n    subgraph \"Frame Reading\"\n        FirstFrame[\"Read First Frame\u003cbr/\u003eread_image_input()\"]\n        LastFrame[\"Read Last Frame\u003cbr/\u003eread_image_input()\"]\n        ResizeCheck[\"Resize Last Frame\u003cbr/\u003eif size mismatch\"]\n    end\n    \n    subgraph \"Encoding\"\n        CLIP[\"CLIP Encoder\u003cbr/\u003eBoth Frames\u003cbr/\u003e[2, 257, 1280]\"]\n        VAE[\"VAE Encoder\u003cbr/\u003eFirst + Zeros + Last\"]\n        T5[\"T5 Text Encoder\"]\n    end\n    \n    Output[\"Encoder Output\"]\n    \n    Input --\u003e FirstFrame\n    Input --\u003e LastFrame\n    FirstFrame --\u003e ResizeCheck\n    LastFrame --\u003e ResizeCheck\n    \n    ResizeCheck --\u003e CLIP\n    ResizeCheck --\u003e VAE\n    Input --\u003e T5\n    \n    CLIP --\u003e Output\n    VAE --\u003e Output\n    T5 --\u003e Output\n```\n\n**FLF2V Implementation** [lightx2v/models/runners/default_runner.py:298-307]():\n\nKey differences from I2V:\n\n1. **Dual Frame Reading**: Reads both `image_path` and `last_frame_path`\n2. **Size Matching**: If frames have different dimensions, resizes and center-crops last frame to match first [lightx2v/models/runners/wan/wan_runner.py:402-410]()\n3. **CLIP Encoding**: Processes both frames, returning `[2, 257, 1280]` features\n4. **VAE Encoding**: Creates sequence `[first_frame, zeros..., last_frame]` where middle frames are generated\n5. **Mask Structure**: Sets `msk[:, 0] = 1` and `msk[:, -1] = 1` for both boundary frames [lightx2v/models/runners/wan/wan_runner.py:424-426]()\n\nThis enables temporally smooth interpolation between specified start and end states.\n\nSources: [lightx2v/models/runners/default_runner.py:298-307](), [lightx2v/models/runners/wan/wan_runner.py:401-412]()\n\n---\n\n## Configuration and Resolution Handling\n\n### Adaptive Resolution System\n\nLightX2V uses an adaptive bucketing system to handle various aspect ratios efficiently:\n\n**Resolution Buckets** [lightx2v/models/runners/default_runner.py:28-32]():\n\n```python\nbucket_config = {\n    0.667: [[480, 832], [544, 960], [720, 1280]],  # Portrait\n    1.500: [[832, 480], [960, 544], [1280, 720]],  # Landscape  \n    1.000: [[480, 480], [576, 576], [960, 960]],   # Square\n}\n```\n\n**Resolution Selection Logic** [lightx2v/models/runners/default_runner.py:39-53]():\n\n1. Calculate input image aspect ratio\n2. Find closest aspect ratio bucket\n3. Select resolution tier based on input area\n4. Apply isotropic crop-resize to target dimensions\n\n### Latent Shape Calculation\n\n**For T2V** [lightx2v/models/runners/wan/wan_runner.py:482-492]():\n- Uses `target_height` and `target_width` from config\n- Calculates latent dimensions: `latent_h = target_h / vae_stride[1]`\n- Temporal dimension: `(target_video_length - 1) // vae_stride[0] + 1`\n\n**For I2V** [lightx2v/models/runners/wan/wan_runner.py:365-412]():\n- Derives from input image dimensions\n- Applies optimal patching for sequence parallelism [lightx2v/models/runners/wan/wan_runner.py:391]()\n- Adjusts for grid splitting in distributed mode [lightx2v/models/runners/wan/wan_runner.py:307-357]()\n\n### Key Configuration Parameters\n\n| Parameter | T2V | I2V | Description | Default Value |\n|-----------|-----|-----|-------------|---------------|\n| `target_video_length` |  |  | Number of frames to generate | 81 |\n| `target_height` |  | - | Output height (T2V only) | 480 |\n| `target_width` |  | - | Output width (T2V only) | 832 |\n| `resize_mode` | - |  | Image resize strategy | \"adaptive\" |\n| `use_image_encoder` | - |  | Enable CLIP encoding | True |\n| `vae_stride` |  |  | Temporal/spatial compression | [4, 16, 16] |\n| `patch_size` |  |  | Transformer patch size | [1, 2, 2] |\n| `num_channels_latents` |  |  | VAE latent channels | 16 |\n| `infer_steps` |  |  | Number of diffusion steps | 40-50 |\n| `sample_guide_scale` |  |  | CFG scale | 4.0-6.0 |\n| `sample_shift` |  |  | Flow shift parameter | 7.0 |\n\n**Resolution Mode Options** [lightx2v/models/runners/default_runner.py:36-52]():\n- `\"adaptive\"`: Select from predefined buckets based on aspect ratio\n- `\"fixed_shape\"`: Force specific dimensions\n- `\"keep_ratio_fixed_area\"`: Maintain aspect ratio with fixed pixel count\n- `\"fixed_min_side\"`: Scale to fixed minimum side length\n\nSources: [lightx2v/models/runners/default_runner.py:24-53](), [lightx2v/models/runners/wan/wan_runner.py:365-492]()\n\n---\n\n## Diffusion Loop Execution\n\n### Three-Stage Inference Pipeline\n\nThe WAN transformer uses a three-stage architecture for each diffusion step [lightx2v/models/networks/wan/model.py:81-84]():\n\n**Stage Architecture Diagram**\n\n```mermaid\ngraph TB\n    Inputs[\"inputs dict\u003cbr/\u003e{text_encoder_output,\u003cbr/\u003eimage_encoder_output}\"]\n    \n    subgraph \"Stage 1: WanPreInfer\"\n        PreInfer[\"WanPreInfer.infer()\"]\n        PatchEmbed[\"patch_embedding\u003cbr/\u003eConv3D (1,2,2) patches\"]\n        TimeEmbed[\"time_embedding\u003cbr/\u003esinusoidal + MLP\"]\n        TextEmbed[\"text_embedding\u003cbr/\u003eproject to model dim\"]\n        ImgEmbed[\"img_emb projection\u003cbr/\u003e(I2V only)\"]\n        RoPE[\"Compute RoPE\u003cbr/\u003ecos/sin cache\"]\n        PreOutput[\"WanPreInferModuleOutput\u003cbr/\u003e{x, embed, embed0,\u003cbr/\u003econtext, cos_sin, grid_sizes}\"]\n    end\n    \n    subgraph \"Stage 2: WanTransformerInfer\"\n        TransInfer[\"WanTransformerInfer.infer()\"]\n        Loop[\"For each block (40 blocks)\"]\n        SelfAttn[\"Self-Attention\u003cbr/\u003ewith RoPE\"]\n        CrossAttn[\"Cross-Attention\u003cbr/\u003eText + Image (I2V)\"]\n        FFN[\"Feed-Forward\u003cbr/\u003ewith AdaLN modulation\"]\n        TransOutput[\"Denoised features\"]\n    end\n    \n    subgraph \"Stage 3: WanPostInfer\"\n        PostInfer[\"WanPostInfer.infer()\"]\n        FinalNorm[\"Final LayerNorm\"]\n        FinalMod[\"Final modulation\"]\n        Head[\"Linear head projection\"]\n        Unpatchify[\"Unpatchify\u003cbr/\u003epatches  [C,T,H,W]\"]\n        NoisePred[\"Predicted Noise\u003cbr/\u003e[16, T_lat, H_lat, W_lat]\"]\n    end\n    \n    Inputs --\u003e PreInfer\n    PreInfer --\u003e PatchEmbed\n    PatchEmbed --\u003e TimeEmbed\n    TimeEmbed --\u003e TextEmbed\n    TextEmbed --\u003e ImgEmbed\n    ImgEmbed --\u003e RoPE\n    RoPE --\u003e PreOutput\n    \n    PreOutput --\u003e TransInfer\n    TransInfer --\u003e Loop\n    Loop --\u003e SelfAttn\n    SelfAttn --\u003e CrossAttn\n    CrossAttn --\u003e FFN\n    FFN --\u003e TransOutput\n    \n    TransOutput --\u003e PostInfer\n    PostInfer --\u003e FinalNorm\n    FinalNorm --\u003e FinalMod\n    FinalMod --\u003e Head\n    Head --\u003e Unpatchify\n    Unpatchify --\u003e NoisePred\n```\n\nSources: [lightx2v/models/networks/wan/infer/pre_infer.py:12-162](), [lightx2v/models/networks/wan/infer/transformer_infer.py:17-362](), [lightx2v/models/networks/wan/infer/post_infer.py:8-32]()\n\n### Diffusion Loop Implementation\n\n**Main Denoising Loop** [lightx2v/models/runners/default_runner.py:173-206]():\n\n```python\ndef run_segment(self, segment_idx=0):\n    infer_steps = self.model.scheduler.infer_steps  # 40-50 steps\n    \n    for step_index in range(infer_steps):\n        # Step 1: Prepare timestep and scale latents\n        self.model.scheduler.step_pre(step_index=step_index)\n        \n        # Step 2: Model inference (3-stage pipeline)\n        self.model.infer(self.inputs)\n        \n        # Step 3: Update latents with predicted noise\n        self.model.scheduler.step_post()\n    \n    return self.model.scheduler.latents\n```\n\n**Per-Step Operations:**\n\n1. **`scheduler.step_pre(step_index)`** [lightx2v/models/schedulers/wan/scheduler.py]():\n   - Retrieves current timestep: `self.timesteps[step_index]`\n   - Sets `timestep_input` tensor for model conditioning\n   - Updates `step_index` counter\n   - No latent pre-scaling (WAN uses rectified flow formulation, not DDPM)\n\n2. **`model.infer(inputs)`** [lightx2v/models/networks/wan/model.py:160-199]():\n   - Executes three-stage pipeline via `_infer_cond_uncond()` [lightx2v/models/networks/wan/model.py:104-123]()\n   - **CFG Parallel Mode** (`cfg_parallel=True`): Splits conditional/unconditional inference across 2 GPUs [lightx2v/models/networks/wan/model.py:170-182]()\n     - Rank 0 computes `noise_pred_cond`\n     - Rank 1 computes `noise_pred_uncond`\n     - Uses `dist.all_gather()` to synchronize results\n   - **Sequential Mode** (`cfg_parallel=False`): Runs both passes serially [lightx2v/models/networks/wan/model.py:184-187]()\n   - **CFG Formula**: `noise_pred = noise_pred_uncond + cfg_scale * (noise_pred_cond - noise_pred_uncond)` [lightx2v/models/networks/wan/model.py:189]()\n   - Stores result in `scheduler.noise_pred`\n\n3. **`scheduler.step_post()`** [lightx2v/models/schedulers/wan/scheduler.py]():\n   - Implements `WanScheduler` (UniPC second-order ODE solver)\n   - Converts noise prediction  x0 prediction via `convert_model_output()` [lightx2v/models/schedulers/wan/scheduler.py:105-123]()\n   - Updates latents using multistep formula: `multistep_uni_p_bh_update()` [lightx2v/models/schedulers/wan/scheduler.py:136-234]()\n   - Maintains `model_outputs` history for higher-order solving\n   - Updates `self.latents` with denoised result\n\n**WanScheduler Configuration:**\n- **Inference Steps**: 40-50 for quality (default), 4 for distilled models\n- **Flow Shift**: `sample_shift=7.0` shifts noise schedule for better quality [lightx2v/models/schedulers/wan/scheduler.py:16]()\n- **CFG Scale**: `sample_guide_scale=4.0-6.0` controls conditioning strength [lightx2v/models/schedulers/wan/scheduler.py:27]()\n- **Solver Order**: Second-order UniPC for balance of speed/quality [lightx2v/models/schedulers/wan/scheduler.py:25]()\n\nSources: [lightx2v/models/runners/default_runner.py:173-206](), [lightx2v/models/networks/wan/model.py:102-199](), [lightx2v/models/schedulers/wan/scheduler.py:11-286]()\n\n---\n\n## Code Entry Points and Runner Classes\n\n### CLI Entry Points\n\n**Command Line Interface** [lightx2v/infer.py:42-156]():\n\n```bash\n# T2V Generation with wan2.1\npython -m lightx2v.infer \\\n  --model_cls wan2.1 \\\n  --task t2v \\\n  --model_path /path/to/Wan2.1-T2V-14B \\\n  --config_json configs/wan21/wan_t2v.json \\\n  --prompt \"A serene mountain landscape at sunset\" \\\n  --negative_prompt \"blurry, low quality\" \\\n  --seed 42 \\\n  --save_result_path output.mp4\n\n# I2V Generation with wan2.1\npython -m lightx2v.infer \\\n  --model_cls wan2.1 \\\n  --task i2v \\\n  --model_path /path/to/Wan2.1-I2V-14B \\\n  --config_json configs/wan21/wan_i2v.json \\\n  --prompt \"Add motion to the scene\" \\\n  --negative_prompt \"static, frozen\" \\\n  --image_path input.jpg \\\n  --seed 42 \\\n  --save_result_path output.mp4\n\n# FLF2V Generation (First-Last-Frame interpolation)\npython -m lightx2v.infer \\\n  --model_cls wan2.1 \\\n  --task flf2v \\\n  --model_path /path/to/Wan2.1-I2V-14B \\\n  --config_json configs/wan21/wan_i2v.json \\\n  --prompt \"Smooth transition\" \\\n  --image_path first_frame.jpg \\\n  --last_frame_path last_frame.jpg \\\n  --seed 42 \\\n  --save_result_path output.mp4\n```\n\n**Key CLI Arguments** [lightx2v/infer.py:44-154]():\n- `--model_cls`: Model variant (`wan2.1`, `wan2.2`, `wan2.2_moe`)\n- `--task`: Task type (`t2v`, `i2v`, `flf2v`)\n- `--model_path`: Path to model checkpoint directory\n- `--config_json`: Path to JSON config file with hyperparameters\n- `--prompt`: Text prompt for generation\n- `--negative_prompt`: Negative prompt for CFG\n- `--image_path`: Conditioning image path (I2V/FLF2V only)\n- `--last_frame_path`: Last frame path (FLF2V only)\n- `--seed`: Random seed for reproducibility\n- `--save_result_path`: Output video file path\n- `--return_result_tensor`: Return tensor instead of saving (ComfyUI mode)\n- `--target_shape`: Override output resolution (e.g., `--target_shape 720 1280`)\n\n### Runner Class Hierarchy\n\n```mermaid\ngraph TB\n    BaseRunner[\"BaseRunner\u003cbr/\u003eAbstract base\u003cbr/\u003e(stop signals, metrics)\"]\n    DefaultRunner[\"DefaultRunner\u003cbr/\u003eCore pipeline: init_modules(),\u003cbr/\u003erun_pipeline(), run_main()\"]\n    WanRunner[\"WanRunner\u003cbr/\u003eT2V, I2V, FLF2V\u003cbr/\u003eload_transformer(),\u003cbr/\u003eload_text_encoder(),\u003cbr/\u003eload_image_encoder(),\u003cbr/\u003eload_vae()\"]\n    \n    WanAudioRunner[\"WanAudioRunner\u003cbr/\u003eS2V (audio-driven)\u003cbr/\u003eload_audio_encoder(),\u003cbr/\u003eload_audio_adapter()\"]\n    \n    Wan22MoeRunner[\"Wan22MoeRunner\u003cbr/\u003eMixture-of-Experts\u003cbr/\u003eMultiModelStruct\u003cbr/\u003e(high/low noise models)\"]\n    \n    WanDistillRunner[\"WanDistillRunner\u003cbr/\u003e4-step distilled\u003cbr/\u003eWanStepDistillScheduler\"]\n    \n    BaseRunner --\u003e DefaultRunner\n    DefaultRunner --\u003e WanRunner\n    WanRunner --\u003e WanAudioRunner\n    WanRunner --\u003e Wan22MoeRunner\n    WanRunner --\u003e WanDistillRunner\n    \n    subgraph \"Task-Specific Methods in DefaultRunner\"\n        T2V[\"_run_input_encoder_local_t2v()\u003cbr/\u003eText only\"]\n        I2V[\"_run_input_encoder_local_i2v()\u003cbr/\u003eText + Image\"]\n        FLF2V[\"_run_input_encoder_local_flf2v()\u003cbr/\u003eText + First + Last frame\"]\n        S2V[\"_run_input_encoder_local_s2v()\u003cbr/\u003eText + Image + Audio\"]\n    end\n    \n    DefaultRunner --\u003e T2V\n    DefaultRunner --\u003e I2V\n    DefaultRunner --\u003e FLF2V\n    DefaultRunner --\u003e S2V\n    \n    subgraph \"Registry Pattern\"\n        Register[\"RUNNER_REGISTER dict\"]\n        Keys[\"'wan2.1'  WanRunner\u003cbr/\u003e'seko_talk'  WanAudioRunner\u003cbr/\u003e'wan2.2_moe'  Wan22MoeRunner\u003cbr/\u003e'wan2.1_distill'  WanDistillRunner\"]\n    end\n    \n    Register -.lookup.-\u003e Keys\n    Keys -.instantiate.-\u003e WanRunner\n    Keys -.instantiate.-\u003e WanAudioRunner\n```\n\n**Key Method Overrides by Runner:**\n\n| Runner | Key Methods | Purpose |\n|--------|-------------|---------|\n| `WanRunner` | `load_transformer()`, `load_text_encoder()`, `load_image_encoder()`, `load_vae()`, `init_scheduler()` | Base WAN model loading for T2V/I2V |\n| `WanAudioRunner` | `load_audio_encoder()`, `load_audio_adapter()`, `init_run_segment()`, `read_audio_input()` | Adds audio encoding and segmentation for S2V |\n| `Wan22MoeRunner` | `load_transformer()` returns `MultiModelStruct` | Dual model switching at noise boundary threshold |\n| `WanDistillRunner` | `init_scheduler()` uses `WanStepDistillScheduler` | 4-step inference with CFG distillation |\n\n**Model Class Hierarchy:**\n\n```mermaid\ngraph TB\n    BaseTransformerModel[\"BaseTransformerModel\u003cbr/\u003elightx2v/models/networks/base_model.py\"]\n    WanModel[\"WanModel\u003cbr/\u003elightx2v/models/networks/wan/model.py\u003cbr/\u003e3-stage inference pipeline\"]\n    WanAudioModel[\"WanAudioModel\u003cbr/\u003elightx2v/models/networks/wan/audio_model.py\u003cbr/\u003eAdds audio adapter support\"]\n    \n    BaseTransformerModel --\u003e WanModel\n    WanModel --\u003e WanAudioModel\n    \n    subgraph \"WanModel Components\"\n        PreInfer[\"WanPreInfer\u003cbr/\u003ePatch embedding,\u003cbr/\u003etime/text embedding\"]\n        TransInfer[\"WanTransformerInfer\u003cbr/\u003e40 DiT blocks with\u003cbr/\u003eself/cross attention\"]\n        PostInfer[\"WanPostInfer\u003cbr/\u003eFinal norm, head,\u003cbr/\u003eunpatchify\"]\n    end\n    \n    WanModel --\u003e PreInfer\n    WanModel --\u003e TransInfer\n    WanModel --\u003e PostInfer\n```\n\nSources: [lightx2v/models/runners/wan/wan_runner.py:63-626](), [lightx2v/models/runners/wan/wan_audio_runner.py:276-788](), [lightx2v/models/networks/wan/model.py:33-200]()\n\n### Runner Registration and Initialization\n\n**Runner Registration** [lightx2v/models/runners/wan/wan_runner.py:63-64]():\n```python\n@RUNNER_REGISTER(\"wan2.1\")\nclass WanRunner(DefaultRunner):\n    def __init__(self, config):\n        super().__init__(config)\n        self.vae_cls = WanVAE  # VAE class selection\n        self.vae_name = config.get(\"vae_name\", \"Wan2.1_VAE.pth\")\n```\n\n**Runner Initialization Flow**\n\n```mermaid\ngraph TB\n    Main[\"main()\u003cbr/\u003elightx2v/infer.py\"]\n    ParseArgs[\"argparse\u003cbr/\u003e--model_cls, --task, etc.\"]\n    SetConfig[\"set_config(args)\u003cbr/\u003eLoad config.json\"]\n    InitInput[\"init_empty_input_info(task)\u003cbr/\u003eCreate InputInfo dataclass\"]\n    \n    InitRunner[\"init_runner(config)\u003cbr/\u003elightx2v/infer.py:35-39\"]\n    Lookup[\"RUNNER_REGISTER[model_cls]\u003cbr/\u003eFactory lookup\"]\n    Construct[\"WanRunner(config)\u003cbr/\u003eConstructor\"]\n    InitModules[\"runner.init_modules()\u003cbr/\u003elightx2v/models/runners/default_runner.py:70-97\"]\n    \n    subgraph \"Module Loading Sequence\"\n        LoadTransformer[\"load_transformer()\u003cbr/\u003e WanModel\u003cbr/\u003elightx2v/models/runners/wan/wan_runner.py:72-79\"]\n        LoadText[\"load_text_encoder()\u003cbr/\u003e T5EncoderModel list\u003cbr/\u003elightx2v/models/runners/wan/wan_runner.py:119-157\"]\n        LoadImage[\"load_image_encoder()\u003cbr/\u003e CLIPModel (I2V only)\u003cbr/\u003elightx2v/models/runners/wan/wan_runner.py:81-117\"]\n        LoadVAE[\"load_vae()\u003cbr/\u003e WanVAE encoder/decoder\u003cbr/\u003elightx2v/models/runners/wan/wan_runner.py:214-220\"]\n        LoadVFI[\"load_vfi_model()\u003cbr/\u003e RIFEWrapper (optional)\u003cbr/\u003elightx2v/models/runners/default_runner.py:105-112\"]\n        SetScheduler[\"model.set_scheduler(scheduler)\u003cbr/\u003eLink scheduler to model\u003cbr/\u003elightx2v/models/runners/default_runner.py:77\"]\n    end\n    \n    RunPipeline[\"runner.run_pipeline(input_info)\u003cbr/\u003elightx2v/models/runners/default_runner.py:462-476\"]\n    RunEncoders[\"run_input_encoder()\u003cbr/\u003eTask-specific method\"]\n    RunMain[\"run_main()\u003cbr/\u003eDiffusion loop\u003cbr/\u003elightx2v/models/runners/default_runner.py:361-391\"]\n    \n    Main --\u003e ParseArgs\n    ParseArgs --\u003e SetConfig\n    SetConfig --\u003e InitInput\n    InitInput --\u003e InitRunner\n    \n    InitRunner --\u003e Lookup\n    Lookup --\u003e Construct\n    Construct --\u003e InitModules\n    \n    InitModules --\u003e LoadTransformer\n    LoadTransformer --\u003e LoadText\n    LoadText --\u003e LoadImage\n    LoadImage --\u003e LoadVAE\n    LoadVAE --\u003e LoadVFI\n    LoadVFI --\u003e SetScheduler\n    \n    SetScheduler --\u003e RunPipeline\n    RunPipeline --\u003e RunEncoders\n    RunEncoders --\u003e RunMain\n```\n\n**Module Loading Details:**\n\n1. **`load_transformer()`** [lightx2v/models/runners/wan/wan_runner.py:72-79]():\n   - Instantiates `WanModel` class with model architecture\n   - Applies LoRA adapters if `lora_configs` specified via `build_wan_model_with_lora()` [lightx2v/models/runners/wan/wan_runner.py:35-60]()\n   - Handles weight quantization (INT8/FP8/NVFP4/MXFP4/6/8) via `dit_quant_scheme` config\n   - Loads checkpoint from `{model_path}/` or subdirectories like `original/`, `fp8/`, `int8/`\n\n2. **`load_text_encoder()`** [lightx2v/models/runners/wan/wan_runner.py:119-157]():\n   - Returns list containing `T5EncoderModel` instance: `[text_encoder]`\n   - **Checkpoint Selection**:\n     - Quantized: `models_t5_umt5-xxl-enc-{int8|fp8}.pth` if `t5_quantized=True`\n     - Original: `models_t5_umt5-xxl-enc-bf16.pth` otherwise\n   - Tokenizer path: `{model_path}/google/umt5-xxl`\n   - CPU offloading: Loads on CPU if `t5_cpu_offload=True` [lightx2v/models/runners/wan/wan_runner.py:121-125]()\n\n3. **`load_image_encoder()`** [lightx2v/models/runners/wan/wan_runner.py:81-117]():\n   - Returns `CLIPModel` instance or `None`\n   - **Task Gating**: Only loaded for I2V tasks `if self.config[\"task\"] in [\"i2v\", \"flf2v\", \"animate\", \"s2v\", \"rs2v\"]` [lightx2v/models/runners/wan/wan_runner.py:83]()\n   - **Checkpoint Selection**:\n     - Quantized: `models_clip_open-clip-xlm-roberta-large-vit-huge-14-{int8|fp8}.pth`\n     - Original: `models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth`\n   - CPU offloading: Loads on CPU if `clip_cpu_offload=True`\n\n4. **`load_vae()`** [lightx2v/models/runners/wan/wan_runner.py:214-220]():\n   - Creates `vae_encoder` and `vae_decoder` instances\n   - Uses `WanVAE` class for wan2.1 or `Wan2_2_VAE` for wan2.2 [lightx2v/models/runners/wan/wan_runner.py:67-70]()\n   - **Checkpoint**: `Wan2.1_VAE.pth` or `Wan2.2_VAE.pth`\n   - **Memory Optimizations**:\n     - Tiling: `use_tiling_vae=True` for large resolutions\n     - LightVAE: `use_lightvae=True` for faster lightweight variant\n     - CPU offload: `vae_cpu_offload=True`\n   - Encoder/decoder may share same instance if `use_tae=False`\n\n5. **`init_scheduler()`** [lightx2v/models/runners/wan/wan_runner.py:222-235]():\n   - Creates scheduler instance based on `feature_caching` config:\n     - `\"NoCaching\"`  `WanScheduler` (standard EulerScheduler)\n     - `\"TaylorSeer\"`  `WanSchedulerTaylorCaching`\n     - `\"Tea\"`, `\"Ada\"`, `\"Mag\"`, `\"FirstBlock\"`, etc.  `WanSchedulerCaching`\n   - Supports changing resolution via `WanScheduler4ChangingResolutionInterface` wrapper\n\nSources: [lightx2v/infer.py:35-187](), [lightx2v/models/runners/default_runner.py:70-131](), [lightx2v/models/runners/wan/wan_runner.py:63-235]()\n\n---\n\n## Server API Integration\n\n### REST API Endpoints\n\n**Video Generation Service** [lightx2v/server/services/generation/base.py:14-89]():\n\nThe server provides async APIs for both T2V and I2V:\n\n```python\n# T2V Request\n{\n    \"task_id\": \"auto-generated\",\n    \"prompt\": \"A serene lake at sunset\",\n    \"negative_prompt\": \"blurry, low quality\",\n    \"target_video_length\": 81,\n    \"seed\": 42,\n    \"target_shape\": [480, 832]\n}\n\n# I2V Request (adds image_path)\n{\n    \"task_id\": \"auto-generated\",\n    \"prompt\": \"Add motion to the scene\",\n    \"image_path\": \"base64://...\" or \"http://...\",\n    \"target_video_length\": 81,\n    \"seed\": 42\n}\n```\n\n**Request Processing** [lightx2v/deploy/worker/hub.py:84-109]():\n\n1. **Image Download/Decode**: Handles HTTP URLs and base64-encoded images\n2. **Path Resolution**: Saves to temporary directory\n3. **Parameter Mapping**: Converts request to InputInfo\n4. **Async Execution**: Submits to inference queue\n\nSources: [lightx2v/server/services/generation/base.py:14-89](), [lightx2v/deploy/worker/hub.py:32-489](), [lightx2v/server/schema.py:18-61]()"])</script><script>self.__next_f.push([1,"31:T4d75,"])</script><script>self.__next_f.push([1,"# WanDistillRunner - 4-Step Distilled Models\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/models/input_encoders/hf/__init__.py](lightx2v/models/input_encoders/hf/__init__.py)\n- [lightx2v/models/networks/__init__.py](lightx2v/models/networks/__init__.py)\n- [lightx2v/models/networks/wan/causvid_model.py](lightx2v/models/networks/wan/causvid_model.py)\n- [lightx2v/models/networks/wan/distill_model.py](lightx2v/models/networks/wan/distill_model.py)\n- [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py](lightx2v/models/networks/wan/infer/causvid/transformer_infer.py)\n- [lightx2v/models/runners/__init__.py](lightx2v/models/runners/__init__.py)\n- [lightx2v/models/runners/wan/wan_distill_runner.py](lightx2v/models/runners/wan/wan_distill_runner.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document covers the **distillation runners** for WAN models, which enable accelerated inference by reducing the number of diffusion steps from 50 to 4. This system achieves approximately **25x speedup** through step distillation and classifier-free guidance (CFG) distillation techniques.\n\nThe distillation system supports three variants:\n- **WAN 2.1 Distill** - Single model with standard step distillation\n- **WAN 2.1 Mean Flow Distill** - Single model with mean flow scheduling\n- **WAN 2.2 MoE Distill** - Dual model (high-noise/low-noise) mixture-of-experts approach\n\nFor standard multi-step WAN inference, see [WanRunner - Text-to-Video and Image-to-Video](#5.3). For audio-driven generation, see [WanAudioRunner - Audio-to-Video Generation](#5.2).\n\n**Sources:** [lightx2v/models/runners/wan/wan_distill_runner.py:1-209](), [lightx2v/models/networks/wan/distill_model.py:1-36]()\n\n## Architecture Overview\n\nThe distillation runner system is organized into specialized runner classes that inherit from `WanRunner` and use custom schedulers designed for few-step inference:\n\n```mermaid\ngraph TB\n    WanRunner[\"WanRunner\u003cbr/\u003e(Base WAN Runner)\"]\n    WanDistillRunner[\"WanDistillRunner\u003cbr/\u003eRUNNER_REGISTER: wan2.1_distill\"]\n    Wan21MeanFlowRunner[\"Wan21MeanFlowDistillRunner\u003cbr/\u003eRUNNER_REGISTER: wan2.1_mean_flow_distill\"]\n    Wan22MoeRunner[\"Wan22MoeDistillRunner\u003cbr/\u003eRUNNER_REGISTER: wan2.2_moe_distill\"]\n    \n    WanDistillModel[\"WanDistillModel\u003cbr/\u003e(Model Class)\"]\n    MultiDistillModelStruct[\"MultiDistillModelStruct\u003cbr/\u003e(Dual Model Manager)\"]\n    \n    WanStepDistillScheduler[\"WanStepDistillScheduler\"]\n    Wan21MeanFlowScheduler[\"Wan21MeanFlowStepDistillScheduler\"]\n    Wan22StepDistillScheduler[\"Wan22StepDistillScheduler\"]\n    \n    WanRunner --\u003e WanDistillRunner\n    WanDistillRunner --\u003e Wan21MeanFlowRunner\n    WanDistillRunner --\u003e Wan22MoeRunner\n    \n    WanDistillRunner -.loads.-\u003e WanDistillModel\n    Wan22MoeRunner -.loads.-\u003e MultiDistillModelStruct\n    MultiDistillModelStruct -.manages.-\u003e WanDistillModel\n    \n    WanDistillRunner -.uses.-\u003e WanStepDistillScheduler\n    Wan21MeanFlowRunner -.uses.-\u003e Wan21MeanFlowScheduler\n    Wan22MoeRunner -.uses.-\u003e Wan22StepDistillScheduler\n```\n\n**Key Characteristics:**\n- **No CFG**: Distilled models do not use classifier-free guidance during inference\n- **Fixed Steps**: Typically 4 inference steps (configurable)\n- **Specialized Schedulers**: Custom timestep calculations optimized for distilled models\n- **Optional Multi-Model**: MoE variant switches between models during inference\n\n**Sources:** [lightx2v/models/runners/wan/wan_distill_runner.py:13-175]()\n\n## WanDistillRunner - Single Model Distillation\n\n### Class Structure\n\n`WanDistillRunner` is the base class for WAN 2.1 distillation, registered as `\"wan2.1_distill\"`:\n\n```mermaid\ngraph LR\n    subgraph \"WanDistillRunner Lifecycle\"\n        Init[\"__init__\u003cbr/\u003e(config)\"]\n        LoadTransformer[\"load_transformer()\u003cbr/\u003e WanDistillModel\"]\n        InitScheduler[\"init_scheduler()\u003cbr/\u003e WanStepDistillScheduler\"]\n        RunPipeline[\"run_pipeline()\u003cbr/\u003e(inherited from WanRunner)\"]\n    end\n    \n    Init --\u003e LoadTransformer\n    LoadTransformer --\u003e InitScheduler\n    InitScheduler --\u003e RunPipeline\n```\n\n### Model Loading\n\nThe `load_transformer` method instantiates `WanDistillModel`, which loads weights from `distill_model.pt`:\n\n| Method | Purpose | Returns |\n|--------|---------|---------|\n| `load_transformer()` | Creates WanDistillModel or LoRA-enhanced variant | `WanDistillModel` instance |\n| `init_scheduler()` | Creates WanStepDistillScheduler (NoCaching only) | Sets `self.scheduler` |\n\n**Key Implementation Details:**\n\n[lightx2v/models/runners/wan/wan_distill_runner.py:18-31]()\n- Line 19: Builds model from `model_path` configuration\n- Line 21-24: Supports LoRA integration through `build_wan_model_with_lora`\n- Line 28-31: Only `NoCaching` feature caching is supported\n\n**Sources:** [lightx2v/models/runners/wan/wan_distill_runner.py:18-31]()\n\n### WanDistillModel Class\n\n`WanDistillModel` extends `WanModel` with distillation-specific weight loading:\n\n```mermaid\ngraph TB\n    WanModel[\"WanModel\u003cbr/\u003e(Base Class)\"]\n    WanDistillModel[\"WanDistillModel\"]\n    \n    WanDistillModel -.inherits.-\u003e WanModel\n    \n    subgraph \"Weight Loading Strategy\"\n        CheckDistillPt[\"Check: distill_model.pt\u003cbr/\u003eexists?\"]\n        LoadDistillPt[\"Load from\u003cbr/\u003edistill_model.pt\"]\n        LoadStandard[\"Load standard WAN weights\u003cbr/\u003e(fallback)\"]\n        \n        CheckDistillPt --\u003e|Yes| LoadDistillPt\n        CheckDistillPt --\u003e|No| LoadStandard\n    end\n    \n    WanDistillModel --\u003e CheckDistillPt\n```\n\n**Weight Loading Logic:**\n\n[lightx2v/models/networks/wan/distill_model.py:24-35]()\n- Line 26: Checks for `distill_model.pt` in model directory\n- Line 28-34: Loads and processes weights with dtype conversion\n- Line 35: Falls back to parent class weight loading if file not found\n\n**Sources:** [lightx2v/models/networks/wan/distill_model.py:16-36]()\n\n## Wan21MeanFlowDistillRunner\n\nThis variant uses mean flow scheduling, which modifies the timestep calculation for improved quality:\n\n```mermaid\ngraph LR\n    WanDistillRunner --\u003e Wan21MeanFlowRunner[\"Wan21MeanFlowDistillRunner\u003cbr/\u003eRUNNER_REGISTER: wan2.1_mean_flow_distill\"]\n    Wan21MeanFlowRunner --\u003e Wan21MeanFlowScheduler[\"Wan21MeanFlowStepDistillScheduler\"]\n```\n\n**Differences from Base WanDistillRunner:**\n\n| Aspect | WanDistillRunner | Wan21MeanFlowDistillRunner |\n|--------|------------------|----------------------------|\n| Scheduler | `WanStepDistillScheduler` | `Wan21MeanFlowStepDistillScheduler` |\n| Timestep Calculation | Standard distillation | Mean flow method |\n| Configuration | Same | Same |\n\n**Sources:** [lightx2v/models/runners/wan/wan_distill_runner.py:34-43]()\n\n## Wan22MoeDistillRunner - Dual Model Architecture\n\n### Overview\n\n`Wan22MoeDistillRunner` implements a mixture-of-experts approach where two specialized models handle different noise levels:\n\n```mermaid\ngraph TB\n    subgraph \"Dual Model Structure\"\n        HighNoiseModel[\"High Noise Model\u003cbr/\u003e(Steps 0-1)\"]\n        LowNoiseModel[\"Low Noise Model\u003cbr/\u003e(Steps 2-3)\"]\n    end\n    \n    subgraph \"MultiDistillModelStruct\"\n        ModelList[\"model[0], model[1]\"]\n        BoundaryIndex[\"boundary_step_index = 2\"]\n        CurrentIndex[\"cur_model_index\"]\n        GetCurrentModel[\"get_current_model_index()\"]\n    end\n    \n    subgraph \"Inference Flow\"\n        Step0[\"Step 0:\u003cbr/\u003eHigh Noise\"]\n        Step1[\"Step 1:\u003cbr/\u003eHigh Noise\"]\n        Step2[\"Step 2:\u003cbr/\u003eLow Noise\"]\n        Step3[\"Step 3:\u003cbr/\u003eLow Noise\"]\n    end\n    \n    HighNoiseModel -.stored in.-\u003e ModelList\n    LowNoiseModel -.stored in.-\u003e ModelList\n    BoundaryIndex -.controls.-\u003e GetCurrentModel\n    GetCurrentModel -.updates.-\u003e CurrentIndex\n    \n    Step0 --\u003e Step1\n    Step1 --\u003e Step2\n    Step2 --\u003e Step3\n    \n    Step0 -.uses.-\u003e HighNoiseModel\n    Step1 -.uses.-\u003e HighNoiseModel\n    Step2 -.uses.-\u003e LowNoiseModel\n    Step3 -.uses.-\u003e LowNoiseModel\n```\n\n### Model Paths and Configuration\n\nThe runner locates high-noise and low-noise models separately:\n\n**High Noise Model Path Resolution:**\n\n[lightx2v/models/runners/wan/wan_distill_runner.py:124-131]()\n1. If `dit_quantized` and `high_noise_quantized_ckpt` specified  use quantized checkpoint\n2. Else if `high_noise_original_ckpt` specified  use original checkpoint\n3. Else  use `{model_path}/high_noise_model` directory\n\n**Low Noise Model Path Resolution:**\n\n[lightx2v/models/runners/wan/wan_distill_runner.py:133-140]()\n- Same logic as high noise model path resolution\n\n**Sources:** [lightx2v/models/runners/wan/wan_distill_runner.py:122-140]()\n\n### MultiDistillModelStruct Class\n\nThis class manages model switching during inference:\n\n| Attribute | Type | Purpose |\n|-----------|------|---------|\n| `model` | `List[WanDistillModel]` | `[high_noise_model, low_noise_model]` |\n| `boundary_step_index` | `int` | Step index where switching occurs (default: 2) |\n| `cur_model_index` | `int` | Currently active model (0 or 1) |\n\n**Model Switching Logic:**\n\n```mermaid\nflowchart TD\n    CheckStep[\"Check: step_index \u003c boundary_step_index?\"]\n    UseHighNoise[\"Use High Noise Model\u003cbr/\u003e(cur_model_index = 0)\"]\n    UseLowNoise[\"Use Low Noise Model\u003cbr/\u003e(cur_model_index = 1)\"]\n    \n    OffloadCheck{\"CPU offload enabled?\u003cbr/\u003eoffload_granularity == 'model'?\"}\n    \n    OffloadPrev[\"Offload Previous Model\u003cbr/\u003eto CPU\"]\n    LoadCurrent[\"Load Current Model\u003cbr/\u003eto CUDA\"]\n    \n    Infer[\"Execute model.infer(inputs)\"]\n    \n    CheckStep --\u003e|Yes| UseHighNoise\n    CheckStep --\u003e|No| UseLowNoise\n    \n    UseHighNoise --\u003e OffloadCheck\n    UseLowNoise --\u003e OffloadCheck\n    \n    OffloadCheck --\u003e|Yes| OffloadPrev\n    OffloadCheck --\u003e|No| Infer\n    \n    OffloadPrev --\u003e LoadCurrent\n    LoadCurrent --\u003e Infer\n```\n\n**Implementation Details:**\n\n[lightx2v/models/runners/wan/wan_distill_runner.py:55-77]()\n- Line 57: Determines which model to use based on `step_index`\n- Line 60-75: Handles CPU offloading when switching between models\n- Line 58, 68: Logs which model is active at each step\n\n**Sources:** [lightx2v/models/runners/wan/wan_distill_runner.py:46-118]()\n\n### Lazy Loading Support\n\nFor memory-constrained scenarios, models can be loaded on-demand:\n\n[lightx2v/models/runners/wan/wan_distill_runner.py:80-117]()\n- Line 80-84: Checks if lazy loading is enabled\n- Line 86-117: Loads models dynamically when needed\n- Supports both standard and LoRA-enhanced models\n\n**Sources:** [lightx2v/models/runners/wan/wan_distill_runner.py:142-169]()\n\n## Step Distillation Schedulers\n\n### Scheduler Hierarchy\n\n```mermaid\ngraph TB\n    BaseScheduler[\"Scheduler Base Class\"]\n    WanStepDistillScheduler[\"WanStepDistillScheduler\u003cbr/\u003e(wan2.1_distill)\"]\n    Wan21MeanFlowScheduler[\"Wan21MeanFlowStepDistillScheduler\u003cbr/\u003e(wan2.1_mean_flow_distill)\"]\n    Wan22StepDistillScheduler[\"Wan22StepDistillScheduler\u003cbr/\u003e(wan2.2_moe_distill)\"]\n    \n    BaseScheduler --\u003e WanStepDistillScheduler\n    BaseScheduler --\u003e Wan21MeanFlowScheduler\n    BaseScheduler --\u003e Wan22StepDistillScheduler\n```\n\n### Key Characteristics\n\n| Scheduler | Timestep Strategy | CFG Support | Step Count |\n|-----------|------------------|-------------|------------|\n| `WanStepDistillScheduler` | Standard distillation | No (distilled out) | 4 (typical) |\n| `Wan21MeanFlowStepDistillScheduler` | Mean flow calculation | No | 4 (typical) |\n| `Wan22StepDistillScheduler` | Two-model switching | No | 4 (typical) |\n\n**Core Scheduler Methods:**\n\n1. **`prepare()`** - Initializes latents and timesteps\n2. **`step_pre()`** - Prepares inputs for current step\n3. **`step_post()`** - Updates latents after model inference\n4. **No CFG** - Distilled models have guidance baked into weights\n\n**Sources:** [lightx2v/models/runners/wan/wan_distill_runner.py:27-31](), [lightx2v/models/runners/wan/wan_distill_runner.py:39-43](), [lightx2v/models/runners/wan/wan_distill_runner.py:171-175]()\n\n## Feature Caching Restrictions\n\nDistillation runners only support `NoCaching` mode:\n\n```python\n# From init_scheduler() implementations\nif self.config[\"feature_caching\"] == \"NoCaching\":\n    self.scheduler = WanStepDistillScheduler(self.config)\nelse:\n    raise NotImplementedError(f\"Unsupported feature_caching type: {self.config['feature_caching']}\")\n```\n\n**Rationale:**\n- Distilled models are already optimized for speed\n- 4-step inference makes caching benefits minimal\n- Caching strategies (Tea, Mag, Taylor) designed for multi-step processes\n\n**Sources:** [lightx2v/models/runners/wan/wan_distill_runner.py:28-31]()\n\n## LoRA Support\n\n### WAN 2.1 Distill LoRA\n\nStandard LoRA application through `build_wan_model_with_lora`:\n\n[lightx2v/models/runners/wan/wan_distill_runner.py:20-24]()\n- Checks for `lora_configs` in configuration\n- Builds model with LoRA layers integrated\n- Uses inherited LoRA methods from `WanModel`\n\n### WAN 2.2 MoE Distill LoRA\n\nThe MoE variant supports **separate LoRA weights** for high-noise and low-noise models:\n\n```mermaid\ngraph TB\n    subgraph \"LoRA Configuration\"\n        HighLoRA[\"high_lora_path\u003cbr/\u003ehigh_lora_strength\"]\n        LowLoRA[\"low_lora_path\u003cbr/\u003elow_lora_strength\"]\n    end\n    \n    subgraph \"Model Application\"\n        HighNoiseModel[\"High Noise Model\u003cbr/\u003emodel[0]\"]\n        LowNoiseModel[\"Low Noise Model\u003cbr/\u003emodel[1]\"]\n    end\n    \n    SwitchLoRA[\"switch_lora()\u003cbr/\u003emethod\"]\n    \n    HighLoRA -.applied to.-\u003e HighNoiseModel\n    LowLoRA -.applied to.-\u003e LowNoiseModel\n    \n    SwitchLoRA --\u003e HighNoiseModel\n    SwitchLoRA --\u003e LowNoiseModel\n```\n\n**Dynamic LoRA Switching:**\n\n[lightx2v/models/runners/wan/wan_distill_runner.py:177-208]()\n\nThe `switch_lora` method allows runtime LoRA changes:\n\n```python\ndef switch_lora(\n    self,\n    high_lora_path: str = None,\n    high_lora_strength: float = 1.0,\n    low_lora_path: str = None,\n    low_lora_strength: float = 1.0\n) -\u003e bool\n```\n\n**Parameters:**\n- `high_lora_path` - Path to high-noise model LoRA weights\n- `high_lora_strength` - Scaling factor for high-noise LoRA (0.0-1.0)\n- `low_lora_path` - Path to low-noise model LoRA weights\n- `low_lora_strength` - Scaling factor for low-noise LoRA (0.0-1.0)\n\n**Implementation:**\n- Line 198-200: Updates high-noise model LoRA if path provided\n- Line 202-205: Updates low-noise model LoRA if path provided\n- Only updates models that are currently loaded (not `None`)\n\n**Sources:** [lightx2v/models/runners/wan/wan_distill_runner.py:177-208]()\n\n## Configuration and Usage\n\n### Required Configuration Keys\n\n| Key | Type | Description | Default |\n|-----|------|-------------|---------|\n| `model_path` | `str` | Base model directory path | Required |\n| `feature_caching` | `str` | Must be `\"NoCaching\"` | `\"NoCaching\"` |\n| `boundary_step_index` | `int` | Model switch point (MoE only) | `2` |\n| `cpu_offload` | `bool` | Enable CPU offloading | `False` |\n| `offload_granularity` | `str` | `\"model\"`, `\"block\"`, or `\"phase\"` | `\"block\"` |\n| `lazy_load` | `bool` | Load models on-demand | `False` |\n| `lora_configs` | `dict` | LoRA configuration (optional) | `None` |\n\n### WAN 2.2 MoE Specific Keys\n\n| Key | Type | Description |\n|-----|------|-------------|\n| `high_noise_quantized_ckpt` | `str` | Path to quantized high-noise checkpoint |\n| `high_noise_original_ckpt` | `str` | Path to original high-noise checkpoint |\n| `low_noise_quantized_ckpt` | `str` | Path to quantized low-noise checkpoint |\n| `low_noise_original_ckpt` | `str` | Path to original low-noise checkpoint |\n\n### Example Configuration\n\n```json\n{\n  \"runner\": \"wan2.2_moe_distill\",\n  \"model_path\": \"/path/to/wan2.2_moe\",\n  \"feature_caching\": \"NoCaching\",\n  \"boundary_step_index\": 2,\n  \"cpu_offload\": true,\n  \"offload_granularity\": \"model\",\n  \"dit_quantized\": false,\n  \"lora_configs\": {\n    \"high_noise_model\": {\n      \"lora_path\": \"/path/to/high_lora.safetensors\",\n      \"lora_strength\": 0.8\n    },\n    \"low_noise_model\": {\n      \"lora_path\": \"/path/to/low_lora.safetensors\",\n      \"lora_strength\": 1.0\n    }\n  }\n}\n```\n\n**Sources:** [lightx2v/models/runners/wan/wan_distill_runner.py:15-16](), [lightx2v/models/runners/wan/wan_distill_runner.py:122-140]()\n\n## Performance Characteristics\n\n### Speedup Analysis\n\n```mermaid\ngraph LR\n    StandardWAN[\"Standard WAN\u003cbr/\u003e50 steps\u003cbr/\u003e~25 seconds\"]\n    DistillWAN[\"Distilled WAN\u003cbr/\u003e4 steps\u003cbr/\u003e~1 second\"]\n    \n    StandardWAN -.25x speedup.-\u003e DistillWAN\n    \n    subgraph \"Speedup Sources\"\n        StepReduction[\"Step Reduction\u003cbr/\u003e50  4 steps\u003cbr/\u003e(12.5x)\"]\n        CFGDistill[\"CFG Distillation\u003cbr/\u003eNo dual inference\u003cbr/\u003e(2x)\"]\n        Combined[\"Combined\u003cbr/\u003e~25x total\"]\n    end\n    \n    StepReduction --\u003e Combined\n    CFGDistill --\u003e Combined\n```\n\n### Memory Considerations\n\n**WAN 2.1 Distill:**\n- Single model loaded: ~14GB VRAM\n- Standard memory profile\n\n**WAN 2.2 MoE Distill:**\n- Two models (if both loaded): ~28GB VRAM\n- With CPU offloading: ~14GB VRAM (one model active)\n- With lazy loading: ~14GB VRAM (load on-demand)\n\n| Configuration | VRAM Usage | Speed | Complexity |\n|---------------|------------|-------|------------|\n| Both models in VRAM | ~28GB | Fastest | Simple |\n| CPU offload + double buffer | ~14GB | Fast | Medium |\n| Lazy loading | ~14GB | Medium | Complex |\n\n**Sources:** [lightx2v/models/runners/wan/wan_distill_runner.py:60-75](), [lightx2v/models/runners/wan/wan_distill_runner.py:143-169]()\n\n## Integration with Pipeline\n\n### Inference Flow\n\n```mermaid\nsequenceDiagram\n    participant Pipeline as LightX2VPipeline\n    participant Runner as WanDistillRunner\n    participant Scheduler as WanStepDistillScheduler\n    participant Model as WanDistillModel/MultiDistillModelStruct\n    \n    Pipeline-\u003e\u003eRunner: run_pipeline(input_info)\n    Runner-\u003e\u003eRunner: run_input_encoder()\n    Runner-\u003e\u003eScheduler: prepare(latents)\n    \n    loop 4 Steps\n        Runner-\u003e\u003eScheduler: step_pre()\n        Scheduler--\u003e\u003eRunner: inputs prepared\n        \n        alt WAN 2.2 MoE\n            Runner-\u003e\u003eModel: get_current_model_index()\n            Model--\u003e\u003eModel: Switch model if needed\n        end\n        \n        Runner-\u003e\u003eModel: infer(inputs)\n        Model--\u003e\u003eRunner: noise prediction\n        Runner-\u003e\u003eScheduler: step_post()\n        Scheduler--\u003e\u003eRunner: updated latents\n    end\n    \n    Runner-\u003e\u003eRunner: run_vae_decoder(latents)\n    Runner--\u003e\u003ePipeline: generated video\n```\n\n### Runner Registration\n\nThe runners are registered with specific string identifiers:\n\n| Registration Key | Class | Import Path |\n|-----------------|-------|-------------|\n| `\"wan2.1_distill\"` | `WanDistillRunner` | `lightx2v.models.runners.wan.wan_distill_runner` |\n| `\"wan2.1_mean_flow_distill\"` | `Wan21MeanFlowDistillRunner` | `lightx2v.models.runners.wan.wan_distill_runner` |\n| `\"wan2.2_moe_distill\"` | `Wan22MoeDistillRunner` | `lightx2v.models.runners.wan.wan_distill_runner` |\n\n**Sources:** [lightx2v/models/runners/wan/wan_distill_runner.py:13-14](), [lightx2v/models/runners/wan/wan_distill_runner.py:34-35](), [lightx2v/models/runners/wan/wan_distill_runner.py:120-121]()\n\n## Limitations and Constraints\n\n### Current Restrictions\n\n1. **No Feature Caching**: Only `NoCaching` mode supported\n   - Tea, Mag, Taylor caching not implemented\n   - 4-step inference makes caching less beneficial\n\n2. **No CFG**: Classifier-free guidance distilled into weights\n   - Cannot adjust guidance scale at inference time\n   - Trade-off: speed vs. runtime controllability\n\n3. **Fixed Step Count**: Optimized for 4 steps\n   - Increasing steps may not improve quality\n   - Models trained for specific step count\n\n### Compatibility\n\n**Compatible With:**\n- Quantization (INT8, FP8, NVFP4)\n- CPU offloading (model, block, phase granularity)\n- LoRA (including multi-LoRA for MoE)\n- Lazy loading\n- Distributed inference\n\n**Not Compatible With:**\n- Feature caching (Tea, Mag, Taylor, Ada)\n- Runtime CFG adjustment\n- Autoregressive generation (use WanCausVidRunner instead)\n\n**Sources:** [lightx2v/models/runners/wan/wan_distill_runner.py:28-31](), [lightx2v/models/runners/wan/wan_distill_runner.py:40-43](), [lightx2v/models/runners/wan/wan_distill_runner.py:172-175]()"])</script><script>self.__next_f.push([1,"32:T609e,"])</script><script>self.__next_f.push([1,"# Mixture-of-Experts Models (Wan 2.2 MoE)\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/input_encoders/hf/__init__.py](lightx2v/models/input_encoders/hf/__init__.py)\n- [lightx2v/models/networks/__init__.py](lightx2v/models/networks/__init__.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/causvid_model.py](lightx2v/models/networks/wan/causvid_model.py)\n- [lightx2v/models/networks/wan/distill_model.py](lightx2v/models/networks/wan/distill_model.py)\n- [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py](lightx2v/models/networks/wan/infer/causvid/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/runners/__init__.py](lightx2v/models/runners/__init__.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_distill_runner.py](lightx2v/models/runners/wan/wan_distill_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the **Wan 2.2 Mixture-of-Experts (MoE)** architecture, which uses a dual-model strategy to optimize video generation quality across different noise levels. The system switches between a specialized high-noise model and a low-noise model during the diffusion denoising process based on a configurable boundary timestep.\n\nFor information about the base Wan 2.1 and Wan 2.2 architectures without MoE, see [5.1](#5.1). For step-distilled variants, see [5.4](#5.4). For audio-driven generation capabilities, see [5.2](#5.2).\n\n---\n\n## Architecture Overview\n\nThe Wan 2.2 MoE architecture splits the diffusion denoising process between two specialized models:\n\n1. **High-Noise Model**: Handles the early denoising steps (high timesteps) where the latent representation contains significant noise\n2. **Low-Noise Model**: Handles the later refinement steps (low timesteps) where fine details are generated\n\nThis specialization allows each model to be optimized for its specific portion of the denoising trajectory, improving overall generation quality.\n\n### Dual-Model Architecture\n\n```mermaid\ngraph TB\n    subgraph \"MultiModelStruct\"\n        Scheduler[\"WanScheduler\u003cbr/\u003etimesteps, step_index\"]\n        Boundary[\"Boundary Logic\u003cbr/\u003eboundary_timestep = 0.875 * 1000\"]\n        \n        HighModel[\"High-Noise Model\u003cbr/\u003eWanModel\u003cbr/\u003emodel[0]\"]\n        LowModel[\"Low-Noise Model\u003cbr/\u003eWanModel\u003cbr/\u003emodel[1]\"]\n    end\n    \n    subgraph \"Early Steps (step_index=0 to N)\"\n        Step0[\"timestep \u003e= boundary_timestep\u003cbr/\u003ee.g., timestep=875\"]\n        UseHigh[\"Use High-Noise Model\u003cbr/\u003ecur_model_index=0\"]\n        GuidanceHigh[\"sample_guide_scale[0]\"]\n    end\n    \n    subgraph \"Late Steps (step_index=N+1 to end)\"\n        StepN[\"timestep \u003c boundary_timestep\u003cbr/\u003ee.g., timestep=100\"]\n        UseLow[\"Use Low-Noise Model\u003cbr/\u003ecur_model_index=1\"]\n        GuidanceLow[\"sample_guide_scale[1]\"]\n    end\n    \n    Scheduler --\u003e Boundary\n    Boundary --\u003e Step0\n    Boundary --\u003e StepN\n    \n    Step0 --\u003e UseHigh\n    UseHigh --\u003e HighModel\n    HighModel --\u003e GuidanceHigh\n    \n    StepN --\u003e UseLow\n    UseLow --\u003e LowModel\n    LowModel --\u003e GuidanceLow\n    \n    style HighModel fill:#ffcccc\n    style LowModel fill:#ccccff\n```\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:495-584]()\n\n---\n\n## MultiModelStruct Class\n\nThe `MultiModelStruct` class orchestrates the dual-model system, managing model selection, scheduler synchronization, and memory optimization.\n\n### Class Structure\n\n```mermaid\nclassDiagram\n    class MultiModelStruct {\n        +List[WanModel] model\n        +Config config\n        +float boundary\n        +float boundary_timestep\n        +int cur_model_index\n        +WanScheduler scheduler\n        \n        +set_scheduler(scheduler)\n        +infer(inputs)\n        +get_current_model_index()\n        +to_cuda(model_index)\n        +offload_cpu(model_index)\n    }\n    \n    class WanModel {\n        +transformer_infer\n        +pre_weight\n        +transformer_weights\n        +infer(inputs)\n        +to_cpu()\n        +to_cuda()\n    }\n    \n    class WanScheduler {\n        +timesteps\n        +step_index\n        +sample_guide_scale\n        +latents\n    }\n    \n    MultiModelStruct \"1\" --\u003e \"2\" WanModel : manages\n    MultiModelStruct \"1\" --\u003e \"1\" WanScheduler : shared\n```\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:495-507]()\n\n### Initialization Parameters\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `model_list` | `List[WanModel]` | Required | List containing `[high_noise_model, low_noise_model]` |\n| `config` | `dict` | Required | Configuration dictionary |\n| `boundary` | `float` | `0.875` | Boundary ratio for switching models (0-1 range) |\n| `num_train_timesteps` | `int` | `1000` | Total training timesteps for boundary calculation |\n\n**Key Attributes:**\n- `boundary_timestep = boundary * num_train_timesteps` - Computed threshold for model switching\n- `cur_model_index` - Tracks active model: `-1` (uninitialized), `0` (high-noise), `1` (low-noise)\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:496-503]()\n\n---\n\n## Model Switching Logic\n\n### Timestep-Based Selection\n\nThe system uses the current diffusion timestep to determine which model should process each denoising step:\n\n```mermaid\nflowchart TD\n    Start[\"Inference Step\u003cbr/\u003escheduler.step_index\"]\n    GetTimestep[\"timestep = scheduler.timesteps[step_index]\"]\n    CheckBoundary{\"timestep \u003e= boundary_timestep?\"}\n    \n    SelectHigh[\"Select High-Noise Model\u003cbr/\u003ecur_model_index = 0\u003cbr/\u003esample_guide_scale = config[0]\"]\n    SelectLow[\"Select Low-Noise Model\u003cbr/\u003ecur_model_index = 1\u003cbr/\u003esample_guide_scale = config[1]\"]\n    \n    CheckOffload{\"cpu_offload \u0026\u0026 offload_granularity == 'model'?\"}\n    \n    ManageMemoryHigh[\"Handle Model Swapping:\u003cbr/\u003e- Offload low-noise to CPU\u003cbr/\u003e- Load high-noise to CUDA\"]\n    ManageMemoryLow[\"Handle Model Swapping:\u003cbr/\u003e- Offload high-noise to CPU\u003cbr/\u003e- Load low-noise to CUDA\"]\n    \n    InferHigh[\"model[0].infer(inputs)\"]\n    InferLow[\"model[1].infer(inputs)\"]\n    \n    Start --\u003e GetTimestep\n    GetTimestep --\u003e CheckBoundary\n    \n    CheckBoundary --\u003e|Yes| SelectHigh\n    CheckBoundary --\u003e|No| SelectLow\n    \n    SelectHigh --\u003e CheckOffload\n    SelectLow --\u003e CheckOffload\n    \n    CheckOffload --\u003e|Yes, High| ManageMemoryHigh\n    CheckOffload --\u003e|Yes, Low| ManageMemoryLow\n    CheckOffload --\u003e|No| InferHigh\n    CheckOffload --\u003e|No| InferLow\n    \n    ManageMemoryHigh --\u003e InferHigh\n    ManageMemoryLow --\u003e InferLow\n```\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:557-577]()\n- [lightx2v/models/runners/wan/wan_runner.py:515-554]()\n\n### Example Switching Sequence\n\nFor a typical 40-step generation with `boundary=0.875` and `num_train_timesteps=1000`:\n- `boundary_timestep = 875`\n- Steps 0-34: `timestep \u003e= 875`  **High-Noise Model**\n- Steps 35-39: `timestep \u003c 875`  **Low-Noise Model**\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:500-501]()\n\n---\n\n## Wan22MoeRunner\n\nThe `Wan22MoeRunner` class extends `WanRunner` to configure and initialize the MoE dual-model system.\n\n### Configuration Structure\n\n```mermaid\ngraph LR\n    subgraph \"Model Paths\"\n        ModelPath[\"config['model_path']\u003cbr/\u003eBase directory\"]\n        \n        HighPath[\"High-Noise Model Path\"]\n        LowPath[\"Low-Noise Model Path\"]\n        \n        HighOptions[\"Options:\u003cbr/\u003e1. config['high_noise_original_ckpt']\u003cbr/\u003e2. config['high_noise_quantized_ckpt']\u003cbr/\u003e3. model_path/high_noise_model/\"]\n        \n        LowOptions[\"Options:\u003cbr/\u003e1. config['low_noise_original_ckpt']\u003cbr/\u003e2. config['low_noise_quantized_ckpt']\u003cbr/\u003e3. model_path/low_noise_model/\"]\n    end\n    \n    subgraph \"Instantiation\"\n        HighModel[\"WanModel\u003cbr/\u003emodel_type='wan2.2_moe_high_noise'\"]\n        LowModel[\"WanModel\u003cbr/\u003emodel_type='wan2.2_moe_low_noise'\"]\n        \n        Multi[\"MultiModelStruct\u003cbr/\u003e[high_noise_model, low_noise_model]\"]\n    end\n    \n    ModelPath --\u003e HighOptions\n    ModelPath --\u003e LowOptions\n    \n    HighOptions --\u003e HighPath\n    LowOptions --\u003e LowPath\n    \n    HighPath --\u003e HighModel\n    LowPath --\u003e LowModel\n    \n    HighModel --\u003e Multi\n    LowModel --\u003e Multi\n```\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:586-625]()\n\n### load_transformer() Method\n\nThe `load_transformer()` method in `Wan22MoeRunner` constructs the `MultiModelStruct`:\n\n```python\n# Conceptual flow from lightx2v/models/runners/wan/wan_runner.py:595-625\ndef load_transformer(self):\n    # Determine paths for high-noise model\n    if config.get(\"dit_quantized\") and config.get(\"high_noise_quantized_ckpt\"):\n        high_noise_model_path = config[\"high_noise_quantized_ckpt\"]\n    elif config.get(\"high_noise_original_ckpt\"):\n        high_noise_model_path = config[\"high_noise_original_ckpt\"]\n    else:\n        high_noise_model_path = os.path.join(config[\"model_path\"], \"high_noise_model\")\n    \n    # Same for low-noise model...\n    \n    # Build models with optional LoRA\n    high_noise_model = WanModel(\n        model_path=high_noise_model_path,\n        config=config,\n        device=init_device,\n        model_type=\"wan2.2_moe_high_noise\"\n    )\n    low_noise_model = WanModel(\n        model_path=low_noise_model_path,\n        config=config,\n        device=init_device,\n        model_type=\"wan2.2_moe_low_noise\"\n    )\n    \n    return MultiModelStruct([high_noise_model, low_noise_model], config)\n```\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:595-625]()\n\n---\n\n## Memory Optimization Strategies\n\n### CPU Offloading at Model Granularity\n\nThe MoE architecture supports offloading inactive models to CPU memory to reduce GPU memory usage:\n\n| Configuration | Behavior |\n|---------------|----------|\n| `cpu_offload=True` \u003cbr/\u003e `offload_granularity=\"model\"` | Only the active model resides in GPU memory; inactive model moved to CPU |\n| `cpu_offload=True` \u003cbr/\u003e `offload_granularity=\"block\"` | Both models on GPU, but individual transformer blocks offloaded |\n| `cpu_offload=False` | Both models kept on GPU throughout inference |\n\n**Model Swapping Operations:**\n\n```mermaid\nsequenceDiagram\n    participant Step as \"Inference Step\"\n    participant Logic as \"get_current_model_index()\"\n    participant HighGPU as \"High-Noise Model (GPU)\"\n    participant HighCPU as \"High-Noise Model (CPU)\"\n    participant LowGPU as \"Low-Noise Model (GPU)\"\n    participant LowCPU as \"Low-Noise Model (CPU)\"\n    \n    Note over Step: timestep \u003e= boundary_timestep\n    Step-\u003e\u003eLogic: Check timestep\n    Logic-\u003e\u003eHighCPU: to_cuda(model_index=0)\n    HighCPU--\u003e\u003eHighGPU: Weights moved to GPU\n    HighGPU-\u003e\u003eStep: Execute inference\n    \n    Note over Step: timestep \u003c boundary_timestep\n    Step-\u003e\u003eLogic: Check timestep\n    Logic-\u003e\u003eHighGPU: offload_cpu(model_index=0)\n    HighGPU--\u003e\u003eHighCPU: Weights moved to CPU\n    Logic-\u003e\u003eLowCPU: to_cuda(model_index=1)\n    LowCPU--\u003e\u003eLowGPU: Weights moved to GPU\n    LowGPU-\u003e\u003eStep: Execute inference\n```\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:561-576]()\n- [lightx2v/models/runners/wan/wan_runner.py:579-583]()\n\n### Lazy Loading Support\n\nFor extremely memory-constrained environments, the system supports lazy loading where models are instantiated only when needed:\n\n**Configuration:**\n```json\n{\n  \"lazy_load\": true,\n  \"cpu_offload\": true,\n  \"offload_granularity\": \"model\"\n}\n```\n\n**Lazy Loading Behavior:**\n1. Initialize `MultiModelStruct([None, None], config)`\n2. Store model paths: `model_struct.high_noise_model_path`, `model_struct.low_noise_model_path`\n3. On first use, dynamically load the required model\n4. After transition boundary, unload previous model and load next\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:517-554]()\n- [lightx2v/models/networks/wan/model.py:438-444]()\n\n---\n\n## Configuration Options\n\n### Core Parameters\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `model_cls` | `str` | `\"wan2.2_moe\"` | Model class identifier for registry |\n| `model_path` | `str` | Required | Base path containing `high_noise_model/` and `low_noise_model/` subdirectories |\n| `boundary` | `float` | `0.875` | Timestep ratio for switching models |\n| `sample_guide_scale` | `List[float]` | `[7.5, 7.5]` | CFG guidance scales for `[high_noise, low_noise]` models |\n| `cpu_offload` | `bool` | `False` | Enable CPU offloading |\n| `offload_granularity` | `str` | `\"block\"` | Offloading granularity: `\"model\"`, `\"block\"`, or `\"phase\"` |\n\n### Model Path Configuration\n\n**Option 1: Directory Structure (Default)**\n```\nmodel_path/\n high_noise_model/\n    non_block.safetensors\n    block_*.safetensors\n low_noise_model/\n     non_block.safetensors\n     block_*.safetensors\n```\n\n**Option 2: Explicit Checkpoint Paths**\n```json\n{\n  \"high_noise_original_ckpt\": \"/path/to/high_noise.safetensors\",\n  \"low_noise_original_ckpt\": \"/path/to/low_noise.safetensors\"\n}\n```\n\n**Option 3: Quantized Checkpoints**\n```json\n{\n  \"dit_quantized\": true,\n  \"high_noise_quantized_ckpt\": \"/path/to/high_noise_fp8.safetensors\",\n  \"low_noise_quantized_ckpt\": \"/path/to/low_noise_fp8.safetensors\"\n}\n```\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:590-625]()\n\n### LoRA Integration\n\nBoth models support LoRA adapters with model-specific configurations:\n\n```json\n{\n  \"lora_configs\": [\n    {\n      \"name\": \"high_noise_model\",\n      \"path\": \"/path/to/high_noise_lora.safetensors\",\n      \"strength\": 1.0\n    },\n    {\n      \"name\": \"low_noise_model\",\n      \"path\": \"/path/to/low_noise_lora.safetensors\",\n      \"strength\": 0.8\n    }\n  ],\n  \"lora_dynamic_apply\": true\n}\n```\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:35-65]()\n- [lightx2v/models/runners/wan/wan_runner.py:524-551]()\n\n---\n\n## Inference Flow\n\n### Complete Generation Pipeline\n\n```mermaid\nflowchart TD\n    Start[\"run_pipeline(input_info)\"]\n    EncodeInputs[\"Run Encoders:\u003cbr/\u003e- Text: T5\u003cbr/\u003e- Image: CLIP + VAE\u003cbr/\u003e- (Audio: Seko)\"]\n    \n    InitScheduler[\"scheduler.prepare()\u003cbr/\u003e- Initialize latents\u003cbr/\u003e- Set timesteps\"]\n    \n    Loop[\"For each step_index\"]\n    \n    PreStep[\"scheduler.step_pre()\"]\n    CheckModel{\"get_current_model_index()\"}\n    \n    SelectHigh[\"Active: High-Noise Model\"]\n    SelectLow[\"Active: Low-Noise Model\"]\n    \n    Infer[\"model[cur_model_index].infer(inputs):\u003cbr/\u003e1. Pre-inference\u003cbr/\u003e2. Transformer blocks\u003cbr/\u003e3. Post-inference\"]\n    \n    PostStep[\"scheduler.step_post():\u003cbr/\u003e- Update latents\u003cbr/\u003e- Apply noise prediction\"]\n    \n    CheckDone{\"All steps complete?\"}\n    \n    DecodeVAE[\"vae_decoder.decode(latents)\"]\n    Output[\"Return generated video\"]\n    \n    Start --\u003e EncodeInputs\n    EncodeInputs --\u003e InitScheduler\n    InitScheduler --\u003e Loop\n    Loop --\u003e PreStep\n    PreStep --\u003e CheckModel\n    \n    CheckModel --\u003e|timestep \u003e= boundary| SelectHigh\n    CheckModel --\u003e|timestep \u003c boundary| SelectLow\n    \n    SelectHigh --\u003e Infer\n    SelectLow --\u003e Infer\n    \n    Infer --\u003e PostStep\n    PostStep --\u003e CheckDone\n    \n    CheckDone --\u003e|No| Loop\n    CheckDone --\u003e|Yes| DecodeVAE\n    DecodeVAE --\u003e Output\n```\n\n**Sources:** \n- [lightx2v/models/runners/default_runner.py:359-389]()\n- [lightx2v/models/runners/wan/wan_runner.py:515-577]()\n\n### Step-by-Step Example\n\nFor a 40-step I2V generation with default `boundary=0.875`:\n\n1. **Steps 0-34** (timesteps 1000875): High-Noise Model\n   - Coarse structure generation\n   - Major compositional elements\n   - Guidance scale: `sample_guide_scale[0]`\n\n2. **Model Transition** (at step 35, timestep ~874)\n   - Optional: Offload high-noise model to CPU\n   - Optional: Load low-noise model from CPU to GPU\n\n3. **Steps 35-39** (timesteps 8740): Low-Noise Model\n   - Fine detail refinement\n   - Texture and sharpness\n   - Guidance scale: `sample_guide_scale[1]`\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:557-577]()\n\n---\n\n## Model Variants\n\n### 1. Standard MoE (wan2.2_moe)\n\n**Registration:** `@RUNNER_REGISTER(\"wan2.2_moe\")`  \n**Runner Class:** `Wan22MoeRunner`  \n**Use Cases:** Text-to-Video (T2V), Image-to-Video (I2V), First-Last-Frame-to-Video (FLF2V)\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:586-625]()\n- [lightx2v/infer.py:52]()\n\n### 2. Audio MoE (wan2.2_moe_audio)\n\n**Registration:** `@RUNNER_REGISTER(\"wan2.2_moe_audio\")`  \n**Runner Class:** `Wan22AudioRunner`  \n**Extends:** Audio-driven video generation with MoE architecture\n\n**Key Differences:**\n- Integrates `SekoAudioEncoder` and `AudioAdapter`\n- Supports multi-person audio segmentation\n- Uses `WanAudioModel` for transformer inference\n- VAE: `Wan2_2_VAE` (audio-optimized)\n\n**Additional Configuration:**\n```json\n{\n  \"model_cls\": \"wan2.2_moe_audio\",\n  \"task\": \"s2v\",\n  \"audio_sr\": 16000,\n  \"target_fps\": 16,\n  \"prev_frame_length\": 5\n}\n```\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_audio_runner.py:276-789]()\n- [lightx2v/infer.py:54]()\n\n### 3. Distilled MoE (wan2.2_moe_distill)\n\n**Registration:** `@RUNNER_REGISTER(\"wan2.2_moe_distill\")`  \n**Runner Class:** `Wan22MoeDistillRunner`  \n**Purpose:** Step-distilled variant for faster inference (4 steps instead of 40)\n\n**Key Differences:**\n- Uses `MultiDistillModelStruct` instead of `MultiModelStruct`\n- Switches based on `boundary_step_index` (e.g., step 2) rather than timestep\n- Scheduler: `Wan22StepDistillScheduler`\n- No CFG (guidance-free generation)\n\n**Configuration:**\n```json\n{\n  \"model_cls\": \"wan2.2_moe_distill\",\n  \"infer_steps\": 4,\n  \"boundary_step_index\": 2,\n  \"enable_cfg\": false\n}\n```\n\n**Step Boundary Logic:**\n- Steps 0-1: High-Noise Model\n- Steps 2-3: Low-Noise Model\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_distill_runner.py:120-176]()\n- [lightx2v/models/runners/wan/wan_distill_runner.py:46-77]()\n- [lightx2v/infer.py:56]()\n\n---\n\n## Comparison with Single-Model Wan 2.2\n\n### Architecture Comparison\n\n| Aspect | Wan 2.2 (Single Model) | Wan 2.2 MoE (Dual Model) |\n|--------|------------------------|--------------------------|\n| **Model Structure** | Single `WanModel` | Two `WanModel` instances in `MultiModelStruct` |\n| **Denoising Strategy** | Uniform model across all timesteps | Specialized models for high/low noise |\n| **Parameter Count** | ~14B parameters | ~14B  2 = ~28B parameters (not loaded simultaneously with offloading) |\n| **Memory Usage (without offload)** | ~26GB VRAM | ~52GB VRAM |\n| **Memory Usage (with model offload)** | ~26GB VRAM | ~26GB VRAM (+ CPU RAM) |\n| **Inference Speed** | Baseline | +10-20% overhead (model switching) |\n| **Quality** | Good | Better (specialized optimization) |\n| **Guidance Scale** | Single value | Dual values (per model) |\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:495-584]()\n- [lightx2v/models/runners/wan/wan_runner.py:586-625]()\n\n### When to Use MoE vs. Single Model\n\n**Use MoE when:**\n- Maximum quality is required\n- Memory is available (or CPU offloading acceptable)\n- Generating at 720p or higher resolution\n- Fine detail preservation is critical\n\n**Use Single Model when:**\n- Memory is extremely constrained\n- Inference speed is paramount\n- Generating at 480p or lower resolution\n- Using quantization (FP8/INT8)\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:586-625]()\n\n---\n\n## Code Integration Points\n\n### Runner Registration\n\n```python\n# From lightx2v/infer.py\nfrom lightx2v.models.runners.wan.wan_runner import Wan22MoeRunner\n\n# Automatic registration via decorator in wan_runner.py:586\n@RUNNER_REGISTER(\"wan2.2_moe\")\nclass Wan22MoeRunner(WanRunner):\n    ...\n```\n\n### Command-Line Usage\n\n```bash\npython -m lightx2v.infer \\\n  --model_cls wan2.2_moe \\\n  --task i2v \\\n  --model_path /path/to/wan2.2_moe \\\n  --config_json configs/wan2.2_moe.json \\\n  --image_path input.jpg \\\n  --save_result_path output.mp4\n```\n\n### Python API Usage\n\n```python\nfrom lightx2v.utils.set_config import set_config\nfrom lightx2v.utils.registry_factory import RUNNER_REGISTER\nfrom lightx2v.utils.input_info import set_input_info\n\n# Configure\nconfig = {\n    \"model_cls\": \"wan2.2_moe\",\n    \"model_path\": \"/path/to/wan2.2_moe\",\n    \"task\": \"i2v\",\n    \"boundary\": 0.875,\n    \"sample_guide_scale\": [7.5, 5.0],\n    \"cpu_offload\": True,\n    \"offload_granularity\": \"model\"\n}\n\n# Initialize runner\nrunner = RUNNER_REGISTER[\"wan2.2_moe\"](config)\nrunner.init_modules()\n\n# Prepare input\ninput_info = set_input_info({\n    \"task\": \"i2v\",\n    \"image_path\": \"input.jpg\",\n    \"prompt\": \"A serene mountain landscape\",\n    \"seed\": 42\n})\n\n# Generate\nresult = runner.run_pipeline(input_info)\n```\n\n**Sources:** \n- [lightx2v/infer.py:30-146]()\n- [lightx2v/models/runners/wan/wan_runner.py:586-625]()\n\n---\n\n## Performance Characteristics\n\n### Memory Footprint\n\n| Configuration | GPU Memory (BF16) | CPU Memory | Notes |\n|---------------|-------------------|------------|-------|\n| Both models on GPU | ~52GB | ~4GB | No offloading |\n| Model-granularity offload | ~26GB | ~26GB | Recommended |\n| Block-granularity offload | ~40GB | ~12GB | Both models on GPU, blocks offloaded |\n| Lazy load + model offload | ~26GB | ~30GB | Minimal GPU usage |\n| FP8 quantization | ~26GB | ~2GB | Quantized weights |\n| FP8 + model offload | ~13GB | ~13GB | Maximum memory efficiency |\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:561-576]()\n- [lightx2v/models/networks/wan/model.py:438-476]()\n\n### Inference Time Breakdown\n\nFor a typical 40-step 720p I2V generation on RTX 4090:\n\n| Phase | Single Model | MoE (No Offload) | MoE (Model Offload) |\n|-------|--------------|------------------|---------------------|\n| Encoding | ~2s | ~2s | ~2s |\n| Steps 0-34 (High) | ~45s | ~45s | ~45s + ~3s (load) |\n| Model Switch | 0s | 0s | ~5s (offload + load) |\n| Steps 35-39 (Low) | ~5s | ~5s | ~5s |\n| VAE Decode | ~3s | ~3s | ~3s |\n| **Total** | **~55s** | **~55s** | **~63s** |\n\n**Overhead:** Model offloading adds ~15% latency but enables execution on lower-memory GPUs.\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:557-577]()\n\n---\n\n## Advanced Topics\n\n### Custom Boundary Tuning\n\nThe `boundary` parameter can be tuned based on the generation task:\n\n| Boundary Value | High-Noise Steps | Low-Noise Steps | Use Case |\n|----------------|------------------|-----------------|----------|\n| `0.95` | 38 steps | 2 steps | Maximum detail refinement |\n| `0.875` (default) | 35 steps | 5 steps | Balanced |\n| `0.75` | 30 steps | 10 steps | More refinement time |\n| `0.50` | 20 steps | 20 steps | Equal split |\n\n**Configuration:**\n```json\n{\n  \"boundary\": 0.75\n}\n```\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:496-501]()\n\n### Per-Model Guidance Scale Tuning\n\nDifferent guidance scales can be applied to each model for optimal quality:\n\n```json\n{\n  \"enable_cfg\": true,\n  \"sample_guide_scale\": [8.0, 6.0]\n}\n```\n\n- `sample_guide_scale[0]`: Guidance for high-noise model (typically higher for structure)\n- `sample_guide_scale[1]`: Guidance for low-noise model (typically lower for natural detail)\n\n**Sources:** \n- [lightx2v/models/runners/wan/wan_runner.py:560, 570]()\n\n### Distributed Inference\n\nThe MoE architecture supports distributed inference with sequence parallelism:\n\n```json\n{\n  \"parallel\": {\n    \"seq_p_size\": 2,\n    \"cfg_p_size\": 2\n  }\n}\n```\n\n**Considerations:**\n- Both models must be sharded identically\n- Model switching occurs synchronously across all ranks\n- Sequence parallel state is maintained during transitions\n\n**Sources:** \n- [lightx2v/models/networks/wan/model.py:486-533]()"])</script><script>self.__next_f.push([1,"33:T6cdf,"])</script><script>self.__next_f.push([1,"# QwenImageRunner - Text/Image-to-Image Generation\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py](lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py)\n- [lightx2v/models/networks/qwen_image/infer/pre_infer.py](lightx2v/models/networks/qwen_image/infer/pre_infer.py)\n- [lightx2v/models/networks/qwen_image/infer/transformer_infer.py](lightx2v/models/networks/qwen_image/infer/transformer_infer.py)\n- [lightx2v/models/networks/qwen_image/model.py](lightx2v/models/networks/qwen_image/model.py)\n- [lightx2v/models/runners/qwen_image/qwen_image_runner.py](lightx2v/models/runners/qwen_image/qwen_image_runner.py)\n- [lightx2v/models/schedulers/qwen_image/scheduler.py](lightx2v/models/schedulers/qwen_image/scheduler.py)\n- [lightx2v/models/video_encoders/hf/qwen_image/vae.py](lightx2v/models/video_encoders/hf/qwen_image/vae.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nQwenImageRunner implements image generation using Qwen-Image models, supporting both text-to-image (T2I) and image-to-image (I2I) tasks. The runner handles vision-language text encoding, transformer-based diffusion, and VAE decoding to generate high-quality images at flexible resolutions. For video generation tasks, see [WanRunner](#5.3). For other image generation models, see [ZImageRunner](#5.7) and [LongCatImageRunner](#5.9).\n\nSources: [lightx2v/models/runners/qwen_image/qwen_image_runner.py:53-60]()\n\n---\n\n## System Architecture\n\nQwenImageRunner orchestrates four main components: a vision-language text encoder (`Qwen25_VLForConditionalGeneration_TextEncoder`), a diffusion transformer (`QwenImageTransformerModel`), a scheduler (`QwenImageScheduler`), and a VAE (`AutoencoderKLQwenImageVAE`).\n\n```mermaid\ngraph TB\n    subgraph \"QwenImageRunner\"\n        RunPipeline[\"run_pipeline()\u003cbr/\u003eMain orchestration\"]\n        InputEncoder[\"run_input_encoder()\u003cbr/\u003eText/Image encoding\"]\n        DiT[\"run_dit()\u003cbr/\u003eDiffusion transformer\"]\n        VAEDecoder[\"run_vae_decoder()\u003cbr/\u003eLatent  Pixel\"]\n    end\n    \n    subgraph \"Text Encoder Options\"\n        Baseline[\"Qwen25_VLForConditionalGeneration\u003cbr/\u003eHuggingFace baseline\"]\n        LightLLMKernel[\"LightLLMKernelTextEncoder\u003cbr/\u003eTriton-optimized\"]\n        LightLLMService[\"LightLLMServiceTextEncoder\u003cbr/\u003eHTTP service\"]\n    end\n    \n    subgraph \"Core Models\"\n        TransformerModel[\"QwenImageTransformerModel\u003cbr/\u003eDiffusion transformer\"]\n        VAEModel[\"AutoencoderKLQwenImageVAE\u003cbr/\u003e16-channel latent space\"]\n        Scheduler[\"QwenImageScheduler\u003cbr/\u003eFlow matching Euler\"]\n    end\n    \n    subgraph \"Task Types\"\n        T2I[\"Text-to-Image (T2I)\u003cbr/\u003eText prompt only\"]\n        I2I[\"Image-to-Image (I2I)\u003cbr/\u003eText + condition images\"]\n    end\n    \n    RunPipeline --\u003e InputEncoder\n    InputEncoder --\u003e DiT\n    DiT --\u003e VAEDecoder\n    \n    InputEncoder --\u003e Baseline\n    InputEncoder --\u003e LightLLMKernel\n    InputEncoder --\u003e LightLLMService\n    \n    DiT --\u003e TransformerModel\n    DiT --\u003e Scheduler\n    VAEDecoder --\u003e VAEModel\n    \n    T2I --\u003e InputEncoder\n    I2I --\u003e InputEncoder\n```\n\nSources: [lightx2v/models/runners/qwen_image/qwen_image_runner.py:53-141](), [lightx2v/models/runners/qwen_image/qwen_image_runner.py:373-405]()\n\n---\n\n## Runner Registration and Configuration\n\nQwenImageRunner is registered in the runner registry with the identifier `\"qwen_image\"` and extends `DefaultRunner` for common pipeline functionality.\n\n| Configuration Key | Description | Default |\n|------------------|-------------|---------|\n| `task` | Task type: \"t2i\" or \"i2i\" | Required |\n| `layered` | Enable layered generation | `False` |\n| `layers` | Number of layers (if layered) | `4` |\n| `resolution` | Target resolution | `1024` |\n| `text_encoder_type` | Text encoder: \"baseline\", \"lightllm_kernel\", \"lightllm_service\" | `\"baseline\"` |\n| `enable_cfg` | Enable classifier-free guidance | Required |\n| `sample_guide_scale` | CFG guidance scale | Required |\n| `infer_steps` | Number of diffusion steps | Required |\n\nSources: [lightx2v/models/runners/qwen_image/qwen_image_runner.py:53-70]()\n\n---\n\n## Inference Pipeline Flow\n\nThe runner follows a three-stage pipeline: input encoding, diffusion denoising, and VAE decoding. The pipeline differs based on whether the task is T2I or I2I.\n\n```mermaid\nflowchart TB\n    Start[\"run_pipeline(input_info)\"]\n    \n    subgraph \"Stage 1: Input Encoding\"\n        TaskSwitch{\"Task Type?\"}\n        T2IEncode[\"_run_input_encoder_local_t2i()\u003cbr/\u003eEncode text prompt\"]\n        I2IEncode[\"_run_input_encoder_local_i2i()\u003cbr/\u003eEncode text + images\"]\n        I2IProcess[\"Process images through\u003cbr/\u003eQwen VL processor\"]\n        VAEEncode[\"VAE encode images\u003cbr/\u003eto latents\"]\n    end\n    \n    subgraph \"Stage 2: Shape Calculation\"\n        SetShape[\"set_target_shape()\u003cbr/\u003eCalculate output dimensions\"]\n        SetImgShapes[\"set_img_shapes()\u003cbr/\u003eSet latent shapes\"]\n    end\n    \n    subgraph \"Stage 3: Diffusion\"\n        DiTLoop[\"_run_dit_local()\u003cbr/\u003eIterative denoising\"]\n        PrepareScheduler[\"scheduler.prepare()\"]\n        StepLoop[\"For each step:\u003cbr/\u003estep_pre  infer  step_post\"]\n    end\n    \n    subgraph \"Stage 4: Decoding\"\n        VAEDecode[\"run_vae_decoder(latents)\u003cbr/\u003eLatent  Pixel space\"]\n        Unpack[\"_unpack_latents()\u003cbr/\u003eUnpack 2x2 blocks\"]\n        Postprocess[\"image_processor.postprocess()\u003cbr/\u003eNormalize and convert\"]\n    end\n    \n    Start --\u003e TaskSwitch\n    TaskSwitch --\u003e|\"t2i\"| T2IEncode\n    TaskSwitch --\u003e|\"i2i\"| I2IEncode\n    I2IEncode --\u003e I2IProcess\n    I2IProcess --\u003e VAEEncode\n    \n    T2IEncode --\u003e SetShape\n    VAEEncode --\u003e SetShape\n    SetShape --\u003e SetImgShapes\n    \n    SetImgShapes --\u003e DiTLoop\n    DiTLoop --\u003e PrepareScheduler\n    PrepareScheduler --\u003e StepLoop\n    \n    StepLoop --\u003e VAEDecode\n    VAEDecode --\u003e Unpack\n    Unpack --\u003e Postprocess\n```\n\nSources: [lightx2v/models/runners/qwen_image/qwen_image_runner.py:373-405](), [lightx2v/models/runners/qwen_image/qwen_image_runner.py:127-141]()\n\n---\n\n## Text Encoding and Vision-Language Processing\n\nQwenImageRunner supports three text encoder implementations for different performance trade-offs. All encoders output text embeddings that condition the diffusion process.\n\n### Text Encoder Types\n\n**Baseline (HuggingFace)**: Standard implementation using `Qwen2_5_VLForConditionalGeneration` directly.\n\n**LightLLM Kernel**: Optimized with Triton kernels for faster inference while keeping the model locally loaded.\n\n**LightLLM Service**: Offloads text encoding to a remote HTTP service, freeing GPU memory.\n\n### Image-to-Image Conditioning\n\nFor I2I tasks, condition images are processed through the vision-language model to extract visual features. Images are resized to two resolutions: a smaller condition size (default 384384) for the VL model and a larger VAE size (10241024) for latent encoding.\n\n```mermaid\ngraph TB\n    subgraph \"T2I Path\"\n        T2IPrompt[\"Text prompt\"]\n        T2ITemplate[\"prompt_template_encode\u003cbr/\u003e{text}\"]\n        T2ITokenize[\"Tokenize with Qwen2Tokenizer\"]\n        T2IEncode[\"Qwen2_5_VL encode\u003cbr/\u003eExtract hidden states\"]\n        T2IOutput[\"prompt_embeds\u003cbr/\u003e[batch, seq_len, dim]\"]\n    end\n    \n    subgraph \"I2I Path\"\n        I2IPrompt[\"Text prompt + Images\"]\n        I2IResize[\"Resize images:\u003cbr/\u003econdition_size (384)\u003cbr/\u003evae_size (1024)\"]\n        I2IProcessor[\"Qwen2VLProcessor\u003cbr/\u003eProcess image tokens\"]\n        I2IEncode[\"Qwen2_5_VL encode\u003cbr/\u003ewith pixel_values\"]\n        I2IVAEEncode[\"VAE encode\u003cbr/\u003econdition images\"]\n        I2IOutput[\"prompt_embeds +\u003cbr/\u003eimage_latents\"]\n    end\n    \n    subgraph \"Prompt Template\"\n        UseImageID{\"USE_IMAGE_ID_IN_PROMPT?\"}\n        WithID[\"Picture 1: \u0026lt;image_pad\u0026gt;\u003cbr/\u003ePicture 2: \u0026lt;image_pad\u0026gt;\"]\n        WithoutID[\"\u0026lt;image_pad\u0026gt;\"]\n    end\n    \n    T2IPrompt --\u003e T2ITemplate\n    T2ITemplate --\u003e T2ITokenize\n    T2ITokenize --\u003e T2IEncode\n    T2IEncode --\u003e T2IOutput\n    \n    I2IPrompt --\u003e I2IResize\n    I2IResize --\u003e UseImageID\n    UseImageID --\u003e|True| WithID\n    UseImageID --\u003e|False| WithoutID\n    WithID --\u003e I2IProcessor\n    WithoutID --\u003e I2IProcessor\n    I2IProcessor --\u003e I2IEncode\n    I2IEncode --\u003e I2IVAEEncode\n    I2IVAEEncode --\u003e I2IOutput\n```\n\nSources: [lightx2v/models/runners/qwen_image/qwen_image_runner.py:90-118](), [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:156-253]()\n\n---\n\n## Target Shape Calculation and Aspect Ratios\n\nQwenImageRunner determines output resolution through aspect ratio presets or custom dimensions. The system ensures dimensions are divisible by 32 (VAE scale factor  2 for packing).\n\n### Supported Aspect Ratios\n\n| Aspect Ratio | Dimensions | Use Case |\n|--------------|-----------|----------|\n| 16:9 | 1664928 | Widescreen |\n| 9:16 | 9281664 | Portrait/Mobile |\n| 1:1 | 13281328 | Square |\n| 4:3 | 14721140 | Standard |\n| 3:4 | 7681024 | Vertical |\n\n### Custom Shape Handling\n\nWhen `target_shape` is provided, dimensions are validated against `max_custom_size` (default 1664) and `min_custom_size` (default 256). If dimensions exceed the maximum, they are proportionally scaled down.\n\n### Latent Shape Calculation\n\nThe final latent shape accounts for:\n1. **VAE compression**: 8 downsampling\n2. **Packing**: 22 block packing (requires height/width divisible by 2)\n3. **Layered generation**: Multiple layers if enabled\n\n```python\n# Standard latent shape\nheight = 2 * (int(height) // (vae_scale_factor * 2))  # vae_scale_factor=8\nwidth = 2 * (int(width) // (vae_scale_factor * 2))\nnum_channels_latents = in_channels // 4  # 16 channels / 4 = 4 per block\ntarget_shape = (1, 1, num_channels_latents, height, width)\n\n# Layered latent shape (if layered=True)\ntarget_shape = (1, layers + 1, num_channels_latents, height, width)\n```\n\nSources: [lightx2v/models/runners/qwen_image/qwen_image_runner.py:281-336](), [lightx2v/models/runners/qwen_image/qwen_image_runner.py:338-354]()\n\n---\n\n## Transformer Architecture and Inference\n\n`QwenImageTransformerModel` implements a joint text-image diffusion transformer with separate processing streams that interact through cross-attention.\n\n### Three-Phase Inference Structure\n\n```mermaid\ngraph LR\n    subgraph \"Pre-Infer Phase\"\n        PreLinear[\"img_in / txt_in\u003cbr/\u003eProject to model dim\"]\n        PreTimestep[\"Timestep embedding\u003cbr/\u003eSinCos  Linear\"]\n        PreNorm[\"txt_norm\u003cbr/\u003eNormalize text\"]\n    end\n    \n    subgraph \"Transformer Blocks\"\n        Block1[\"Block 0\"]\n        Block2[\"Block 1\"]\n        BlockN[\"Block N\"]\n        \n        Block1 --\u003e Block2\n        Block2 --\u003e BlockN\n    end\n    \n    subgraph \"Post-Infer Phase\"\n        PostNorm[\"img_norm_out\"]\n        PostLinear[\"linear\u003cbr/\u003eProject to latent dim\"]\n    end\n    \n    PreLinear --\u003e Block1\n    PreTimestep --\u003e Block1\n    PreNorm --\u003e Block1\n    \n    BlockN --\u003e PostNorm\n    PostNorm --\u003e PostLinear\n```\n\n### Transformer Block Structure\n\nEach transformer block processes image and text streams in parallel, then combines them through joint attention.\n\n```mermaid\ngraph TB\n    subgraph \"Single Transformer Block\"\n        Input[\"hidden_states (img)\u003cbr/\u003eencoder_hidden_states (txt)\u003cbr/\u003etemb (timestep)\"]\n        \n        subgraph \"Image Stream\"\n            ImgMod1[\"img_mod\u003cbr/\u003eModulation params\"]\n            ImgNorm1[\"img_norm1\"]\n            ImgQKV[\"to_q / to_k / to_v\u003cbr/\u003eImage QKV\"]\n            ImgRoPE[\"Apply 3D RoPE\u003cbr/\u003e(frame, height, width)\"]\n        end\n        \n        subgraph \"Text Stream\"\n            TxtMod1[\"txt_mod\u003cbr/\u003eModulation params\"]\n            TxtNorm1[\"txt_norm1\"]\n            TxtQKV[\"add_q_proj / add_k_proj / add_v_proj\u003cbr/\u003eText QKV\"]\n            TxtRoPE[\"Apply 1D RoPE\u003cbr/\u003e(sequence position)\"]\n        end\n        \n        subgraph \"Joint Attention\"\n            Concat[\"Concatenate [txt, img]\"]\n            FlashAttn[\"Flash Attention 3\u003cbr/\u003eor Sage Attention\"]\n            Split[\"Split  txt / img\"]\n            OutProj[\"to_out / to_add_out\"]\n            Gate1[\"Gate with img_gate1 / txt_gate1\"]\n        end\n        \n        subgraph \"Feed-Forward\"\n            FFNMod[\"img_mod2 / txt_mod2\"]\n            FFNNorm[\"img_norm2 / txt_norm2\"]\n            FFN[\"GELU + Linear\"]\n            Gate2[\"Gate with img_gate2 / txt_gate2\"]\n        end\n        \n        Input --\u003e ImgMod1\n        Input --\u003e TxtMod1\n        \n        ImgMod1 --\u003e ImgNorm1\n        ImgNorm1 --\u003e ImgQKV\n        ImgQKV --\u003e ImgRoPE\n        \n        TxtMod1 --\u003e TxtNorm1\n        TxtNorm1 --\u003e TxtQKV\n        TxtQKV --\u003e TxtRoPE\n        \n        ImgRoPE --\u003e Concat\n        TxtRoPE --\u003e Concat\n        Concat --\u003e FlashAttn\n        FlashAttn --\u003e Split\n        Split --\u003e OutProj\n        OutProj --\u003e Gate1\n        \n        Gate1 --\u003e FFNMod\n        FFNMod --\u003e FFNNorm\n        FFNNorm --\u003e FFN\n        FFN --\u003e Gate2\n    end\n```\n\n### Modulation Mechanism\n\nAdaptive Layer Normalization (AdaLN) modulates layer normalization with timestep-conditioned scale, shift, and gate parameters. This allows the model to adapt its processing based on the noise level.\n\n```python\n# Modulation formula\nshift, scale, gate = mod_params.chunk(3, dim=-1)\nmodulated = (normalized_x * (1 + scale)) + shift\noutput = residual + gate * layer_output(modulated)\n```\n\nFor layered generation with `zero_cond_t=True`, the modulation index selects different parameters for the condition layer versus generated layers.\n\nSources: [lightx2v/models/networks/qwen_image/model.py:48-146](), [lightx2v/models/networks/qwen_image/infer/transformer_infer.py:278-367]()\n\n---\n\n## VAE Encoding and Decoding\n\n`AutoencoderKLQwenImageVAE` implements a 16-channel latent space with packing for efficient processing.\n\n### Latent Space Properties\n\n| Property | Value | Description |\n|----------|-------|-------------|\n| Channels | 16 | Latent channels before packing |\n| Compression | 8 | Spatial downsampling factor |\n| Packing | 22 | Blocks packed into sequence |\n| Mean | 16 values | Per-channel normalization mean |\n| Std | 16 values | Per-channel normalization std |\n\n### Packing and Unpacking\n\nLatents are packed into 22 blocks to create a sequence representation that can be processed by transformers. This converts spatial dimensions to sequence length.\n\n```python\n# Packing (encode)\nlatents: [batch, 16, height, width] \n view as [batch, 16, height/2, 2, width/2, 2]\n permute to [batch, height/2, width/2, 16/4, 2, 2]\n reshape to [batch, (height/2 * width/2), 16*4]\n# Result: sequence of length (height/2 * width/2), dim 64\n\n# Unpacking (decode)\nlatents: [batch, num_patches, 64]\n view as [batch, height/2, width/2, 16/4, 2, 2]\n permute to [batch, 16/4, height/2, 2, width/2, 2]\n reshape to [batch, 16, height, width]\n```\n\n### Tiling Support\n\nFor large images, VAE tiling can be enabled with `use_tiling_vae=True` to process the image in overlapping tiles, preventing memory overflow.\n\nSources: [lightx2v/models/video_encoders/hf/qwen_image/vae.py:19-141]()\n\n---\n\n## Scheduler and Sampling\n\n`QwenImageScheduler` uses flow matching with Euler discrete sampling, based on Diffusers' `FlowMatchEulerDiscreteScheduler`.\n\n### Flow Matching Parameters\n\nThe scheduler uses a shift parameter `` that adapts based on the image resolution:\n\n```python\nmu = calculate_shift(\n    image_seq_len,\n    base_seq_len=256,      # Base sequence length\n    max_seq_len=4096,      # Maximum sequence length\n    base_shift=0.5,        # Shift at base resolution\n    max_shift=1.15         # Shift at max resolution\n)\n```\n\nHigher resolutions use larger `` values to adjust the noise schedule for better sample quality.\n\n### Timestep Schedule\n\nThe scheduler generates timesteps from sigma values:\n```python\nsigmas = np.linspace(1.0, 1/infer_steps, infer_steps)\ntimesteps = sigmas * 1000  # Scale to [0, 1000]\n```\n\n### 3D Rotary Position Embeddings\n\nQwen-Image uses 3D RoPE to encode spatial structure with separate frequencies for frame, height, and width dimensions.\n\n```mermaid\ngraph LR\n    subgraph \"RoPE Structure\"\n        Axes[\"3 Axes:\u003cbr/\u003eFrame (16 dim)\u003cbr/\u003eHeight (56 dim)\u003cbr/\u003eWidth (56 dim)\"]\n        \n        subgraph \"Image RoPE\"\n            ImgFrame[\"Frame indices:\u003cbr/\u003e0 for single frame\"]\n            ImgHeight[\"Height indices:\u003cbr/\u003e0 to height-1\"]\n            ImgWidth[\"Width indices:\u003cbr/\u003e0 to width-1\"]\n        end\n        \n        subgraph \"Text RoPE\"\n            TxtSeq[\"Sequence indices:\u003cbr/\u003emax_vid_index to\u003cbr/\u003emax_vid_index + seq_len\"]\n        end\n        \n        Combine[\"Combine with\u003cbr/\u003epolar coordinates\u003cbr/\u003ecos + i*sin\"]\n    end\n    \n    Axes --\u003e ImgFrame\n    Axes --\u003e ImgHeight\n    Axes --\u003e ImgWidth\n    Axes --\u003e TxtSeq\n    \n    ImgFrame --\u003e Combine\n    ImgHeight --\u003e Combine\n    ImgWidth --\u003e Combine\n    TxtSeq --\u003e Combine\n```\n\nFor `scale_rope=True`, the height and width indices are centered around zero using both positive and negative frequencies.\n\nSources: [lightx2v/models/schedulers/qwen_image/scheduler.py:419-596]()\n\n---\n\n## Layered Generation\n\nLayered generation enables creating images with transparency layers (RGBA) by generating multiple image layers simultaneously. This is useful for compositing and layer-based editing.\n\n### Configuration\n\n```python\nconfig = {\n    \"layered\": True,\n    \"layers\": 4,  # Number of generated layers (excluding condition)\n    \"resolution\": 640  # Base resolution for layer generation\n}\n```\n\n### Layer Structure\n\nWhen `layered=True`:\n- **Input**: Condition layer (existing image)\n- **Output**: N generated layers plus condition layer\n- **Total frames**: `layers + 1` (e.g., 5 frames for 4 layers + 1 condition)\n\n### Latent Shape Differences\n\n**Standard generation**:\n```python\ntarget_shape = (1, 1, num_channels_latents, height, width)\nlatents_packed = [batch, height/2 * width/2, 64]\n```\n\n**Layered generation**:\n```python\ntarget_shape = (1, layers+1, num_channels_latents, height, width)\nlatents_packed = [batch, (layers+1) * height/2 * width/2, 64]\n```\n\n### Layer-Specific RoPE\n\nLayered generation uses `QwenEmbedLayer3DRope` which assigns different frame indices to each layer:\n- Generated layers: frame indices 0, 1, 2, 3\n- Condition layer: frame index -1 (negative index for distinction)\n\nThis allows the model to distinguish between generated layers and the conditioning layer through positional encoding.\n\nSources: [lightx2v/models/runners/qwen_image/qwen_image_runner.py:60-63](), [lightx2v/models/schedulers/qwen_image/scheduler.py:304-417](), [lightx2v/models/video_encoders/hf/qwen_image/vae.py:77-86]()\n\n---\n\n## CFG and Parallel Processing\n\nClassifier-free guidance (CFG) computes both conditional and unconditional predictions, then combines them to strengthen prompt adherence.\n\n### Standard CFG Implementation\n\n```python\n# Conditional prediction\nnoise_pred_cond = model.infer(latents, prompt_embeds)\n\n# Unconditional prediction\nnoise_pred_uncond = model.infer(latents, negative_prompt_embeds)\n\n# Combine with guidance scale\ncombined = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n\n# Normalize to preserve magnitude\nnoise_pred = combined * (||noise_pred_cond|| / ||combined||)\n```\n\n### CFG Parallel Mode\n\nWhen `cfg_parallel=True`, conditional and unconditional predictions are computed on separate GPUs simultaneously:\n\n```mermaid\ngraph LR\n    subgraph \"CFG Parallel (2 GPUs)\"\n        GPU0[\"GPU 0:\u003cbr/\u003eConditional\u003cbr/\u003eprompt_embeds\"]\n        GPU1[\"GPU 1:\u003cbr/\u003eUnconditional\u003cbr/\u003enegative_prompt_embeds\"]\n        \n        AllGather[\"all_gather()\u003cbr/\u003eSynchronize results\"]\n        Combine[\"Combine predictions\u003cbr/\u003ewith guidance_scale\"]\n    end\n    \n    GPU0 --\u003e AllGather\n    GPU1 --\u003e AllGather\n    AllGather --\u003e Combine\n```\n\nThis doubles throughput when two GPUs are available by overlapping the two inference passes.\n\nSources: [lightx2v/models/networks/qwen_image/model.py:106-146]()\n\n---\n\n## Sequence Parallelism\n\nFor very large resolutions, sequence parallelism splits the spatial tokens across multiple GPUs to reduce memory footprint per device.\n\n### Sequence Splitting\n\n```python\n# Before seq parallel\nhidden_states: [seq_len, hidden_dim]\n\n# After seq parallel (rank 0 of 4 GPUs)\nhidden_states_local: [seq_len/4, hidden_dim]\n```\n\nPadding is added if sequence length is not evenly divisible by world size.\n\n### Communication Pattern\n\n```mermaid\ngraph TB\n    subgraph \"Forward Pass\"\n        Chunk[\"Chunk sequence\u003cbr/\u003eacross GPUs\"]\n        LocalCompute[\"Local attention\u003cbr/\u003eon each GPU\"]\n        AllGather[\"all_gather()\u003cbr/\u003eGather results\"]\n    end\n    \n    subgraph \"Attention Options\"\n        SeqParallelAttn[\"calculate_parallel()\u003cbr/\u003eSequence-parallel attention\"]\n        HeadParallel[\"enable_head_parallel\u003cbr/\u003eSplit attention heads\"]\n        FP8Comm[\"seq_p_fp8_comm\u003cbr/\u003eFP8 communication\"]\n    end\n    \n    Chunk --\u003e LocalCompute\n    LocalCompute --\u003e AllGather\n    \n    LocalCompute --\u003e SeqParallelAttn\n    SeqParallelAttn --\u003e HeadParallel\n    SeqParallelAttn --\u003e FP8Comm\n```\n\nFP8 communication (`seq_p_fp8_comm=True`) quantizes activation tensors to FP8 during all-gather operations to reduce bandwidth requirements.\n\nSources: [lightx2v/models/networks/qwen_image/model.py:72-88](), [lightx2v/models/networks/qwen_image/infer/transformer_infer.py:208-221]()\n\n---\n\n## CPU Offloading and Lazy Loading\n\nQwenImageRunner supports two memory optimization strategies for large models on limited VRAM.\n\n### CPU Offloading\n\nModels are kept on CPU and moved to GPU only when needed:\n\n```python\nconfig = {\n    \"cpu_offload\": True,\n    \"offload_granularity\": \"model\"  # or \"block\", \"phase\"\n}\n```\n\n**Offload sequence**: `text_encoder  transformer  vae`\n\nModels are offloaded in this order after their inference stage completes.\n\n### Lazy Loading\n\nWeights are loaded on-demand during inference rather than at initialization:\n\n```python\nconfig = {\n    \"lazy_load\": True,\n    \"cpu_offload\": True  # Required with lazy_load\n}\n```\n\nWith lazy loading, the model weights are loaded immediately before the DiT inference stage and unloaded after:\n\n```python\n# In _run_dit_local\nif config.get(\"lazy_load\"):\n    self.model = self.load_transformer()  # Load weights\n    self.model.set_scheduler(self.scheduler)\n    \nlatents, generator = self.run(total_steps)  # Run inference\n\n# Model automatically garbage collected after\n```\n\nThis pattern is repeated for text encoders and VAE, loading each component only when needed.\n\nSources: [lightx2v/models/runners/qwen_image/qwen_image_runner.py:127-165](), [lightx2v/models/runners/qwen_image/qwen_image_runner.py:182-208]()\n\n---\n\n## LoRA Support\n\nQwenImageRunner supports LoRA (Low-Rank Adaptation) for model customization with two application modes:\n\n### Dynamic LoRA Application\n\nWhen `lora_dynamic_apply=True`, LoRA weights are applied during inference without merging into base weights:\n\n```python\nlora_configs = [{\n    \"path\": \"/path/to/lora\",\n    \"strength\": 1.0\n}]\n\nconfig = {\n    \"lora_configs\": lora_configs,\n    \"lora_dynamic_apply\": True\n}\n```\n\nThis mode is compatible with quantized models and enables runtime LoRA switching.\n\n### Static LoRA Merging\n\nWhen `lora_dynamic_apply=False`, LoRA weights are merged into the base model at initialization:\n\n```python\nmodel = QwenImageTransformerModel(**model_kwargs)\nlora_adapter = LoraAdapter(model)\nlora_adapter.apply_lora(lora_configs)\n```\n\n**Restrictions**:\n- Cannot be used with quantized models (`dit_quantized=False`)\n- Cannot be used with lazy loading (`lazy_load=False`)\n\nSources: [lightx2v/models/runners/qwen_image/qwen_image_runner.py:35-50](), [lightx2v/models/runners/qwen_image/qwen_image_runner.py:77-88]()\n\n---\n\n## Module Weight Structure\n\nThe transformer model is divided into three weight groups for efficient memory management and potential offloading:\n\n### Weight Components\n\n```mermaid\ngraph TB\n    subgraph \"QwenImageTransformerWeights\"\n        PreWeights[\"QwenImagePreWeights\u003cbr/\u003e- img_in\u003cbr/\u003e- txt_norm\u003cbr/\u003e- txt_in\u003cbr/\u003e- time_text_embed\"]\n        \n        TransformerWeights[\"QwenImageTransformerWeights\u003cbr/\u003e- blocks[N]\u003cbr/\u003e   compute_phases[4]\u003cbr/\u003e      img_attn (to_q/k/v)\u003cbr/\u003e      txt_attn (add_q/k/v_proj)\u003cbr/\u003e      cross_attn (calculate)\u003cbr/\u003e      ffn (img_mlp, txt_mlp)\"]\n        \n        PostWeights[\"QwenImagePostWeights\u003cbr/\u003e- img_norm_out\u003cbr/\u003e- linear\"]\n    end\n    \n    PreWeights --\u003e TransformerWeights\n    TransformerWeights --\u003e PostWeights\n```\n\n### Compute Phases per Block\n\nEach transformer block has 4 compute phases executed sequentially:\n\n1. **Phase 0**: Image attention (self-attention for image tokens)\n2. **Phase 1**: Text attention (self-attention for text tokens)\n3. **Phase 2**: Cross-attention (joint attention between text and image)\n4. **Phase 3**: Feed-forward networks (MLP for both streams)\n\nThis structure enables fine-grained CPU offloading at the `phase` granularity level.\n\nSources: [lightx2v/models/networks/qwen_image/model.py:18-31]()\n\n---\n\n## Image Preprocessing and Preferred Resolutions\n\nFor I2I tasks, input images are preprocessed to match expected resolutions for the VL model and VAE.\n\n### Preferred Resolution List\n\nQwenImage prefers specific resolutions optimized for the model's training distribution:\n\n| Width | Height | Aspect Ratio |\n|-------|--------|--------------|\n| 672 | 1568 | ~0.43 |\n| 720 | 1456 | ~0.49 |\n| 800 | 1328 | ~0.60 |\n| 1024 | 1024 | 1.00 |\n| 1328 | 800 | ~1.66 |\n| 1456 | 720 | ~2.02 |\n| 1568 | 672 | ~2.33 |\n\nThe full list includes 17 resolutions spanning vertical to horizontal orientations.\n\n### Image Resize Strategy\n\n```python\n# Calculate target dimensions matching aspect ratio\ntarget_area = CONDITION_IMAGE_SIZE  # e.g., 384 * 384\nratio = image_width / image_height\nwidth = sqrt(target_area * ratio)\nheight = width / ratio\n\n# Round to multiples of 32\nwidth = round(width / 32) * 32\nheight = round(height / 32) * 32\n```\n\nTwo resized versions are created:\n1. **Condition image**: Smaller (384) for VL model visual features\n2. **VAE image**: Larger (1024) for latent encoding\n\nSources: [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:24-52](), [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:128-134]()\n\n---\n\n## Advanced Configuration Options\n\n### Attention Type Selection\n\n```python\nconfig = {\n    \"attn_type\": \"flash_attn3\"  # or \"flash_attn2\", \"sage_attn\"\n}\n```\n\nDetermines which optimized attention kernel is used for the joint attention computation.\n\n### RoPE Type Selection\n\n```python\nconfig = {\n    \"rope_type\": \"flashinfer\"  # or \"torch\", \"torch_naive\"\n}\n```\n\n- **flashinfer**: Pre-computed cos/sin caching with optimized kernels\n- **torch**: PyTorch complex number implementation\n- **torch_naive**: Simple rotary embedding formula\n\n### Modulation Type\n\n```python\nconfig = {\n    \"modulate_type\": \"triton\"  # or \"torch\"\n}\n```\n\nWhen `\"triton\"`, uses fused Triton kernels for scale-shift-gate operations, improving performance by reducing memory round-trips.\n\n### Zero Conditional Timestep\n\n```python\nconfig = {\n    \"zero_cond_t\": True  # For layered generation\n}\n```\n\nWhen enabled, the condition layer receives a timestep embedding of zero, while generated layers receive the actual timestep. This helps the model distinguish conditioning information from generation targets.\n\nSources: [lightx2v/models/networks/qwen_image/infer/transformer_infer.py:20-46](), [lightx2v/models/schedulers/qwen_image/scheduler.py:432]()\n\n---\n\n## Performance Monitoring\n\nQwenImageRunner integrates with the monitoring system to track inference metrics:\n\n```python\n# Metrics tracked\n- lightx2v_input_prompt_len: Length of input text prompt\n- lightx2v_input_image_len: Pixel count of input images (width  height)\n- lightx2v_run_text_encode_duration: Text encoding time\n- lightx2v_run_vae_encoder_image_duration: VAE encoding time\n- lightx2v_run_vae_decode_duration: VAE decoding time\n```\n\nThese metrics are recorded when `GET_RECORDER_MODE()` returns True, typically in production deployments.\n\nSources: [lightx2v/models/runners/qwen_image/qwen_image_runner.py:210-259]()"])</script><script>self.__next_f.push([1,"34:T8eff,"])</script><script>self.__next_f.push([1,"# ZImageRunner - Fast Image Generation\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [app/utils/image_page.py](app/utils/image_page.py)\n- [configs/z_image/z_image_turbo_t2i_offload.json](configs/z_image/z_image_turbo_t2i_offload.json)\n- [examples/z-image-turbo/z_image_turbo.py](examples/z-image-turbo/z_image_turbo.py)\n- [lightx2v/models/input_encoders/hf/z_image/qwen3_model.py](lightx2v/models/input_encoders/hf/z_image/qwen3_model.py)\n- [lightx2v/models/networks/base_model.py](lightx2v/models/networks/base_model.py)\n- [lightx2v/models/networks/z_image/model.py](lightx2v/models/networks/z_image/model.py)\n- [lightx2v/models/networks/z_image/weights/post_weights.py](lightx2v/models/networks/z_image/weights/post_weights.py)\n- [lightx2v/models/networks/z_image/weights/transformer_weights.py](lightx2v/models/networks/z_image/weights/transformer_weights.py)\n- [lightx2v/models/runners/z_image/z_image_runner.py](lightx2v/models/runners/z_image/z_image_runner.py)\n- [lightx2v/models/video_encoders/hf/z_image/vae.py](lightx2v/models/video_encoders/hf/z_image/vae.py)\n- [scripts/z_image/z_image_turbo_t2i.sh](scripts/z_image/z_image_turbo_t2i.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\n`ZImageRunner` is a specialized runner implementation for the Z-Image-Turbo model family, providing high-performance text-to-image (T2I) and image-to-image (I2I) generation. This page documents the runner's architecture, components, task support, and optimization strategies.\n\nFor general image generation with Qwen-Image models using dual-stream architecture and Qwen2.5VL text encoding, see [QwenImageRunner](#5.6). For the overall runner system architecture, see [Runner System and Registry Pattern](#4.1).\n\nSources: [lightx2v/models/runners/z_image/z_image_runner.py:1-401]()\n\n---\n\n## Overview and Positioning\n\n### Registration and Initialization\n\n`ZImageRunner` is registered in the runner factory with the identifier `\"z_image\"` and extends `DefaultRunner`:\n\n```python\n@RUNNER_REGISTER(\"z_image\")\nclass ZImageRunner(DefaultRunner):\n    model_cpu_offload_seq = \"text_encoder-\u003etransformer-\u003evae\"\n    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\"]\n```\n\nThe runner is specifically designed for Z-Image-Turbo models, which are optimized for fast inference with:\n- **Single-stream transformer architecture** (unlike QwenImageRunner's dual-stream)\n- **Qwen3 text encoder** (4B parameters, int4 quantizable)\n- **FP8 quantization support** for transformer weights\n- **Turbo scheduler** with default 9 inference steps\n- **Ada GPU optimization** via Q8F kernels\n\nSources: [lightx2v/models/runners/z_image/z_image_runner.py:52-59]()\n\n### Key Differentiators from QwenImageRunner\n\n| Feature | ZImageRunner | QwenImageRunner |\n|---------|-------------|-----------------|\n| Text Encoder | Qwen3Model (4B) | Qwen2.5VL (3B/7B) |\n| Transformer Architecture | Single-stream | Dual-stream (image/text) |\n| Modulation | AdaLN (if present) | AdaLN (layered/non-layered) |\n| Primary Use Case | Fast T2I with turbo mode | T2I/I2I with image understanding |\n| Default Steps | 9 (turbo) | 16-20 |\n| Quantization Focus | FP8-SGL, Q8F for Ada GPUs | FP8, INT8 |\n\nSources: [lightx2v/models/runners/z_image/z_image_runner.py:79-89](), [lightx2v/models/input_encoders/hf/z_image/qwen3_model.py:25-46]()\n\n---\n\n## Architecture Components\n\n### Component Hierarchy\n\n```mermaid\ngraph TB\n    subgraph \"ZImageRunner Components\"\n        Runner[\"ZImageRunner\u003cbr/\u003e(z_image_runner.py)\"]\n        \n        subgraph \"Model Layer\"\n            Model[\"ZImageTransformerModel\u003cbr/\u003e(z_image/model.py)\"]\n            Scheduler[\"ZImageScheduler\u003cbr/\u003e(z_image/scheduler.py)\"]\n        end\n        \n        subgraph \"Encoders\"\n            TextEnc[\"Qwen3Model_TextEncoder\u003cbr/\u003e(qwen3_model.py)\"]\n            VAE[\"AutoencoderKLZImageVAE\u003cbr/\u003e(z_image/vae.py)\"]\n        end\n        \n        subgraph \"Weight Structure\"\n            PreWeight[\"ZImagePreWeights\u003cbr/\u003e(pre_weights.py)\"]\n            TransWeight[\"ZImageTransformerWeights\u003cbr/\u003e(transformer_weights.py)\"]\n            PostWeight[\"ZImagePostWeights\u003cbr/\u003e(post_weights.py)\"]\n        end\n        \n        subgraph \"Inference Modules\"\n            PreInfer[\"ZImagePreInfer\u003cbr/\u003e(pre_infer.py)\"]\n            TransInfer[\"ZImageTransformerInfer\u003cbr/\u003e(transformer_infer.py)\"]\n            PostInfer[\"ZImagePostInfer\u003cbr/\u003e(post_infer.py)\"]\n        end\n    end\n    \n    Runner --\u003e Model\n    Runner --\u003e TextEnc\n    Runner --\u003e VAE\n    Runner --\u003e Scheduler\n    \n    Model --\u003e PreWeight\n    Model --\u003e TransWeight\n    Model --\u003e PostWeight\n    \n    Model --\u003e PreInfer\n    Model --\u003e TransInfer\n    Model --\u003e PostInfer\n    \n    PreInfer --\u003e PreWeight\n    TransInfer --\u003e TransWeight\n    PostInfer --\u003e PostWeight\n```\n\nSources: [lightx2v/models/runners/z_image/z_image_runner.py:60-89](), [lightx2v/models/networks/z_image/model.py:17-48]()\n\n### Model Loading Sequence\n\nThe `load_model()` method initializes components in dependency order:\n\n```python\ndef load_model(self):\n    self.model = self.load_transformer()        # ZImageTransformerModel\n    self.text_encoders = self.load_text_encoder()  # [Qwen3Model_TextEncoder]\n    self.vae = self.load_vae()                 # AutoencoderKLZImageVAE\n```\n\n**Transformer Loading with LoRA Support:**\n\n```python\ndef load_transformer(self):\n    z_image_model_kwargs = {\n        \"model_path\": os.path.join(self.config[\"model_path\"], \"transformer\"),\n        \"config\": self.config,\n        \"device\": self.init_device,\n    }\n    lora_configs = self.config.get(\"lora_configs\")\n    if not lora_configs:\n        model = ZImageTransformerModel(**z_image_model_kwargs)\n    else:\n        model = build_z_image_model_with_lora(\n            ZImageTransformerModel, self.config, \n            z_image_model_kwargs, lora_configs\n        )\n    return model\n```\n\nSources: [lightx2v/models/runners/z_image/z_image_runner.py:60-89](), [lightx2v/models/runners/z_image/z_image_runner.py:34-49]()\n\n---\n\n## Task Support and Initialization\n\n### Supported Tasks\n\n`ZImageRunner` supports two task modes configured via `config[\"task\"]`:\n\n| Task | Description | Required Inputs |\n|------|-------------|----------------|\n| `t2i` | Text-to-Image | `prompt`, `negative_prompt` (optional), `aspect_ratio` |\n| `i2i` | Image-to-Image | `prompt`, `image_path` (comma-separated), `negative_prompt` (optional) |\n\n**Task-Specific Runner Initialization:**\n\n```python\ndef init_modules(self):\n    if not self.config.get(\"lazy_load\", False) and not self.config.get(\"unload_modules\", False):\n        self.load_model()\n        self.model.set_scheduler(self.scheduler)\n    \n    self.run_dit = self._run_dit_local\n    \n    if self.config[\"task\"] == \"t2i\":\n        self.run_input_encoder = self._run_input_encoder_local_t2i\n    elif self.config[\"task\"] == \"i2i\":\n        self.run_input_encoder = self._run_input_encoder_local_i2i\n    else:\n        assert NotImplementedError\n```\n\nSources: [lightx2v/models/runners/z_image/z_image_runner.py:91-104]()\n\n### T2I Task Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Runner as ZImageRunner\n    participant TextEnc as Qwen3Model_TextEncoder\n    participant DIT as ZImageTransformerModel\n    participant Scheduler as ZImageScheduler\n    participant VAE as AutoencoderKLZImageVAE\n    \n    User-\u003e\u003eRunner: run_pipeline(input_info)\n    \n    Note over Runner: T2I Mode\n    Runner-\u003e\u003eRunner: run_input_encoder (T2I)\n    Runner-\u003e\u003eTextEnc: infer([prompt])\n    TextEnc--\u003e\u003eRunner: prompt_embeds (seq_len, hidden_dim)\n    \n    opt CFG Enabled\n        Runner-\u003e\u003eTextEnc: infer([negative_prompt])\n        TextEnc--\u003e\u003eRunner: negative_prompt_embeds\n    end\n    \n    Runner-\u003e\u003eRunner: set_target_shape()\n    Note over Runner: Calculate from aspect_ratio\u003cbr/\u003ee.g., \"16:9\" -\u003e [1664, 928]\n    \n    Runner-\u003e\u003eScheduler: prepare(input_info)\n    Scheduler--\u003e\u003eRunner: initial_latents\n    \n    loop For 9 inference steps\n        Runner-\u003e\u003eScheduler: step_pre(step_index)\n        Runner-\u003e\u003eDIT: infer(inputs)\n        Note over DIT: Pre-infer -\u003e Transformer -\u003e Post-infer\n        DIT--\u003e\u003eScheduler: noise_pred\n        Runner-\u003e\u003eScheduler: step_post()\n        Scheduler--\u003e\u003eRunner: updated_latents\n    end\n    \n    Runner-\u003e\u003eVAE: decode(final_latents)\n    VAE--\u003e\u003eRunner: images (PIL or tensor)\n    Runner--\u003e\u003eUser: save_result_path or tensor\n```\n\nSources: [lightx2v/models/runners/z_image/z_image_runner.py:115-128](), [lightx2v/models/runners/z_image/z_image_runner.py:371-401]()\n\n### I2I Task Flow\n\nFor I2I tasks, the runner processes input images through the text encoder's image preprocessing pipeline:\n\n```python\ndef _run_input_encoder_local_i2i(self):\n    image_paths_list = self.input_info.image_path.split(\",\")\n    images_list = []\n    for image_path in image_paths_list:\n        _, image = self.read_image_input(image_path)\n        images_list.append(image)\n    \n    prompt = self.input_info.prompt\n    # Text encoder handles both text and image preprocessing\n    text_encoder_output = self.run_text_encoder(\n        prompt, images_list, neg_prompt=self.input_info.negative_prompt\n    )\n    \n    # VAE encode images to latents\n    image_encoder_output_list = []\n    for vae_image in text_encoder_output[\"image_info\"][\"vae_image_list\"]:\n        image_encoder_output = self.run_vae_encoder(image=vae_image)\n        image_encoder_output_list.append(image_encoder_output)\n    \n    return {\n        \"text_encoder_output\": text_encoder_output,\n        \"image_encoder_output\": image_encoder_output_list,\n    }\n```\n\n**Image Reading with Auto-Resizing:**\n\nThe runner ensures images are divisible by `vae_scale_factor * 2`:\n\n```python\ndef read_image_input(self, img_path):\n    img_ori = Image.open(img_path).convert(\"RGB\") if not isinstance(img_path, Image.Image) else img_path\n    width, height = img_ori.size\n    \n    vae_scale_factor = self.config[\"vae_scale_factor\"]\n    vae_scale = vae_scale_factor * 2  # Typically 16 (8 * 2)\n    \n    if height % vae_scale != 0 or width % vae_scale != 0:\n        new_height = (height // vae_scale) * vae_scale\n        new_width = (width // vae_scale) * vae_scale\n        img_ori = img_ori.resize((new_width, new_height), Image.Resampling.LANCZOS)\n    \n    img = TF.to_tensor(img_ori).sub_(0.5).div_(0.5).unsqueeze(0).to(AI_DEVICE)\n    return img, img_ori\n```\n\nSources: [lightx2v/models/runners/z_image/z_image_runner.py:160-184](), [lightx2v/models/runners/z_image/z_image_runner.py:130-158]()\n\n---\n\n## Text Encoding with Qwen3\n\n### Qwen3Model_TextEncoder Architecture\n\nUnlike QwenImageRunner's Qwen2.5VL encoder, ZImageRunner uses a lightweight Qwen3 4B model:\n\n```python\nclass Qwen3Model_TextEncoder:\n    def __init__(self, config):\n        self.config = config\n        self.tokenizer_max_length = 512\n        self.cpu_offload = config.get(\"qwen3_cpu_offload\", config.get(\"cpu_offload\", False))\n        self.qwen3_quantized = config.get(\"qwen3_quantized\", False)\n        self.load()\n    \n    def load(self):\n        if self.qwen3_quantized:\n            assert self.config[\"qwen3_quant_scheme\"] == \"int4\"\n            self.text_encoder = Qwen3Model.from_pretrained(\n                self.config[\"qwen3_quantized_ckpt\"],  # e.g., \"JunHowie/Qwen3-4B-GPTQ-Int4\"\n                torch_dtype=GET_DTYPE(), \n                device_map=AI_DEVICE\n            )\n        else:\n            qwen3_original_ckpt = self.config.get(\n                \"qwen3_original_ckpt\", \n                os.path.join(self.config[\"model_path\"], \"text_encoder\")\n            )\n            self.text_encoder = Qwen3Model.from_pretrained(\n                qwen3_original_ckpt, \n                torch_dtype=GET_DTYPE(), \n                device_map=AI_DEVICE if not self.cpu_offload else \"cpu\"\n            )\n```\n\n**Key Features:**\n- **Chat template with thinking enabled**: Uses `apply_chat_template` with `enable_thinking=True`\n- **INT4 quantization support**: Can load GPTQ-quantized models for 4x memory reduction\n- **512 token max length**: Shorter than QwenImageRunner's typical limits\n- **Hidden state extraction**: Uses second-to-last layer (`hidden_states[-2]`)\n\nSources: [lightx2v/models/input_encoders/hf/z_image/qwen3_model.py:25-46]()\n\n### Text Encoding with Mask Handling\n\n```python\n@torch.no_grad()\ndef infer(self, prompt, image_list=None):\n    if isinstance(prompt, str):\n        prompt = [prompt]\n    \n    # Apply chat template\n    for i, prompt_item in enumerate(prompt):\n        messages = [{\"role\": \"user\", \"content\": prompt_item}]\n        prompt_tokens = self.tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True, \n            enable_thinking=True\n        )\n        prompt[i] = prompt_tokens\n    \n    # Tokenize with padding\n    text_inputs = self.tokenizer(\n        prompt, max_length=self.tokenizer_max_length, \n        padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n    ).to(AI_DEVICE)\n    prompt_masks = text_inputs.attention_mask.bool().to(AI_DEVICE)\n    \n    # Extract hidden states\n    prompt_embeds = self.text_encoder(\n        input_ids=text_inputs.input_ids,\n        attention_mask=prompt_masks,\n        output_hidden_states=True,\n    ).hidden_states[-2]\n    \n    # Extract valid tokens using mask\n    embedding_list = []\n    for i in range(len(prompt_embeds)):\n        extracted = prompt_embeds[i][prompt_masks[i]]\n        embedding_list.append(extracted)\n    \n    return embedding_list, image_info\n```\n\n**Mask-Based Extraction:**\n\nThe runner stores sequence lengths for each prompt, including negative prompts:\n\n```python\ndef run_text_encoder(self, text, image_list=None, neg_prompt=None):\n    if self.config[\"task\"] == \"t2i\":\n        prompt_embeds_list, _ = self.text_encoders[0].infer([text])\n        prompt_embeds = prompt_embeds_list[0]\n        self.input_info.txt_seq_lens = [prompt_embeds.shape[0]]\n        \n        if self.config[\"enable_cfg\"] and neg_prompt is not None:\n            neg_prompt_embeds_list, _ = self.text_encoders[0].infer([neg_prompt])\n            neg_prompt_embeds = neg_prompt_embeds_list[0]\n            self.input_info.txt_seq_lens.append(neg_prompt_embeds.shape[0])\n```\n\nSources: [lightx2v/models/input_encoders/hf/z_image/qwen3_model.py:62-103](), [lightx2v/models/runners/z_image/z_image_runner.py:186-243]()\n\n---\n\n## Transformer Architecture\n\n### Single-Stream Design\n\n`ZImageTransformerModel` implements a **single-stream architecture** where image and text tokens are concatenated and processed together:\n\n```mermaid\ngraph TB\n    subgraph \"ZImageTransformerModel Structure\"\n        Input[\"Latent Input\u003cbr/\u003e(B, C, H, W)\"]\n        \n        subgraph \"Pre-Inference\"\n            PatchEmbed[\"Patch Embedding\u003cbr/\u003e+ Position Embedding\"]\n            Concat[\"Concatenate\u003cbr/\u003eImage + Text Tokens\"]\n        end\n        \n        subgraph \"Transformer Blocks (n_layers)\"\n            Block1[\"Block 0\"]\n            Block2[\"Block 1\"]\n            BlockN[\"Block N-1\"]\n            \n            subgraph \"Single Block\"\n                Mod[\"AdaLN Modulation\u003cbr/\u003e(optional)\"]\n                Attn[\"Self-Attention\u003cbr/\u003e+ RMS Norm\"]\n                FFN[\"Feed-Forward\u003cbr/\u003e+ RMS Norm\"]\n            end\n        end\n        \n        subgraph \"Refiner Blocks (n_refiner_layers)\"\n            NoiseRefiner[\"Noise Refiner\u003cbr/\u003e(with modulation)\"]\n            ContextRefiner[\"Context Refiner\u003cbr/\u003e(no modulation)\"]\n        end\n        \n        subgraph \"Post-Inference\"\n            Extract[\"Extract Image Tokens\"]\n            NormOut[\"RMS Norm\"]\n            ProjOut[\"Linear Projection\"]\n            Unpatch[\"Unpatchify\"]\n        end\n        \n        Output[\"Noise Prediction\u003cbr/\u003e(B, C, H, W)\"]\n    end\n    \n    Input --\u003e PatchEmbed\n    PatchEmbed --\u003e Concat\n    Concat --\u003e Block1\n    Block1 --\u003e Block2\n    Block2 --\u003e BlockN\n    BlockN --\u003e NoiseRefiner\n    NoiseRefiner --\u003e ContextRefiner\n    ContextRefiner --\u003e Extract\n    Extract --\u003e NormOut\n    NormOut --\u003e ProjOut\n    ProjOut --\u003e Unpatch\n    Unpatch --\u003e Output\n    \n    Mod -.part of.-\u003e Block1\n    Attn -.part of.-\u003e Block1\n    FFN -.part of.-\u003e Block1\n```\n\nSources: [lightx2v/models/networks/z_image/model.py:17-48](), [lightx2v/models/networks/z_image/weights/transformer_weights.py:9-60]()\n\n### Weight Structure\n\n#### Transformer Block Components\n\nEach transformer block consists of three phases:\n\n```python\nclass ZImageTransformerBlock(WeightModule):\n    def __init__(self, block_idx, task, mm_type, config, ...):\n        self.compute_phases = WeightModuleList([\n            # Phase 0: Modulation (if has_modulation=True)\n            ZImageAdaLNModulation(...) if self.has_modulation else WeightModule(),\n            # Phase 1: Attention\n            ZImageAttention(...),\n            # Phase 2: Feed-Forward\n            ZImageFFN(...),\n        ])\n```\n\n**ZImageAttention Structure:**\n\n```python\nclass ZImageAttention(WeightModule):\n    def __init__(self, block_idx, ...):\n        # Pre-attention normalization\n        self.add_module(\"attention_norm1\", RMS_WEIGHT_REGISTER[...])\n        self.add_module(\"attention_norm2\", RMS_WEIGHT_REGISTER[...])\n        \n        # QK normalization\n        self.add_module(\"norm_q\", RMS_WEIGHT_REGISTER[...])\n        self.add_module(\"norm_k\", RMS_WEIGHT_REGISTER[...])\n        \n        # QKV projections (no bias)\n        self.add_module(\"to_q\", MM_WEIGHT_REGISTER[mm_type](..., bias=None))\n        self.add_module(\"to_k\", MM_WEIGHT_REGISTER[mm_type](..., bias=None))\n        self.add_module(\"to_v\", MM_WEIGHT_REGISTER[mm_type](..., bias=None))\n        \n        # Output projection (no bias)\n        self.add_module(\"to_out\", WeightModuleList([\n            MM_WEIGHT_REGISTER[mm_type](..., bias=None)\n        ]))\n        \n        # Attention computation\n        self.add_module(\"calculate\", ATTN_WEIGHT_REGISTER[self.attn_type]())\n```\n\n**Key Differences from WAN/Qwen:**\n- No bias terms in attention layers\n- RMS normalization instead of LayerNorm in some paths\n- QK normalization applied separately\n- Single attention path (no cross-attention)\n\nSources: [lightx2v/models/networks/z_image/weights/transformer_weights.py:115-192](), [lightx2v/models/networks/z_image/weights/transformer_weights.py:238-386]()\n\n#### Feed-Forward Network\n\n```python\nclass ZImageFFN(WeightModule):\n    def __init__(self, block_idx, ...):\n        # SiLU gating with w1, w2, w3\n        self.add_module(\"w1\", MM_WEIGHT_REGISTER[mm_type](..., bias=None))\n        self.add_module(\"w2\", MM_WEIGHT_REGISTER[mm_type](..., bias=None))\n        self.add_module(\"w3\", MM_WEIGHT_REGISTER[mm_type](..., bias=None))\n        \n        # FFN normalization\n        self.add_module(\"ffn_norm1\", RMS_WEIGHT_REGISTER[...])\n        self.add_module(\"ffn_norm2\", RMS_WEIGHT_REGISTER[...])\n```\n\nThe FFN uses a gated linear unit architecture: `output = w2(SiLU(w1(x)) * w3(x))`\n\nSources: [lightx2v/models/networks/z_image/weights/transformer_weights.py:388-482]()\n\n### Inference Flow with CFG\n\n```python\n@torch.no_grad()\ndef infer(self, inputs):\n    latents = self.scheduler.latents\n    latents_input = latents\n    \n    if self.config[\"enable_cfg\"]:\n        if self.config[\"cfg_parallel\"]:\n            # CFG Parallel Processing (2 GPUs)\n            cfg_p_group = self.config[\"device_mesh\"].get_group(mesh_dim=\"cfg_p\")\n            cfg_p_rank = dist.get_rank(cfg_p_group)\n            \n            if cfg_p_rank == 0:\n                noise_pred = self._infer_cond_uncond(\n                    latents_input, \n                    inputs[\"text_encoder_output\"][\"prompt_embeds\"], \n                    infer_condition=True\n                )\n            else:\n                noise_pred = self._infer_cond_uncond(\n                    latents_input, \n                    inputs[\"text_encoder_output\"][\"negative_prompt_embeds\"], \n                    infer_condition=False\n                )\n            \n            # All-gather predictions\n            noise_pred_list = [torch.zeros_like(noise_pred) for _ in range(2)]\n            dist.all_gather(noise_pred_list, noise_pred, group=cfg_p_group)\n            noise_pred_cond = noise_pred_list[0]\n            noise_pred_uncond = noise_pred_list[1]\n        else:\n            # Sequential CFG Processing\n            noise_pred_cond = self._infer_cond_uncond(\n                latents_input, \n                inputs[\"text_encoder_output\"][\"prompt_embeds\"], \n                infer_condition=True\n            )\n            noise_pred_uncond = self._infer_cond_uncond(\n                latents_input, \n                inputs[\"text_encoder_output\"][\"negative_prompt_embeds\"], \n                infer_condition=False\n            )\n        \n        # CFG with rescaling\n        comb_pred = noise_pred_uncond + self.scheduler.sample_guide_scale * (noise_pred_cond - noise_pred_uncond)\n        noise_pred_cond_norm = torch.norm(noise_pred_cond, dim=-1, keepdim=True)\n        noise_norm = torch.norm(comb_pred, dim=-1, keepdim=True)\n        self.scheduler.noise_pred = comb_pred * (noise_pred_cond_norm / noise_norm)\n    else:\n        # No CFG\n        noise_pred = self._infer_cond_uncond(\n            latents_input, \n            inputs[\"text_encoder_output\"][\"prompt_embeds\"], \n            infer_condition=True\n        )\n        self.scheduler.noise_pred = noise_pred\n```\n\nSources: [lightx2v/models/networks/z_image/model.py:84-147]()\n\n---\n\n## Aspect Ratio and Resolution Handling\n\n### Aspect Ratio Calculation\n\n`ZImageRunner` supports predefined aspect ratios and custom dimensions:\n\n```python\ndef get_input_target_shape(self):\n    default_aspect_ratios = {\n        \"16:9\": [1664, 928],\n        \"9:16\": [928, 1664],\n        \"1:1\": [1328, 1328],\n        \"4:3\": [1472, 1140],\n        \"3:4\": [768, 1024],\n    }\n    as_maps = self.config.get(\"aspect_ratios\", {})\n    as_maps.update(default_aspect_ratios)\n    max_size = self.config.get(\"max_custom_size\", 1664)\n    min_size = self.config.get(\"min_custom_size\", 256)\n    \n    # Option 1: Custom shape [height, width]\n    if len(self.input_info.target_shape) == 2:\n        height, width = self.input_info.target_shape\n        if width \u003e max_size or height \u003e max_size:\n            scale = max_size / max(width, height)\n            width, height = int(width * scale), int(height * scale)\n        width, height = max(width, min_size), max(height, min_size)\n        return (width, height)\n    \n    # Option 2: Aspect ratio lookup\n    aspect_ratio = self.input_info.aspect_ratio or self.config.get(\"aspect_ratio\")\n    if aspect_ratio in as_maps:\n        width, height = as_maps[aspect_ratio]\n        return (width, height)\n```\n\nSources: [lightx2v/models/runners/z_image/z_image_runner.py:276-307]()\n\n### Target Shape Setting with Packing\n\nThe runner calculates latent dimensions accounting for VAE scale factor and packing:\n\n```python\ndef set_target_shape(self):\n    height, width = self.get_input_target_shape()\n    \n    # VAE applies 8x compression, packing requires divisibility by 2\n    vae_scale_factor = self.config[\"vae_scale_factor\"]  # Typically 8\n    height = 2 * (int(height) // (vae_scale_factor * 2))\n    width = 2 * (int(width) // (vae_scale_factor * 2))\n    \n    num_channels_latents = self.config.get(\"num_channels_latents\", 16)\n    self.input_info.target_shape = (1, num_channels_latents, height, width)\n```\n\n**Image Shapes for Patchification:**\n\n```python\ndef set_img_shapes(self):\n    _, _, latent_height, latent_width = self.input_info.target_shape\n    \n    patch_size = self.config.get(\"patch_size\", 2)\n    patch_height = latent_height // patch_size\n    patch_width = latent_width // patch_size\n    \n    image_shapes = [(1, patch_height, patch_width)]\n    self.input_info.image_shapes = image_shapes\n```\n\n**Example Calculation:**\n- Input: `aspect_ratio=\"16:9\"`  `(1664, 928)` pixels\n- After VAE alignment: `height = 2 * (928 // 16) = 928`, `width = 2 * (1664 // 16) = 1664`\n- Latent shape: `(1, 16, 928, 1664)` (16 latent channels)\n- After packing: `(1, 464, 832)` patches (464 = 928/2, 832 = 1664/2)\n\nSources: [lightx2v/models/runners/z_image/z_image_runner.py:309-338]()\n\n---\n\n## VAE Encoding and Decoding\n\n### AutoencoderKLZImageVAE\n\nThe VAE uses HuggingFace's `AutoencoderKL` with Z-Image-specific configuration:\n\n```python\nclass AutoencoderKLZImageVAE:\n    def __init__(self, config):\n        self.latent_channels = 16\n        self.vae_latents_mean = None\n        self.vae_latents_std = None\n    \n    def load(self):\n        self.model = AutoencoderKL.from_pretrained(\n            os.path.join(self.config[\"model_path\"], \"vae\")\n        ).to(self.device).to(GET_DTYPE())\n        \n        self.image_processor = VaeImageProcessor(\n            vae_scale_factor=self.config[\"vae_scale_factor\"] * 2  # 16\n        )\n```\n\n**Key Parameters:**\n- **16 latent channels** (vs 4 for SD VAE)\n- **8x spatial compression** (configurable via `vae_scale_factor`)\n- **Scaling and shift factors** for latent normalization\n\nSources: [lightx2v/models/video_encoders/hf/z_image/vae.py:27-44]()\n\n### Decoding with Scaling\n\n```python\n@torch.no_grad()\ndef decode(self, latents, input_info):\n    if self.cpu_offload:\n        self.model.to(torch.device(AI_DEVICE))\n    \n    latents = latents.to(next(self.model.parameters()).dtype)\n    \n    # Inverse scaling: latents = (latents / scaling_factor) + shift_factor\n    if hasattr(self.model.config, \"scaling_factor\") and hasattr(self.model.config, \"shift_factor\"):\n        scaling_factor = self.model.config.scaling_factor\n        shift_factor = self.model.config.shift_factor\n        latents = (latents / scaling_factor) + shift_factor\n    \n    images = self.model.decode(latents, return_dict=False)[0]\n    images = self.image_processor.postprocess(\n        images, output_type=\"pt\" if input_info.return_result_tensor else \"pil\"\n    )\n    \n    if self.cpu_offload:\n        self.model.to(torch.device(\"cpu\"))\n    \n    return images\n```\n\nSources: [lightx2v/models/video_encoders/hf/z_image/vae.py:60-77]()\n\n### Encoding with Packing\n\nFor I2I tasks, the VAE encodes input images with packing:\n\n```python\n@torch.no_grad()\ndef encode_vae_image(self, image):\n    if self.cpu_offload:\n        self.model.to(torch.device(AI_DEVICE))\n    \n    image = image.to(self.model.device)\n    \n    if image.shape[1] != self.latent_channels:\n        image_latents = self._encode_vae_image(image=image)\n        # Apply scaling (inverse of decoding)\n        if hasattr(self.model.config, \"scaling_factor\") and hasattr(self.model.config, \"shift_factor\"):\n            image_latents = (image_latents - self.model.config.shift_factor) * self.model.config.scaling_factor\n    else:\n        image_latents = image\n    \n    image_latents = torch.cat([image_latents], dim=0)\n    \n    if self.cpu_offload:\n        self.model.to(torch.device(\"cpu\"))\n    \n    return image_latents\n```\n\nSources: [lightx2v/models/video_encoders/hf/z_image/vae.py:97-116]()\n\n---\n\n## Scheduler and Inference Loop\n\n### ZImageScheduler\n\nThe runner initializes `ZImageScheduler` with turbo defaults:\n\n```python\ndef init_scheduler(self):\n    self.scheduler = ZImageScheduler(self.config)\n```\n\n**Typical Configuration:**\n- `infer_steps`: 9 (turbo mode) or configurable\n- `sample_guide_scale`: CFG scale (default 1.0 for turbo)\n- `enable_cfg`: Boolean flag for classifier-free guidance\n\nSources: [lightx2v/models/runners/z_image/z_image_runner.py:340-341]()\n\n### Inference Loop Execution\n\n```python\ndef run(self, total_steps=None):\n    if total_steps is None:\n        total_steps = self.model.scheduler.infer_steps\n    \n    for step_index in range(total_steps):\n        logger.info(f\"==\u003e step_index: {step_index + 1} / {total_steps}\")\n        \n        # Scheduler pre-processing\n        with ProfilingContext4DebugL1(\"step_pre\"):\n            self.model.scheduler.step_pre(step_index=step_index)\n        \n        # Main transformer inference\n        with ProfilingContext4DebugL1(\" infer_main\"):\n            self.model.infer(self.inputs)\n        \n        # Scheduler post-processing\n        with ProfilingContext4DebugL1(\"step_post\"):\n            self.model.scheduler.step_post()\n        \n        # Progress callback\n        if self.progress_callback:\n            self.progress_callback(((step_index + 1) / total_steps) * 100, 100)\n    \n    return self.model.scheduler.latents, self.model.scheduler.generator\n```\n\nSources: [lightx2v/models/runners/z_image/z_image_runner.py:256-274]()\n\n### Complete Pipeline Execution\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Runner as ZImageRunner\n    participant Encoder as run_input_encoder\n    participant DIT as run_dit\n    participant VAE as run_vae_decoder\n    \n    User-\u003e\u003eRunner: run_pipeline(input_info)\n    \n    Runner-\u003e\u003eEncoder: Execute (T2I or I2I)\n    Note over Encoder: Text encoding\u003cbr/\u003e(+ image for I2I)\n    Encoder--\u003e\u003eRunner: inputs dict\n    \n    Runner-\u003e\u003eRunner: set_target_shape()\n    Runner-\u003e\u003eRunner: set_img_shapes()\n    \n    Runner-\u003e\u003eDIT: run_dit(total_steps)\n    \n    opt Lazy Load / Unload Modules\n        DIT-\u003e\u003eDIT: load_transformer()\n        DIT-\u003e\u003eDIT: set_scheduler()\n    end\n    \n    DIT-\u003e\u003eDIT: scheduler.prepare(input_info)\n    \n    loop For each inference step\n        DIT-\u003e\u003eDIT: scheduler.step_pre()\n        DIT-\u003e\u003eDIT: model.infer()\n        DIT-\u003e\u003eDIT: scheduler.step_post()\n    end\n    \n    DIT--\u003e\u003eRunner: latents, generator\n    \n    Runner-\u003e\u003eVAE: run_vae_decoder(latents)\n    \n    opt Lazy Load\n        VAE-\u003e\u003eVAE: load_vae()\n    end\n    \n    VAE-\u003e\u003eVAE: decode(latents, input_info)\n    VAE--\u003e\u003eRunner: images\n    \n    opt Save to Disk\n        Runner-\u003e\u003eRunner: images[0].save(save_result_path)\n    end\n    \n    Runner-\u003e\u003eRunner: end_run()\n    Runner--\u003e\u003eUser: {\"images\": images} or None\n```\n\nSources: [lightx2v/models/runners/z_image/z_image_runner.py:371-401]()\n\n---\n\n## Optimization Support\n\n### Quantization Strategies\n\n`ZImageRunner` supports multiple quantization schemes:\n\n| Scheme | Component | Description |\n|--------|-----------|-------------|\n| `fp8-sgl` | Transformer | FP8 E4M3 via SGLang kernels (recommended) |\n| `fp8-q8f` | Transformer | FP8 via Q8F kernels (Ada GPU optimized) |\n| `int8-vllm` | Transformer | INT8 via VLLM kernels |\n| `int4` | Qwen3 Text Encoder | GPTQ 4-bit quantization |\n\n**Configuration Example:**\n\n```json\n{\n    \"dit_quantized\": true,\n    \"dit_quantized_ckpt\": \"lightx2v/Z-Image-Turbo-Quantized/z_image_turbo_scaled_fp8_e4m3fn.safetensors\",\n    \"dit_quant_scheme\": \"fp8-sgl\",\n    \n    \"qwen3_quantized\": true,\n    \"qwen3_quantized_ckpt\": \"JunHowie/Qwen3-4B-GPTQ-Int4\",\n    \"qwen3_quant_scheme\": \"int4\"\n}\n```\n\nSources: [configs/z_image/z_image_turbo_t2i_offload.json:1-17]()\n\n### CPU Offloading\n\nThe runner supports model-level and block-level offloading:\n\n```python\nmodel_cpu_offload_seq = \"text_encoder-\u003etransformer-\u003evae\"\n```\n\n**Offload Granularity Options:**\n\n| Granularity | Description | VRAM Savings | Speed Impact |\n|-------------|-------------|--------------|--------------|\n| `model` | Offload entire model between encoder/DIT/VAE | High | ~2x slower |\n| `block` | Keep 2 transformer blocks in GPU | Medium | ~30% slower |\n| `phase` | Offload at phase level (not recommended for Z-Image) | Very High | ~50% slower |\n\n**Configuration:**\n\n```json\n{\n    \"cpu_offload\": true,\n    \"offload_granularity\": \"model\",\n    \"qwen3_cpu_offload\": true,\n    \"vae_cpu_offload\": true\n}\n```\n\nThe runner manages offloading in `_run_dit_local`:\n\n```python\ndef _run_dit_local(self, total_steps=None):\n    if self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n        self.model = self.load_transformer()\n        self.model.set_scheduler(self.scheduler)\n    \n    self.model.scheduler.prepare(self.input_info)\n    latents, generator = self.run(total_steps)\n    return latents, generator\n```\n\nSources: [lightx2v/models/runners/z_image/z_image_runner.py:54-55](), [lightx2v/models/runners/z_image/z_image_runner.py:106-113]()\n\n### Lazy Loading\n\nFor minimal VRAM scenarios, lazy loading streams weights from disk:\n\n```python\nif self.lazy_load:\n    self.transformer_weights = self.transformer_weight_class(\n        self.config, \n        self.lazy_load_path,  # Path to safetensors directory\n        self.lora_path\n    )\n```\n\nThe weight module structure supports lazy loading:\n\n```python\nclass ZImageTransformerWeights(WeightModule):\n    def __init__(self, config, lazy_load_path=None, lora_path=None):\n        self.lazy_load = self.config.get(\"lazy_load\", False)\n        if self.lazy_load:\n            # Only load non-block weights initially\n            non_block_file = os.path.join(lazy_load_path, \"non_block.safetensors\")\n            # Blocks loaded on-demand during inference\n```\n\nSources: [lightx2v/models/networks/z_image/weights/transformer_weights.py:9-104]()\n\n### LoRA Dynamic Application\n\nThe runner supports two LoRA modes:\n\n**1. Dynamic Application (Recommended for Quantized Models):**\n\n```python\nlora_dynamic_apply = config.get(\"lora_dynamic_apply\", False)\n\nif lora_dynamic_apply:\n    lora_path = lora_configs[0][\"path\"]\n    lora_strength = lora_configs[0][\"strength\"]\n    model_kwargs[\"lora_path\"] = lora_path\n    model_kwargs[\"lora_strength\"] = lora_strength\n    model = z_image_module(**model_kwargs)\n```\n\n**2. Merged Application (For Non-Quantized Models):**\n\n```python\nelse:\n    assert not config.get(\"dit_quantized\", False), \"Online LoRA only for quantized models\"\n    assert not config.get(\"lazy_load\", False), \"Lazy load mode does not support LoRA merging\"\n    model = z_image_module(**model_kwargs)\n    lora_adapter = LoraAdapter(model)\n    lora_adapter.apply_lora(lora_configs)\n```\n\nSources: [lightx2v/models/runners/z_image/z_image_runner.py:34-49]()\n\n---\n\n## Configuration Examples\n\n### Turbo T2I Configuration\n\n```json\n{\n    \"aspect_ratio\": \"16:9\",\n    \"num_channels_latents\": 16,\n    \"infer_steps\": 9,\n    \"attn_type\": \"flash_attn3\",\n    \"enable_cfg\": false,\n    \"sample_guide_scale\": 0.0,\n    \"patch_size\": 2,\n    \"cpu_offload\": true,\n    \"offload_granularity\": \"model\",\n    \"qwen3_quantized\": true,\n    \"qwen3_quant_scheme\": \"int4\",\n    \"qwen3_quantized_ckpt\": \"JunHowie/Qwen3-4B-GPTQ-Int4\",\n    \"dit_quantized\": true,\n    \"dit_quant_scheme\": \"fp8-sgl\",\n    \"dit_quantized_ckpt\": \"lightx2v/Z-Image-Turbo-Quantized/z_image_turbo_scaled_fp8_e4m3fn.safetensors\"\n}\n```\n\n**Key Settings:**\n- **No CFG**: `enable_cfg=false` for turbo mode\n- **Flash Attention 3**: H100/Ada optimization\n- **9 inference steps**: Typical turbo configuration\n- **Model-level offload**: Suitable for consumer GPUs\n\nSources: [configs/z_image/z_image_turbo_t2i_offload.json:1-17]()\n\n### Shell Script Invocation\n\n```bash\n#!/bin/bash\n\nlightx2v_path=/path/to/LightX2V\nmodel_path=Tongyi-MAI/Z-Image-Turbo\n\nexport CUDA_VISIBLE_DEVICES=0\n\npython -m lightx2v.infer \\\n--model_cls z_image \\\n--task t2i \\\n--model_path $model_path \\\n--config_json ${lightx2v_path}/configs/z_image/z_image_turbo_t2i.json \\\n--prompt 'Young Chinese woman in red Hanfu, intricate embroidery...' \\\n--negative_prompt \" \" \\\n--save_result_path ${lightx2v_path}/save_results/z_image_turbo.png \\\n--seed 42 \\\n--aspect_ratio \"16:9\"\n```\n\nSources: [scripts/z_image/z_image_turbo_t2i.sh:1-22]()\n\n### Python API Usage\n\n```python\nfrom lightx2v import LightX2VPipeline\n\n# Initialize pipeline\npipe = LightX2VPipeline(\n    model_path=\"Tongyi-MAI/Z-Image-Turbo\",\n    model_cls=\"z_image\",\n    task=\"t2i\",\n)\n\n# Enable FP8 quantization\npipe.enable_quantize(\n    dit_quantized=True,\n    dit_quantized_ckpt=\"lightx2v/Z-Image-Turbo-Quantized/z_image_turbo_scaled_fp8_e4m3fn.safetensors\",\n    quant_scheme=\"fp8-sgl\",\n)\n\n# Enable model-level offloading\npipe.enable_offload(\n    cpu_offload=True,\n    offload_granularity=\"model\",\n)\n\n# Create generator\npipe.create_generator(\n    attn_mode=\"flash_attn3\",\n    aspect_ratio=\"16:9\",\n    infer_steps=9,\n    guidance_scale=1,\n)\n\n# Generate image\npipe.generate(\n    seed=42,\n    prompt=\"A coffee shop entrance...\",\n    negative_prompt=\"\",\n    save_result_path=\"/path/to/output.png\",\n)\n```\n\nSources: [examples/z-image-turbo/z_image_turbo.py:1-58]()\n\n---\n\n## Performance Characteristics\n\n### Memory Footprint\n\n| Configuration | VRAM Usage | Inference Time (RTX 4090) |\n|--------------|------------|---------------------------|\n| FP16 baseline | ~18 GB | ~1.2s/iter |\n| FP8 + model offload | ~8 GB | ~2.5s/iter |\n| FP8 + INT4 text encoder + model offload | ~6 GB | ~2.8s/iter |\n| Lazy load + INT4 | ~4 GB | ~4.0s/iter |\n\n### Comparison with QwenImageRunner\n\n| Metric | ZImageRunner (Turbo) | QwenImageRunner |\n|--------|---------------------|-----------------|\n| Default Inference Steps | 9 | 16-20 |\n| CFG Default | Disabled | Enabled (5.0) |\n| Text Encoder Size | 4B (Qwen3) | 3-7B (Qwen2.5VL) |\n| Latent Channels | 16 | 16 |\n| Attention Architecture | Single-stream | Dual-stream |\n| Primary Optimization | Turbo + FP8 | Versatility |\n\nSources: [lightx2v/models/runners/z_image/z_image_runner.py:1-401](), [app/utils/image_page.py:1-514]()"])</script><script>self.__next_f.push([1,"35:T4a29,"])</script><script>self.__next_f.push([1,"# WanCausVidRunner - Autoregressive Video\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/models/input_encoders/hf/__init__.py](lightx2v/models/input_encoders/hf/__init__.py)\n- [lightx2v/models/networks/__init__.py](lightx2v/models/networks/__init__.py)\n- [lightx2v/models/networks/wan/causvid_model.py](lightx2v/models/networks/wan/causvid_model.py)\n- [lightx2v/models/networks/wan/distill_model.py](lightx2v/models/networks/wan/distill_model.py)\n- [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py](lightx2v/models/networks/wan/infer/causvid/transformer_infer.py)\n- [lightx2v/models/runners/__init__.py](lightx2v/models/runners/__init__.py)\n- [lightx2v/models/runners/wan/wan_distill_runner.py](lightx2v/models/runners/wan/wan_distill_runner.py)\n\n\u003c/details\u003e\n\n\n\n## Overview\n\nThe CausVid architecture implements autoregressive video generation for WAN models through the `WanCausVidModel` class. Unlike standard WAN models that process entire video sequences at once, CausVid generates video incrementally by maintaining KV caches across generation steps. This enables efficient long-form video generation by avoiding redundant attention computation over previously generated frames.\n\n**Key Components:**\n- **Model Class**: `WanCausVidModel` [lightx2v/models/networks/wan/causvid_model.py]()\n- **Transformer Inference**: `WanTransformerInferCausVid` [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py]()\n- **Cache Parameters**: `kv_start`, `kv_end` control incremental generation\n\n**Note**: CausVid is implemented as a model architecture variant rather than a separate runner. WAN runners can use `WanCausVidModel` for autoregressive generation.\n\n**Related Pages:**\n- Standard WAN video generation: See page 5.3\n- WAN distilled models: See page 5.4\n\n## Architecture Components\n\n### Class Hierarchy\n\n`WanCausVidModel` extends `WanModel` with causal inference support:\n\n**CausVid Model Architecture**\n```mermaid\ngraph TB\n    WanModel[\"WanModel\"]\n    WanCausVidModel[\"WanCausVidModel\"]\n    \n    WanPreInfer[\"WanPreInfer\"]\n    WanTransformerInferCausVid[\"WanTransformerInferCausVid\"]\n    WanPostInfer[\"WanPostInfer\"]\n    \n    WanPreWeights[\"WanPreWeights\"]\n    WanTransformerWeights[\"WanTransformerWeights\"]\n    WanPostWeights[\"WanPostWeights\"]\n    \n    KVCache[\"self.kv_cache\u003cbr/\u003eList[Dict]\"]\n    CrossAttnCache[\"self.crossattn_cache\u003cbr/\u003eList[Dict]\"]\n    \n    WanModel --\u003e WanCausVidModel\n    \n    WanCausVidModel --\u003e WanPreInfer\n    WanCausVidModel --\u003e WanTransformerInferCausVid\n    WanCausVidModel --\u003e WanPostInfer\n    \n    WanCausVidModel --\u003e WanPreWeights\n    WanCausVidModel --\u003e WanTransformerWeights\n    WanCausVidModel --\u003e WanPostWeights\n    \n    WanTransformerInferCausVid --\u003e KVCache\n    WanTransformerInferCausVid --\u003e CrossAttnCache\n```\n\n**Sources:** [lightx2v/models/networks/wan/causvid_model.py:20-31](), [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:11-20]()\n\n### Inference Pipeline Components\n\n| Component | Class | File Path | Purpose |\n|-----------|-------|-----------|---------|\n| Pre-inference | `WanPreInfer` | [lightx2v/models/networks/wan/infer/pre_infer.py]() | Input embedding, position encoding |\n| Transformer | `WanTransformerInferCausVid` | [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py]() | Autoregressive transformer with KV caching |\n| Post-inference | `WanPostInfer` | [lightx2v/models/networks/wan/infer/post_infer.py]() | Output projection, noise prediction |\n\n**Sources:** [lightx2v/models/networks/wan/causvid_model.py:28-31]()\n\n## Weight Loading\n\n### Checkpoint Path Resolution\n\n`WanCausVidModel._load_ckpt()` searches for `causvid_model.pt` and falls back to standard loading:\n\n**Weight Loading Flow**\n```mermaid\ngraph LR\n    Start[\"_load_ckpt()\"]\n    FindPath[\"find_torch_model_path()\u003cbr/\u003ecausvid_model.pt\"]\n    CheckExists{\"os.path.exists()\"}\n    LoadCausVid[\"torch.load()\u003cbr/\u003edtype conversion\u003cbr/\u003epin_memory()\"]\n    FallbackLoad[\"super()._load_ckpt()\"]\n    Return[\"return weight_dict\"]\n    \n    Start --\u003e FindPath\n    FindPath --\u003e CheckExists\n    CheckExists --\u003e|\"True\"| LoadCausVid\n    CheckExists --\u003e|\"False\"| FallbackLoad\n    LoadCausVid --\u003e Return\n    FallbackLoad --\u003e Return\n```\n\n**Sources:** [lightx2v/models/networks/wan/causvid_model.py:33-43]()\n\n### Dtype Conversion\n\nWeights are converted to appropriate dtypes:\n- **Standard layers**: `GET_DTYPE()` (from `unified_dtype` config)\n- **Sensitive layers**: `GET_SENSITIVE_DTYPE()` (typically higher precision)\n- All weights are pinned to CPU memory before device transfer\n\n**Sources:** [lightx2v/models/networks/wan/causvid_model.py:38-39]()\n\n## KV Cache Management\n\n### Self-Attention KV Cache\n\nCausVid maintains per-block KV caches to avoid recomputing attention over previously generated frames.\n\n**KV Cache Data Structure**\n```mermaid\ngraph TB\n    KVCache[\"self.kv_cache\u003cbr/\u003eList[Dict]\u003cbr/\u003elength = blocks_num\"]\n    \n    Block0[\"block_idx = 0\"]\n    Block1[\"block_idx = 1\"]\n    BlockN[\"block_idx = N\"]\n    \n    CacheDict[\"Dict:\u003cbr/\u003ek: Tensor[kv_size, num_heads, head_dim]\u003cbr/\u003ev: Tensor[kv_size, num_heads, head_dim]\"]\n    \n    KVSizeCalc[\"kv_size = num_frames * frame_seq_length\"]\n    \n    KVCache --\u003e Block0\n    KVCache --\u003e Block1\n    KVCache --\u003e BlockN\n    \n    Block0 --\u003e CacheDict\n    CacheDict --\u003e KVSizeCalc\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:21-33]()\n\n### Cache Initialization\n\n`_init_kv_cache()` allocates zero tensors for the full video sequence:\n\n```\nkv_size = num_frames * frame_seq_length\nfor block_idx in range(blocks_num):\n    kv_cache[block_idx] = {\n        'k': torch.zeros([kv_size, num_heads, head_dim], dtype, device),\n        'v': torch.zeros([kv_size, num_heads, head_dim], dtype, device)\n    }\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:21-33]()\n\n### Cross-Attention Cache\n\nCross-attention conditioning (text/image) is computed once and cached:\n\n**Cross-Attention Cache Structure**\n```mermaid\ngraph TB\n    CrossCache[\"self.crossattn_cache\u003cbr/\u003eList[Dict]\u003cbr/\u003elength = blocks_num\"]\n    \n    Entry[\"Dict:\u003cbr/\u003ek: Tensor[text_len, num_heads, head_dim]\u003cbr/\u003ev: Tensor[text_len, num_heads, head_dim]\u003cbr/\u003eis_init: bool\"]\n    \n    InitCheck{\"is_init?\"}\n    ComputeKV[\"weights.cross_attn_k/v.apply()\u003cbr/\u003eweights.cross_attn_norm_k.apply()\"]\n    ReuseKV[\"Use cached K, V\"]\n    \n    CrossCache --\u003e Entry\n    Entry --\u003e InitCheck\n    InitCheck --\u003e|\"False\"| ComputeKV\n    InitCheck --\u003e|\"True\"| ReuseKV\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:35-47](), [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:150-158]()\n\nThe `is_init` flag prevents redundant computation of conditioning keys and values.\n\n## Incremental Inference with kv_start and kv_end\n\n### Inference Method Signature\n\n`WanCausVidModel.infer()` accepts cache position parameters for incremental generation:\n\n```python\ndef infer(self, inputs, kv_start, kv_end):\n    # Update KV cache slice [kv_start:kv_end]\n    # Attend to cache [:kv_end]\n```\n\n**Sources:** [lightx2v/models/networks/wan/causvid_model.py:45-46]()\n\n### Inference Flow\n\n**Incremental Inference Pipeline**\n```mermaid\nsequenceDiagram\n    participant Caller\n    participant Model as WanCausVidModel\n    participant PreInfer as WanPreInfer\n    participant TransInfer as WanTransformerInferCausVid\n    participant PostInfer as WanPostInfer\n    \n    Caller-\u003e\u003eModel: infer(inputs, kv_start, kv_end)\n    \n    alt cpu_offload enabled\n        Model-\u003e\u003eModel: pre_weight.to_cuda()\u003cbr/\u003etransformer_weights.post_weights_to_cuda()\n    end\n    \n    Model-\u003e\u003ePreInfer: infer(pre_weight, inputs, kv_start, kv_end)\n    PreInfer--\u003e\u003eModel: embed, grid_sizes, pre_infer_out\n    \n    Model-\u003e\u003eTransInfer: infer(weights, grid_sizes, embed, ..., kv_start, kv_end)\n    \n    loop for each block\n        TransInfer-\u003e\u003eTransInfer: Compute Q, K, V\n        TransInfer-\u003e\u003eTransInfer: kv_cache[block_idx][k/v][kv_start:kv_end] = K/V\n        TransInfer-\u003e\u003eTransInfer: attention(Q, cache[:kv_end])\n    end\n    \n    TransInfer--\u003e\u003eModel: x\n    \n    Model-\u003e\u003ePostInfer: infer(x, embed, grid_sizes)\n    PostInfer--\u003e\u003eModel: noise_pred\n    \n    Model-\u003e\u003eModel: scheduler.noise_pred = noise_pred\n    \n    alt cpu_offload enabled\n        Model-\u003e\u003eModel: pre_weight.to_cpu()\u003cbr/\u003etransformer_weights.post_weights_to_cpu()\n    end\n```\n\n**Sources:** [lightx2v/models/networks/wan/causvid_model.py:45-58](), [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:49-96]()\n\n### Self-Attention Implementation\n\n`infer_self_attn()` updates KV cache incrementally:\n\n**Self-Attention with KV Cache**\n```mermaid\ngraph TB\n    Input[\"norm1_out\"]\n    \n    QKVProj[\"weights.self_attn_q/k/v.apply()\"]\n    QKVNorm[\"weights.self_attn_norm_q/k.apply()\"]\n    \n    ComputeFreqs[\"compute_freqs_causvid()\u003cbr/\u003estart_frame = kv_start // prod(grid_sizes)\"]\n    \n    ApplyRoPE[\"apply_rotary_emb(q/k, freqs_i)\"]\n    \n    UpdateCache[\"kv_cache[block_idx][k/v][kv_start:kv_end] = k/v\"]\n    \n    Attention[\"weights.self_attn_1.apply()\u003cbr/\u003eq @ kv_cache[k][:kv_end]\u003cbr/\u003e@ kv_cache[v][:kv_end]\"]\n    \n    Output[\"weights.self_attn_o.apply()\u003cbr/\u003ex = x + y * embed0[2]\"]\n    \n    Input --\u003e QKVProj\n    QKVProj --\u003e QKVNorm\n    QKVNorm --\u003e ComputeFreqs\n    ComputeFreqs --\u003e ApplyRoPE\n    ApplyRoPE --\u003e UpdateCache\n    UpdateCache --\u003e Attention\n    Attention --\u003e Output\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:98-139]()\n\n#### Key Operations\n\n| Operation | Code | Purpose |\n|-----------|------|---------|\n| **Position Encoding** | `compute_freqs_causvid()` | Calculate RoPE for current frame position |\n| **Cache Update** | `kv_cache[block_idx]['k'][kv_start:kv_end] = k` | Store new K/V tensors |\n| **Attention Range** | `kv_cache[block_idx]['k'][:kv_end]` | Attend to all cached tokens |\n| **Sequence Lengths** | `_calculate_q_k_len()` | Compute cu_seqlens for variable-length attention |\n\n**Sources:** [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:108-130]()\n\n## Cross-Attention Implementation\n\n### Conditional Generation\n\n`infer_cross_attn()` caches text/image conditioning:\n\n**Cross-Attention Flow**\n```mermaid\ngraph TB\n    Input[\"weights.norm3.apply(x)\"]\n    \n    TaskCheck{\"self.task in\u003cbr/\u003e[i2v, s2v, rs2v]?\"}\n    \n    SplitContext[\"context_img = context[:257]\u003cbr/\u003econtext_text = context[257:]\"]\n    \n    CacheCheck{\"crossattn_cache[block_idx][is_init]?\"}\n    \n    ComputeKV[\"weights.cross_attn_k/v.apply(context)\u003cbr/\u003eweights.cross_attn_norm_k.apply()\u003cbr/\u003eStore in cache, is_init = True\"]\n    \n    ReuseKV[\"k/v = crossattn_cache[block_idx][k/v]\"]\n    \n    TextAttn[\"weights.cross_attn_1.apply()\u003cbr/\u003eq @ k @ v\"]\n    \n    ImgAttn[\"weights.cross_attn_k/v_img.apply()\u003cbr/\u003eweights.cross_attn_2.apply()\"]\n    \n    Combine[\"attn_out = text_attn + img_attn\"]\n    \n    Output[\"weights.cross_attn_o.apply()\u003cbr/\u003ex = x + attn_out\"]\n    \n    Input --\u003e TaskCheck\n    TaskCheck --\u003e|\"True\"| SplitContext\n    TaskCheck --\u003e|\"False\"| CacheCheck\n    SplitContext --\u003e CacheCheck\n    \n    CacheCheck --\u003e|\"False\"| ComputeKV\n    CacheCheck --\u003e|\"True\"| ReuseKV\n    \n    ComputeKV --\u003e TextAttn\n    ReuseKV --\u003e TextAttn\n    \n    TaskCheck --\u003e|\"True\"| ImgAttn\n    TextAttn --\u003e ImgAttn\n    ImgAttn --\u003e Combine\n    Combine --\u003e Output\n    TextAttn --\u003e Output\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:141-197]()\n\n### Context Splitting for I2V/S2V\n\n| Task Type | Context Split | Description |\n|-----------|---------------|-------------|\n| **T2V** | Full context | Text conditioning only |\n| **I2V/S2V** | `context[:257]` | 257 image tokens (CLIP) |\n| **I2V/S2V** | `context[257:]` | Text tokens |\n\n**Sources:** [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:144-146]()\n\nImage and text attention outputs are summed before output projection.\n\n## CPU Offloading\n\n### Model-Level Offloading\n\n`WanCausVidModel.infer()` supports model-level CPU offloading:\n\n**Offloading Flow**\n```mermaid\ngraph TB\n    Start[\"infer()\"]\n    \n    CheckOffload{\"config[cpu_offload]?\"}\n    \n    ToGPU[\"pre_weight.to_cuda()\u003cbr/\u003etransformer_weights.post_weights_to_cuda()\"]\n    \n    PreInfer[\"pre_infer.infer()\"]\n    TransInfer[\"transformer_infer.infer()\"]\n    PostInfer[\"post_infer.infer()\"]\n    \n    ToCPU[\"pre_weight.to_cpu()\u003cbr/\u003etransformer_weights.post_weights_to_cpu()\"]\n    \n    End[\"return\"]\n    \n    Start --\u003e CheckOffload\n    CheckOffload --\u003e|\"True\"| ToGPU\n    CheckOffload --\u003e|\"False\"| PreInfer\n    ToGPU --\u003e PreInfer\n    PreInfer --\u003e TransInfer\n    TransInfer --\u003e PostInfer\n    PostInfer --\u003e CheckOffload\n    CheckOffload --\u003e|\"True\"| ToCPU\n    CheckOffload --\u003e|\"False\"| End\n    ToCPU --\u003e End\n```\n\n**Sources:** [lightx2v/models/networks/wan/causvid_model.py:47-58]()\n\n### Block-Level Offloading\n\n`WanTransformerInferCausVid` extends `WanOffloadTransformerInfer` for block-level streaming:\n\n**Block Offloading with WeightAsyncStreamManager**\n```mermaid\ngraph LR\n    Block0[\"block_idx = 0\"]\n    LoadB0[\"active_weights[0] = weights.blocks[0]\u003cbr/\u003eto_cuda()\"]\n    Compute0[\"infer_block(active_weights[0])\"]\n    Prefetch1[\"prefetch_weights(1, weights.blocks)\"]\n    Swap1[\"swap_weights()\"]\n    \n    Block1[\"block_idx = 1\"]\n    Compute1[\"infer_block(active_weights[0])\"]\n    \n    Block0 --\u003e LoadB0\n    LoadB0 --\u003e Compute0\n    Compute0 --\u003e Prefetch1\n    Prefetch1 --\u003e Swap1\n    Swap1 --\u003e Block1\n    Block1 --\u003e Compute1\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:54-78]()\n\nSee page 6.3 for `WeightAsyncStreamManager` details.\n\n## Configuration Parameters\n\n### Required Config Keys\n\n`WanTransformerInferCausVid.__init__()` requires:\n\n| Parameter | Type | Purpose |\n|-----------|------|---------|\n| `num_frames` | int | Total frames in video sequence |\n| `num_frame_per_block` | int | Frames per generation step |\n| `frame_seq_length` | int | Tokens per frame (HW) |\n| `text_len` | int | Maximum text conditioning length |\n\n**Sources:** [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:14-17]()\n\n### Cache Size Calculation\n\n```\nkv_size = num_frames * frame_seq_length\n```\n\nDetermines total KV cache allocation for the video sequence.\n\n**Sources:** [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:23]()\n\n## Limitations\n\n### Parallel Attention Not Supported\n\n`WanTransformerInferCausVid` does not support sequence parallelism:\n\n```python\nif self.parallel_attention:\n    raise NotImplementedError(\"Parallel attention is not implemented for causvid inference\")\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:110-111](), [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:132-133]()\n\nCausVid cannot use Ulysses or Ring sequence parallelism. See page 6.5 for distributed inference options.\n\n## Implementation Details\n\n### Rotary Position Embeddings\n\n`compute_freqs_causvid()` calculates frame-aware RoPE:\n\n```python\nfreqs_i = compute_freqs_causvid(\n    dim=q.size(2) // 2,                          # head_dim / 2\n    grid_sizes=grid_sizes,\n    freqs=freqs,\n    start_frame=kv_start // math.prod(grid_sizes[0][1:])  # Current frame index\n)\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:108]()\n\n`start_frame` is computed by dividing `kv_start` by spatial resolution (HW).\n\n### FFN with Modulation\n\n`infer_ffn()` applies adaptive layer normalization:\n\n```python\nnorm2_out = LayerNorm(x)\ny = ffn_0(norm2_out * (1 + embed0[4]) + embed0[3])  # Scale + shift\ny = GELU(y, approximate='tanh')\ny = ffn_2(y)\nx = x + y * embed0[5]  # Gated residual\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:199-206]()\n\n`embed0` contains 6 modulation parameters: `[scale_attn, shift_attn, gate_attn, shift_ffn, scale_ffn, gate_ffn]`\n\n### Transformer Block Structure\n\n`infer_block()` executes three phases:\n\n**Block Inference Phases**\n```mermaid\ngraph LR\n    Input[\"x\"]\n    \n    Modulation[\"compute_phases[0].modulation\u003cbr/\u003echunk embed0 into 6 params\"]\n    \n    SelfAttn[\"compute_phases[1]\u003cbr/\u003einfer_self_attn()\"]\n    \n    CrossAttn[\"compute_phases[2]\u003cbr/\u003einfer_cross_attn()\"]\n    \n    FFN[\"compute_phases[3]\u003cbr/\u003einfer_ffn()\"]\n    \n    Output[\"x\"]\n    \n    Input --\u003e Modulation\n    Modulation --\u003e SelfAttn\n    SelfAttn --\u003e CrossAttn\n    CrossAttn --\u003e FFN\n    FFN --\u003e Output\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:208-221]()\n\n---\n\n## Comparison with Standard Wan Models\n\n### Key Differences\n\n| Aspect | WanModel (Standard) | WanCausVidModel (Autoregressive) |\n|--------|---------------------|----------------------------------|\n| Inference class | `WanTransformerInfer` | `WanTransformerInferCausVid` |\n| Attention computation | Full sequence each step | Incremental with KV cache |\n| Memory complexity | O(n) per step | O(n) per step after caching |\n| Cross-attention | Computed each step | Cached after first step |\n| Inference signature | `infer(inputs)` | `infer(inputs, kv_start, kv_end)` |\n| Parallel attention | Supported | Not supported (NotImplementedError) |\n| Video generation | Full sequence | Frame-by-frame incremental |\n\n**Sources:** [lightx2v/models/networks/wan/causvid_model.py:20-31](), [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:11-221]()\n\n### Performance Implications\n\nThe autoregressive approach with KV caching provides several advantages:\n\n1. **Memory Efficiency**: Only stores K and V tensors once, not recomputed each diffusion step\n2. **Computational Efficiency**: Attention complexity is O(n) instead of O(n) for new tokens\n3. **Long Sequence Support**: Can generate longer videos by processing incrementally\n4. **Cache Reuse**: Cross-attention conditioning is computed once and reused\n\nHowever, it has trade-offs:\n\n1. **Memory Overhead**: Requires persistent cache storage for all past tokens\n2. **No Parallelism**: Cannot use sequence parallelism for distributed generation\n3. **Specialized Runner**: Requires specific runner support for incremental generation workflow\n\n**Sources:** [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:21-48]()\n\n---\n\n## Summary\n\nThe **WanCausVidModel** implements autoregressive video generation through:\n\n- **KV Cache**: Per-block storage of self-attention keys and values for all past tokens\n- **Cross-Attention Cache**: One-time computation and reuse of conditioning information\n- **Incremental Inference**: `kv_start` and `kv_end` parameters control which cache slice to update\n- **Causal Position Embeddings**: Frame-aware rotary embeddings via `compute_freqs_causvid()`\n- **Offloading Support**: CPU offloading at both model-level and block-level granularity\n\nThis architecture enables efficient long-form video generation by avoiding redundant computation of attention over previously generated frames.\n\n**Sources:** [lightx2v/models/networks/wan/causvid_model.py:1-59](), [lightx2v/models/networks/wan/infer/causvid/transformer_infer.py:1-222]()"])</script><script>self.__next_f.push([1,"36:T59e3,"])</script><script>self.__next_f.push([1,"# LongCatImageRunner and LTX2Runner\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/models/networks/hunyuan_video/model.py](lightx2v/models/networks/hunyuan_video/model.py)\n- [lightx2v/models/networks/longcat_image/model.py](lightx2v/models/networks/longcat_image/model.py)\n- [lightx2v/models/networks/lora_adapter.py](lightx2v/models/networks/lora_adapter.py)\n- [lightx2v/models/networks/ltx2/model.py](lightx2v/models/networks/ltx2/model.py)\n- [lightx2v/models/networks/wan/animate_model.py](lightx2v/models/networks/wan/animate_model.py)\n- [lightx2v/models/runners/longcat_image/longcat_image_runner.py](lightx2v/models/runners/longcat_image/longcat_image_runner.py)\n- [lightx2v/models/runners/ltx2/ltx2_runner.py](lightx2v/models/runners/ltx2/ltx2_runner.py)\n\n\u003c/details\u003e\n\n\n\nThis document covers two specialized model runners: **LongCatImageRunner** for high-resolution image generation and **LTX2Runner** for synchronized audio and video synthesis. These runners extend `DefaultRunner` to support their unique architectures and multi-modal capabilities.\n\nFor basic runner concepts and the standard three-stage pipeline, see [Runner System and Registry Pattern](#4.1). For other model runners, see [WAN Model Family](#5.1), [QwenImageRunner](#5.6), and [ZImageRunner](#5.7).\n\n---\n\n## Overview\n\n**LongCatImageRunner** (`longcat_image`) specializes in high-resolution image generation with support for both text-to-image (T2I) and image-to-image (I2I) tasks. It uses a Qwen-based vision-language model for text encoding and supports prompt rewriting and flexible aspect ratios.\n\n**LTX2Runner** (`ltx2`) is a multi-modal runner that generates synchronized video and audio from text or images. It features a two-stage pipeline with optional upsampling, dual VAE decoders for video and audio, and support for image conditioning with configurable strength.\n\n| Feature | LongCatImageRunner | LTX2Runner |\n|---------|-------------------|------------|\n| **Tasks** | T2I, I2I | T2AV, I2AV |\n| **Output Modalities** | Image only | Video + Audio |\n| **Text Encoder** | Qwen2.5-VL | Gemma |\n| **Latent Channels** | 16 (image) | 128 (video), 8 (audio) |\n| **VAE** | Single image VAE | Dual VAEs (video, audio) |\n| **Multi-Stage** | No | Yes (2 stages with upsampler) |\n| **Image Conditioning** | I2I via VL encoding | I2AV with strength masks |\n\nSources: [lightx2v/models/runners/longcat_image/longcat_image_runner.py:60-61](), [lightx2v/models/runners/ltx2/ltx2_runner.py:24-25]()\n\n---\n\n## LongCatImageRunner Architecture\n\n### Component Structure\n\n```mermaid\ngraph TB\n    subgraph \"LongCatImageRunner Registration\"\n        Registry[\"RUNNER_REGISTER('longcat_image')\"]\n        Runner[\"LongCatImageRunner\u003cbr/\u003e(DefaultRunner)\"]\n    end\n    \n    subgraph \"Model Components\"\n        TextEnc[\"LongCatImageTextEncoder\u003cbr/\u003eQwen2.5-VL-based\"]\n        Transformer[\"LongCatImageTransformerModel\u003cbr/\u003e10 double-stream + 20 single-stream blocks\"]\n        VAE[\"LongCatImageVAE\u003cbr/\u003e8x spatial compression\u003cbr/\u003e2x packing factor\"]\n        Scheduler[\"LongCatImageScheduler\u003cbr/\u003eEuler-based\"]\n    end\n    \n    subgraph \"Task-Specific Encoders\"\n        T2IEnc[\"_run_input_encoder_local_t2i\u003cbr/\u003eText only encoding\"]\n        I2IEnc[\"_run_input_encoder_local_i2i\u003cbr/\u003eText + Image VL encoding\"]\n        I2IDiT[\"_run_dit_local_i2i\u003cbr/\u003eVAE encoding for conditioning\"]\n    end\n    \n    Registry --\u003e Runner\n    Runner --\u003e TextEnc\n    Runner --\u003e Transformer\n    Runner --\u003e VAE\n    Runner --\u003e Scheduler\n    \n    Runner --\u003e|task='t2i'| T2IEnc\n    Runner --\u003e|task='i2i'| I2IEnc\n    Runner --\u003e|task='i2i'| I2IDiT\n    \n    T2IEnc --\u003e TextEnc\n    I2IEnc --\u003e TextEnc\n    I2IDiT --\u003e VAE\n```\n\n**LongCatImageRunner Component Mapping**\n\nThe runner is registered as `\"longcat_image\"` and loads four primary components. For T2I tasks, only text encoding is performed. For I2I tasks, the input image is encoded both through the vision-language model (for conditioning) and the VAE (for latent initialization).\n\nSources: [lightx2v/models/runners/longcat_image/longcat_image_runner.py:60-101]()\n\n---\n\n### Task Support: T2I and I2I\n\nLongCatImageRunner supports two task modes, configured via `config[\"task\"]`:\n\n**Text-to-Image (T2I)**\n- Default task mode\n- Text encoder processes prompt only via `infer([text])`\n- Generates images from scratch using noise latents\n- Flow: `_run_input_encoder_local_t2i()`  `_run_dit_local()`  `run_vae_decoder()`\n\n**Image-to-Image (I2I)**\n- Configured with `task=\"i2i\"` and `image_path` parameter\n- Text encoder uses vision-language encoding via `infer_with_image([text], prompt_image)`\n- Input image preprocessed to half size for prompt encoding\n- VAE encoder converts full-size image to latents for conditioning\n- Scheduler uses `prepare_i2i()` with image latents\n\nSources: [lightx2v/models/runners/longcat_image/longcat_image_runner.py:94-101](), [lightx2v/models/runners/longcat_image/longcat_image_runner.py:144-177]()\n\n---\n\n### Text Encoding with Vision-Language Support\n\n```mermaid\ngraph LR\n    subgraph \"T2I Text Encoding\"\n        T2IPrompt[\"prompt: str\"]\n        T2IInfer[\"text_encoders[0].infer([text])\u003cbr/\u003eText-only encoding\"]\n        T2IEmbeds[\"prompt_embeds\u003cbr/\u003e[1, seq_len, hidden_dim]\"]\n    end\n    \n    subgraph \"I2I VL Encoding\"\n        I2IPrompt[\"prompt: str\"]\n        I2IImage[\"prompt_image\u003cbr/\u003ePIL Image @ half size\"]\n        I2IInfer[\"text_encoders[0].infer_with_image\u003cbr/\u003e([text], prompt_image)\u003cbr/\u003eVision-language encoding\"]\n        I2IEmbeds[\"prompt_embeds\u003cbr/\u003e[1, seq_len, hidden_dim]\u003cbr/\u003eImage-conditioned\"]\n    end\n    \n    T2IPrompt --\u003e T2IInfer\n    T2IInfer --\u003e T2IEmbeds\n    \n    I2IPrompt --\u003e I2IInfer\n    I2IImage --\u003e I2IInfer\n    I2IInfer --\u003e I2IEmbeds\n```\n\n**Text Encoding Flow Comparison**\n\nFor I2I tasks, the input image is resized to half the target dimensions and passed to the text encoder along with the prompt. This produces image-conditioned text embeddings that guide the generation. The Qwen-based encoder processes both modalities jointly.\n\nSources: [lightx2v/models/runners/longcat_image/longcat_image_runner.py:204-232](), [lightx2v/models/runners/longcat_image/longcat_image_runner.py:240-263]()\n\n---\n\n### Latent Space and Resolution Handling\n\nLongCatImageRunner uses a 16-channel latent space with 8x VAE spatial compression and 2x packing factor:\n\n```python\n# Latent shape calculation (from set_target_shape)\nvae_scale_factor = 8\nheight = 2 * (int(height) // (vae_scale_factor * 2))  # Divisible by 16\nwidth = 2 * (int(width) // (vae_scale_factor * 2))    # Divisible by 16\nnum_channels_latents = 16\ntarget_shape = (1, num_channels_latents, height, width)\n```\n\n**Aspect Ratio Support**\n\nThe runner provides predefined aspect ratios and supports custom dimensions:\n\n| Aspect Ratio | Dimensions | Use Case |\n|--------------|------------|----------|\n| `16:9` | 1344  768 | Landscape (default) |\n| `9:16` | 768  1344 | Portrait |\n| `1:1` | 1024  1024 | Square |\n| `4:3` | 1152  864 | Standard |\n| Custom | User-defined | `target_shape=[H, W]` |\n\nCustom dimensions are clamped between `min_custom_size` (256) and `max_custom_size` (1664) pixels.\n\nSources: [lightx2v/models/runners/longcat_image/longcat_image_runner.py:336-364](), [lightx2v/models/runners/longcat_image/longcat_image_runner.py:301-335]()\n\n---\n\n### LongCatImageRunner Pipeline Flow\n\n```mermaid\nflowchart TB\n    Start[/\"input_info\u003cbr/\u003e(prompt, task, image_path)\"/]\n    \n    subgraph \"1. Input Encoding Phase\"\n        TaskCheck{\"task type\"}\n        T2IEncode[\"_run_input_encoder_local_t2i\u003cbr/\u003e text_encoders[0].infer([prompt])\u003cbr/\u003e Returns text_encoder_output\"]\n        I2IEncode[\"_run_input_encoder_local_i2i\u003cbr/\u003e Load image from image_path\u003cbr/\u003e Resize to half size for VL\u003cbr/\u003e text_encoders[0].infer_with_image\u003cbr/\u003e _preprocess_image (full size)\u003cbr/\u003e Returns text + image tensors\"]\n    end\n    \n    subgraph \"2. Shape Configuration\"\n        SetShape[\"set_target_shape\u003cbr/\u003e get_custom_shape (aspect ratios)\u003cbr/\u003e Apply VAE scale factor (8x)\u003cbr/\u003e Apply packing factor (2x)\u003cbr/\u003e Set target_shape=[1, 16, H, W]\"]\n        SetImgShape[\"set_img_shapes\u003cbr/\u003e image_shapes for attention\"]\n    end\n    \n    subgraph \"3. DiT Inference Phase\"\n        DiTCheck{\"task type\"}\n        T2IDiT[\"_run_dit_local\u003cbr/\u003e scheduler.prepare(input_info)\u003cbr/\u003e run (denoising loop)\"]\n        I2IDiT[\"_run_dit_local_i2i\u003cbr/\u003e Load VAE if needed\u003cbr/\u003e scheduler.prepare_i2i(input_info, image_tensor, vae)\u003cbr/\u003e run (denoising loop)\"]\n    end\n    \n    subgraph \"4. Denoising Loop\"\n        LoopStart[\"for step_index in range(total_steps)\"]\n        StepPre[\"scheduler.step_pre(step_index)\"]\n        ModelInfer[\"model.infer(inputs)\u003cbr/\u003eLongCatImageTransformerModel\"]\n        StepPost[\"scheduler.step_post\"]\n        LoopEnd[\"latents, generator\"]\n    end\n    \n    subgraph \"5. VAE Decoding\"\n        VAEDecode[\"run_vae_decoder\u003cbr/\u003e vae.decode(latents, input_info)\u003cbr/\u003e Returns PIL Image\"]\n    end\n    \n    Output[/\"Final Image\u003cbr/\u003e(PIL or saved to file)\"/]\n    \n    Start --\u003e TaskCheck\n    TaskCheck --\u003e|\"t2i\"| T2IEncode\n    TaskCheck --\u003e|\"i2i\"| I2IEncode\n    \n    T2IEncode --\u003e SetShape\n    I2IEncode --\u003e SetShape\n    SetShape --\u003e SetImgShape\n    \n    SetImgShape --\u003e DiTCheck\n    DiTCheck --\u003e|\"t2i\"| T2IDiT\n    DiTCheck --\u003e|\"i2i\"| I2IDiT\n    \n    T2IDiT --\u003e LoopStart\n    I2IDiT --\u003e LoopStart\n    \n    LoopStart --\u003e StepPre\n    StepPre --\u003e ModelInfer\n    ModelInfer --\u003e StepPost\n    StepPost --\u003e|\"repeat\"| LoopStart\n    StepPost --\u003e|\"done\"| LoopEnd\n    \n    LoopEnd --\u003e VAEDecode\n    VAEDecode --\u003e Output\n```\n\n**LongCatImageRunner Complete Pipeline**\n\nThe pipeline diverges based on task type at two points: input encoding (T2I uses text-only, I2I uses vision-language encoding) and DiT preparation (I2I performs VAE encoding of the input image for conditioning). Both paths converge at the denoising loop.\n\nSources: [lightx2v/models/runners/longcat_image/longcat_image_runner.py:389-413](), [lightx2v/models/runners/longcat_image/longcat_image_runner.py:281-299]()\n\n---\n\n## LTX2Runner Architecture\n\n### Multi-Modal Component Structure\n\n```mermaid\ngraph TB\n    subgraph \"LTX2Runner Registration\"\n        Registry[\"RUNNER_REGISTER('ltx2')\"]\n        Runner[\"LTX2Runner\u003cbr/\u003e(DefaultRunner)\"]\n    end\n    \n    subgraph \"Encoder Components\"\n        TextEnc[\"LTX2TextEncoder\u003cbr/\u003eGemma-based\u003cbr/\u003eDual context (video, audio)\"]\n    end\n    \n    subgraph \"Transformer Model\"\n        Model[\"LTX2Model\u003cbr/\u003eDual-stream transformer\"]\n        PreInfer[\"LTX2PreInfer\u003cbr/\u003eSeparate video/audio processing\"]\n        TransInfer[\"LTX2TransformerInfer\u003cbr/\u003eJoint video-audio blocks\"]\n        PostInfer[\"LTX2PostInfer\u003cbr/\u003eSeparate video/audio outputs\"]\n    end\n    \n    subgraph \"Dual VAE Decoders\"\n        VideoVAE[\"LTX2VideoVAE\u003cbr/\u003e8x temporal\u003cbr/\u003e8x spatial compression\u003cbr/\u003eEncoder for I2AV\"]\n        AudioVAE[\"LTX2AudioVAE\u003cbr/\u003eAudio mel-spectrogram decoder\"]\n    end\n    \n    subgraph \"Upsampler (Optional)\"\n        Upsampler[\"LTX2Upsampler\u003cbr/\u003e2x spatial upsampling\u003cbr/\u003eStage 2 refinement\"]\n    end\n    \n    Registry --\u003e Runner\n    Runner --\u003e TextEnc\n    Runner --\u003e Model\n    Runner --\u003e VideoVAE\n    Runner --\u003e AudioVAE\n    Runner --\u003e|\"use_upsampler=True\"| Upsampler\n    \n    Model --\u003e PreInfer\n    Model --\u003e TransInfer\n    Model --\u003e PostInfer\n```\n\n**LTX2Runner Component Architecture**\n\nLTX2Runner manages separate processing streams for video and audio throughout the pipeline. The text encoder produces dual contexts, the transformer processes both modalities jointly, and separate VAE decoders generate final video frames and audio waveforms.\n\nSources: [lightx2v/models/runners/ltx2/ltx2_runner.py:24-67](), [lightx2v/models/runners/ltx2/ltx2_runner.py:105-134]()\n\n---\n\n### Task Support: T2AV and I2AV\n\nLTX2Runner supports synchronized audio-video generation in two modes:\n\n**Text-to-Audio+Video (T2AV)**\n- Default task mode\n- Generates both video and audio from text prompt\n- Flow: `_run_input_encoder_local_t2av()`  `run_main()`  dual VAE decoding\n\n**Image-to-Audio+Video (I2AV)**\n- Configured with `task=\"i2av\"` and `image_path`\n- Supports multiple images: `image_path=\"frame0.jpg,frame3.jpg,frame7.jpg\"`\n- Each image has configurable `image_strength` (0.0 = fully denoise, 1.0 = keep original)\n- Flow: `_run_input_encoder_local_i2av()`  `run_vae_encoder()`  conditioning setup\n\nSources: [lightx2v/models/runners/ltx2/ltx2_runner.py:168-192](), [lightx2v/models/runners/ltx2/ltx2_runner.py:199-290]()\n\n---\n\n### Multi-Stage Pipeline with Upsampling\n\nLTX2Runner implements a two-stage generation pipeline when `use_upsampler=True`:\n\n```mermaid\nflowchart TB\n    subgraph \"Stage 1: Base Generation\"\n        S1Init[\"Prepare scheduler\u003cbr/\u003e video_latent_shape: [128, F, H, W]\u003cbr/\u003e audio_latent_shape: [8, A, M]\u003cbr/\u003e distilled_sigma_values\"]\n        S1Denoise[\"run_segment\u003cbr/\u003e Denoising loop (infer_steps)\u003cbr/\u003e Returns v_latent, a_latent\"]\n        S1Out[\"Low-res latents\u003cbr/\u003ev_latent: [128, F, H, W]\u003cbr/\u003ea_latent: [8, A, M]\"]\n    end\n    \n    subgraph \"Stage 2: Upsampling (Optional)\"\n        S2Check{\"use_upsampler?\"}\n        S2Upsample[\"run_upsampler\u003cbr/\u003e upsampler.upsample(v_latent)\u003cbr/\u003e 2x spatial resolution\u003cbr/\u003e Reset scheduler with upsample sigmas\"]\n        S2Init[\"Update latent shapes\u003cbr/\u003e target_shape *= 2\u003cbr/\u003e video_latent_shape: [128, F, 2H, 2W]\u003cbr/\u003e audio_latent_shape unchanged\"]\n        S2Denoise[\"run_segment (Stage 2)\u003cbr/\u003e Denoising loop with upsampled latent\u003cbr/\u003e cleanup_inputs=True\"]\n        S2Out[\"High-res latents\u003cbr/\u003ev_latent: [128, F, 2H, 2W]\u003cbr/\u003ea_latent: [8, A, M]\"]\n    end\n    \n    subgraph \"Decoding Phase\"\n        VAEDecode[\"run_vae_decoder\u003cbr/\u003e video_vae.decode(v_latent)\u003cbr/\u003e audio_vae.decode(a_latent)\u003cbr/\u003e Returns video frames, audio waveform\"]\n    end\n    \n    S1Init --\u003e S1Denoise\n    S1Denoise --\u003e S1Out\n    S1Out --\u003e S2Check\n    S2Check --\u003e|\"True\"| S2Upsample\n    S2Check --\u003e|\"False\"| VAEDecode\n    S2Upsample --\u003e S2Init\n    S2Init --\u003e S2Denoise\n    S2Denoise --\u003e S2Out\n    S2Out --\u003e VAEDecode\n```\n\n**Two-Stage Pipeline Architecture**\n\nStage 1 generates base latents at the configured resolution. If upsampling is enabled, Stage 2 spatially upsamples the video latent 2x and performs additional denoising steps for high-resolution refinement. Audio latents remain unchanged between stages.\n\n**Configuration Parameters**\n\n| Parameter | Stage 1 | Stage 2 |\n|-----------|---------|---------|\n| `target_height` | H | 2H |\n| `target_width` | W | 2W |\n| `distilled_sigma_values` | Base sigmas | `distilled_sigma_values_upsample` |\n| Initial video latent | Noise or conditioned | Upsampled from Stage 1 |\n| Initial audio latent | Noise | From Stage 1 (unchanged) |\n| Video denoise mask | From I2AV conditioning | None (full denoising) |\n\nSources: [lightx2v/models/runners/ltx2/ltx2_runner.py:348-383](), [lightx2v/models/runners/ltx2/ltx2_runner.py:444-473]()\n\n---\n\n### Dual Latent Spaces\n\nLTX2Runner manages separate latent spaces for video and audio with different shapes and compression ratios:\n\n```python\n# Video latent shape calculation\nvideo_latent_shape = (\n    num_channels_latents,  # 128 channels\n    (target_video_length - 1) // vae_scale_factors[0] + 1,  # Temporal: 8x compression\n    target_height // vae_scale_factors[1],  # Spatial height: 8x compression\n    target_width // vae_scale_factors[2],   # Spatial width: 8x compression\n)\n\n# Audio latent shape calculation\nduration = target_video_length / fps\nlatents_per_second = audio_sampling_rate / audio_hop_length / audio_scale_factor\naudio_frames = round(duration * latents_per_second)\naudio_latent_shape = (\n    8,             # 8 channels\n    audio_frames,  # Time dimension (aligned with video duration)\n    audio_mel_bins # Frequency bins (128)\n)\n```\n\n**Latent Space Specifications**\n\n| Modality | Channels | Compression | Dimensions |\n|----------|----------|-------------|------------|\n| Video | 128 | 8x (T), 8x (H), 8x (W) | `[128, (L-1)//8+1, H//8, W//8]` |\n| Audio | 8 | Temporal alignment | `[8, duration*latents_per_sec, 128]` |\n\nThe scheduler maintains separate `video_latent_state` and `audio_latent_state` objects, each with their own noise schedules and update logic.\n\nSources: [lightx2v/models/runners/ltx2/ltx2_runner.py:136-166]()\n\n---\n\n### Image Conditioning for I2AV\n\nLTX2Runner supports conditioning on multiple images at specified frames with controllable strength:\n\n```mermaid\nflowchart LR\n    subgraph \"Input Specification\"\n        ImagePaths[\"image_path:\u003cbr/\u003e'frame0.jpg,frame3.jpg,frame7.jpg'\"]\n        Strengths[\"image_strength:\u003cbr/\u003e[1.0, 0.5, 0.3]\"]\n    end\n    \n    subgraph \"run_vae_encoder Processing\"\n        LoadResize[\"For each image:\u003cbr/\u003e Load from path\u003cbr/\u003e Resize to target_height, target_width\"]\n        VAEEncode[\"video_vae.encode(image)\u003cbr/\u003e Returns latent [C, 1, H_lat, W_lat]\"]\n        FrameMap[\"Map pixel frame to latent frame\u003cbr/\u003e latent_idx = (frame_idx - 1) // 8 + 1\"]\n        SetLatent[\"Set initial_video_latent\u003cbr/\u003e[:, latent_idx, :, :]\"]\n        SetMask[\"Set video_denoise_mask\u003cbr/\u003e[:, latent_idx, :, :] = 1.0 - strength\"]\n    end\n    \n    subgraph \"Outputs\"\n        InitLatent[\"initial_video_latent\u003cbr/\u003e[C, F, H, W]\u003cbr/\u003eConditioned frames set\"]\n        DenoiseMask[\"video_denoise_mask\u003cbr/\u003e[1, F, H, W]\u003cbr/\u003e1.0 = denoise\u003cbr/\u003e0.0 = keep original\"]\n    end\n    \n    ImagePaths --\u003e LoadResize\n    Strengths --\u003e SetMask\n    LoadResize --\u003e VAEEncode\n    VAEEncode --\u003e FrameMap\n    FrameMap --\u003e SetLatent\n    FrameMap --\u003e SetMask\n    SetLatent --\u003e InitLatent\n    SetMask --\u003e DenoiseMask\n```\n\n**Image Conditioning Pipeline**\n\nEach conditioned image is encoded to a latent and placed at its corresponding temporal index. The denoise mask controls how much the model should modify each frame: `strength=1.0` keeps the conditioned frame unchanged, `strength=0.0` allows full denoising.\n\n**Strength Parameter Behavior**\n\n| Strength | Denoise Mask Value | Effect |\n|----------|-------------------|--------|\n| 0.0 | 1.0 | Fully denoise (generate new content) |\n| 0.5 | 0.5 | Partial denoising (blend) |\n| 1.0 | 0.0 | Keep original (no denoising) |\n\nSources: [lightx2v/models/runners/ltx2/ltx2_runner.py:199-290]()\n\n---\n\n### LTX2Runner Pipeline Flow\n\n```mermaid\nflowchart TB\n    Start[/\"input_info\u003cbr/\u003e(prompt, task, image_path, use_upsampler)\"/]\n    \n    subgraph \"1. Input Encoding\"\n        TaskCheck{\"task type\"}\n        T2AV[\"_run_input_encoder_local_t2av\u003cbr/\u003e get_latent_shape_with_target_hw\u003cbr/\u003e run_text_encoder\u003cbr/\u003e video_denoise_mask = None\u003cbr/\u003e initial_video_latent = None\"]\n        I2AV[\"_run_input_encoder_local_i2av\u003cbr/\u003e get_latent_shape_with_target_hw\u003cbr/\u003e run_text_encoder\u003cbr/\u003e run_vae_encoder\u003cbr/\u003e Returns denoise_mask, initial_latent\"]\n    end\n    \n    subgraph \"2. Stage 1: Base Generation\"\n        InitRun[\"init_run\u003cbr/\u003e _prepare_scheduler (with conditioning)\u003cbr/\u003e Load transformer if lazy_load\"]\n        Segment1[\"run_segment (segment_idx=0)\u003cbr/\u003e Denoising loop (infer_steps)\u003cbr/\u003e Returns v_latent, a_latent\"]\n    end\n    \n    subgraph \"3. Stage 2: Upsampling (Optional)\"\n        UpsampleCheck{\"use_upsampler?\"}\n        RunUpsampler[\"run_upsampler\u003cbr/\u003e upsampler.upsample(v_latent)\u003cbr/\u003e Reset scheduler sigmas\u003cbr/\u003e Update target_shape, latent_shapes\u003cbr/\u003e _prepare_scheduler (upsampled_v_latent)\u003cbr/\u003e run_segment (Stage 2, cleanup_inputs=True)\"]\n    end\n    \n    subgraph \"4. VAE Decoding\"\n        VAEDecode[\"run_vae_decoder\u003cbr/\u003e video = video_vae.decode(v_latent)\u003cbr/\u003e audio = audio_vae.decode(a_latent)\u003cbr/\u003e Returns video frames, audio waveform\"]\n    end\n    \n    subgraph \"5. Output Processing\"\n        SaveVideo[\"save_video\u003cbr/\u003e Combine video + audio\u003cbr/\u003e Save to save_result_path\"]\n    end\n    \n    Output[/\"Final Output\u003cbr/\u003e(video file with audio)\"/]\n    \n    Start --\u003e TaskCheck\n    TaskCheck --\u003e|\"t2av\"| T2AV\n    TaskCheck --\u003e|\"i2av\"| I2AV\n    \n    T2AV --\u003e InitRun\n    I2AV --\u003e InitRun\n    \n    InitRun --\u003e Segment1\n    Segment1 --\u003e UpsampleCheck\n    \n    UpsampleCheck --\u003e|\"True\"| RunUpsampler\n    UpsampleCheck --\u003e|\"False\"| VAEDecode\n    \n    RunUpsampler --\u003e VAEDecode\n    VAEDecode --\u003e SaveVideo\n    SaveVideo --\u003e Output\n```\n\n**LTX2Runner Complete Pipeline**\n\nThe pipeline begins with task-specific input encoding, proceeds through Stage 1 base generation, optionally performs Stage 2 upsampling, then decodes both video and audio latents. The final output is a synchronized video file with embedded audio.\n\nSources: [lightx2v/models/runners/ltx2/ltx2_runner.py:444-493](), [lightx2v/models/runners/ltx2/ltx2_runner.py:495-553]()\n\n---\n\n## Shared Denoising Loop Implementation\n\nBoth runners inherit the standard denoising loop from `DefaultRunner` but customize specific stages:\n\n```mermaid\ngraph LR\n    subgraph \"LongCatImageRunner.run\"\n        LCPrepare[\"scheduler.step_pre\"]\n        LCInfer[\"model.infer(inputs)\u003cbr/\u003eLongCatImageTransformerModel\"]\n        LCPost[\"scheduler.step_post\"]\n    end\n    \n    subgraph \"LTX2Runner.run_segment\"\n        LTPrepare[\"scheduler.step_pre\"]\n        LTInfer[\"model.infer(inputs)\u003cbr/\u003eLTX2Model\u003cbr/\u003eDual-stream processing\"]\n        LTPost[\"scheduler.step_post\u003cbr/\u003eUpdates video_latent_state\u003cbr/\u003eUpdates audio_latent_state\"]\n    end\n    \n    LCPrepare --\u003e LCInfer\n    LCInfer --\u003e LCPost\n    \n    LTPrepare --\u003e LTInfer\n    LTInfer --\u003e LTPost\n```\n\n**Denoising Loop Comparison**\n\nBoth runners follow the standard `step_pre  infer  step_post` pattern but with model-specific inference implementations. LTX2Runner's scheduler manages dual latent states while LongCatImageRunner's scheduler handles single image latents.\n\nSources: [lightx2v/models/runners/longcat_image/longcat_image_runner.py:281-299](), [lightx2v/models/runners/ltx2/ltx2_runner.py:495-553]()\n\n---\n\n## Configuration Examples\n\n### LongCatImageRunner Configuration\n\n```json\n{\n  \"model_cls\": \"longcat_image\",\n  \"model_path\": \"/path/to/longcat_image\",\n  \"task\": \"t2i\",\n  \"resolution\": 1024,\n  \"aspect_ratio\": \"16:9\",\n  \"enable_cfg\": true,\n  \"guidance_scale\": 5.0,\n  \"infer_steps\": 50,\n  \"vae_scale_factor\": 8,\n  \"transformer_in_channels\": 64,\n  \"enable_prompt_rewrite\": false,\n  \"cfg_parallel\": false\n}\n```\n\n### LTX2Runner Configuration\n\n```json\n{\n  \"model_cls\": \"ltx2\",\n  \"model_path\": \"/path/to/ltx2\",\n  \"task\": \"t2av\",\n  \"target_height\": 480,\n  \"target_width\": 720,\n  \"target_video_length\": 121,\n  \"fps\": 24,\n  \"audio_sampling_rate\": 24000,\n  \"audio_hop_length\": 256,\n  \"audio_scale_factor\": 8,\n  \"audio_mel_bins\": 128,\n  \"num_channels_latents\": 128,\n  \"vae_scale_factors\": [8, 8, 8],\n  \"use_upsampler\": true,\n  \"distilled_sigma_values\": [14.6146, 6.4927, 2.4841, 0.0292],\n  \"distilled_sigma_values_upsample\": [2.4841, 0.0292],\n  \"enable_cfg\": true,\n  \"guidance_scale\": 3.0,\n  \"infer_steps\": 4,\n  \"use_tiling_vae\": false,\n  \"tensor_parallel\": false,\n  \"seq_parallel\": false\n}\n```\n\nSources: [lightx2v/models/runners/longcat_image/longcat_image_runner.py:65-68](), [lightx2v/models/runners/ltx2/ltx2_runner.py:136-166]()"])</script><script>self.__next_f.push([1,"37:T63b1,"])</script><script>self.__next_f.push([1,"# ShotPipeline - Multi-Clip Generation\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [configs/seko_talk/shot/rs2v/rs2v.json](configs/seko_talk/shot/rs2v/rs2v.json)\n- [configs/seko_talk/shot/stream/f2v.json](configs/seko_talk/shot/stream/f2v.json)\n- [configs/seko_talk/shot/stream/s2v.json](configs/seko_talk/shot/stream/s2v.json)\n- [lightx2v/models/schedulers/scheduler.py](lightx2v/models/schedulers/scheduler.py)\n- [lightx2v/models/schedulers/wan/audio/scheduler.py](lightx2v/models/schedulers/wan/audio/scheduler.py)\n- [lightx2v/shot_runner/rs2v_infer.py](lightx2v/shot_runner/rs2v_infer.py)\n- [lightx2v/shot_runner/shot_base.py](lightx2v/shot_runner/shot_base.py)\n- [lightx2v/shot_runner/stream_infer.py](lightx2v/shot_runner/stream_infer.py)\n- [lightx2v/utils/input_info.py](lightx2v/utils/input_info.py)\n- [lightx2v/utils/set_config.py](lightx2v/utils/set_config.py)\n\n\u003c/details\u003e\n\n\n\n**Purpose:** This document describes the ShotPipeline system for generating long-form video by chaining multiple clips together. ShotPipeline enables generation beyond the temporal limits of individual models by managing clip transitions, temporal consistency, and audio segmentation across multiple inference stages.\n\nFor information about individual audio-to-video generation with single clips, see [WanAudioRunner - Audio-to-Video Generation](#5.2). For general runner architecture, see [Runner System and Registry Pattern](#4.1).\n\n## Overview\n\nThe ShotPipeline system addresses the challenge of generating videos longer than a single model's maximum temporal window (typically 33-81 frames) by:\n\n1. **Clip Segmentation** - Breaking long audio/video into overlapping segments\n2. **Temporal Consistency** - Using overlap frames or latents to maintain consistency between clips\n3. **Multi-Model Orchestration** - Switching between different runners for different segments\n4. **Reference State Management** - Managing reference frames for long-form consistency (RS2V)\n\nTwo main pipeline implementations exist:\n- `ShotRS2VPipeline` - Referenced Speech-to-Video for long-form audio with reference state tracking\n- `ShotStreamPipeline` - Streaming pipeline that alternates between models for quality/speed trade-offs\n\n```mermaid\ngraph TB\n    subgraph \"ShotPipeline System\"\n        Base[\"ShotPipeline\u003cbr/\u003e(Base Class)\"]\n        RS2V[\"ShotRS2VPipeline\u003cbr/\u003e(Referenced S2V)\"]\n        Stream[\"ShotStreamPipeline\u003cbr/\u003e(Model Switching)\"]\n    end\n    \n    subgraph \"Configuration\"\n        MainConfig[\"Main JSON Config\u003cbr/\u003elightx2v_path, clip_configs\"]\n        ClipConfig[\"ClipConfig\u003cbr/\u003ename, config_json\"]\n        DefaultInput[\"default_input_info\u003cbr/\u003eper-clip defaults\"]\n    end\n    \n    subgraph \"Clip Management\"\n        ClipGen[\"clip_generators\u003cbr/\u003e{name: runner}\"]\n        ClipInputs[\"clip_inputs\u003cbr/\u003e{name: input_info}\"]\n        LoadConfigs[\"load_clip_configs()\"]\n    end\n    \n    subgraph \"Temporal Consistency\"\n        OverlapFrame[\"overlap_frame\u003cbr/\u003e(decoded pixels)\"]\n        OverlapLatent[\"overlap_latent\u003cbr/\u003e(latent space)\"]\n        RefState[\"ref_state\u003cbr/\u003e(reference indicator)\"]\n    end\n    \n    subgraph \"Audio Processing\"\n        AudioReader[\"SlidingWindowReader\u003cbr/\u003eRS2V_SlidingWindowReader\"]\n        AudioClip[\"audio_clip\u003cbr/\u003e(per-segment audio)\"]\n        AudioAlign[\"Audio-Video Alignment\u003cbr/\u003e(frame padding)\"]\n    end\n    \n    MainConfig --\u003e LoadConfigs\n    LoadConfigs --\u003e ClipConfig\n    ClipConfig --\u003e ClipGen\n    ClipConfig --\u003e DefaultInput\n    \n    Base --\u003e RS2V\n    Base --\u003e Stream\n    \n    ClipGen --\u003e Base\n    ClipInputs --\u003e Base\n    \n    RS2V --\u003e AudioReader\n    RS2V --\u003e RefState\n    Stream --\u003e AudioReader\n    \n    AudioReader --\u003e AudioClip\n    AudioClip --\u003e ClipGen\n    \n    ClipGen --\u003e OverlapFrame\n    ClipGen --\u003e OverlapLatent\n    OverlapLatent --\u003e AudioAlign\n```\n\n**Sources:** [lightx2v/shot_runner/shot_base.py:1-135](), [lightx2v/shot_runner/rs2v_infer.py:1-153](), [lightx2v/shot_runner/stream_infer.py:1-125]()\n\n## Base ShotPipeline Architecture\n\nThe `ShotPipeline` base class provides the infrastructure for managing multiple clip generators and coordinating their execution.\n\n### Core Components\n\n| Component | Type | Purpose |\n|-----------|------|---------|\n| `clip_generators` | `dict[str, Runner]` | Initialized runners for each clip type |\n| `clip_inputs` | `dict[str, InputInfo]` | Input information for each clip |\n| `progress_callback` | `callable` | Optional callback for progress updates |\n\n### Key Methods\n\n**Initialization and Configuration:**\n\n```mermaid\ngraph LR\n    Init[\"__init__\u003cbr/\u003e(clip_configs)\"]\n    CreateGen[\"create_clip_generator\u003cbr/\u003e(clip_config)\"]\n    InitRunner[\"_init_runner\u003cbr/\u003e(config)\"]\n    \n    Init --\u003e CreateGen\n    CreateGen --\u003e InitRunner\n    InitRunner --\u003e Registry[\"RUNNER_REGISTER\u003cbr/\u003e[model_cls]\"]\n    Registry --\u003e Runner[\"runner.init_modules()\"]\n```\n\nThe initialization flow loads each clip configuration, instantiates the appropriate runner from the registry, and calls `init_modules()` to load model weights.\n\n**Input Management:**\n\nThe `check_input_info()` method merges user input with clip defaults:\n\n```mermaid\ngraph TB\n    UserInput[\"User Input Info\"]\n    DefaultInfo[\"default_input_info\u003cbr/\u003e(from config)\"]\n    FillDefaults[\"fill_input_info_from_defaults()\"]\n    Normalize[\"normalize_unset_to_none()\"]\n    Ready[\"Ready for Inference\"]\n    \n    UserInput --\u003e FillDefaults\n    DefaultInfo --\u003e FillDefaults\n    FillDefaults --\u003e Normalize\n    Normalize --\u003e Ready\n```\n\nThe `update_input_info()` method synchronizes external inputs across all clip input infos, particularly useful for shared parameters like `seed`, `image_path`, `audio_path`, `prompt`, and `save_result_path` [lightx2v/shot_runner/shot_base.py:87-105]().\n\n**Sources:** [lightx2v/shot_runner/shot_base.py:63-135]()\n\n## Configuration System\n\nShotPipeline uses a hierarchical JSON configuration system to define multi-clip workflows.\n\n### Main Configuration Structure\n\n```json\n{\n  \"lightx2v_path\": \"\u003cbase_path\u003e\",\n  \"parallel\": {\n    \"cfg_p_size\": 1,\n    \"seq_p_size\": 1\n  },\n  \"clip_configs\": [\n    {\n      \"name\": \"clip_name\",\n      \"path\": \"relative/config/path.json\"\n    }\n  ]\n}\n```\n\nThe `load_clip_configs()` function processes this structure [lightx2v/shot_runner/shot_base.py:35-61]():\n\n```mermaid\ngraph TB\n    MainJSON[\"Main JSON\u003cbr/\u003e(pipeline config)\"]\n    LoadFunc[\"load_clip_configs()\"]\n    CheckParallel{\"parallel config?\"}\n    InitParallel[\"init_parallel_env()\"]\n    \n    subgraph \"For Each Clip\"\n        LoadClipJSON[\"Load clip JSON\u003cbr/\u003efrom path\"]\n        SetConfig[\"set_config()\"]\n        AddParallel[\"Add parallel config\"]\n        CreateClipConfig[\"ClipConfig(name, config_json)\"]\n    end\n    \n    MainJSON --\u003e LoadFunc\n    LoadFunc --\u003e CheckParallel\n    CheckParallel --\u003e|Yes| InitParallel\n    CheckParallel --\u003e LoadClipJSON\n    InitParallel --\u003e LoadClipJSON\n    LoadClipJSON --\u003e SetConfig\n    SetConfig --\u003e AddParallel\n    AddParallel --\u003e CreateClipConfig\n```\n\n### Per-Clip Configuration\n\nEach clip configuration includes model settings and default input values:\n\n| Field | Purpose | Example |\n|-------|---------|---------|\n| `model_cls` | Runner type | `\"seko_talk\"` |\n| `task` | Task type | `\"s2v\"`, `\"rs2v\"` |\n| `target_video_length` | Clip length in frames | `33`, `81` |\n| `prev_frame_length` | Overlap length | `5`, `1` |\n| `default_input_info` | Default parameters | `{\"infer_steps\": 4, \"prompt\": \"...\"}` |\n\n**Example configurations:**\n- RS2V configuration: [configs/seko_talk/shot/rs2v/rs2v.json:1-28]()\n- Stream S2V configuration: [configs/seko_talk/shot/stream/s2v.json:1-38]()\n- Stream F2V configuration: [configs/seko_talk/shot/stream/f2v.json:1-44]()\n\n**Sources:** [lightx2v/shot_runner/shot_base.py:23-61](), [configs/seko_talk/shot/rs2v/rs2v.json:1-28](), [configs/seko_talk/shot/stream/s2v.json:1-38]()\n\n## ShotRS2VPipeline - Referenced Speech-to-Video\n\n`ShotRS2VPipeline` generates long-form audio-driven video by segmenting audio into overlapping clips and maintaining a reference state sequence for temporal consistency.\n\n### Architecture\n\n```mermaid\ngraph TB\n    InputAudio[\"Input Audio\u003cbr/\u003e(full duration)\"]\n    ResampleAudio[\"Resample to model_sr\u003cbr/\u003e(16kHz)\"]\n    TrimDuration[\"Trim to video_duration\u003cbr/\u003e(optional)\"]\n    AudioReader[\"RS2V_SlidingWindowReader\u003cbr/\u003e(overlapping windows)\"]\n    \n    subgraph \"Clip Loop\"\n        NextFrame[\"audio_clip, pad_len = next_frame()\"]\n        CheckEnd{\"audio_clip == None?\"}\n        \n        SetFlags[\"Set is_first, is_last\u003cbr/\u003ebased on index and pad_len\"]\n        SetRefState[\"Set ref_state\u003cbr/\u003e(from sequence pattern)\"]\n        SetSeed[\"Increment seed\"]\n        SetAudioClip[\"Set audio_clip in input_info\"]\n        \n        RunClip[\"pipe.run_clip_pipeline()\"]\n        \n        TrimPadding[\"Trim pad_len from output\"]\n        AppendVideo[\"Append to gen_video_list\"]\n        AppendAudio[\"Append to cut_audio_list\"]\n        UpdateOverlap[\"Update overlap_latent\"]\n    end\n    \n    ConcatVideo[\"Concatenate video clips\"]\n    ConcatAudio[\"Concatenate audio clips\"]\n    SaveOutput[\"Save merged video + audio\"]\n    \n    InputAudio --\u003e ResampleAudio\n    ResampleAudio --\u003e TrimDuration\n    TrimDuration --\u003e AudioReader\n    \n    AudioReader --\u003e NextFrame\n    NextFrame --\u003e CheckEnd\n    CheckEnd --\u003e|No| SetFlags\n    CheckEnd --\u003e|Yes| ConcatVideo\n    \n    SetFlags --\u003e SetRefState\n    SetRefState --\u003e SetSeed\n    SetSeed --\u003e SetAudioClip\n    SetAudioClip --\u003e RunClip\n    \n    RunClip --\u003e TrimPadding\n    TrimPadding --\u003e AppendVideo\n    TrimPadding --\u003e AppendAudio\n    AppendVideo --\u003e UpdateOverlap\n    UpdateOverlap --\u003e NextFrame\n    \n    ConcatVideo --\u003e SaveOutput\n    ConcatAudio --\u003e SaveOutput\n```\n\n### Reference State Sequence\n\nThe `get_reference_state_sequence()` function generates a pattern that determines when the model should use the reference frame [lightx2v/shot_runner/rs2v_infer.py:16-23]():\n\n| Video Duration | Pattern | Behavior |\n|----------------|---------|----------|\n| \u003e 3 seconds | `[0, 1, 1]` repeating | Reference every 3rd clip |\n|  3 seconds | `[0, 1, 1, 1, 1, 1, 1]` repeating | Reference every 7th clip |\n\nThe `ref_state` value is passed to the underlying runner via the `RS2VInputInfo` dataclass [lightx2v/utils/input_info.py:140]().\n\n### Audio Segmentation\n\n`RS2V_SlidingWindowReader` manages overlapping audio windows [lightx2v/shot_runner/rs2v_infer.py:54]():\n\n| Parameter | First Clip | Subsequent Clips |\n|-----------|------------|------------------|\n| Window Length | `target_video_length` frames | `target_video_length + 3` frames |\n| Overlap | 0 | 3 frames |\n| Padding | Yes (for last clip) | Yes (for last clip) |\n\nThe reader returns `(audio_clip, pad_len)` where `pad_len` indicates how much zero-padding was added to reach the target length. This padding is trimmed from the output video [lightx2v/shot_runner/rs2v_infer.py:88-90]().\n\n### Clip Processing Loop\n\nThe main generation loop [lightx2v/shot_runner/rs2v_infer.py:66-92]():\n\n1. **Read Audio Segment** - Get next overlapping audio window\n2. **Set Flags** - Mark `is_first` (idx==0) and `is_last` (pad_len\u003e0)\n3. **Set Reference State** - Get state from cyclic pattern\n4. **Increment Seed** - Ensure different random state per clip\n5. **Run Inference** - Call `pipe.run_clip_pipeline(clip_input_info)`\n6. **Trim Padding** - Remove padded frames: `gen_clip_video[:, :, :gen_clip_video.shape[2] - video_pad_len]`\n7. **Update Overlap** - Set `overlap_latent` from last latent frame: `gen_latents[:, -1:]`\n\n### Temporal Consistency Mechanism\n\nOverlap latents are passed between clips via the `RS2VInputInfo` structure:\n\n```mermaid\ngraph LR\n    Clip1[\"Clip 1\u003cbr/\u003eGeneration\"]\n    ExtractLatent[\"Extract Last Latent\u003cbr/\u003egen_latents[:, -1:]\"]\n    StoreOverlap[\"overlap_latent\u003cbr/\u003e(in input_info)\"]\n    Clip2[\"Clip 2\u003cbr/\u003eGeneration\"]\n    \n    Clip1 --\u003e ExtractLatent\n    ExtractLatent --\u003e StoreOverlap\n    StoreOverlap --\u003e Clip2\n    \n    Clip2 --\u003e ExtractLatent\n```\n\nThe scheduler uses this overlap to initialize portions of the latent tensor, ensuring smooth transitions [lightx2v/models/schedulers/wan/audio/scheduler.py:90-93]():\n\n```python\nif self.prev_latents is not None:\n    self.latents = (1.0 - self.mask) * self.prev_latents + self.mask * self.latents\n```\n\n**Sources:** [lightx2v/shot_runner/rs2v_infer.py:25-107](), [lightx2v/shot_runner/utils.py](), [lightx2v/models/schedulers/wan/audio/scheduler.py:42-45]()\n\n## ShotStreamPipeline - Model Switching Strategy\n\n`ShotStreamPipeline` alternates between two different models to balance temporal consistency and motion dynamics for long-form generation.\n\n### Dual-Model Architecture\n\n```mermaid\ngraph TB\n    Pipeline[\"ShotStreamPipeline\"]\n    \n    subgraph \"Model A: S2V\"\n        S2V[\"s2v_clip runner\u003cbr/\u003e(High Consistency)\"]\n        S2VConfig[\"prev_frame_length: 5\u003cbr/\u003eStrong temporal coherence\"]\n    end\n    \n    subgraph \"Model B: F2V\"\n        F2V[\"f2v_clip runner\u003cbr/\u003e(High Dynamics)\"]\n        F2VConfig[\"prev_frame_length: 1\u003cbr/\u003eBetter motion response\u003cbr/\u003e+ LoRA for movement\"]\n    end\n    \n    Pipeline --\u003e S2V\n    Pipeline --\u003e F2V\n    \n    S2V --\u003e S2VConfig\n    F2V --\u003e F2VConfig\n    \n    AlternateLogic[\"Alternate by Index:\u003cbr/\u003ei % 2 == 0  S2V\u003cbr/\u003ei % 2 == 1  F2V\"]\n```\n\n### Characteristics of Each Model\n\n| Model | Overlap Frames | Strengths | Weaknesses | Config |\n|-------|----------------|-----------|------------|--------|\n| **S2V** | 5 frames | Strong temporal consistency, stable identity | Less dynamic motion response | [configs/seko_talk/shot/stream/s2v.json]() |\n| **F2V** | 1 frame | Better motion dynamics, responsive gestures | Weaker temporal consistency | [configs/seko_talk/shot/stream/f2v.json]() |\n\nThe F2V model can be enhanced with a LoRA for improved motion generation [configs/seko_talk/shot/stream/f2v.json:27-32]() and supports additional action prompts [lightx2v/shot_runner/stream_infer.py:64]().\n\n### Generation Loop with Model Switching\n\n```mermaid\ngraph TB\n    Start[\"Start Loop\u003cbr/\u003ei = 0, overlap = 0\"]\n    ReadAudio[\"Read Audio Window\u003cbr/\u003e(with overlap)\"]\n    CheckDone{\"Audio Done?\"}\n    \n    SelectModel{\"i % 2 == 0?\"}\n    UseS2V[\"Use s2v_clip\u003cbr/\u003einputs = s2v_input_info\"]\n    UseF2V[\"Use f2v_clip\u003cbr/\u003einputs = f2v_input_info\u003cbr/\u003e+ action prompt\"]\n    \n    PrepOverlap[\"Set inputs.overlap_frame\u003cbr/\u003e(from global_tail_video)\"]\n    IncrementSeed[\"inputs.seed += i\"]\n    SetAudio[\"inputs.audio_clip = audio_clip\"]\n    \n    RunClip[\"pipe.run_clip_pipeline(inputs)\"]\n    \n    AlignLen[\"aligned_len = gen_clip_video.shape[2] - overlap\"]\n    AppendVideo[\"Append gen_clip_video[:,:,:aligned_len]\"]\n    AppendAudio[\"Append audio_clip[:aligned_len * audio_per_frame]\"]\n    \n    UpdateState[\"overlap = pipe.prev_frame_length\u003cbr/\u003eglobal_tail_video = gen_clip_video[:,:,-max_tail_len:]\u003cbr/\u003ei += 1\"]\n    \n    MergeOutputs[\"Concatenate all clips\"]\n    \n    Start --\u003e ReadAudio\n    ReadAudio --\u003e CheckDone\n    CheckDone --\u003e|No| SelectModel\n    CheckDone --\u003e|Yes| MergeOutputs\n    \n    SelectModel --\u003e|Yes| UseS2V\n    SelectModel --\u003e|No| UseF2V\n    \n    UseS2V --\u003e PrepOverlap\n    UseF2V --\u003e PrepOverlap\n    \n    PrepOverlap --\u003e IncrementSeed\n    IncrementSeed --\u003e SetAudio\n    SetAudio --\u003e RunClip\n    \n    RunClip --\u003e AlignLen\n    AlignLen --\u003e AppendVideo\n    AppendVideo --\u003e AppendAudio\n    AppendAudio --\u003e UpdateState\n    UpdateState --\u003e ReadAudio\n```\n\n### Temporal Consistency with Variable Overlap\n\nUnlike RS2V which uses latent-space overlap, `ShotStreamPipeline` uses pixel-space overlap frames [lightx2v/shot_runner/stream_infer.py:74-81]():\n\n1. **Maintain Global Tail** - Store last `max_tail_len` frames of each clip in `global_tail_video`\n2. **Extract Required Overlap** - When switching to next model, extract `pipe.prev_frame_length` frames from tail\n3. **Set Overlap Frame** - Pass extracted frames via `inputs.overlap_frame`\n4. **Align Output** - Trim overlap frames from concatenation: `gen_clip_video[:, :, :aligned_len]`\n\nThe `max_tail_len` is calculated as the maximum `prev_frame_length` across all models [lightx2v/shot_runner/stream_infer.py:26]():\n\n```python\nself.max_tail_len = max(\n    s2v.config.get(\"prev_frame_length\", None), \n    f2v.config.get(\"prev_frame_length\", None)\n)\n```\n\n**Sources:** [lightx2v/shot_runner/stream_infer.py:17-93](), [configs/seko_talk/shot/stream/s2v.json:1-38](), [configs/seko_talk/shot/stream/f2v.json:1-44]()\n\n## Input Info Structures for Multi-Clip Tasks\n\nShot pipelines use specialized `InputInfo` dataclasses with fields for temporal state management.\n\n### S2VInputInfo Fields\n\nStandard S2V fields plus overlap state [lightx2v/utils/input_info.py:88-112]():\n\n| Field | Type | Purpose |\n|-------|------|---------|\n| `seed` | `int` | Random seed (incremented per clip) |\n| `prompt`, `negative_prompt` | `str` | Text conditioning |\n| `image_path`, `audio_path` | `str` | Input media paths |\n| `audio_num` | `int` | Number of audio inputs (multi-person) |\n| `with_mask` | `bool` | Use spatial masks for multi-person |\n| `stream_config` | `dict` | Streaming configuration |\n| `overlap_frame` | `Tensor` | Previous clip's last frames (pixels) |\n| `overlap_latent` | `Tensor` | Previous clip's last latent (unused in S2V) |\n| `audio_clip` | `Tensor` | Pre-segmented audio for this clip |\n\n### RS2VInputInfo Fields\n\nExtends S2VInputInfo with reference state management [lightx2v/utils/input_info.py:115-144]():\n\n| Additional Field | Type | Purpose |\n|-----------------|------|---------|\n| `ref_state` | `int` | Reference indicator (0=use reference, 1=generate freely) |\n| `is_first` | `bool` | First clip in sequence |\n| `is_last` | `bool` | Last clip in sequence |\n\n### SekoTalkInputs\n\nAlternative input structure using `UNSET` sentinel values for flexible initialization [lightx2v/utils/input_info.py:318-376]():\n\n```python\n@dataclass\nclass SekoTalkInputs:\n    infer_steps: int | Any = UNSET\n    seed: int | Any = UNSET\n    # ... other fields with UNSET defaults\n    \n    @classmethod\n    def from_args(cls, args, **overrides):\n        \"\"\"Build from args with override priority\"\"\"\n        \n    def normalize_unset_to_none(self):\n        \"\"\"Replace UNSET with None before inference\"\"\"\n```\n\nThe `init_input_info_from_args()` factory function creates appropriate input structures [lightx2v/utils/input_info.py:371-376]():\n\n```python\ndef init_input_info_from_args(task, args, **overrides):\n    if task in [\"s2v\", \"rs2v\"]:\n        return SekoTalkInputs.from_args(args, **overrides)\n```\n\n**Sources:** [lightx2v/utils/input_info.py:88-144](), [lightx2v/utils/input_info.py:318-376]()\n\n## Scheduler Integration with Multi-Clip State\n\nThe `EulerScheduler` for audio tasks integrates with ShotPipeline's overlap mechanisms through `prepare_latents()` and masking.\n\n### Latent Initialization with Previous State\n\nWhen `prev_latents` is provided (wan2.2_audio), the scheduler blends previous and new latents [lightx2v/models/schedulers/wan/audio/scheduler.py:79-94]():\n\n```mermaid\ngraph TB\n    InitLatents[\"prepare_latents(seed, latent_shape)\"]\n    GenRandom[\"Generate random noise\u003cbr/\u003etorch.randn(latent_shape)\"]\n    CheckPrev{\"prev_latents exists?\"}\n    CreateMask[\"masks_like(latents,\u003cbr/\u003ezero=True, prev_len=prev_len)\"]\n    BlendLatents[\"latents = (1 - mask) * prev_latents\u003cbr/\u003e+ mask * latents\"]\n    ReturnLatents[\"Return prepared latents\"]\n    \n    InitLatents --\u003e GenRandom\n    GenRandom --\u003e CheckPrev\n    CheckPrev --\u003e|No| ReturnLatents\n    CheckPrev --\u003e|Yes| CreateMask\n    CreateMask --\u003e BlendLatents\n    BlendLatents --\u003e ReturnLatents\n```\n\nThe mask ensures only new regions are randomized while preserving previous state [lightx2v/models/schedulers/wan/audio/scheduler.py:91-93]().\n\n### Reset for Next Clip\n\nThe `reset()` method prepares the scheduler for the next clip [lightx2v/models/schedulers/wan/audio/scheduler.py:118-122]():\n\n```python\ndef reset(self, seed, latent_shape, image_encoder_output=None):\n    if self.config[\"model_cls\"] == \"wan2.2_audio\":\n        self.prev_latents = image_encoder_output[\"prev_latents\"]\n        self.prev_len = image_encoder_output[\"prev_len\"]\n    self.prepare_latents(seed, latent_shape, dtype=torch.float32)\n```\n\n### Timestep Handling for Overlapping Frames\n\nThe `step_pre()` method adjusts timesteps based on the mask [lightx2v/models/schedulers/wan/audio/scheduler.py:57-77]():\n\n1. **Compute Audio Adapter Embeddings** - Time embeddings for audio features\n2. **Create Masked Timesteps** - Zero out timesteps for overlap regions: `(self.mask[0] * self.timestep_input).flatten()`\n3. **Pad to Max Length** - Ensure consistent tensor size across sequence parallel groups\n4. **Add Reference Frame Padding** - Append zero timesteps for reference frame tokens\n\n**Sources:** [lightx2v/models/schedulers/wan/audio/scheduler.py:13-144]()\n\n## Usage Patterns and Examples\n\n### RS2V Pipeline Usage\n\nCommand-line invocation [lightx2v/shot_runner/rs2v_infer.py:123-152]():\n\n```bash\npython -m lightx2v.shot_runner.rs2v_infer \\\n    --config_json configs/seko_talk/shot/rs2v/main.json \\\n    --seed 42 \\\n    --prompt \"A person speaking to the camera\" \\\n    --image_path assets/inputs/audio/speaker.png \\\n    --audio_path assets/inputs/audio/speech.mp3 \\\n    --save_result_path output.mp4\n```\n\nPython API usage:\n\n```python\nfrom lightx2v.shot_runner.shot_base import load_clip_configs\nfrom lightx2v.shot_runner.rs2v_infer import ShotRS2VPipeline\n\n# Load configuration\nclip_configs = load_clip_configs(\"configs/seko_talk/shot/rs2v/main.json\")\n\n# Initialize pipeline\npipeline = ShotRS2VPipeline(clip_configs)\n\n# Prepare input\nclass Args:\n    seed = 42\n    prompt = \"A person speaking\"\n    image_path = \"speaker.png\"\n    audio_path = \"speech.mp3\"\n    save_result_path = \"output.mp4\"\n    video_duration = 20  # seconds\n\n# Generate\ngen_video, audio, sample_rate = pipeline.generate(Args())\n```\n\n### Stream Pipeline Usage\n\nCommand-line invocation [lightx2v/shot_runner/stream_infer.py:95-124]():\n\n```bash\npython -m lightx2v.shot_runner.stream_infer \\\n    --config_json configs/seko_talk/shot/stream/main.json \\\n    --seed 42 \\\n    --prompt \"A person gesturing while speaking\" \\\n    --image_path speaker.png \\\n    --audio_path speech.mp3 \\\n    --save_result_path output.mp4\n```\n\n### Configuration Template\n\nMain JSON structure for multi-clip pipelines:\n\n```json\n{\n  \"lightx2v_path\": \"/path/to/models\",\n  \"parallel\": {\n    \"cfg_p_size\": 1,\n    \"seq_p_size\": 2\n  },\n  \"clip_configs\": [\n    {\n      \"name\": \"rs2v_clip\",\n      \"path\": \"configs/seko_talk/shot/rs2v/rs2v.json\"\n    }\n  ]\n}\n```\n\nIndividual clip configuration structure:\n\n```json\n{\n  \"model_cls\": \"seko_talk\",\n  \"task\": \"rs2v\",\n  \"model_path\": \"/path/to/model\",\n  \"target_video_length\": 81,\n  \"target_fps\": 16,\n  \"audio_sr\": 16000,\n  \"default_input_info\": {\n    \"infer_steps\": 4,\n    \"resize_mode\": \"adaptive\",\n    \"prompt\": \"Default prompt\",\n    \"image_path\": \"default.png\",\n    \"audio_path\": \"default.mp3\"\n  }\n}\n```\n\n### Progress Callback Integration\n\nSet a callback to monitor generation progress:\n\n```python\ndef progress_callback(current_clip, total_clips):\n    print(f\"Processing clip {current_clip}/{total_clips}\")\n\npipeline.set_progress_callback(progress_callback)\nresult = pipeline.run_pipeline(input_info)\n```\n\nThe callback is invoked in the generation loop [lightx2v/shot_runner/rs2v_infer.py:82-83]() and [lightx2v/shot_runner/shot_base.py:117-118]().\n\n**Sources:** [lightx2v/shot_runner/rs2v_infer.py:123-152](), [lightx2v/shot_runner/stream_infer.py:95-124](), [lightx2v/shot_runner/shot_base.py:112-134]()\n\n## Key Design Patterns\n\n### 1. Clip Orchestration Pattern\n\nShotPipeline uses a registry-based approach to instantiate runners for each clip:\n\n```mermaid\ngraph LR\n    Config[\"ClipConfig\"]\n    Registry[\"RUNNER_REGISTER\"]\n    GetRunner[\"RUNNER_REGISTER\u003cbr/\u003e[model_cls]\"]\n    InitModules[\"runner.init_modules()\"]\n    Ready[\"Ready for Inference\"]\n    \n    Config --\u003e GetRunner\n    Registry --\u003e GetRunner\n    GetRunner --\u003e InitModules\n    InitModules --\u003e Ready\n```\n\nThis pattern enables dynamic composition of different model types within a single pipeline [lightx2v/shot_runner/shot_base.py:106-110]().\n\n### 2. Default Configuration Inheritance\n\nThe `check_input_info()` method implements a two-level default system:\n\n1. **System Defaults** - Hard-coded in InputInfo dataclass definitions\n2. **Clip Defaults** - Specified in `default_input_info` section of clip config\n3. **User Overrides** - Provided at runtime\n\nPriority: User Overrides \u003e Clip Defaults \u003e System Defaults\n\n### 3. State Management Between Clips\n\nThree state passing mechanisms:\n\n| Mechanism | Used By | Format | Location in Code |\n|-----------|---------|--------|------------------|\n| `overlap_frame` | ShotStreamPipeline | Pixel tensor (B, C, F, H, W) | [lightx2v/shot_runner/stream_infer.py:74]() |\n| `overlap_latent` | ShotRS2VPipeline | Latent tensor (B, F, H, W) | [lightx2v/shot_runner/rs2v_infer.py:91]() |\n| `ref_state` | ShotRS2VPipeline | Integer flag (0/1) | [lightx2v/shot_runner/rs2v_infer.py:78]() |\n\n### 4. Audio-Video Synchronization\n\nAlignment calculation ensures frame-accurate synchronization [lightx2v/shot_runner/stream_infer.py:76-78]():\n\n```python\naligned_len = gen_clip_video.shape[2] - overlap\ngen_video_list.append(gen_clip_video[:, :, :aligned_len])\ncut_audio_list.append(audio_clip[:aligned_len * audio_per_frame])\n```\n\nThis handles the temporal overlap between clips by trimming redundant frames and corresponding audio samples.\n\n**Sources:** [lightx2v/shot_runner/shot_base.py:74-78](), [lightx2v/shot_runner/shot_base.py:106-110](), [lightx2v/shot_runner/stream_infer.py:74-81](), [lightx2v/shot_runner/rs2v_infer.py:78-91]()"])</script><script>self.__next_f.push([1,"38:T5613,"])</script><script>self.__next_f.push([1,"# Performance Optimization\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [README_zh.md](README_zh.md)\n- [lightx2v_platform/README.md](lightx2v_platform/README.md)\n- [lightx2v_platform/README_zh.md](lightx2v_platform/README_zh.md)\n- [scripts/hunyuan_video_15/README.md](scripts/hunyuan_video_15/README.md)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides an overview of the performance optimization techniques implemented in LightX2V that enable efficient video/image generation inference across diverse hardware platforms. LightX2V employs a composable multi-layer optimization architecture where techniques can be combined multiplicatively to achieve 100x+ performance range - from high-end server GPUs (8x H100) to consumer hardware (single RTX 4090 with 8GB VRAM).\n\nThe optimization system consists of seven independent layers: **Model Precision**, **Attention Operations**, **Memory Management**, **Computation Parallelism**, **Feature Caching**, **Kernel Implementation**, and **Hardware Abstraction**. Each layer provides specific memory/speed trade-offs that compose orthogonally.\n\nFor detailed information on specific optimization techniques:\n- Quantization formats and backends: See [Quantization System](#6.1)\n- Attention operator selection: See [Attention Operators and Sparse Patterns](#6.2)\n- CPU offloading strategies: See [Memory Management and CPU Offloading](#6.3)\n- Caching mechanisms: See [Feature Caching and Streaming](#6.4)\n- Distributed inference: See [Distributed and Parallel Inference](#6.5)\n- LoRA management: See [LoRA Dynamic Application](#6.6)\n\n## Seven-Layer Optimization Architecture\n\nThe optimization system is structured as seven independent but composable layers, each addressing different performance bottlenecks.\n\n### Optimization Stack Diagram\n\n```mermaid\ngraph TB\n    subgraph \"Layer 1: Model Precision\"\n        FP32[\"FP32 Baseline\u003cbr/\u003e~4GB/1B params\"]\n        FP16[\"FP16/BF16\u003cbr/\u003e2x reduction\"]\n        INT8[\"INT8\u003cbr/\u003e4x reduction\"]\n        FP8[\"FP8\u003cbr/\u003e2-4x reduction\"]\n        NVFP4[\"NVFP4\u003cbr/\u003e8x reduction\"]\n        MxFP[\"MxFP4/6/8\u003cbr/\u003eBlock scaling\"]\n    end\n    \n    subgraph \"Layer 2: Attention Operations\"\n        VanillaAttn[\"Vanilla Attention\u003cbr/\u003eO(n) memory\"]\n        FlashAttn2[\"Flash Attention 2\u003cbr/\u003eIO-aware\"]\n        FlashAttn3[\"Flash Attention 3\u003cbr/\u003eHopper optimized\"]\n        SageAttn[\"Sage Attention 2\u003cbr/\u003eINT8 KV cache\u003cbr/\u003e2x throughput\"]\n        NbhdAttn[\"Neighborhood Attn\u003cbr/\u003eLocal + sparse\"]\n    end\n    \n    subgraph \"Layer 3: Memory Management\"\n        NoOffload[\"No Offload\u003cbr/\u003eFull GPU\"]\n        ModelOffload[\"Model Offload\u003cbr/\u003eLoad per infer\"]\n        BlockOffload[\"Block Offload\u003cbr/\u003e2 blocks in GPU\"]\n        PhaseOffload[\"Phase Offload\u003cbr/\u003eSingle phase in GPU\"]\n        LazyLoad[\"Lazy Load\u003cbr/\u003eDiskCPUGPU\"]\n    end\n    \n    subgraph \"Layer 4: Computation Parallelism\"\n        CFGParallel[\"CFG Parallelism\u003cbr/\u003e2x throughput\"]\n        SeqParallel[\"Sequence Parallelism\u003cbr/\u003eUlysses/Ring\"]\n        TensorParallel[\"Tensor Parallelism\"]\n    end\n    \n    subgraph \"Layer 5: Feature Caching\"\n        TeaCache[\"TeaCache\u003cbr/\u003eSkip redundant blocks\"]\n        MagCache[\"MagCache\u003cbr/\u003eFeature reuse\"]\n        KVCache[\"KV Cache\u003cbr/\u003eAutoregressive\"]\n    end\n    \n    subgraph \"Layer 6: Kernel Implementation\"\n        Triton[\"Triton Kernels\"]\n        VLLM[\"VLLM Kernels\"]\n        SGL[\"SGL Kernels\"]\n        TorchAO[\"TorchAO\"]\n        Q8F[\"Q8F Kernels\"]\n    end\n    \n    subgraph \"Layer 7: Hardware Abstraction\"\n        Platform[\"lightx2v_platform\u003cbr/\u003eHardware agnostic\"]\n        CUDA[\"CUDA Backend\"]\n        MLU[\"Cambricon MLU\"]\n        NPU[\"Ascend NPU\"]\n    end\n    \n    FP32 -.-\u003e|optimize| FP16\n    FP16 -.-\u003e|optimize| INT8\n    FP16 -.-\u003e|optimize| FP8\n    FP8 -.-\u003e|optimize| NVFP4\n    \n    VanillaAttn -.-\u003e|optimize| FlashAttn2\n    FlashAttn2 -.-\u003e|optimize| FlashAttn3\n    FlashAttn2 -.-\u003e|optimize| SageAttn\n    \n    NoOffload -.-\u003e|optimize| ModelOffload\n    ModelOffload -.-\u003e|optimize| BlockOffload\n    BlockOffload -.-\u003e|optimize| PhaseOffload\n    PhaseOffload -.-\u003e|optimize| LazyLoad\n    \n    INT8 --\u003e Triton\n    INT8 --\u003e VLLM\n    FP8 --\u003e SGL\n    FP8 --\u003e TorchAO\n    NVFP4 --\u003e Q8F\n    \n    Triton --\u003e Platform\n    VLLM --\u003e Platform\n    Platform --\u003e CUDA\n    Platform --\u003e MLU\n    Platform --\u003e NPU\n```\n\n**Sources:** [lightx2v/models/networks/wan/model.py:1-200](), [lightx2v/common/ops/mm/mm_weight.py:99-450](), [lightx2v/models/networks/wan/infer/transformer_infer.py:17-74](), [lightx2v/models/runners/default_runner.py:55-242]()\n\n## Optimization Layer Details\n\n### Layer 1: Model Precision\n\nQuantization reduces memory footprint by using lower-precision formats for weights and activations:\n\n| Format | Memory Reduction | Quality Impact | Backend Support |\n|--------|-----------------|----------------|-----------------|\n| FP32 | Baseline (4GB/1B) | Reference | PyTorch native |\n| FP16/BF16 | 2x | Negligible | PyTorch native |\n| INT8 | 4x | Minimal (\u003c2% quality loss) | Triton, VLLM, TorchAO, Q8F |\n| FP8 | 2-4x | Minimal (\u003c1% quality loss) | VLLM, SGL, TorchAO, Triton |\n| NVFP4 | 8x | Moderate (requires QAT) | Q8F (Ada GPUs) |\n| MxFP4/6/8 | 4-8x | Configurable | Cutlass kernels |\n\n**Configuration keys:**\n- `dit_quantized`: Enable DiT model quantization\n- `dit_quant_scheme`: Select format (e.g., `\"fp8-sgl\"`, `\"int8-vllm\"`)\n- `t5_quantized`, `clip_quantized`, `adapter_quantized`: Component-specific quantization\n- `weight_auto_quant`: Runtime quantization (slower initialization)\n\n### Layer 2: Attention Operations\n\nAttention operators optimize the most computationally expensive operation in transformers:\n\n| Operator | Memory | Speed | Special Features |\n|----------|--------|-------|------------------|\n| Vanilla | O(n) | Baseline | Reference implementation |\n| Flash Attention 2 | O(n) | ~2x faster | IO-aware, fused kernels |\n| Flash Attention 3 | O(n) | ~3x faster (H100) | Hopper architecture optimized |\n| Sage Attention 2 | O(n) | ~4x faster | INT8 KV cache quantization |\n| Neighborhood | O(nk) | ~5x faster | Local + sparse patterns |\n\n**Configuration keys:**\n- `self_attn_1_type`: Self-attention operator (`\"flash_attn2\"`, `\"flash_attn3\"`, `\"sage_attn\"`, `\"nbhd_attn\"`)\n- `cross_attn_1_type`, `cross_attn_2_type`: Cross-attention operators\n- `nbhd_attn_setting`: Neighborhood attention coefficients\n- `window_size`: Local attention window dimensions\n\n### Layer 3: Memory Management\n\nFive-level offloading hierarchy enables execution on low-VRAM systems:\n\n| Strategy | GPU Memory | Speed | Use Case |\n|----------|-----------|-------|----------|\n| No Offload | 100% model | Baseline | High-end GPUs (80GB+) |\n| Model Offload | Only during infer | 2x slower | 24GB GPUs |\n| Block Offload | 2 blocks resident | ~1.3x slower | 16GB GPUs |\n| Phase Offload | Single phase | ~1.5x slower | 12GB GPUs |\n| Lazy Load | Minimal (active compute) | ~2-3x slower | 8GB GPUs + system RAM |\n\n**Configuration keys:**\n- `cpu_offload`: Enable offloading (bool)\n- `offload_granularity`: Level (`\"model\"`, `\"block\"`, `\"phase\"`)\n- `lazy_load`: Enable lazy weight streaming from disk\n- `unload_modules`: Unload encoders after use\n\n### Layer 4: Computation Parallelism\n\nDistributed inference strategies for multi-GPU systems:\n\n| Strategy | Throughput Gain | GPU Requirement | Communication Overhead |\n|----------|----------------|-----------------|----------------------|\n| CFG Parallelism | 2x | 2 GPUs minimum | Low (final gather only) |\n| Sequence Parallelism (Ulysses) | Linear with GPUs | Any | Medium (attention all-gather) |\n| Sequence Parallelism (Ring) | Linear with GPUs | Any | Low (ring communication) |\n| Tensor Parallelism | Sub-linear | Any | High (per-layer communication) |\n\n**Configuration structure:**\n```json\n{\n  \"parallel\": {\n    \"cfg_p_size\": 2,           // CFG parallelism degree\n    \"seq_p_size\": 4,           // Sequence parallelism degree\n    \"seq_p_attn_type\": \"ulysses\",  // or \"ring\"\n    \"seq_p_fp8_comm\": true,    // FP8 communication\n    \"seq_p_head_parallel\": true\n  },\n  \"enable_cfg\": true,\n  \"cfg_parallel\": true\n}\n```\n\n### Layer 5: Feature Caching\n\nCaching mechanisms skip redundant computation:\n\n| Mechanism | Speedup | Quality Impact | Applicable Models |\n|-----------|---------|----------------|-------------------|\n| TeaCache | ~1.3x | Negligible | All diffusion models |\n| MagCache | ~1.25x | Minimal | All diffusion models |\n| KV Cache | Critical for AR | None | Autoregressive models only |\n| VAE Cache (CACHE_T=2) | ~1.5x decode | None | Video models |\n\n**Configuration keys:**\n- `feature_caching`: Type (`\"NoCaching\"`, `\"Tea\"`, `\"Mag\"`, `\"Ada\"`)\n- `teacache_thresh`: TeaCache similarity threshold (default: 0.26)\n- `use_stream_vae`: Enable VAE streaming with cache\n\n### Layer 6: Kernel Implementation\n\nBackend-specific optimized kernels for quantized operations:\n\n| Backend | Formats | GPU Support | Performance Notes |\n|---------|---------|-------------|-------------------|\n| Triton | INT8, FP8 | CUDA, ROCm | Custom kernels, good portability |\n| VLLM | INT8, FP8 | CUDA, ROCm | High throughput, mature |\n| SGL | FP8 | CUDA (compute 7.5) | Fastest FP8, torch 2.8+ required |\n| TorchAO | INT8, FP8 | CUDA, ROCm | Official PyTorch, good compatibility |\n| Q8F | INT8, FP8, NVFP4 | Ada (RTX 40xx, L40S) | Ada-optimized, best for NVFP4 |\n\n**Configuration scheme format:** `\"{format}-{backend}\"` (e.g., `\"fp8-sgl\"`, `\"int8-vllm\"`)\n\n### Layer 7: Hardware Abstraction\n\nThe `lightx2v_platform` module abstracts hardware-specific operations, enabling portable optimization implementations across 8+ backends (NVIDIA CUDA, Cambricon MLU, Ascend NPU, AMD ROCm, MetaX, Hygon, MThreads, Enflame).\n\n**Sources:** [lightx2v/models/networks/wan/model.py:56-80](), [lightx2v/common/ops/mm/mm_weight.py:330-450](), [lightx2v/models/networks/wan/infer/transformer_infer.py:17-74](), [lightx2v/models/runners/wan/wan_runner.py:159-221](), [lightx2v/models/runners/default_runner.py:99-242]()\n\n## Optimization Composability\n\nOptimizations combine multiplicatively. Example configuration achieving ~32x memory reduction and ~16x speedup on 8x H100:\n\n```json\n{\n  \"dit_quantized\": true,\n  \"dit_quant_scheme\": \"fp8-sgl\",              // Layer 1: 2x memory\n  \"self_attn_1_type\": \"sage_attn\",            // Layer 2: 2x speed\n  \"cpu_offload\": true,                         \n  \"offload_granularity\": \"block\",             // Layer 3: 4x memory\n  \"parallel\": {\n    \"cfg_p_size\": 2,                          // Layer 4: 2x throughput\n    \"seq_p_size\": 4                           // Layer 4: 4x batch capacity\n  },\n  \"feature_caching\": \"Tea\",                    // Layer 5: 1.3x speed\n  \"enable_cfg\": true,\n  \"cfg_parallel\": true\n}\n```\n\n**Combined effect:** 2x  4x = 8x memory reduction, 2x  2x  1.3x = 5.2x speedup, 4x parallelism scale\n\n### Configuration Flow Diagram\n\n```mermaid\ngraph LR\n    ConfigJSON[\"config.json\"] --\u003e AutoCalc[\"auto_calc_config()\"]\n    AutoCalc --\u003e SetConfig[\"set_config()\"]\n    SetConfig --\u003e RunnerInit[\"Runner.__init__()\"]\n    \n    RunnerInit --\u003e LoadModel[\"load_model()\"]\n    LoadModel --\u003e LoadTransformer[\"load_transformer()\"]\n    LoadModel --\u003e LoadEncoders[\"load_text_encoder()\u003cbr/\u003eload_image_encoder()\"]\n    LoadModel --\u003e LoadVAE[\"load_vae()\"]\n    \n    LoadTransformer --\u003e QuantCheck{quantized?}\n    QuantCheck --\u003e|yes| QuantConfig[\"dit_quant_scheme\u003cbr/\u003e MMWeight backend\"]\n    QuantCheck --\u003e|no| DefaultWeight[\"MMWeight Default\"]\n    \n    LoadEncoders --\u003e OffloadCheck{cpu_offload?}\n    OffloadCheck --\u003e|yes| CPUDevice[\"device=cpu\u003cbr/\u003elazy_load=true\"]\n    OffloadCheck --\u003e|no| GPUDevice[\"device=cuda\"]\n    \n    LoadVAE --\u003e ParallelCheck{parallel?}\n    ParallelCheck --\u003e|yes| InitDeviceMesh[\"init_device_mesh()\u003cbr/\u003eset_parallel_config()\"]\n    ParallelCheck --\u003e|no| SingleGPU[\"Single GPU mode\"]\n    \n    RunnerInit --\u003e InitScheduler[\"init_scheduler()\"]\n    InitScheduler --\u003e FeatureCacheCheck{feature_caching?}\n    FeatureCacheCheck --\u003e|Tea/Mag| CachingScheduler[\"WanSchedulerCaching\"]\n    FeatureCacheCheck --\u003e|NoCaching| BaseScheduler[\"WanScheduler\"]\n    \n    InitScheduler --\u003e InitModules[\"init_modules()\"]\n    InitModules --\u003e CompileCheck{compile?}\n    CompileCheck --\u003e|yes| CompileModel[\"model.compile()\u003cbr/\u003etorch.compile()\"]\n    CompileCheck --\u003e|no| Ready[\"Ready for inference\"]\n```\n\n**Sources:** [lightx2v/utils/set_config.py:1-174](), [lightx2v/models/runners/default_runner.py:55-131](), [lightx2v/models/runners/wan/wan_runner.py:63-236](), [lightx2v/infer.py:35-187]()\n\n## Performance Benchmarks\n\n### Inference Speed Comparison\n\nBased on WAN2.1 video generation (480p, 81 frames, 4-step distilled model):\n\n| Configuration | Hardware | Speed (s/iter) | Memory (GB) | Notes |\n|--------------|----------|----------------|-------------|-------|\n| FP16 baseline | 8x H100 80GB | 0.35 | 320 | Reference speed |\n| FP8 + SageAttn | 8x H100 80GB | 0.18 | 160 | ~2x faster |\n| FP8 + CFG parallel | 8x H100 80GB | 0.09 | 160 | ~4x faster |\n| FP8 + block offload | 1x RTX 4090 24GB | 2.35 | 18 | Consumer GPU |\n| INT8 + lazy load | 1x RTX 4090 8GB | ~10.0 | 8 | Minimal VRAM |\n\n### Memory Usage Breakdown\n\nFor WAN2.1 text-to-video (480p, 81 frames):\n\n| Component | FP16 (GB) | FP8 (GB) | INT8 (GB) | NVFP4 (GB) |\n|-----------|-----------|----------|-----------|------------|\n| DiT Model (14B params) | 28.0 | 14.0 | 7.0 | 3.5 |\n| T5 Encoder | 9.6 | 4.8 | 2.4 | 1.2 |\n| CLIP Encoder | 3.2 | 1.6 | 0.8 | 0.4 |\n| VAE | 1.6 | 0.8 | 0.8 | 0.8 |\n| Working memory | 12.0 | 8.0 | 6.0 | 5.0 |\n| **Total** | **54.4** | **29.2** | **17.0** | **10.9** |\n\n**With block offloading (2 blocks resident):** Total GPU memory reduces to ~12GB (FP8) or ~6GB (INT8)\n\n**Sources:** Configuration files and documentation benchmarks\n\n## Configuration Examples\n\n### High-End Server (8x H100)\n\n```json\n{\n  \"dit_quantized\": true,\n  \"dit_quant_scheme\": \"fp8-sgl\",\n  \"self_attn_1_type\": \"flash_attn3\",\n  \"parallel\": {\n    \"cfg_p_size\": 2,\n    \"seq_p_size\": 4,\n    \"seq_p_attn_type\": \"ulysses\",\n    \"seq_p_fp8_comm\": true\n  },\n  \"feature_caching\": \"Tea\",\n  \"compile\": true\n}\n```\n\n### Consumer GPU (RTX 4090 24GB)\n\n```json\n{\n  \"dit_quantized\": true,\n  \"dit_quant_scheme\": \"fp8-q8f\",\n  \"self_attn_1_type\": \"sage_attn\",\n  \"cpu_offload\": true,\n  \"offload_granularity\": \"block\",\n  \"t5_quantized\": true,\n  \"t5_quant_scheme\": \"int8-q8f\",\n  \"clip_quantized\": true,\n  \"clip_quant_scheme\": \"int8-q8f\"\n}\n```\n\n### Low-VRAM System (8GB GPU + 16GB RAM)\n\n```json\n{\n  \"dit_quantized\": true,\n  \"dit_quant_scheme\": \"int8-vllm\",\n  \"self_attn_1_type\": \"flash_attn2\",\n  \"cpu_offload\": true,\n  \"offload_granularity\": \"phase\",\n  \"lazy_load\": true,\n  \"t5_quantized\": true,\n  \"t5_quant_scheme\": \"int8-vllm\",\n  \"t5_lazy_load\": true,\n  \"clip_quantized\": true,\n  \"clip_quant_scheme\": \"int8-vllm\",\n  \"vae_cpu_offload\": true,\n  \"unload_modules\": true\n}\n```\n\n**Sources:** [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:1-51](), [docs/EN/source/getting_started/quickstart.md:1-250]()\n\n## Runner-Level Optimization Integration\n\nThe optimization techniques are integrated at the runner level through configuration-driven initialization:\n\n```mermaid\ngraph TB\n    RunPipeline[\"run_pipeline(input_info)\"] --\u003e RunEncoder[\"run_input_encoder()\"]\n    \n    RunEncoder --\u003e LoadCheck{lazy_load or\u003cbr/\u003eunload_modules?}\n    LoadCheck --\u003e|yes| LoadOnDemand[\"Load encoder on-demand\"]\n    LoadCheck --\u003e|no| UseLoaded[\"Use pre-loaded encoder\"]\n    \n    LoadOnDemand --\u003e EncodeData[\"Encode input\"]\n    UseLoaded --\u003e EncodeData\n    \n    EncodeData --\u003e UnloadCheck{unload_modules?}\n    UnloadCheck --\u003e|yes| UnloadEncoder[\"del encoder\u003cbr/\u003etorch.cuda.empty_cache()\"]\n    UnloadCheck --\u003e|no| KeepEncoder[\"Keep encoder in memory\"]\n    \n    UnloadEncoder --\u003e RunMain[\"run_main()\"]\n    KeepEncoder --\u003e RunMain\n    \n    RunMain --\u003e InitRun[\"init_run()\"]\n    InitRun --\u003e OffloadSetup{cpu_offload?}\n    OffloadSetup --\u003e|model| ToGPU[\"model.to_cuda()\"]\n    OffloadSetup --\u003e|block/phase| NonBlockToGPU[\"non_block_weights_to_cuda()\"]\n    OffloadSetup --\u003e|no| SkipOffload[\"Skip offload setup\"]\n    \n    ToGPU --\u003e RunSegment[\"run_segment()\"]\n    NonBlockToGPU --\u003e RunSegment\n    SkipOffload --\u003e RunSegment\n    \n    RunSegment --\u003e LoopSteps[\"for step in infer_steps\"]\n    \n    LoopSteps --\u003e StepPre[\"scheduler.step_pre()\"]\n    StepPre --\u003e OffloadBlock{block offload?}\n    OffloadBlock --\u003e|yes| LoadBlock[\"offload_manager.to_cuda(block_idx)\"]\n    OffloadBlock --\u003e|no| NoBlockLoad[\"Skip block load\"]\n    \n    LoadBlock --\u003e ModelInfer[\"model.infer(inputs)\"]\n    NoBlockLoad --\u003e ModelInfer\n    \n    ModelInfer --\u003e InferCond[\"_infer_cond_uncond()\"]\n    InferCond --\u003e PreInfer[\"pre_infer.infer()\"]\n    \n    PreInfer --\u003e TransformerInfer[\"transformer_infer.infer()\"]\n    TransformerInfer --\u003e BlockLoop[\"for block in blocks\"]\n    \n    BlockLoop --\u003e AttnCheck{attention_type?}\n    AttnCheck --\u003e|flash_attn2| FlashAttn2[\"flash_attn2.apply()\"]\n    AttnCheck --\u003e|flash_attn3| FlashAttn3[\"flash_attn3.apply()\"]\n    AttnCheck --\u003e|sage_attn| SageAttn[\"sage_attn.apply()\"]\n    AttnCheck --\u003e|nbhd_attn| NbhdAttn[\"nbhd_attn.apply()\"]\n    \n    FlashAttn2 --\u003e MMWeight[\"mm_weight.apply()\"]\n    FlashAttn3 --\u003e MMWeight\n    SageAttn --\u003e MMWeight\n    NbhdAttn --\u003e MMWeight\n    \n    MMWeight --\u003e QuantCheck{quantized?}\n    QuantCheck --\u003e|fp8| FP8Kernel[\"fp8_gemm_triton/vllm/sgl\"]\n    QuantCheck --\u003e|int8| INT8Kernel[\"int8_gemm_triton/vllm\"]\n    QuantCheck --\u003e|no| TorchMM[\"torch.mm()\"]\n    \n    FP8Kernel --\u003e NextBlock[\"Continue block loop\"]\n    INT8Kernel --\u003e NextBlock\n    TorchMM --\u003e NextBlock\n    \n    NextBlock --\u003e BlockLoopDone{more blocks?}\n    BlockLoopDone --\u003e|yes| BlockLoop\n    BlockLoopDone --\u003e|no| PostInfer[\"post_infer.infer()\"]\n    \n    PostInfer --\u003e StepPost[\"scheduler.step_post()\"]\n    StepPost --\u003e LoopDone{more steps?}\n    LoopDone --\u003e|yes| LoopSteps\n    LoopDone --\u003e|no| VAEDecode[\"run_vae_decoder()\"]\n    \n    VAEDecode --\u003e OffloadCleanup{cpu_offload?}\n    OffloadCleanup --\u003e|model| ToCPU[\"model.to_cpu()\"]\n    OffloadCleanup --\u003e|block/phase| NonBlockToCPU[\"non_block_weights_to_cpu()\"]\n    OffloadCleanup --\u003e|no| SkipCleanup[\"Skip cleanup\"]\n    \n    ToCPU --\u003e Return[\"Return generated video\"]\n    NonBlockToCPU --\u003e Return\n    SkipCleanup --\u003e Return\n```\n\n**Sources:** [lightx2v/models/runners/default_runner.py:173-476](), [lightx2v/models/networks/wan/model.py:102-200](), [lightx2v/models/networks/wan/infer/transformer_infer.py:77-362]()\n\n## Compile Mode (torch.compile)\n\nLightX2V supports `torch.compile()` for graph-level optimization, particularly beneficial for fixed-resolution inference:\n\n**Configuration:**\n```json\n{\n  \"compile\": true,\n  \"compile_shapes\": [\n    [480, 832],   // 480p landscape\n    [544, 960],   // 540p landscape  \n    [720, 1280],  // 720p landscape\n    [480, 480]    // 480p square\n  ],\n  \"compile_max_audios\": 2  // For audio models\n}\n```\n\n**Compilation process:**\n1. `model.compile(compile_shapes)` pre-compiles graphs for each resolution\n2. `model.select_graph_for_compile(input_info)` selects appropriate graph at runtime\n3. Compiled graphs skip Python overhead and enable kernel fusion\n\n**Performance impact:** ~1.2-1.5x speedup after warm-up, but adds initialization time (~2-5 min per shape)\n\n**Sources:** [lightx2v/models/networks/wan/audio_model.py:61-136](), [lightx2v/models/runners/default_runner.py:95-98](), [lightx2v/utils/custom_compiler.py]()\n\n## Profiling and Monitoring\n\nLightX2V includes built-in profiling for optimization analysis:\n\n**Configuration:**\n```python\n# Environment variable\nos.environ[\"PROFILING_DEBUG_LEVEL\"] = \"1\"  # or \"2\" for detailed\n\n# In code\nfrom lightx2v.utils.profiler import ProfilingContext4DebugL1\n\nwith ProfilingContext4DebugL1(\"Custom operation\"):\n    # Code to profile\n    pass\n```\n\n**Metrics integration:** When `GET_RECORDER_MODE()` returns True, the system exports Prometheus metrics via `lightx2v.server.metrics.monitor_cli`:\n- `lightx2v_run_per_step_dit_duration`: Per-step DiT inference time\n- `lightx2v_run_text_encode_duration`: Text encoder time\n- `lightx2v_run_vae_decode_duration`: VAE decoder time\n- `lightx2v_worker_request_duration`: End-to-end request time\n\n**Sources:** [lightx2v/utils/profiler.py](), [lightx2v/server/metrics.py](), [lightx2v/models/runners/default_runner.py:176-201]()\n\n## Hardware-Specific Optimizations\n\n### NVIDIA GPUs\n\n- **Ampere (A100):** Use `flash_attn2`, FP8 quantization with `vllm` backend\n- **Hopper (H100):** Use `flash_attn3`, FP8 quantization with `sgl` backend, enable `seq_p_fp8_comm`\n- **Ada (RTX 40xx, L40S):** Use `sage_attn`, NVFP4/FP8 quantization with `q8f` backend\n- **Consumer (RTX 30xx):** Use `flash_attn2`, INT8 quantization with `vllm`, enable `cpu_offload`\n\n### Non-NVIDIA Hardware\n\nLightX2V's `lightx2v_platform` abstraction supports:\n- **Cambricon MLU:** Custom attention operators registered via `ATTN_WEIGHT_REGISTER`\n- **Ascend NPU:** Custom RoPE implementation via `ROPE_REGISTER`\n- **AMD ROCm:** Triton kernels compatible with ROCm backend\n\nHardware-specific implementations override platform-agnostic defaults through the registry system.\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:36-58](), [lightx2v_platform/]()\n\n## Trade-off Decision Matrix\n\n| Priority | Hardware | Recommended Configuration | Expected Performance |\n|----------|----------|--------------------------|---------------------|\n| **Max Quality** | 8x H100 | FP16, no offload, flash_attn3, CFG parallel | 0.35s/iter, 320GB VRAM |\n| **Balanced** | 4x A100 | FP8-vllm, no offload, flash_attn2, seq parallel | 0.8s/iter, 160GB VRAM |\n| **Memory Limited** | 1x RTX 4090 | FP8-q8f, block offload, sage_attn | 2.3s/iter, 18GB VRAM |\n| **Consumer** | 1x RTX 3090 | INT8-vllm, phase offload, flash_attn2 | 5s/iter, 12GB VRAM |\n| **Minimal VRAM** | RTX 3060 | INT8-vllm, lazy_load, flash_attn2, unload_modules | 10-15s/iter, 8GB VRAM + RAM |\n\n**Quality degradation:** FP8  0% vs FP16, INT8  1-2% vs FP16, NVFP4  3-5% vs FP16 (requires QAT models)\n\n**Sources:** Documentation benchmarks, [docs/EN/source/getting_started/quickstart.md:14-32]()"])</script><script>self.__next_f.push([1,"39:T746c,"])</script><script>self.__next_f.push([1,"# Quantization System\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json](configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json)\n- [docs/EN/source/getting_started/quickstart.md](docs/EN/source/getting_started/quickstart.md)\n- [docs/ZH_CN/source/getting_started/quickstart.md](docs/ZH_CN/source/getting_started/quickstart.md)\n- [lightx2v/__init__.py](lightx2v/__init__.py)\n- [lightx2v/common/ops/attn/utils/ring_comm.py](lightx2v/common/ops/attn/utils/ring_comm.py)\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/common/ops/mm/triton_kernels.py](lightx2v/common/ops/mm/triton_kernels.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/utils/global_paras.py](lightx2v/utils/global_paras.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n- [pyproject.toml](pyproject.toml)\n- [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh](scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh)\n- [tools/convert/converter.py](tools/convert/converter.py)\n- [tools/convert/quant/__init__.py](tools/convert/quant/__init__.py)\n- [tools/convert/quant/quant.py](tools/convert/quant/quant.py)\n- [tools/convert/quant_adapter.py](tools/convert/quant_adapter.py)\n- [tools/convert/readme.md](tools/convert/readme.md)\n- [tools/convert/readme_zh.md](tools/convert/readme_zh.md)\n\n\u003c/details\u003e\n\n\n\nThe quantization system in LightX2V provides comprehensive model compression capabilities through weight and activation quantization. This system enables running large video generation models on consumer hardware by reducing memory footprint and accelerating inference through low-precision compute kernels. The quantization framework supports multiple precision formats (INT8, FP8, NVFP4, MXFP4/6/8) and integrates with various backend kernels (VLLM, SGL, TorchAO, Triton, Q8).\n\nFor information about model conversion and quantization workflows, see the conversion tools in [tools/convert/](#8.1). For runtime optimization strategies, see [Memory Management and CPU Offloading](#6.3).\n\n## Supported Quantization Schemes\n\nLightX2V supports a wide range of quantization formats organized into three main categories: integer quantization, floating-point quantization, and microscaling formats.\n\n### Integer Quantization\n\n**INT8** provides 8-bit integer quantization with symmetric per-channel weight quantization. The system supports multiple backend implementations:\n- `int8-vllm`: VLLM kernel implementation (recommended for NVIDIA GPUs)\n- `int8-sgl`: SGL kernel implementation (requires torch 2.8.0)\n- `int8-torchao`: TorchAO implementation\n- `int8-triton`: Custom Triton kernel implementation\n- `int8-q8f`: Q8 kernel (optimized for Ada architecture)\n- `int8-tmo`: Moffett kernel\n- `int8-npu`: NPU-specific implementation\n\n### Floating-Point Quantization\n\n**FP8** uses 8-bit floating-point format (E4M3) with per-channel symmetric quantization. Supported backends include:\n- `fp8-vllm`: VLLM kernel implementation\n- `fp8-sgl`: SGL kernel implementation  \n- `fp8-torchao`: TorchAO implementation\n- `fp8-triton`: Custom Triton kernel\n- `fp8-q8f`: Q8 kernel for Ada architecture\n\n### Microscaling Formats\n\n**NVFP4** provides 4-bit NVIDIA floating-point format with block-wise scaling for extreme compression.\n\n**MXFP Formats** implement microscaling floating-point quantization:\n- `mxfp4`: 4-bit microscaling floating-point\n- `mxfp6`: 6-bit microscaling floating-point  \n- `mxfp8`: 8-bit microscaling floating-point\n- `mxfp6-mxfp8`: Hybrid format using MXFP6 for weights and MXFP8 for activations\n\nSources: [lightx2v/common/ops/mm/mm_weight.py:26-73](), [tools/convert/readme.md](), [tools/convert/readme_zh.md]()\n\n## Weight Quantization Architecture\n\n```mermaid\nclassDiagram\n    class MMWeightTemplate {\n        +weight_name: str\n        +bias_name: str\n        +lazy_load: bool\n        +lora_prefix: str\n        +has_lora_branch: bool\n        +load(weight_dict)\n        +apply(input_tensor)\n        +apply_lora(input_tensor)\n        +register_lora(weight_dict)\n        +update_lora(weight_dict)\n        +remove_lora()\n    }\n    \n    class MMWeight {\n        +weight: Tensor\n        +bias: Tensor\n        +pin_weight: Tensor\n        +apply(input_tensor)\n    }\n    \n    class MMWeightQuantTemplate {\n        +weight_scale_name: str\n        +load_func: callable\n        +act_quant_func: callable\n        +bias_force_fp32: bool\n        +scale_force_fp32: bool\n        +load_quantized(weight_dict)\n        +post_process()\n    }\n    \n    class MMWeightInt8PerChannelSym {\n        +weight: int8 Tensor\n        +weight_scale: float32 Tensor\n        +apply_int8_vllm()\n        +apply_int8_sgl()\n        +apply_int8_torchao()\n        +apply_int8_triton()\n    }\n    \n    class MMWeightFp8PerChannelSym {\n        +weight: float8_e4m3fn Tensor\n        +weight_scale: float32 Tensor\n        +apply_fp8_vllm()\n        +apply_fp8_sgl()\n        +apply_fp8_torchao()\n        +apply_fp8_triton()\n    }\n    \n    class MMWeightNvfp4 {\n        +weight: uint8 Tensor\n        +weight_scale: float32 Tensor\n        +apply_nvfp4()\n    }\n    \n    class MMWeightMxfp4 {\n        +weight: uint8 Tensor\n        +weight_scale: float32 Tensor\n        +apply_mxfp4()\n    }\n    \n    MMWeightTemplate \u003c|-- MMWeight\n    MMWeightTemplate \u003c|-- MMWeightQuantTemplate\n    MMWeightQuantTemplate \u003c|-- MMWeightInt8PerChannelSym\n    MMWeightQuantTemplate \u003c|-- MMWeightFp8PerChannelSym\n    MMWeightQuantTemplate \u003c|-- MMWeightNvfp4\n    MMWeightQuantTemplate \u003c|-- MMWeightMxfp4\n```\n\n**Diagram: Quantized Weight Class Hierarchy**\n\nThe weight quantization system is built on a hierarchical class structure. `MMWeightTemplate` provides the base interface for all weight modules, including LoRA support. `MMWeightQuantTemplate` extends this with quantization-specific functionality including weight scale management and post-processing hooks. Concrete quantization schemes inherit from this template and implement scheme-specific `apply()` methods that invoke the appropriate kernel backend.\n\nSources: [lightx2v/common/ops/mm/mm_weight.py:105-256](), [lightx2v/common/ops/mm/mm_weight.py:336-375]()\n\n## Quantization Weight Classes\n\n### MMWeightQuantTemplate Base Class\n\nThe base quantization template provides common infrastructure for all quantized weight implementations. Key attributes include:\n\n- `weight_scale_name`: Name pattern for weight scaling factors (e.g., `\"layer.weight_scale\"`)\n- `weight_need_transpose`: Whether weights need transposition during loading\n- `act_quant_func`: Function pointer for activation quantization\n- `bias_force_fp32`: Forces bias tensors to FP32 precision for numerical stability\n- `scale_force_fp32`: Forces scale tensors to FP32 precision\n\nThe template implements a three-stage loading pipeline:\n\n1. **Load Quantized Weights**: Loads quantized weight tensors and scale factors from storage\n2. **Post-Process**: Applies transposition, dtype conversion, and buffer management\n3. **Register Buffers**: Creates CUDA/CPU buffers for offloading if configured\n\nSources: [lightx2v/common/ops/mm/mm_weight.py:336-431]()\n\n### INT8 Quantization Implementation\n\n```mermaid\ngraph TB\n    subgraph \"INT8 Weight Loading\"\n        LoadWeights[\"Load INT8 Weights\u003cbr/\u003etorch.int8\"]\n        LoadScales[\"Load Weight Scales\u003cbr/\u003etorch.float32\"]\n        LoadBias[\"Load Bias\u003cbr/\u003etorch.float32\"]\n    end\n    \n    subgraph \"Runtime Inference\"\n        Input[\"Input Activation\u003cbr/\u003ebfloat16/float16\"]\n        ActQuant[\"Activation Quantization\u003cbr/\u003eper_token_quant_int8\"]\n        IntGEMM[\"INT8 GEMM\u003cbr/\u003ecutlass_scaled_mm\"]\n        Dequant[\"Implicit Dequantization\u003cbr/\u003escale * output\"]\n        AddBias[\"Add Bias\"]\n        Output[\"Output\u003cbr/\u003ebfloat16/float16\"]\n    end\n    \n    LoadWeights --\u003e IntGEMM\n    LoadScales --\u003e Dequant\n    LoadBias --\u003e AddBias\n    Input --\u003e ActQuant\n    ActQuant --\u003e IntGEMM\n    IntGEMM --\u003e Dequant\n    Dequant --\u003e AddBias\n    AddBias --\u003e Output\n```\n\n**Diagram: INT8 Quantization Data Flow**\n\nThe `MMWeightInt8PerChannelSym` class implements symmetric per-channel INT8 quantization. During initialization, weights are loaded as `torch.int8` tensors with per-output-channel scaling factors stored as `torch.float32`. The quantization formula is:\n\n```\nweight_quantized = round(weight_fp / scale).clamp(-128, 127).to(int8)\n```\n\nAt inference time, activations are quantized per-token using the `act_quant_func`, then an optimized INT8 matrix multiplication kernel computes the result. The output is implicitly dequantized using the scale factors and returned in the original precision.\n\nBackend-specific implementations:\n- **VLLM** (`apply_int8_vllm`): Uses `vllm_ops.cutlass_scaled_mm` for fused INT8 GEMM\n- **SGL** (`apply_int8_sgl`): Uses `sgl_kernel.int8_scaled_mm` \n- **TorchAO** (`apply_int8_torchao`): Uses `torchao.quantization.utils.quant_int8_per_token_matmul`\n- **Triton** (`apply_int8_triton`): Uses custom Triton kernel `int8_gemm_bias_triton`\n\nSources: [lightx2v/common/ops/mm/mm_weight.py:451-619]()\n\n### FP8 Quantization Implementation\n\nThe `MMWeightFp8PerChannelSym` class implements E4M3 floating-point quantization with similar structure to INT8 but using `torch.float8_e4m3fn` dtype. FP8 provides better dynamic range than INT8 for the same bit width, making it suitable for models sensitive to quantization error.\n\nThe quantization process:\n```\nscale = max(abs(weight)) / fp8_max_value\nweight_fp8 = (weight_fp / scale).to(float8_e4m3fn)\n```\n\nFP8 quantization supports the same kernel backends as INT8, with functions like `apply_fp8_vllm`, `apply_fp8_sgl`, etc. The kernels automatically handle the floating-point multiply-accumulate semantics.\n\nSources: [lightx2v/common/ops/mm/mm_weight.py:621-786]()\n\n### NVFP4 and Microscaling Formats\n\n**NVFP4** (`MMWeightNvfp4`) implements 4-bit floating-point quantization with block-wise scaling:\n- Weights are packed into `uint8` tensors (2 weights per byte)\n- Each block of weights shares a common `float32` scale factor\n- Uses `cutlass_scaled_nvfp4_mm` kernel for inference\n\n**MXFP Formats** (`MMWeightMxfp4`, `MMWeightMxfp6`, `MMWeightMxfp8`) implement OCP microscaling standards:\n- Weights are quantized to 4/6/8-bit microscaling floating-point\n- Block sizes typically 32-128 elements\n- Supports hybrid precision (e.g., MXFP6 weights + MXFP8 activations)\n- Uses specialized CUTLASS kernels for optimal performance\n\nThese formats provide better compression than FP8 while maintaining reasonable accuracy through fine-grained scaling.\n\nSources: [lightx2v/common/ops/mm/mm_weight.py:788-946](), [lightx2v/common/ops/mm/mm_weight.py:948-1113]()\n\n## Activation Quantization\n\n```mermaid\ngraph LR\n    subgraph \"Activation Quantization Methods\"\n        PerToken[\"Per-Token Quantization\u003cbr/\u003esglang_int8_act_quant\"]\n        TorchAO[\"TorchAO Quantization\u003cbr/\u003etorchao_int8_quant\"]\n        Triton[\"Triton Quantization\u003cbr/\u003eint8_quantize_triton\"]\n    end\n    \n    Input[\"Input Activations\u003cbr/\u003eShape: [M, K]\u003cbr/\u003edtype: bfloat16\"]\n    \n    subgraph \"Scale Computation\"\n        AbsMax[\"Compute AbsMax\u003cbr/\u003eper row\"]\n        Scale[\"scale = absmax / 127\"]\n    end\n    \n    Quantize[\"Quantize: round(x / scale)\u003cbr/\u003eclamp to [-128, 127]\"]\n    Output[\"Quantized Output\u003cbr/\u003eShape: [M, K]\u003cbr/\u003edtype: int8\"]\n    Scales[\"Scales Output\u003cbr/\u003eShape: [M, 1]\u003cbr/\u003edtype: float32\"]\n    \n    Input --\u003e PerToken\n    Input --\u003e TorchAO\n    Input --\u003e Triton\n    \n    PerToken --\u003e AbsMax\n    TorchAO --\u003e AbsMax\n    Triton --\u003e AbsMax\n    \n    AbsMax --\u003e Scale\n    Scale --\u003e Quantize\n    Quantize --\u003e Output\n    Scale --\u003e Scales\n```\n\n**Diagram: Activation Quantization Pipeline**\n\nActivation quantization dynamically quantizes input activations during inference. The system implements per-token (per-row) symmetric quantization to minimize quantization error while maintaining computational efficiency.\n\n### Per-Token Quantization\n\nFor each token (row) in the input activation tensor, the system:\n1. Computes the maximum absolute value: `absmax = max(abs(row))`\n2. Calculates the scale: `scale = absmax / 127`\n3. Quantizes: `quantized = round(row / scale).clamp(-128, 127)`\n\nThis per-token approach adapts to the dynamic range of each token, providing better accuracy than per-tensor quantization.\n\n### Backend Implementations\n\nDifferent backends provide optimized implementations:\n\n- **SGL**: `sglang_int8_act_quant` - Optimized for SGL kernel backend\n- **TorchAO**: `torchao_int8_quant` - Uses TorchAO's `quantize_activation_per_token_absmax`\n- **Triton**: `int8_quantize_triton` - Custom Triton kernel for maximum performance\n- **FP8 Triton**: `fp8_quantize_triton` - FP8 variant using E4M3 format\n\nSources: [lightx2v/common/ops/mm/mm_weight.py:48-90](), [lightx2v/common/ops/mm/triton_kernels.py:1-300]()\n\n## Quantization Configuration\n\nQuantization is configured through multiple configuration parameters in model runners and the main config system.\n\n### DiT Model Quantization\n\n```mermaid\ngraph TB\n    ConfigFile[\"Configuration JSON\"]\n    \n    subgraph \"DiT Quantization Config\"\n        DitQuant[\"dit_quantized: bool\"]\n        DitScheme[\"dit_quant_scheme: str\u003cbr/\u003ee.g., 'int8-vllm'\"]\n        WeightAutoQuant[\"weight_auto_quant: bool\"]\n        DitQuantCkpt[\"dit_quantized_ckpt: path\"]\n    end\n    \n    subgraph \"Model Loading\"\n        CheckQuantized{dit_quantized?}\n        LoadFP[\"Load FP Weights\u003cbr/\u003eMMWeight\"]\n        LoadQuant[\"Load Quantized Weights\u003cbr/\u003eMMWeightInt8/Fp8/etc\"]\n        SetMMType[\"Set mm_type in config\"]\n    end\n    \n    ConfigFile --\u003e DitQuant\n    ConfigFile --\u003e DitScheme\n    ConfigFile --\u003e WeightAutoQuant\n    ConfigFile --\u003e DitQuantCkpt\n    \n    DitQuant --\u003e CheckQuantized\n    CheckQuantized --\u003e|False| LoadFP\n    CheckQuantized --\u003e|True| DitScheme\n    DitScheme --\u003e SetMMType\n    SetMMType --\u003e LoadQuant\n    WeightAutoQuant --\u003e LoadQuant\n```\n\n**Diagram: DiT Quantization Configuration Flow**\n\nKey configuration parameters:\n\n- **`dit_quantized`**: Boolean flag enabling quantization for the DiT transformer\n- **`dit_quant_scheme`**: String specifying the quantization format (e.g., `\"int8-vllm\"`, `\"fp8-sgl\"`, `\"mxfp4\"`)\n- **`weight_auto_quant`**: If true, quantizes weights on-the-fly from FP16/BF16 checkpoints\n- **`dit_quantized_ckpt`**: Path to pre-quantized checkpoint file\n\nExample configuration:\n```json\n{\n  \"dit_quantized\": true,\n  \"dit_quant_scheme\": \"int8-vllm\",\n  \"dit_quantized_ckpt\": \"models/wan2.1_int8.safetensors\"\n}\n```\n\nSources: [lightx2v/models/networks/wan/weights/transformer_weights.py:11-54]()\n\n### Text Encoder Quantization\n\nText encoders (T5, CLIP) use similar configuration patterns:\n\n**T5 Encoder**:\n- `t5_quantized`: Enable T5 quantization\n- `t5_quant_scheme`: Quantization format (e.g., `\"int8-vllm\"`)\n- `t5_quantized_ckpt`: Path to quantized T5 checkpoint\n- `t5_original_ckpt`: Path to original checkpoint (if using auto-quantization)\n\n**CLIP Encoder**:\n- `clip_quantized`: Enable CLIP quantization\n- `clip_quant_scheme`: Quantization format\n- `clip_quantized_ckpt`: Path to quantized CLIP checkpoint\n- `clip_original_ckpt`: Path to original checkpoint\n\nThe system automatically selects the appropriate checkpoint based on the quantization scheme. For example, with `\"int8-vllm\"` scheme, it looks for files with `-int8` suffix.\n\nSources: [lightx2v/models/runners/wan/wan_runner.py:81-157]()\n\n### Audio Adapter Quantization\n\nAudio adapters used in speech-to-video models support quantization:\n\n```python\nadapter_quantized = config.get(\"adapter_quantized\", False)\nadapter_quant_scheme = config.get(\"adapter_quant_scheme\", None)\n```\n\nThe system maps quantization schemes to checkpoint filenames:\n- `\"fp8\"` schemes  `\"audio_adapter_model_fp8.safetensors\"`\n- `\"int8\"` schemes  `\"audio_adapter_model_int8.safetensors\"`\n- `\"mxfp4\"`  `\"audio_adapter_model_mxfp4.safetensors\"`\n- `\"mxfp6\"`  `\"audio_adapter_model_mxfp6.safetensors\"`\n\nSources: [lightx2v/models/networks/wan/audio_model.py:27-54](), [lightx2v/models/runners/wan/wan_audio_runner.py:756-781]()\n\n## Weight Module Integration\n\n```mermaid\ngraph TB\n    subgraph \"Weight Module Creation\"\n        Config[\"Config with\u003cbr/\u003edit_quant_scheme\"]\n        MMRegister[\"MM_WEIGHT_REGISTER\"]\n        GetClass[\"Get Weight Class\u003cbr/\u003eby scheme name\"]\n        CreateInstance[\"Create MMWeight\u003cbr/\u003einstance\"]\n    end\n    \n    subgraph \"Weight Module Types\"\n        Default[\"MMWeight\u003cbr/\u003e'Default'\"]\n        Int8VLLM[\"MMWeightInt8PerChannelSym\u003cbr/\u003e'int8-vllm'\"]\n        Fp8SGL[\"MMWeightFp8PerChannelSym\u003cbr/\u003e'fp8-sgl'\"]\n        NVFP4[\"MMWeightNvfp4\u003cbr/\u003e'nvfp4'\"]\n        MXFP4[\"MMWeightMxfp4\u003cbr/\u003e'mxfp4'\"]\n    end\n    \n    subgraph \"Transformer Block Weight Structure\"\n        BlockWeights[\"WanTransformerAttentionBlock\"]\n        Phase0[\"compute_phases[0]\u003cbr/\u003eSelf-Attention\"]\n        Phase1[\"compute_phases[1]\u003cbr/\u003eCross-Attention\"]\n        Phase2[\"compute_phases[2]\u003cbr/\u003eFFN\"]\n        \n        Phase0Weights[\"self_attn_q\u003cbr/\u003eself_attn_k\u003cbr/\u003eself_attn_v\u003cbr/\u003eself_attn_o\"]\n        Phase1Weights[\"cross_attn_q\u003cbr/\u003ecross_attn_k\u003cbr/\u003ecross_attn_v\u003cbr/\u003ecross_attn_o\"]\n        Phase2Weights[\"ffn_0\u003cbr/\u003effn_2\"]\n    end\n    \n    Config --\u003e MMRegister\n    MMRegister --\u003e GetClass\n    GetClass --\u003e Default\n    GetClass --\u003e Int8VLLM\n    GetClass --\u003e Fp8SGL\n    GetClass --\u003e NVFP4\n    GetClass --\u003e MXFP4\n    \n    CreateInstance --\u003e BlockWeights\n    BlockWeights --\u003e Phase0\n    BlockWeights --\u003e Phase1\n    BlockWeights --\u003e Phase2\n    \n    Phase0 --\u003e Phase0Weights\n    Phase1 --\u003e Phase1Weights\n    Phase2 --\u003e Phase2Weights\n```\n\n**Diagram: Weight Module Registry and Integration**\n\nThe quantization system integrates into the model architecture through a registry pattern. The `MM_WEIGHT_REGISTER` maps quantization scheme names to concrete weight class implementations. During model initialization, the framework:\n\n1. Reads `dit_quant_scheme` from config\n2. Queries `MM_WEIGHT_REGISTER` for the appropriate class\n3. Instantiates weight modules with the selected class\n4. Organizes modules into transformer block structure\n\nEach transformer block contains three compute phases:\n- **Phase 0**: Self-attention with Q, K, V, O projections\n- **Phase 1**: Cross-attention with text and image conditioning\n- **Phase 2**: Feed-forward network\n\nAll linear layers in these phases use the same quantized weight class, ensuring consistent quantization throughout the model.\n\nSources: [lightx2v/utils/registry_factory.py](), [lightx2v/models/networks/wan/weights/transformer_weights.py:145-400]()\n\n## Triton Kernel Implementation\n\nLightX2V implements custom Triton kernels for quantized matrix multiplication to achieve optimal performance when specialized libraries are unavailable.\n\n### INT8 GEMM Kernel\n\n```mermaid\ngraph TB\n    subgraph \"int8_gemm_bias_kernel\"\n        LoadA[\"Load INT8 Input\u003cbr/\u003eTile: [BLOCK_M, BLOCK_K]\"]\n        LoadB[\"Load INT8 Weight\u003cbr/\u003eTile: [BLOCK_K, BLOCK_N]\"]\n        LoadScale[\"Load FP32 Scales\u003cbr/\u003ePer output channel\"]\n        LoadBias[\"Load FP32 Bias\"]\n        \n        Accumulate[\"INT32 Accumulation\u003cbr/\u003eacc += A @ B\"]\n        Dequant[\"Dequantize\u003cbr/\u003eout = acc * scale_in * scale_w\"]\n        AddBiasOut[\"Add Bias\u003cbr/\u003eout += bias\"]\n        Store[\"Store Output\u003cbr/\u003eBF16/FP16\"]\n    end\n    \n    subgraph \"Autotuning\"\n        Configs[\"Config Space\u003cbr/\u003eBLOCK_M: 16-128\u003cbr/\u003eBLOCK_N: 64-256\u003cbr/\u003eBLOCK_K: 32-128\"]\n        Autotune[\"@autotune decorator\u003cbr/\u003eSelects best config\"]\n    end\n    \n    LoadA --\u003e Accumulate\n    LoadB --\u003e Accumulate\n    Accumulate --\u003e LoadScale\n    LoadScale --\u003e Dequant\n    Dequant --\u003e LoadBias\n    LoadBias --\u003e AddBiasOut\n    AddBiasOut --\u003e Store\n    \n    Configs --\u003e Autotune\n    Autotune --\u003e LoadA\n```\n\n**Diagram: INT8 Triton Kernel Pipeline**\n\nThe INT8 GEMM kernel (`int8_gemm_bias_kernel`) performs quantized matrix multiplication with integrated dequantization and bias addition. Key features:\n\n**Block-wise Processing**: The kernel divides the computation into tiles of size `[BLOCK_M, BLOCK_N, BLOCK_K]` for efficient memory access and compute.\n\n**Accumulation in INT32**: Intermediate results accumulate in INT32 to prevent overflow during the sum of INT8 products.\n\n**Fused Dequantization**: The kernel multiplies the INT32 accumulator by input and weight scales in a single operation, avoiding separate dequantization kernels.\n\n**Autotuning**: The `@autotune` decorator evaluates multiple tile size configurations and selects the optimal one for the target hardware. Configuration space includes:\n```python\nconfigs = [\n    Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 64}),\n    Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32}),\n    Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}),\n    # ... more configurations\n]\n```\n\nSources: [lightx2v/common/ops/mm/triton_kernels.py:1-300]()\n\n### FP8 GEMM Kernel\n\nThe FP8 kernel (`fp8_gemm_bias_kernel`) follows a similar structure but operates on `float8_e4m3fn` tensors. Key differences:\n\n- **Native FP8 Accumulation**: Uses FP32 accumulators for better numerical stability\n- **Simpler Dequantization**: FP8 scales apply directly without integer conversion\n- **Better Dynamic Range**: E4M3 format provides ~[-240, 240] range vs INT8's [-128, 127]\n\nThe kernel interface:\n```python\nfp8_gemm_bias_triton(\n    input,           # [M, K] float8_e4m3fn\n    weight,          # [N, K] float8_e4m3fn  \n    scale_input,     # [M, 1] float32\n    scale_weight,    # [N, 1] float32\n    bias,            # [N] float32\n    output_dtype     # target dtype (bf16/fp16)\n)\n```\n\nSources: [lightx2v/common/ops/mm/triton_kernels.py:300-600]()\n\n## Model Conversion and Quantization Tool\n\nThe `converter.py` tool provides end-to-end functionality for model quantization and format conversion.\n\n### Quantization Workflow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Converter\n    participant LoadWeights\n    participant Quantizer\n    participant SaveWeights\n    \n    User-\u003e\u003eConverter: python converter.py\u003cbr/\u003e--source model.pth\u003cbr/\u003e--quantized --bits 8\u003cbr/\u003e--linear_type int8\n    Converter-\u003e\u003eLoadWeights: load_pt_safetensors()\n    LoadWeights--\u003e\u003eConverter: weight_dict\n    \n    Converter-\u003e\u003eQuantizer: quantize_model(weights)\n    \n    loop For each layer\n        Quantizer-\u003e\u003eQuantizer: Check if target_key matches\n        Quantizer-\u003e\u003eQuantizer: Apply quantization scheme\n        Quantizer-\u003e\u003eQuantizer: Store quantized weight\n        Quantizer-\u003e\u003eQuantizer: Store scale factor\n    end\n    \n    Quantizer--\u003e\u003eConverter: quantized_weights\n    Converter-\u003e\u003eSaveWeights: save_weights(output_path)\n    SaveWeights--\u003e\u003eUser: model_int8.safetensors\n```\n\n**Diagram: Model Quantization Workflow**\n\n### Quantization Function\n\nThe `quantize_model` function in `converter.py` implements the quantization logic:\n\n```python\ndef quantize_model(\n    weights,                    # Input weight dictionary\n    w_bit=8,                   # Weight bits (currently 8)\n    target_keys=[\"attn\", \"ffn\"],  # Layers to quantize\n    ignore_key=None,           # Keys to skip\n    linear_type=\"int8\",        # Quantization type\n    non_linear_dtype=torch.float  # Non-quantized layer dtype\n):\n```\n\n**Process**:\n1. **Filter Layers**: Identifies weights matching `target_keys` patterns (attention, FFN)\n2. **Apply Quantization**: \n   - For INT8: Uses `IntegerQuantizer` with per-channel symmetric quantization\n   - For FP8: Uses `FloatQuantizer` with E4M3 format\n   - For NVFP4/MXFP: Uses specialized quantizers with block-wise scaling\n3. **Store Results**: Saves quantized weights with `.weight` suffix and scales with `.weight_scale` suffix\n4. **Preserve Precision**: Keeps normalization layers, embeddings, and other non-linear operations in original precision\n\nSources: [tools/convert/converter.py:314-500]()\n\n### Command-Line Interface\n\nThe converter tool provides extensive CLI options:\n\n**Basic Quantization**:\n```bash\npython tools/convert/converter.py \\\n  --source models/wan2.1.safetensors \\\n  --output models/wan2.1_int8 \\\n  --quantized \\\n  --bits 8 \\\n  --linear_type int8\n```\n\n**Advanced Options**:\n- `--target_keys`: Specify layers to quantize (default: `[\"attn\", \"ffn\"]`)\n- `--ignore_key`: Skip specific layers (e.g., `\"norm\"`, `\"embedding\"`)\n- `--adapter_keys`: Additional adapter layers to quantize\n- `--comfyui_mode`: Generate ComfyUI-compatible checkpoints\n\n**Output Formats**:\n- `--output_ext .safetensors`: Use SafeTensors format (recommended)\n- `--output_ext .pth`: Use PyTorch format\n- `--save_num_layers`: Split output by number of layers per file\n- `--save_blocks_per_chunk`: Split output by transformer blocks per file\n\nSources: [tools/convert/converter.py:1-100](), [tools/convert/readme.md]()\n\n### Architecture Conversion Integration\n\nThe quantization tool integrates with architecture conversion, allowing quantization during format conversion:\n\n```bash\n# Convert Diffusers  LightX2V and quantize\npython tools/convert/converter.py \\\n  --source diffusers_model/ \\\n  --output lightx2v_model_int8/ \\\n  --direction backward \\\n  --model_type wan_dit \\\n  --quantized \\\n  --linear_type int8\n```\n\nThis single command:\n1. Loads Diffusers-format weights\n2. Converts key naming (e.g., `attn1.to_q.weight`  `blocks.X.self_attn.q.weight`)\n3. Applies INT8 quantization\n4. Saves in LightX2V-compatible format\n\nSources: [tools/convert/converter.py:36-312]()\n\n## Calibration and Mixed Precision\n\nFor quantization-aware training or calibration-based quantization, the system supports collecting activation statistics.\n\n### Calibration Mode\n\nEnable calibration by setting `do_mm_calib: true` in config. This mode:\n1. Runs inference on calibration dataset\n2. Records activation ranges for each layer\n3. Computes optimal quantization scales\n4. Saves calibration data to `calib.pt`\n\nThe calibration data is stored in the global `CALIB` dictionary and can be used for:\n- Determining per-channel vs per-tensor quantization strategies\n- Identifying outlier channels that need special handling\n- Tuning quantization parameters for minimal accuracy loss\n\nSources: [lightx2v/models/runners/default_runner.py:236-240](), [lightx2v/common/ops/mm/mm_weight.py:20-22]()\n\n### Mixed Precision Strategies\n\nThe system supports mixed precision by selectively quantizing different layer types:\n\n**Sensitive Layers**: Keep in higher precision\n- Normalization layers (LayerNorm, RMSNorm)\n- Embedding layers\n- First/last layer projections\n- Time/position embeddings\n\n**Quantizable Layers**: Aggressive quantization\n- Attention projections (Q, K, V, O)\n- Feed-forward networks\n- Cross-attention projections\n\nConfiguration through `sensitive_layer` set in model initialization:\n```python\nself.sensitive_layer = {\n    \"norm\", \"embedding\", \"modulation\", \n    \"time\", \"img_emb.proj\", \"before_proj\", \"after_proj\"\n}\n```\n\nSensitive layers automatically use `GET_SENSITIVE_DTYPE()` which defaults to FP32 for numerical stability even when the model uses BF16/FP16 inference.\n\nSources: [lightx2v/models/networks/wan/model.py:41-50](), [lightx2v/utils/envs.py]()\n\n## Performance Considerations\n\n### Memory Footprint\n\nQuantization significantly reduces memory requirements:\n\n| Precision | Memory per Parameter | Example (1B params) |\n|-----------|---------------------|---------------------|\n| FP16/BF16 | 2 bytes | 2 GB |\n| INT8 | 1 byte + scales | 1.02 GB |\n| FP8 | 1 byte + scales | 1.02 GB |\n| NVFP4 | 0.5 bytes + scales | 0.52 GB |\n| MXFP4 | 0.5 bytes + scales | 0.52 GB |\n\nThe scale overhead is typically \u003c2% of total size due to per-channel (not per-element) scaling.\n\n### Inference Speed\n\nQuantization provides variable speedup depending on hardware:\n\n**NVIDIA GPUs**:\n- **Ampere (A100)**: INT8 ~1.5x, FP8 ~1.8x speedup over BF16\n- **Ada (RTX 4090)**: INT8 ~2x, FP8 ~2.5x speedup over FP16\n- **Hopper (H100)**: INT8 ~2x, FP8 ~3x speedup with FP8 tensor cores\n\n**Chinese Domestic Chips**:\n- Quantization crucial for meeting memory constraints\n- INT8 kernels optimized per vendor (Cambricon, MetaX, Hygon, etc.)\n\nThe actual speedup depends on:\n- Batch size (larger batches better utilize quantized compute)\n- Model size (memory-bound models benefit more)\n- Attention mechanism overhead (quantization helps less with attention compute)\n\nSources: [README files and performance benchmarks mentioned throughout]()\n\n---\n\nThis quantization system enables LightX2V to run efficiently on a wide range of hardware by reducing memory footprint by 2-4x and accelerating inference by 1.5-3x while maintaining high generation quality through careful scheme selection and mixed-precision strategies."])</script><script>self.__next_f.push([1,"3a:T5c35,"])</script><script>self.__next_f.push([1,"# Attention Operators and Sparse Patterns\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json](configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json)\n- [docs/EN/source/getting_started/quickstart.md](docs/EN/source/getting_started/quickstart.md)\n- [docs/ZH_CN/source/getting_started/quickstart.md](docs/ZH_CN/source/getting_started/quickstart.md)\n- [lightx2v/__init__.py](lightx2v/__init__.py)\n- [lightx2v/common/ops/attn/utils/ring_comm.py](lightx2v/common/ops/attn/utils/ring_comm.py)\n- [lightx2v/common/ops/mm/triton_kernels.py](lightx2v/common/ops/mm/triton_kernels.py)\n- [pyproject.toml](pyproject.toml)\n- [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh](scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh)\n\n\u003c/details\u003e\n\n\n\nThis page documents the attention operator implementations and sparse attention patterns supported by LightX2V. Attention operators are critical for memory efficiency and compute performance during transformer inference. The system provides multiple backend implementations optimized for different hardware and sequence length characteristics.\n\nFor quantization techniques that can be combined with attention operators, see [Quantization System](#6.1). For distributed inference strategies including sequence parallelism, see [Distributed and Parallel Inference](#6.5).\n\n---\n\n## Overview\n\nLightX2V supports multiple attention implementations that can be selected via configuration parameters. The attention system handles three distinct attention operations in each transformer block:\n\n1. **Self-Attention** (`self_attn_1`): Spatial-temporal attention across video tokens\n2. **Cross-Attention (Text)** (`cross_attn_1`): Attention to text conditioning\n3. **Cross-Attention (Image)** (`cross_attn_2`): Attention to image conditioning (for I2V/S2V tasks)\n\nEach attention type can be independently configured with different operators and sparse patterns.\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:18-24]()\n\n---\n\n## Attention Operator Types\n\n### Supported Operators\n\n```mermaid\ngraph TB\n    Config[\"Configuration\u003cbr/\u003e(JSON/Python API)\"]\n    \n    subgraph \"Attention Operator Selection\"\n        SelfAttn[\"self_attn_1_type\"]\n        CrossAttn1[\"cross_attn_1_type\"]\n        CrossAttn2[\"cross_attn_2_type\"]\n    end\n    \n    subgraph \"Flash Attention Family\"\n        Flash2[\"flash_attn2\u003cbr/\u003eGeneral Purpose\u003cbr/\u003eO(N) memory\"]\n        Flash3[\"flash_attn3\u003cbr/\u003eHopper GPU Optimized\u003cbr/\u003eH100+ only\"]\n    end\n    \n    subgraph \"Optimized Variants\"\n        Sage[\"sage_attn\u003cbr/\u003eINT8 KV Cache\u003cbr/\u003e2x throughput\"]\n    end\n    \n    subgraph \"Sparse Patterns\"\n        Nbhd[\"nbhd_attn\u003cbr/\u003eNeighborhood Attention\u003cbr/\u003eLocal + Sparse\"]\n        Radial[\"radial_attn\u003cbr/\u003eRadial Patterns\u003cbr/\u003eCircular locality\"]\n    end\n    \n    Config --\u003e SelfAttn\n    Config --\u003e CrossAttn1\n    Config --\u003e CrossAttn2\n    \n    SelfAttn --\u003e Flash2\n    SelfAttn --\u003e Flash3\n    SelfAttn --\u003e Sage\n    SelfAttn --\u003e Nbhd\n    SelfAttn --\u003e Radial\n    \n    CrossAttn1 --\u003e Flash2\n    CrossAttn1 --\u003e Flash3\n    \n    CrossAttn2 --\u003e Flash2\n    CrossAttn2 --\u003e Flash3\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:18-24](), [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:8-13]()\n\n---\n\n### Flash Attention 2 and 3\n\nFlash Attention is the default attention operator, providing memory-efficient attention via kernel fusion and IO-aware tiling.\n\n**Key Characteristics:**\n- **Flash Attention 2**: General-purpose implementation for Ampere (A100) and newer architectures\n- **Flash Attention 3**: Hopper-optimized (H100+) with improved occupancy and asynchronous execution\n- **Memory Complexity**: O(N) instead of O(N) for vanilla attention\n- **Interface**: Uses cumulative sequence lengths (`cu_seqlens_q`, `cu_seqlens_kv`)\n\n**Implementation:**\n\nThe system initializes cumulative sequence length tensors on first use and caches them for subsequent steps:\n\n[lightx2v/models/networks/wan/infer/transformer_infer.py:193-197]()\n\n```python\nif self.self_attn_cu_seqlens_qkv is None:\n    if self.self_attn_1_type in [\"flash_attn2\", \"flash_attn3\"]:\n        self.self_attn_cu_seqlens_qkv = torch.tensor([0, q.shape[0]]).cumsum(0, dtype=torch.int32).to(q.device, non_blocking=True)\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:193-233](), [docs/EN/source/getting_started/quickstart.md:88-98]()\n\n---\n\n### SageAttention\n\nSageAttention (INT8 KV Cache) quantizes key-value caches to INT8 while maintaining query precision, achieving ~2x throughput improvement over Flash Attention 2.\n\n**Configuration:**\n```json\n{\n    \"self_attn_1_type\": \"sage_attn\",\n    \"cross_attn_1_type\": \"flash_attn2\"\n}\n```\n\n**Features:**\n- INT8 quantization of K/V matrices during attention computation\n- Maintains FP16/BF16 precision for queries\n- 2x memory reduction for KV cache\n- ~2x throughput increase vs Flash Attention 2\n- Compatible with all GPU architectures\n\n**Installation:**\n```bash\ngit clone https://github.com/thu-ml/SageAttention.git\ncd SageAttention \u0026\u0026 CUDA_ARCHITECTURES=\"8.0,8.6,8.9,9.0,12.0\" pip install -v -e .\n```\n\n**Sources:** [docs/EN/source/getting_started/quickstart.md:99-103](), High-level architecture diagrams\n\n---\n\n### Neighborhood Attention (nbhd_attn)\n\nNeighborhood attention implements sparse patterns by restricting attention to local neighborhoods with configurable sparsity coefficients.\n\n**Configuration Example:**\n```json\n{\n    \"self_attn_1_type\": \"nbhd_attn\",\n    \"nbhd_attn_setting\": {\n        \"coefficient\": [1.0, 0.25, 0.056]\n    }\n}\n```\n\n**Sparse Pattern Mechanism:**\n\nThe `coefficient` array defines attention scope for each spatial dimension:\n- **coefficient[0]**: Temporal dimension coverage (1.0 = full)\n- **coefficient[1]**: Height dimension coverage (0.25 = 25% neighborhood)\n- **coefficient[2]**: Width dimension coverage (0.056 = ~5.6% neighborhood)\n\nThis creates a 3D attention window that balances local coherence with computational efficiency.\n\n**Use Cases:**\n- Long video sequences where full attention is prohibitive\n- Distributed inference with sequence parallelism (see [Distributed and Parallel Inference](#6.5))\n- Memory-constrained deployments\n\n**Sources:** [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:8-11](), [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh:12-21]()\n\n---\n\n### Radial Attention\n\nRadial attention implements circular attention patterns, useful for modeling rotational symmetries and radial dependencies in video content.\n\n**Configuration:**\n```json\n{\n    \"self_attn_1_type\": \"radial_attn\"\n}\n```\n\n**Characteristics:**\n- Circular attention masks based on distance from center\n- Useful for specific video generation tasks with radial structure\n- Configurable radius parameters\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:18-24](), High-level architecture diagrams\n\n---\n\n## Configuration Interface\n\n### Per-Attention Configuration\n\nEach of the three attention operations can be independently configured:\n\n```mermaid\ngraph LR\n    subgraph \"Transformer Block\"\n        Phase0[\"Phase 0\u003cbr/\u003eSelf-Attention\"]\n        Phase1[\"Phase 1\u003cbr/\u003eCross-Attention\"]\n        Phase2[\"Phase 2\u003cbr/\u003eFeed-Forward\"]\n    end\n    \n    subgraph \"Configuration Parameters\"\n        SelfType[\"self_attn_1_type\"]\n        CrossType1[\"cross_attn_1_type\"]\n        CrossType2[\"cross_attn_2_type\"]\n    end\n    \n    SelfType --\u003e Phase0\n    CrossType1 --\u003e Phase1\n    CrossType2 --\u003e Phase1\n    \n    Phase0 --\u003e Phase1\n    Phase1 --\u003e Phase2\n```\n\n**Configuration Table:**\n\n| Parameter | Applies To | Valid Values | Default |\n|-----------|------------|--------------|---------|\n| `self_attn_1_type` | Self-attention | `flash_attn2`, `flash_attn3`, `sage_attn`, `nbhd_attn`, `radial_attn` | `flash_attn2` |\n| `cross_attn_1_type` | Text cross-attention | `flash_attn2`, `flash_attn3` | `flash_attn2` |\n| `cross_attn_2_type` | Image cross-attention | `flash_attn2`, `flash_attn3` | `flash_attn2` |\n| `attention_type` | Global override | Same as above | `flash_attn2` |\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:18-24](), [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:8-13]()\n\n---\n\n### Configuration via JSON\n\n**Method 1: JSON Configuration File**\n```json\n{\n    \"self_attn_1_type\": \"nbhd_attn\",\n    \"nbhd_attn_setting\": {\n        \"coefficient\": [1.0, 0.25, 0.056]\n    },\n    \"cross_attn_1_type\": \"flash_attn3\",\n    \"cross_attn_2_type\": \"flash_attn3\"\n}\n```\n\n**Method 2: Python API**\n```python\nfrom lightx2v import LightX2VPipeline\n\ngenerator = pipeline.create_generator(\n    task=\"audio2video\",\n    model_cls=\"seko_talk\",\n    model_path=\"/path/to/model\",\n    config_modify={\n        \"self_attn_1_type\": \"sage_attn\",\n        \"cross_attn_1_type\": \"flash_attn2\"\n    }\n)\n```\n\n**Sources:** [lightx2v/utils/set_config.py:44-49](), [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json]()\n\n---\n\n## Implementation Architecture\n\n### Attention Module Hierarchy\n\n```mermaid\ngraph TB\n    TransformerInfer[\"WanTransformerInfer\u003cbr/\u003elightx2v/models/networks/wan/infer/transformer_infer.py\"]\n    \n    subgraph \"Attention Application Points\"\n        SelfAttnLogic[\"infer_self_attn()\u003cbr/\u003eLine 169-241\"]\n        CrossAttnLogic[\"infer_cross_attn()\u003cbr/\u003eLine 243-316\"]\n    end\n    \n    subgraph \"Weight Modules\"\n        SelfAttnWeights[\"WanSelfAttention\u003cbr/\u003ePhase 0 Weights\"]\n        CrossAttnWeights[\"WanCrossAttention\u003cbr/\u003ePhase 1 Weights\"]\n    end\n    \n    subgraph \"Operator Application\"\n        SelfOp[\"self_attn_1.apply()\u003cbr/\u003eBackend operator\"]\n        CrossOp1[\"cross_attn_1.apply()\u003cbr/\u003eText conditioning\"]\n        CrossOp2[\"cross_attn_2.apply()\u003cbr/\u003eImage conditioning\"]\n    end\n    \n    TransformerInfer --\u003e SelfAttnLogic\n    TransformerInfer --\u003e CrossAttnLogic\n    \n    SelfAttnLogic --\u003e SelfAttnWeights\n    CrossAttnLogic --\u003e CrossAttnWeights\n    \n    SelfAttnWeights --\u003e SelfOp\n    CrossAttnWeights --\u003e CrossOp1\n    CrossAttnWeights --\u003e CrossOp2\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:17-362]()\n\n---\n\n### Self-Attention Implementation Flow\n\nThe self-attention operation processes spatial-temporal tokens with RoPE (Rotary Position Embeddings):\n\n```mermaid\nsequenceDiagram\n    participant Infer as infer_block()\n    participant SelfAttn as infer_self_attn()\n    participant RoPE as apply_rope_func()\n    participant Backend as Attention Backend\n    \n    Infer-\u003e\u003eSelfAttn: x, modulation params\n    \n    Note over SelfAttn: Modulate with scale/shift\n    SelfAttn-\u003e\u003eSelfAttn: norm1_out = norm1(x) * scale + shift\n    \n    Note over SelfAttn: Project Q, K, V\n    SelfAttn-\u003e\u003eSelfAttn: q = self_attn_q(norm1_out)\n    SelfAttn-\u003e\u003eSelfAttn: k = self_attn_k(norm1_out)\n    SelfAttn-\u003e\u003eSelfAttn: v = self_attn_v(norm1_out)\n    \n    Note over SelfAttn: Apply RoPE normalization\n    SelfAttn-\u003e\u003eSelfAttn: q = self_attn_norm_q(q)\n    SelfAttn-\u003e\u003eSelfAttn: k = self_attn_norm_k(k)\n    \n    Note over SelfAttn: Reshape to [S, H, D]\n    SelfAttn-\u003e\u003eSelfAttn: q = q.view(s, n, d)\n    SelfAttn-\u003e\u003eSelfAttn: k = k.view(s, n, d)\n    SelfAttn-\u003e\u003eSelfAttn: v = v.view(s, n, d)\n    \n    Note over RoPE: Apply rotary embeddings\n    SelfAttn-\u003e\u003eRoPE: apply_rope(q, k, cos_sin)\n    RoPE--\u003e\u003eSelfAttn: q_rope, k_rope\n    \n    Note over SelfAttn: Initialize cu_seqlens (first call)\n    SelfAttn-\u003e\u003eSelfAttn: cu_seqlens_qkv = [0, seq_len]\n    \n    Note over Backend: Call attention operator\n    SelfAttn-\u003e\u003eBackend: apply(q, k, v, cu_seqlens)\n    Backend--\u003e\u003eSelfAttn: attn_out\n    \n    Note over SelfAttn: Output projection\n    SelfAttn-\u003e\u003eSelfAttn: y = self_attn_o(attn_out)\n    \n    SelfAttn--\u003e\u003eInfer: y (attention output)\n```\n\n**Key Implementation Details:**\n\n1. **Modulation**: AdaLN-style modulation with scale and shift parameters [lightx2v/models/networks/wan/infer/transformer_infer.py:169-186]()\n\n2. **RoPE Application**: 3D positional embeddings for temporal, height, and width dimensions [lightx2v/models/networks/wan/infer/transformer_infer.py:191]()\n\n3. **Cumulative Sequence Lengths**: Cached tensor for flash attention interface [lightx2v/models/networks/wan/infer/transformer_infer.py:193-197]()\n\n4. **Backend Selection**: Dispatched based on `self_attn_1_type` configuration [lightx2v/models/networks/wan/infer/transformer_infer.py:224-233]()\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:169-241]()\n\n---\n\n### Cross-Attention Implementation\n\nCross-attention operates over text and optional image conditioning:\n\n[lightx2v/models/networks/wan/infer/transformer_infer.py:243-316]()\n\n**Dual Cross-Attention Pattern:**\n```mermaid\ngraph LR\n    Q[\"Query\u003cbr/\u003e(from video tokens)\"]\n    \n    subgraph \"Text Conditioning\"\n        K_text[\"K_text\u003cbr/\u003e(text embeddings)\"]\n        V_text[\"V_text\"]\n        Attn1[\"cross_attn_1\u003cbr/\u003e(Text)\"]\n    end\n    \n    subgraph \"Image Conditioning\"\n        K_img[\"K_img\u003cbr/\u003e(CLIP features)\"]\n        V_img[\"V_img\"]\n        Attn2[\"cross_attn_2\u003cbr/\u003e(Image)\"]\n    end\n    \n    Out[\"Output\u003cbr/\u003e(sum of both)\"]\n    \n    Q --\u003e Attn1\n    K_text --\u003e Attn1\n    V_text --\u003e Attn1\n    \n    Q --\u003e Attn2\n    K_img --\u003e Attn2\n    V_img --\u003e Attn2\n    \n    Attn1 --\u003e Out\n    Attn2 --\u003e Out\n```\n\n**Context Splitting:**\n\nFor I2V/S2V tasks, the context tensor contains both image and text features:\n- `context[:257]`: CLIP image features (257 tokens)\n- `context[257:]`: Text features (remaining tokens)\n\n[lightx2v/models/networks/wan/infer/transformer_infer.py:250-254]()\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:243-316]()\n\n---\n\n## RoPE Integration\n\n### Rotary Position Embeddings\n\nLightX2V uses 3D RoPE for spatial-temporal position encoding:\n\n```mermaid\ngraph TB\n    subgraph \"RoPE Frequency Computation\"\n        HeadDim[\"head_dim = dim / num_heads\"]\n        Splits[\"Split into 3 parts:\u003cbr/\u003e[t_dim, h_dim, w_dim]\"]\n    end\n    \n    subgraph \"Grid Sizes\"\n        GridT[\"T (temporal frames)\"]\n        GridH[\"H (spatial height)\"]\n        GridW[\"W (spatial width)\"]\n    end\n    \n    subgraph \"Frequency Application\"\n        FreqT[\"freqs[0][:T]\u003cbr/\u003eTemporal frequencies\"]\n        FreqH[\"freqs[1][:H]\u003cbr/\u003eHeight frequencies\"]\n        FreqW[\"freqs[2][:W]\u003cbr/\u003eWidth frequencies\"]\n    end\n    \n    Result[\"Combined RoPE tensor\u003cbr/\u003e[T*H*W, 1, head_dim/2]\"]\n    \n    HeadDim --\u003e Splits\n    \n    GridT --\u003e FreqT\n    GridH --\u003e FreqH\n    GridW --\u003e FreqW\n    \n    FreqT --\u003e Result\n    FreqH --\u003e Result\n    FreqW --\u003e Result\n```\n\n**Frequency Computation:**\n\n[lightx2v/models/networks/wan/infer/utils.py:127-140]()\n\nThe system splits the head dimension into three parts for temporal, height, and width dimensions, then broadcasts frequencies across the 3D grid.\n\n**RoPE Backends:**\n\n| Backend | Implementation | Use Case |\n|---------|----------------|----------|\n| `flashinfer` | FlashInfer library with inplace ops | Default, best performance |\n| `torch` | PyTorch complex view operations | Fallback, general compatibility |\n| `torch_naive` | Explicit sin/cos computation | Debug/reference |\n| Platform-specific | Via `ROPE_REGISTER` | Hardware-specific optimizations |\n\n**Configuration:**\n```json\n{\n    \"rope_type\": \"flashinfer\",\n    \"rope_chunk\": false,\n    \"rope_chunk_size\": 100\n}\n```\n\n**Chunked RoPE**: For memory efficiency with very long sequences, RoPE can be applied in chunks [lightx2v/models/networks/wan/infer/utils.py:71-98]()\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:36-58](), [lightx2v/models/networks/wan/infer/utils.py:12-124]()\n\n---\n\n## Window Size Configuration\n\nThe `window_size` parameter controls local attention windows for sparse patterns:\n\n[lightx2v/models/networks/wan/infer/transformer_infer.py:30]()\n\n```python\nself.window_size = config.get(\"window_size\", (-1, -1))\n```\n\n**Window Size Values:**\n- `(-1, -1)`: Full attention (no windowing)\n- `(window_h, window_w)`: Local attention window for height/width dimensions\n- Used with neighborhood attention patterns\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:30]()\n\n---\n\n## Sequence Parallelism Integration\n\nAttention operators integrate with sequence parallelism for distributed inference:\n\n```mermaid\ngraph TB\n    subgraph \"Sequence Parallel Attention\"\n        SeqPConfig[\"seq_parallel: true\u003cbr/\u003eseq_p_size: 8\"]\n        ParallelModule[\"self_attn_1_parallel\u003cbr/\u003e(wrapper)\"]\n    end\n    \n    subgraph \"Communication\"\n        AllGather[\"All-Gather\u003cbr/\u003e(collect Q,K,V)\"]\n        Compute[\"Local Attention\u003cbr/\u003e(per rank)\"]\n        ReduceScatter[\"Reduce-Scatter\u003cbr/\u003e(distribute output)\"]\n    end\n    \n    subgraph \"Optimization Options\"\n        FP8Comm[\"seq_p_fp8_comm\u003cbr/\u003e(FP8 communication)\"]\n        HeadParallel[\"seq_p_head_parallel\u003cbr/\u003e(head-wise split)\"]\n        TensorFusion[\"seq_p_tensor_fusion\u003cbr/\u003e(fuse comm)\"]\n    end\n    \n    SeqPConfig --\u003e ParallelModule\n    \n    ParallelModule --\u003e AllGather\n    AllGather --\u003e Compute\n    Compute --\u003e ReduceScatter\n    \n    FP8Comm -.-\u003e AllGather\n    HeadParallel -.-\u003e Compute\n    TensorFusion -.-\u003e AllGather\n```\n\n**Implementation:**\n\nWhen sequence parallelism is enabled, attention is wrapped with communication primitives:\n\n[lightx2v/models/networks/wan/infer/transformer_infer.py:209-222]()\n\n```python\nif self.config[\"seq_parallel\"]:\n    attn_out = phase.self_attn_1_parallel.apply(\n        q=q, k=k, v=v,\n        slice_qkv_len=img_qkv_len,\n        cu_seqlens_qkv=self.self_attn_cu_seqlens_qkv,\n        attention_module=phase.self_attn_1,\n        attention_type=self.self_attn_1_type,\n        seq_p_group=self.seq_p_group,\n        use_fp8_comm=self.seq_p_fp8_comm,\n        use_tensor_fusion=self.seq_p_tensor_fusion,\n        enable_head_parallel=self.enable_head_parallel,\n    )\n```\n\nFor more details on distributed strategies, see [Distributed and Parallel Inference](#6.5).\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:64-72](), [lightx2v/models/networks/wan/infer/transformer_infer.py:209-222]()\n\n---\n\n## Performance Characteristics\n\n### Attention Operator Comparison\n\n| Operator | Memory | Throughput | Hardware Req. | Notes |\n|----------|--------|------------|---------------|-------|\n| **flash_attn2** | O(N) | 1.0x baseline | Ampere+ (A100) | General purpose, stable |\n| **flash_attn3** | O(N) | 1.2x | Hopper (H100+) | Best for H100 |\n| **sage_attn** | O(N/2) | 2.0x | All CUDA | INT8 KV cache, 2x memory savings |\n| **nbhd_attn** | O(kN) | 3-10x | All CUDA | k = sparsity coefficient |\n| **radial_attn** | O(kN) | 2-5x | All CUDA | Circular patterns |\n\n### Memory Scaling\n\n**Full Attention:**\n```\nMemory = O(batch_size  num_heads  seq_len)\n```\n\n**Flash Attention:**\n```\nMemory = O(batch_size  num_heads  seq_len)\n```\n\n**Neighborhood Attention:**\n```\nMemory = O(batch_size  num_heads  seq_len  window_size)\n```\n\nFor a typical 81-frame video at 480832 resolution:\n- **Sequence Length**: ~16,000 tokens\n- **Full Attention**: ~16GB per layer\n- **Flash Attention**: ~100MB per layer\n- **Neighborhood Attention (coefficient=0.25)**: ~25MB per layer\n\n**Sources:** High-level architecture diagrams, [lightx2v/models/networks/wan/infer/transformer_infer.py:169-241]()\n\n---\n\n## Configuration Examples\n\n### Example 1: Maximum Performance (H100)\n\n```json\n{\n    \"self_attn_1_type\": \"flash_attn3\",\n    \"cross_attn_1_type\": \"flash_attn3\",\n    \"cross_attn_2_type\": \"flash_attn3\",\n    \"rope_type\": \"flashinfer\"\n}\n```\n\n**Use Case:** 8H100 distributed inference with full attention\n\n---\n\n### Example 2: Memory Optimized (RTX 4090)\n\n```json\n{\n    \"self_attn_1_type\": \"sage_attn\",\n    \"cross_attn_1_type\": \"flash_attn2\",\n    \"cross_attn_2_type\": \"flash_attn2\",\n    \"rope_type\": \"torch\"\n}\n```\n\n**Use Case:** Single consumer GPU with 24GB VRAM\n\n---\n\n### Example 3: Long Video Generation\n\n```json\n{\n    \"self_attn_1_type\": \"nbhd_attn\",\n    \"nbhd_attn_setting\": {\n        \"coefficient\": [1.0, 0.25, 0.056]\n    },\n    \"cross_attn_1_type\": \"flash_attn3\",\n    \"cross_attn_2_type\": \"flash_attn3\",\n    \"parallel\": {\n        \"seq_p_size\": 8,\n        \"seq_p_attn_type\": \"ulysses\"\n    }\n}\n```\n\n**Use Case:** 360-second video generation with 8-GPU sequence parallelism\n\n**Sources:** [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json](), [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh]()\n\n---\n\n## Debugging and Profiling\n\n### Attention State Reset\n\nAttention tensors (cumulative sequence lengths) are cached and reused across diffusion steps. They are reset at the start of each transformer inference pass:\n\n[lightx2v/models/networks/wan/infer/transformer_infer.py:81-87]()\n\n```python\ndef reset_infer_states(self):\n    self.self_attn_cu_seqlens_qkv = None\n    self.cross_attn_cu_seqlens_q = None\n    self.cross_attn_cu_seqlens_kv = None\n    self.cross_attn_cu_seqlens_kv_img = None\n```\n\n### Memory Cleanup\n\nWhen `clean_cuda_cache` is enabled, intermediate attention tensors are explicitly deleted:\n\n[lightx2v/models/networks/wan/infer/transformer_infer.py:199-201]()\n\n```python\nif self.clean_cuda_cache:\n    del norm1_out, shift_msa, scale_msa\n    torch.cuda.empty_cache()\n```\n\n**Configuration:**\n```json\n{\n    \"clean_cuda_cache\": true\n}\n```\n\n**Use Case:** Extremely memory-constrained environments; trades performance for memory safety.\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:59](), [lightx2v/models/networks/wan/infer/transformer_infer.py:81-87]()\n\n---\n\n## Installation and Dependencies\n\n### Flash Attention\n\n**Installation:**\n```bash\n# Flash Attention 2 (Ampere+)\ngit clone https://github.com/Dao-AILab/flash-attention.git --recursive\ncd flash-attention \u0026\u0026 python setup.py install\n\n# Flash Attention 3 (Hopper only)\ncd flash-attention/hopper \u0026\u0026 python setup.py install\n```\n\n### SageAttention\n\n**Installation:**\n```bash\ngit clone https://github.com/thu-ml/SageAttention.git\ncd SageAttention \u0026\u0026 \\\nCUDA_ARCHITECTURES=\"8.0,8.6,8.9,9.0,12.0\" \\\nEXT_PARALLEL=4 \\\nNVCC_APPEND_FLAGS=\"--threads 8\" \\\nMAX_JOBS=32 \\\npip install -v -e .\n```\n\n### Platform-Specific RoPE\n\nPlatform-specific RoPE implementations are registered via `ROPE_REGISTER`:\n\n[lightx2v/models/networks/wan/infer/transformer_infer.py:43-51]()\n\n```python\nif rope_type in ROPE_REGISTER:\n    rope_class = ROPE_REGISTER[rope_type]\n    self.rope_instance = rope_class()\n```\n\nThis allows hardware vendors to provide optimized RoPE kernels for their platforms (Cambricon, Ascend, AMD, etc.).\n\n**Sources:** [docs/EN/source/getting_started/quickstart.md:86-105](), [lightx2v/models/networks/wan/infer/transformer_infer.py:36-58]()\n\n---\n\n## Summary\n\nLightX2V's attention system provides:\n\n1. **Multiple Backend Options**: Flash Attention 2/3, SageAttention, sparse patterns\n2. **Independent Configuration**: Per-attention-operation type selection\n3. **Sparse Pattern Support**: Neighborhood and radial attention for memory efficiency\n4. **Distributed Integration**: Seamless integration with sequence parallelism\n5. **Platform Extensibility**: Registry-based system for hardware-specific optimizations\n\nThe attention operator selection is one of the most impactful performance knobs in the system, enabling:\n- **2x throughput** with SageAttention\n- **3-10x memory reduction** with sparse patterns\n- **Cross-GPU scaling** with sequence parallelism\n\nFor combined optimization strategies, see related pages: [Quantization System](#6.1), [Memory Management and CPU Offloading](#6.3), [Distributed and Parallel Inference](#6.5).\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py](), [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json](), High-level architecture diagrams"])</script><script>self.__next_f.push([1,"3b:T8aff,"])</script><script>self.__next_f.push([1,"# Memory Management and Offloading\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [app/README.md](app/README.md)\n- [app/gradio_demo.py](app/gradio_demo.py)\n- [app/run_gradio.sh](app/run_gradio.sh)\n- [app/run_gradio_win.bat](app/run_gradio_win.bat)\n- [app/utils/image_page.py](app/utils/image_page.py)\n- [configs/z_image/z_image_turbo_t2i_offload.json](configs/z_image/z_image_turbo_t2i_offload.json)\n- [docs/EN/source/deploy_guides/deploy_gradio.md](docs/EN/source/deploy_guides/deploy_gradio.md)\n- [docs/EN/source/deploy_guides/deploy_local_windows.md](docs/EN/source/deploy_guides/deploy_local_windows.md)\n- [docs/ZH_CN/source/deploy_guides/deploy_gradio.md](docs/ZH_CN/source/deploy_guides/deploy_gradio.md)\n- [docs/ZH_CN/source/deploy_guides/deploy_local_windows.md](docs/ZH_CN/source/deploy_guides/deploy_local_windows.md)\n- [examples/z-image-turbo/z_image_turbo.py](examples/z-image-turbo/z_image_turbo.py)\n- [lightx2v/common/offload/manager.py](lightx2v/common/offload/manager.py)\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/input_encoders/hf/z_image/qwen3_model.py](lightx2v/models/input_encoders/hf/z_image/qwen3_model.py)\n- [lightx2v/models/networks/base_model.py](lightx2v/models/networks/base_model.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/networks/z_image/model.py](lightx2v/models/networks/z_image/model.py)\n- [lightx2v/models/networks/z_image/weights/post_weights.py](lightx2v/models/networks/z_image/weights/post_weights.py)\n- [lightx2v/models/networks/z_image/weights/transformer_weights.py](lightx2v/models/networks/z_image/weights/transformer_weights.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/runners/z_image/z_image_runner.py](lightx2v/models/runners/z_image/z_image_runner.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/models/video_encoders/hf/z_image/vae.py](lightx2v/models/video_encoders/hf/z_image/vae.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n- [scripts/win/run_wan_i2v.bat](scripts/win/run_wan_i2v.bat)\n- [scripts/win/run_wan_t2v.bat](scripts/win/run_wan_t2v.bat)\n- [scripts/z_image/z_image_turbo_t2i.sh](scripts/z_image/z_image_turbo_t2i.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the memory management and offloading mechanisms in LightX2V that enable large video generation models to run on GPUs with limited VRAM. The system provides multi-level offloading strategies (CPU, disk) with configurable granularity (model, block, phase) to balance memory usage and inference speed. For general performance optimization strategies, see [Performance Optimization](#6). For specific quantization techniques that complement offloading, see [Quantization System](#6.1).\n\n## Offloading Strategies\n\nLightX2V implements a hierarchical offloading system that can move model weights between three storage tiers: GPU VRAM, CPU RAM, and disk storage. The system uses asynchronous streaming to overlap data transfers with computation, minimizing performance impact.\n\n### CPU Offloading\n\nCPU offloading moves inactive model components from GPU VRAM to CPU RAM during inference. This enables running 14B parameter models on GPUs with as little as 8GB VRAM by keeping only the currently executing block or phase on the GPU.\n\n**Key Features:**\n- **Asynchronous Transfer**: Uses CUDA streams to overlap weight transfers with computation\n- **Prefetching**: Loads the next block/phase while the current one executes\n- **Configurable Ratio**: `offload_ratio` parameter controls what fraction of blocks to offload (0.0 to 1.0)\n\n**Configuration Parameters:**\n- `cpu_offload`: Boolean flag to enable CPU offloading\n- `offload_granularity`: Controls offload unit size (`\"model\"`, `\"block\"`, `\"phase\"`)\n- `offload_ratio`: Fraction of blocks to keep in CPU memory (default: 1.0)\n\nSources: [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:13-41](), [app/gradio_demo.py:469-476]()\n\n### Lazy Loading (Disk Offloading)\n\nLazy loading extends CPU offloading to use disk storage as a third tier. Model weights are loaded from disk on-demand into a pinned memory buffer, then transferred to GPU. This is essential for systems with limited CPU RAM (\u003c 32GB).\n\n**Key Features:**\n- **Disk Worker Threads**: Background threads load weights from disk asynchronously\n- **Pinned Memory Buffer**: LRU-managed buffer holds recently used weights in CPU memory\n- **Automatic Prefetching**: Predictively loads upcoming blocks based on execution order\n\n**Configuration Parameters:**\n- `lazy_load`: Boolean flag to enable disk-based lazy loading\n- `unload_modules`: Automatically unload modules after inference\n- `num_disk_workers`: Number of background threads for disk I/O (default: 2)\n- `max_memory`: Maximum CPU memory for pinned buffer in GB (default: 2)\n\nSources: [lightx2v/common/offload/manager.py:66-81](), [app/gradio_demo.py:278-284]()\n\n### Granularity Levels\n\nThe system supports three granularity levels that trade off memory savings against transfer overhead:\n\n| Granularity | Unit Size | Transfer Overhead | Memory Savings | Use Case |\n|-------------|-----------|-------------------|----------------|----------|\n| `model` | Entire model | Lowest | Lowest | High VRAM (40GB+) |\n| `block` | Transformer block (28 total) | Medium | Medium | Medium VRAM (16-40GB) |\n| `phase` | Sub-block phase (4 per block) | Highest | Highest | Low VRAM (8-16GB) |\n\n**Phase-level Granularity Details:**\nEach transformer block is divided into 4 phases:\n1. **Phase 0**: Self-attention modulation and pre-processing\n2. **Phase 1**: Cross-attention computation\n3. **Phase 2**: Feed-forward network (FFN)\n4. **Phase 3**: Post-adapter processing (audio/animate models only)\n\nSources: [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:14-42](), [docs/EN/source/deploy_guides/deploy_gradio.md:219-232]()\n\n## Core Components\n\n### WeightAsyncStreamManager\n\nThe `WeightAsyncStreamManager` class (defined at [lightx2v/common/offload/manager.py:11]()) orchestrates asynchronous weight transfers between CPU and GPU using three CUDA streams to overlap computation with data movement.\n\n**Class Structure: WeightAsyncStreamManager**\n\n```mermaid\ngraph TB\n    subgraph \"CUDA Streams (torch.cuda.Stream)\"\n        CompStream[\"self.compute_stream\u003cbr/\u003epriority=-1\"]\n        CUDALoad[\"self.cuda_load_stream\u003cbr/\u003epriority=0\"]\n        CPULoad[\"self.cpu_load_stream\u003cbr/\u003epriority=0\"]\n    end\n    \n    subgraph \"Active Weights Array (self.active_weights)\"\n        Active0[\"active_weights[0]\u003cbr/\u003eCurrent executing\"]\n        Active1[\"active_weights[1]\u003cbr/\u003eRecently offloaded\"]\n        Active2[\"active_weights[2]\u003cbr/\u003ePrefetch target\"]\n    end\n    \n    subgraph \"Weight States\"\n        GPU[\"GPU VRAM\u003cbr/\u003eWeightModule.to_cuda_async()\"]\n        CPU[\"CPU RAM\u003cbr/\u003eWeightModule.to_cpu_async()\"]\n    end\n    \n    CompStream --\u003e|\"Executes on\"| Active0\n    CUDALoad --\u003e|\"blocks[idx].to_cuda_async()\"| Active2\n    CPULoad --\u003e|\"blocks[idx].to_cpu_async()\"| Active1\n    \n    Active2 -.-\u003e|\"swap_weights() rotates\"| Active0\n    Active0 -.-\u003e|\"swap_weights() rotates\"| Active1\n    \n    Active0 --\u003e GPU\n    Active1 --\u003e CPU\n    Active2 --\u003e GPU\n```\n\n**Core Methods:**\n- `__init__(blocks_num, offload_ratio, phases_num)`: Initialize streams and active_weights array [lightx2v/common/offload/manager.py:12-16]()\n- `init(blocks_num, phases_num, offload_ratio)`: Reset state, calculate `offload_blocks_num` and `offload_phases_num` [lightx2v/common/offload/manager.py:18-26]()\n- `prefetch_weights(block_idx, blocks_weights)`: Load next block to GPU on `cuda_load_stream`, offload previous block to CPU on `cpu_load_stream` [lightx2v/common/offload/manager.py:28-36]()\n- `swap_weights()`: Synchronize all three streams and rotate `active_weights` pointers [lightx2v/common/offload/manager.py:38-46]()\n- `prefetch_phase(block_idx, phase_idx, blocks)`: Phase-granularity version of prefetch [lightx2v/common/offload/manager.py:48-57]()\n- `swap_phases()`: Phase-granularity version of swap [lightx2v/common/offload/manager.py:58-63]()\n\n**Offload Ratio Control:**\nThe `offload_ratio` parameter (0.0 to 1.0) determines how many blocks are offloaded to CPU. Only blocks with `block_idx \u003c offload_blocks_num` are moved to CPU; later blocks remain on GPU to reduce transfer overhead for final layers.\n\nSources: [lightx2v/common/offload/manager.py:11-64]()\n\n### Disk-Based Lazy Loading\n\nWhen `lazy_load=True` is enabled, the `WeightAsyncStreamManager` extends its functionality to support on-demand loading from disk storage using background threads and a pinned memory buffer.\n\n**Lazy Loading Infrastructure**\n\n```mermaid\ngraph TB\n    subgraph \"Initialization\"\n        InitLazy[\"init_lazy_load(num_workers)\"]\n        Executor[\"ThreadPoolExecutor\u003cbr/\u003emax_workers=num_workers\"]\n        PrefetchFutures[\"prefetch_futures[]\u003cbr/\u003eFuture tracking\"]\n    end\n    \n    subgraph \"Prefetch Cycle\"\n        StartPrefetch[\"start_prefetch_block(block_idx)\"]\n        SubmitTask[\"executor.submit(load_state_dict_from_disk)\"]\n        CPUBuffer[\"cpu_buffers[1]\u003cbr/\u003eTarget buffer\"]\n    end\n    \n    subgraph \"Synchronization\"\n        SwapCPU[\"swap_cpu_buffers()\"]\n        WaitFutures[\"future.result() for all\"]\n        RotateBuffers[\"cpu_buffers = [1, 0]\"]\n    end\n    \n    subgraph \"Weight Transfer\"\n        DiskFile[\"block_N.safetensors\"]\n        LoadDisk[\"load_state_dict_from_disk()\"]\n        PinnedMem[\"Pinned CPU memory\"]\n        GPU[\"GPU via prefetch_weights()\"]\n    end\n    \n    InitLazy --\u003e Executor\n    Executor --\u003e StartPrefetch\n    StartPrefetch --\u003e SubmitTask\n    SubmitTask --\u003e CPUBuffer\n    CPUBuffer --\u003e SwapCPU\n    SwapCPU --\u003e WaitFutures\n    WaitFutures --\u003e RotateBuffers\n    \n    DiskFile --\u003e LoadDisk\n    LoadDisk --\u003e PinnedMem\n    PinnedMem --\u003e GPU\n```\n\n**Core Lazy Loading Methods:**\n- `init_lazy_load(num_workers)`: Initialize thread pool executor and futures list [lightx2v/common/offload/manager.py:106-110]()\n- `start_prefetch_block(block_idx, adapter_block_idx)`: Submit disk load tasks for next block/phases to executor [lightx2v/common/offload/manager.py:112-121]()\n- `swap_cpu_buffers()`: Wait for all prefetch futures to complete, then swap CPU buffer pointers [lightx2v/common/offload/manager.py:123-131]()\n- `warm_up_cpu_buffers(blocks_num)`: Pre-load all blocks from disk sequentially to warm up cache [lightx2v/common/offload/manager.py:91-104]()\n\n**Disk Loading Process:**\nEach phase/block's `load_state_dict_from_disk()` method [lightx2v/common/ops/embedding/embedding_weight.py:98-110]() opens the corresponding safetensors file and loads tensors into pinned CPU memory. For phase-level offloading, the file path is `block_{block_idx}.safetensors`, while block-level uses the main model file.\n\n**Thread Pool Cleanup:**\nThe executor is shut down in the destructor, with futures checked for completion to prevent resource leaks [lightx2v/common/offload/manager.py:133-140]().\n\nSources: [lightx2v/common/offload/manager.py:106-141](), [lightx2v/common/ops/embedding/embedding_weight.py:98-110]()\n\n### Text Encoder Offloading\n\nText encoders (T5, CLIP) can be offloaded separately from the main transformer model to reduce peak memory usage during inference.\n\n**T5 Encoder Offload Structure**\n\nThe T5 encoder implements block-level offloading through specialized weight classes:\n\n```mermaid\ngraph TB\n    subgraph \"T5OffloadBlocksWeights\"\n        MainClass[\"T5OffloadBlocksWeights\"]\n        BlockBuffers[\"offload_block_buffers[0]\u003cbr/\u003eCUDA buffer\"]\n        CPUBuffers[\"offload_block_cpu_buffers[0]\u003cbr/\u003eCPU buffer (lazy_load only)\"]\n        Blocks[\"blocks[0..N]\u003cbr/\u003eT5OffloadSelfAttention\"]\n    end\n    \n    subgraph \"T5OffloadSelfAttention\"\n        SelfAttn[\"T5OffloadSelfAttention\"]\n        Norm1[\"norm1 (RMS)\"]\n        Norm2[\"norm2 (RMS)\"]\n        PosEmbed[\"pos_embedding\"]\n        Phases[\"compute_phases[2]\"]\n    end\n    \n    subgraph \"Compute Phases\"\n        Phase0[\"Phase 0:\u003cbr/\u003eT5OffloadAttention\"]\n        Phase1[\"Phase 1:\u003cbr/\u003eT5OffloadFeedForward\"]\n    end\n    \n    MainClass --\u003e BlockBuffers\n    MainClass --\u003e CPUBuffers\n    MainClass --\u003e Blocks\n    \n    Blocks --\u003e SelfAttn\n    SelfAttn --\u003e Norm1\n    SelfAttn --\u003e Norm2\n    SelfAttn --\u003e PosEmbed\n    SelfAttn --\u003e Phases\n    \n    Phases --\u003e Phase0\n    Phases --\u003e Phase1\n```\n\n**T5 Weight Classes:**\n- `T5OffloadBlocksWeights`: Container for all T5 encoder blocks with CPU/CUDA buffers [lightx2v/models/input_encoders/hf/wan/t5/model.py:52-68]()\n- `T5OffloadSelfAttention`: Single T5 block containing normalization, position embedding, and compute phases [lightx2v/models/input_encoders/hf/wan/t5/model.py:71-97]()\n- `T5OffloadAttention`: Q/K/V/O linear layers for attention [lightx2v/models/input_encoders/hf/wan/t5/model.py:100-120]()\n- `T5OffloadFeedForward`: FFN layers including gate, fc1, fc2 [lightx2v/models/input_encoders/hf/wan/t5/model.py:123-141]()\n\n**Configuration:**\nT5 offloading is controlled by separate parameters:\n- `t5_cpu_offload`: Enable T5 encoder offloading (boolean)\n- `t5_offload_granularity`: T5 offload granularity (\"model\", \"block\", or \"phase\")\n\nWhen `t5_cpu_offload=True`, the T5 encoder uses the same `WeightAsyncStreamManager` infrastructure as the main transformer, but with its own independent buffer management.\n\nSources: [lightx2v/models/input_encoders/hf/wan/t5/model.py:52-141](), [app/gradio_demo.py:652-829]()\n\n## Inference Pipeline Integration\n\n### WanOffloadTransformerInfer\n\nThe `WanOffloadTransformerInfer` class extends `WanTransformerInfer` to integrate offloading into the transformer inference pipeline by dynamically selecting the appropriate inference method based on configuration.\n\n**Inference Method Selection: WanOffloadTransformerInfer.__init__()**\n\n```mermaid\ngraph TD\n    Config[\"config dict\u003cbr/\u003e{cpu_offload, offload_granularity,\u003cbr/\u003eoffload_ratio, lazy_load}\"]\n    \n    Init[\"WanOffloadTransformerInfer.__init__()\"]\n    \n    CheckOffload{\"self.config.get('cpu_offload')\"}\n    \n    CheckGran{\"offload_granularity\u003cbr/\u003evalue?\"}\n    \n    CheckLazyBlock{\"lazy_load?\"}\n    CheckLazyPhase{\"lazy_load?\"}\n    \n    BlockOffload[\"self.infer_func =\u003cbr/\u003eself.infer_with_blocks_offload\"]\n    BlockLazy[\"self.infer_func =\u003cbr/\u003eself.infer_with_blocks_lazy_offload\"]\n    \n    PhaseOffload[\"self.infer_func =\u003cbr/\u003eself.infer_with_phases_offload\"]\n    PhaseLazy[\"self.infer_func =\u003cbr/\u003eself.infer_with_phases_lazy_offload\"]\n    \n    ModelLevel[\"self.infer_func =\u003cbr/\u003eself.infer_without_offload\"]\n    \n    CreateMgr[\"Create WeightAsyncStreamManager\u003cbr/\u003eor LazyWeightAsyncStreamManager\"]\n    \n    Config --\u003e Init\n    Init --\u003e CheckOffload\n    CheckOffload --\u003e|\"True\"| CheckGran\n    CheckOffload --\u003e|\"False\"| NoOffload[\"Use parent class infer\"]\n    \n    CheckGran --\u003e|\"'block'\"| CheckLazyBlock\n    CheckGran --\u003e|\"'phase'\"| CheckLazyPhase\n    CheckGran --\u003e|\"'model'\"| ModelLevel\n    \n    CheckLazyBlock --\u003e|\"False\"| BlockOffload\n    CheckLazyBlock --\u003e|\"True\"| BlockLazy\n    \n    CheckLazyPhase --\u003e|\"False\"| PhaseOffload\n    CheckLazyPhase --\u003e|\"True\"| PhaseLazy\n    \n    BlockOffload --\u003e CreateMgr\n    BlockLazy --\u003e CreateMgr\n    PhaseOffload --\u003e CreateMgr\n    PhaseLazy --\u003e CreateMgr\n```\n\n**Manager Initialization:**\nBased on the `lazy_load` flag, the appropriate stream manager is created:\n- `lazy_load=False`: Creates `WeightAsyncStreamManager(blocks_num, offload_ratio, phases_num)` [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:45-49]()\n- `lazy_load=True`: Creates `LazyWeightAsyncStreamManager(blocks_num, offload_ratio, phases_num, num_disk_workers, max_memory, offload_gra)` [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:51-58]()\n\n**Additional State:**\n- `self.offload_ratio`: Extracted from config, defaults to 1.0 [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:14-17]()\n- `self.phase_params`: Dictionary to store intermediate values during phase-level offloading [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:29-39]()\n\nSources: [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:10-59]()\n\n### Block-Level Offloading Flow\n\nBlock-level offloading processes transformer blocks sequentially using triple-buffering: one block executes on GPU while the next prefetches and the previous offloads to CPU.\n\n**Method: infer_with_blocks_offload()**\n\n```mermaid\nsequenceDiagram\n    participant Main as Main Thread\n    participant CompStream as compute_stream\n    participant CUDAStream as cuda_load_stream\n    participant CPUStream as cpu_load_stream\n    participant VRAM as GPU VRAM\n    participant RAM as CPU RAM\n    \n    Note over Main: Initialization (block_idx=0)\n    Main-\u003e\u003eMain: self.weights_stream_mgr.active_weights[0] = blocks[0]\n    Main-\u003e\u003eVRAM: blocks[0].to_cuda()\n    \n    loop For block_idx in range(len(blocks))\n        alt block_idx \u003c len(blocks) - 1\n            Main-\u003e\u003eCUDAStream: weights_stream_mgr.prefetch_weights(block_idx+1, blocks)\n            CUDAStream-\u003e\u003eVRAM: blocks[block_idx+1].to_cuda_async()\n            \n            alt block_idx \u003c offload_blocks_num\n                CPUStream-\u003e\u003eRAM: active_weights[1].to_cpu_async()\n            end\n        end\n        \n        Main-\u003e\u003eCompStream: with torch.cuda.stream(compute_stream):\n        CompStream-\u003e\u003eCompStream: x = self.infer_block(blocks[block_idx], x, pre_infer_out)\n        \n        Main-\u003e\u003eMain: weights_stream_mgr.swap_weights()\n        Note over Main: Synchronizes all streams\u003cbr/\u003eRotates active_weights[0,1,2]\n    end\n```\n\n**Detailed Code Flow:**\n\n1. **Initialization** [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:63-65]():\n   ```python\n   if block_idx == 0:\n       self.weights_stream_mgr.active_weights[0] = blocks[0]\n       self.weights_stream_mgr.active_weights[0].to_cuda()\n   ```\n\n2. **Prefetch Next Block** [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:67-68]():\n   ```python\n   if block_idx \u003c len(blocks) - 1:\n       self.weights_stream_mgr.prefetch_weights(block_idx + 1, blocks)\n   ```\n   This loads `blocks[block_idx+1]` to GPU on `cuda_load_stream` and offloads `active_weights[1]` to CPU on `cpu_load_stream` if `block_idx \u003c offload_blocks_num`\n\n3. **Execute Current Block** [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:70-71]():\n   ```python\n   with torch.cuda.stream(self.weights_stream_mgr.compute_stream):\n       x = self.infer_block(blocks[block_idx], x, pre_infer_out)\n   ```\n\n4. **Synchronize and Swap** [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:72]():\n   ```python\n   self.weights_stream_mgr.swap_weights()\n   ```\n   Calls `synchronize()` on all three streams, then rotates `active_weights` array\n\n**Memory Optimization:**\nThe `offload_ratio` parameter (set in `self.offload_ratio`) controls how many blocks are offloaded. For example, `offload_ratio=0.8` means only the first 80% of blocks are moved to CPU, while the final 20% remain on GPU to reduce transfer overhead in later layers.\n\nSources: [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:60-74](), [lightx2v/common/offload/manager.py:28-46]()\n\n### Phase-Level Offloading Flow\n\nPhase-level offloading provides finer-grained control by dividing each transformer block into 4 phases and offloading them individually, achieving ~75% memory reduction compared to block-level offloading.\n\n**Transformer Block Phase Structure**\n\n```mermaid\ngraph TB\n    subgraph \"WanTransformerAttentionBlock\"\n        Block[\"blocks[block_idx]\"]\n        Phases[\"compute_phases[]\u003cbr/\u003eWeightModuleList\"]\n    end\n    \n    subgraph \"Phase 0: Self-Attention Setup\"\n        P0[\"compute_phases[0]\"]\n        P0Mod[\"modulation (AdaLN)\"]\n        P0Norm[\"norm1 (LayerNorm)\"]\n        P0QKV[\"to_q, to_k, to_v\"]\n        P0Attn[\"self_attn_1 (FlashAttn)\"]\n        P0Out[\"to_out\"]\n    end\n    \n    subgraph \"Phase 1: Cross-Attention\"\n        P1[\"compute_phases[1]\"]\n        P1QKV[\"to_q, to_k, to_v\"]\n        P1Attn[\"cross_attn_1\"]\n    end\n    \n    subgraph \"Phase 2: FFN\"\n        P2[\"compute_phases[2]\"]\n        P2FFN0[\"ffn_0\"]\n        P2FFN2[\"ffn_2\"]\n        P2Norm[\"norm2\"]\n    end\n    \n    subgraph \"Phase 3: Post-Adapter (Model-Specific)\"\n        P3[\"compute_phases[3]\"]\n        P3Audio[\"Audio: PerceiverAttentionCA\"]\n        P3Animate[\"Animate: Motion Cross-Attn\"]\n        P3Vace[\"Vace: after_proj\"]\n    end\n    \n    Block --\u003e Phases\n    Phases --\u003e P0\n    Phases --\u003e P1\n    Phases --\u003e P2\n    Phases --\u003e P3\n    \n    P0 --\u003e P0Mod\n    P0 --\u003e P0Norm\n    P0 --\u003e P0QKV\n    P0 --\u003e P0Attn\n    P0 --\u003e P0Out\n    \n    P1 --\u003e P1QKV\n    P1 --\u003e P1Attn\n    \n    P2 --\u003e P2FFN0\n    P2 --\u003e P2FFN2\n    P2 --\u003e P2Norm\n    \n    P3 --\u003e P3Audio\n    P3 --\u003e P3Animate\n    P3 --\u003e P3Vace\n```\n\n**Method: infer_phase() - Phase Execution Logic**\n\nThe `infer_phase()` method [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:172-223]() executes each phase based on its index:\n\n| Phase | Operations | Stored State | Code Reference |\n|-------|-----------|--------------|----------------|\n| 0 | Compute modulation params, execute self-attention | `shift_msa`, `scale_msa`, `gate_msa`, `c_shift_msa`, `c_scale_msa`, `c_gate_msa`, `y_out` | [transformer_infer.py:178-197]() |\n| 1 | Execute cross-attention with context | `attn_out`, updated `x` | [transformer_infer.py:198-205]() |\n| 2 | Execute FFN, post-process, optionally save adapter hint | `y` | [transformer_infer.py:206-220]() |\n| 3 | Execute post-adapter (audio/animate specific) | Updated `x` | [transformer_infer.py:221-222]() |\n\n**Phase-Level Offload Loop:**\n\n```mermaid\nsequenceDiagram\n    participant Main as infer_phases()\n    participant Prefetch as prefetch_phase()\n    participant Compute as compute_stream\n    participant Swap as swap_phases()\n    \n    loop For each block_idx\n        loop For phase_idx in [0,1,2,3]\n            alt First phase (block=0, phase=0)\n                Main-\u003e\u003eMain: Load phase to active_weights[0]\n                Main-\u003e\u003eMain: phase.to_cuda()\n            end\n            \n            Main-\u003e\u003eCompute: with torch.cuda.stream(compute_stream):\n            Compute-\u003e\u003eCompute: x = infer_phase(active_weights[0], x, pre_infer_out)\n            \n            alt Not last phase\n                Main-\u003e\u003ePrefetch: prefetch_phase(next_block_idx, next_phase_idx, blocks)\n                Prefetch-\u003e\u003ePrefetch: Load next phase to active_weights[2]\n                Prefetch-\u003e\u003ePrefetch: Offload old phase from active_weights[1]\n            end\n            \n            Main-\u003e\u003eSwap: swap_phases()\n            Note over Swap: Synchronize streams\u003cbr/\u003eRotate active_weights\n            \n            alt clean_cuda_cache enabled\n                Main-\u003e\u003eMain: del attn_out, y_out, y\n                Main-\u003e\u003eMain: torch.cuda.empty_cache()\n            end\n        end\n    end\n```\n\n**Memory Savings:**\nPhase-level offloading keeps only 1/4 of a block on GPU at a time. For a 14B model with 28 blocks:\n- Block-level: ~500MB per block on GPU\n- Phase-level: ~125MB per phase on GPU (75% reduction)\n\nThis enables 720p generation on 8GB GPUs when combined with quantization and other optimizations.\n\nSources: [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:109-144](), [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:146-224]()\n\n### Lazy Loading Block Offload\n\nLazy loading extends block offload with disk-based weight loading.\n\n```mermaid\nsequenceDiagram\n    participant Main as Main Thread\n    participant DiskWorker as Disk Worker\n    participant PinBuffer as Pinned Memory Buffer\n    participant GPU as GPU Memory\n    \n    Note over Main: Initialization\n    Main-\u003e\u003eMain: prefetch_weights_from_disk(blocks)\n    Main-\u003e\u003eMain: _sync_prefetch_block(blocks)\n    \n    loop Fill initial buffer\n        Main-\u003e\u003ePinBuffer: Load blocks 0..N from disk\n        Note over PinBuffer: Fills to ~90% capacity\n    end\n    \n    Note over Main: Inference Loop\n    loop For each block_idx\n        alt Block in buffer\n            Main-\u003e\u003ePinBuffer: get(block_idx)\n            PinBuffer--\u003e\u003eMain: Return block\n        else Block not in buffer\n            Main-\u003e\u003eMain: Wait for disk worker\n            Note over DiskWorker: Loading in background\n            DiskWorker-\u003e\u003ePinBuffer: push(block_idx, block)\n        end\n        \n        Main-\u003e\u003eGPU: block.to_cuda_async()\n        Main-\u003e\u003eMain: Execute inference\n        Main-\u003e\u003eMain: block.to_cpu_async()\n        Main-\u003e\u003ePinBuffer: pop(old_block)\n        Main-\u003e\u003eDiskWorker: _async_prefetch_block(next)\n        DiskWorker-\u003e\u003eDiskWorker: Enqueue disk load task\n    end\n```\n\n**Key Differences from Standard Offload:**\n1. Initial buffer fill via `_sync_prefetch_block()`: [lightx2v/common/offload/manager.py:167-185]()\n2. Waiting for disk loads with timeout: [lightx2v/common/offload/manager.py:204-218]()\n3. Asynchronous prefetch after each block: [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:97]()\n4. Buffer management with pop_front: [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:94-95]()\n\nSources: [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:76-107](), [lightx2v/common/offload/manager.py:187-293]()\n\n## Configuration and Usage\n\n### Configuration Parameters\n\nThe offloading system is configured through dictionary-based configuration passed to runners and models.\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `cpu_offload` | bool | False | Enable CPU offloading for transformer |\n| `offload_granularity` | str | \"block\" | Offload unit: \"model\", \"block\", or \"phase\" |\n| `lazy_load` | bool | False | Enable disk-based lazy loading |\n| `unload_modules` | bool | False | Unload encoders/VAE after inference |\n| `num_disk_workers` | int | 4 | Number of background threads for disk I/O (lazy_load only) |\n| `clean_cuda_cache` | bool | False | Clear CUDA cache between phases |\n| `t5_cpu_offload` | bool | False | Offload T5 text encoder separately |\n| `t5_offload_granularity` | str | \"model\" | T5 encoder offload granularity |\n| `audio_adapter.cpu_offload` | bool | False | Offload audio adapter time embedding (audio models only) |\n\n**Example Configuration:**\n```python\nconfig = {\n    \"cpu_offload\": True,\n    \"offload_granularity\": \"phase\",\n    \"offload_ratio\": 1.0,\n    \"lazy_load\": True,\n    \"num_disk_workers\": 2,\n    \"max_memory\": 2,\n    \"clean_cuda_cache\": True,\n}\n```\n\nSources: [app/gradio_demo.py:469-476](), [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:13-59]()\n\n### Hardware-Based Recommendations\n\nThe Gradio auto-configuration system provides hardware-specific recommendations based on available GPU and CPU memory.\n\n#### GPU Memory Rules (14B Model, 720p)\n\n| GPU Memory | Configuration |\n|------------|---------------|\n| 80GB+ | No offloading needed |\n| 48GB | `cpu_offload=True, offload_ratio=0.5, t5_cpu_offload=True` |\n| 40GB | `cpu_offload=True, offload_ratio=0.8, t5_cpu_offload=True` |\n| 32GB | `cpu_offload=True, offload_ratio=1.0, t5_cpu_offload=True` |\n| 24GB | + `offload_granularity=\"block\", use_tiling_vae=True, precision_mode=\"bf16\"` |\n| 16GB | + `offload_granularity=\"phase\", rotary_chunk=True` |\n| 12GB | + `clean_cuda_cache=True, use_tiny_vae=True` |\n| 8GB | + `quantization (fp8/int8), lazy_load=True, unload_modules=True` |\n\n#### CPU Memory Rules (14B Model)\n\n| CPU Memory | Configuration |\n|------------|---------------|\n| 128GB+ | No special configuration |\n| 64GB | Enable quantization |\n| 32GB | + `lazy_load=True` |\n| 16GB | + `unload_modules=True`, full quantization (DIT, T5, CLIP) |\n\nSources: [app/gradio_demo.py:652-829]()\n\n### Model-Specific Offloading Integration\n\nDifferent model variants integrate offloading with specialized requirements based on their architecture.\n\n#### WanAudioModel Integration\n\nThe `WanAudioModel` class includes audio adapter components that require dynamic offloading during inference.\n\n**Audio Adapter Scheduler Offloading:**\nThe audio scheduler manages CPU offloading of the audio adapter's time embedding layer:\n\n```python\n# In EulerScheduler.step_pre()\nif self.audio_adapter.cpu_offload:\n    self.audio_adapter.time_embedding.to(AI_DEVICE)\nself.audio_adapter_t_emb = self.audio_adapter.time_embedding(self.timestep_input)\nif self.audio_adapter.cpu_offload:\n    self.audio_adapter.time_embedding.to(\"cpu\")\n```\n\nThis pattern moves the time embedding to GPU only when needed, then immediately moves it back to CPU to free VRAM [lightx2v/models/schedulers/wan/audio/scheduler.py:33-37]().\n\n**Post-Adapter Phase Integration:**\nAudio models set `self.has_post_adapter = True` and `self.phases_num = 4` to enable phase 3 (post-adapter) offloading [lightx2v/models/networks/wan/infer/audio/transformer_infer.py:21-22]().\n\nPhase 3 implements `perceiver_attention_ca()` which performs cross-attention between audio encoder outputs and video latents. The phase contains:\n- `norm_kv`: Normalizes audio encoder output\n- `norm_q`: Normalizes latent queries\n- `to_q`, `to_kv`: Linear projections for Q, K, V\n- `to_out`: Output projection\n- `shift_scale_gate`: Time-based modulation parameters\n\nSources: [lightx2v/models/schedulers/wan/audio/scheduler.py:31-37](), [lightx2v/models/networks/wan/infer/audio/transformer_infer.py:18-112]()\n\n#### WanAnimateModel Integration\n\nThe `WanAnimateModel` uses motion and face encoders with post-adapter phases.\n\n**Post-Adapter Offloading:**\nThe animation model sets `self.has_post_adapter = True` and `self.phases_num = 4` to enable phase 3 offloading [lightx2v/models/networks/wan/infer/animate/transformer_infer.py:8-11]().\n\nPhase 3 executes motion cross-attention:\n- Normalizes motion features: `pre_norm_motion.apply(motion_vec)` \n- Cross-attention between frame features and motion: Uses `adapter_attn.apply()` with FlashAttention\n- Projects output: `linear2.apply(attn)`\n\nSources: [lightx2v/models/networks/wan/infer/animate/transformer_infer.py:7-39]()\n\n#### WanVaceModel Integration\n\nThe `WanVaceModel` includes separate VACE blocks for video context encoding.\n\n**Dual Block Offloading:**\nThe model maintains two sets of blocks:\n1. **VACE blocks** (`self.vace_blocks`): Process video context, offloaded with `self.infer_vace_blocks()` [lightx2v/models/networks/wan/infer/vace/transformer_infer.py:22-30]()\n2. **Main blocks** (`self.blocks`): Standard transformer blocks\n\nThe stream manager is reinitialized between VACE and main block processing:\n```python\n# After VACE blocks\nif hasattr(self, \"weights_stream_mgr\"):\n    self.weights_stream_mgr.init(self.blocks_num, self.phases_num, self.offload_ratio)\n```\n\nSources: [lightx2v/models/networks/wan/infer/vace/transformer_infer.py:5-38](), [lightx2v/models/networks/wan/weights/vace/transformer_weights.py:12-39]()\n\n## Memory Management Flow\n\n### Complete Offload Lifecycle\n\n```mermaid\nstateDiagram-v2\n    [*] --\u003e Initialization\n    \n    Initialization --\u003e DiskStorage: lazy_load=True\n    Initialization --\u003e CPUMemory: lazy_load=False\n    \n    state DiskStorage {\n        [*] --\u003e DiskFile\n        DiskFile --\u003e DiskWorker: load_from_disk()\n        DiskWorker --\u003e PinnedBuffer: push()\n    }\n    \n    state CPUMemory {\n        [*] --\u003e CPUTensor\n    }\n    \n    PinnedBuffer --\u003e Prefetch: get()\n    CPUTensor --\u003e Prefetch: from blocks\n    \n    state Prefetch {\n        [*] --\u003e CUDALoadStream\n        CUDALoadStream --\u003e AsyncTransfer: to_cuda_async()\n    }\n    \n    AsyncTransfer --\u003e GPUActive\n    \n    state GPUActive {\n        [*] --\u003e ComputeStream\n        ComputeStream --\u003e Inference: infer_block()\n    }\n    \n    GPUActive --\u003e Offload: swap_weights()\n    \n    state Offload {\n        [*] --\u003e CPULoadStream\n        CPULoadStream --\u003e AsyncReturn: to_cpu_async()\n    }\n    \n    Offload --\u003e CPUMemory: offload_ratio check\n    Offload --\u003e Eviction: lazy_load=True\n    \n    state Eviction {\n        [*] --\u003e PopBuffer: pop()\n        PopBuffer --\u003e Release\n    }\n    \n    Eviction --\u003e [*]\n    GPUActive --\u003e [*]: Final block\n```\n\n**State Transitions:**\n1. **Initialization**: Model weights loaded from disk or kept in memory\n2. **Prefetch**: Next block/phase moved to GPU asynchronously\n3. **Active Computation**: Current block executes on GPU\n4. **Offload**: Previous block moved back to CPU or evicted from buffer\n5. **Eviction**: Buffer space reclaimed for new prefetches (lazy load only)\n\nSources: [lightx2v/common/offload/manager.py:11-380](), [lightx2v/models/networks/wan/infer/offload/transformer_infer.py:10-242]()\n\n### Stream Synchronization Points\n\n```mermaid\ngraph TB\n    subgraph \"Time Step T\"\n        T_Comp[\"Compute Stream:\u003cbr/\u003eExecute Block N\"]\n        T_CUDA[\"CUDA Load Stream:\u003cbr/\u003eLoad Block N+1\"]\n        T_CPU[\"CPU Load Stream:\u003cbr/\u003eOffload Block N-1\"]\n    end\n    \n    subgraph \"Synchronization\"\n        Sync[\"swap_weights()\"]\n        SyncComp[\"compute_stream.synchronize()\"]\n        SyncCUDA[\"cuda_load_stream.synchronize()\"]\n        SyncCPU[\"cpu_load_stream.synchronize()\"]\n    end\n    \n    subgraph \"Time Step T+1\"\n        T1_Comp[\"Compute Stream:\u003cbr/\u003eExecute Block N+1\"]\n        T1_CUDA[\"CUDA Load Stream:\u003cbr/\u003eLoad Block N+2\"]\n        T1_CPU[\"CPU Load Stream:\u003cbr/\u003eOffload Block N\"]\n    end\n    \n    T_Comp --\u003e Sync\n    T_CUDA --\u003e Sync\n    T_CPU --\u003e Sync\n    \n    Sync --\u003e SyncComp\n    Sync --\u003e SyncCUDA\n    Sync --\u003e SyncCPU\n    \n    SyncComp --\u003e T1_Comp\n    SyncCUDA --\u003e T1_CUDA\n    SyncCPU --\u003e T1_CPU\n```\n\n**Synchronization Guarantees:**\n- All streams synchronized before pointer swap: [lightx2v/common/offload/manager.py:38-40]()\n- Ensures data consistency across stream boundaries\n- Prevents race conditions on weight tensors\n- Minimizes synchronization overhead by grouping operations\n\nSources: [lightx2v/common/offload/manager.py:37-46](), [lightx2v/common/offload/manager.py:58-63]()\n\n### Error Handling and Timeouts\n\nThe lazy loading system includes timeout mechanisms to detect stalled disk operations:\n\n**Timeout Logic:**\n```python\nstart_time = time.time()\nwhile not self.pin_memory_buffer.exists(obj_key):\n    time.sleep(0.001)\n    if time.time() - start_time \u003e 5:\n        raise TimeoutError(f\"Load timeout: block={block_idx}\")\n```\n\n**Error Scenarios:**\n1. **Disk I/O timeout**: 5-15 second timeouts for disk loads\n2. **Missing prefetch task**: Fallback to synchronous loading\n3. **Buffer overflow**: LRU eviction prevents memory exhaustion\n4. **Worker thread failure**: Logged but doesn't crash inference\n\nSources: [lightx2v/common/offload/manager.py:204-218](), [lightx2v/common/offload/manager.py:242-256]()"])</script><script>self.__next_f.push([1,"3c:T56f4,"])</script><script>self.__next_f.push([1,"# Feature Caching and Streaming\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n\n\u003c/details\u003e\n\n\n\nFeature caching and streaming are performance optimization techniques that reduce computation and memory requirements during video generation inference. Feature caching skips redundant transformer block computations by reusing cached intermediate features across denoising steps, while streaming decoding processes VAE output in chunks to reduce peak memory usage and enable real-time generation.\n\nFor model quantization techniques, see [Quantization System](#6.1). For memory offloading strategies, see [Memory Management and CPU Offloading](#6.3).\n\n## Overview and Purpose\n\nLightX2V implements two complementary optimization strategies:\n\n1. **Feature Caching**: Exploits temporal redundancy in the diffusion denoising process by caching and reusing intermediate transformer features across steps\n2. **Streaming Decode**: Processes VAE decoder output in temporal chunks rather than as a complete batch\n\nThese optimizations are particularly important for long-form video generation where memory constraints and inference latency are critical concerns.\n\n**Key Benefits**:\n- Reduced computation through selective feature reuse (2-3x speedup typical)\n- Lower peak memory consumption during VAE decode\n- Enables real-time streaming generation for audio-to-video tasks\n- Graceful quality vs speed trade-offs\n\nSources: [lightx2v/models/networks/wan/model.py:60-79]()\n\n## Feature Caching Strategies\n\n### Caching Strategy Architecture\n\n```mermaid\ngraph TB\n    Config[\"feature_caching config\u003cbr/\u003e(Tea, Mag, Taylor, etc)\"]\n    \n    subgraph \"Strategy Selection\"\n        ModelInit[\"WanModel._init_infer_class()\"]\n        StrategyMap[\"Strategy  Class Mapping\"]\n    end\n    \n    subgraph \"Caching Implementations\"\n        NoCaching[\"WanTransformerInfer\u003cbr/\u003e(baseline, no caching)\"]\n        Tea[\"WanTransformerInferTeaCaching\"]\n        Mag[\"WanTransformerInferMagCaching\"]\n        Taylor[\"WanTransformerInferTaylorCaching\"]\n        Ada[\"WanTransformerInferAdaCaching\"]\n        FirstBlock[\"WanTransformerInferFirstBlock\"]\n        DynamicBlock[\"WanTransformerInferDynamicBlock\"]\n        Custom[\"WanTransformerInferCustomCaching\"]\n    end\n    \n    subgraph \"Execution Flow\"\n        InferBlock[\"infer_block()\u003cbr/\u003eFor each transformer block\"]\n        CacheDecision{\"Should cache\u003cbr/\u003ethis block/step?\"}\n        ComputeFull[\"Compute full\u003cbr/\u003eblock inference\"]\n        ReuseCache[\"Reuse cached\u003cbr/\u003efeatures\"]\n        UpdateCache[\"Update cache\u003cbr/\u003estorage\"]\n    end\n    \n    Config --\u003e ModelInit\n    ModelInit --\u003e StrategyMap\n    \n    StrategyMap --\u003e NoCaching\n    StrategyMap --\u003e Tea\n    StrategyMap --\u003e Mag\n    StrategyMap --\u003e Taylor\n    StrategyMap --\u003e Ada\n    StrategyMap --\u003e FirstBlock\n    StrategyMap --\u003e DynamicBlock\n    StrategyMap --\u003e Custom\n    \n    Tea --\u003e InferBlock\n    Mag --\u003e InferBlock\n    Taylor --\u003e InferBlock\n    \n    InferBlock --\u003e CacheDecision\n    CacheDecision --\u003e|\"Yes, cache\"| ComputeFull\n    CacheDecision --\u003e|\"No, reuse\"| ReuseCache\n    ComputeFull --\u003e UpdateCache\n```\n\n**Strategy Selection in WanModel**\n\nSources: [lightx2v/models/networks/wan/model.py:56-79]()\n\n### Available Caching Strategies\n\n| Strategy | Class | Description | Use Case |\n|----------|-------|-------------|----------|\n| **NoCaching** | `WanTransformerInfer` | Baseline with no caching | Full quality, maximum computation |\n| **Tea** | `WanTransformerInferTeaCaching` | Time-aware caching | General purpose speedup |\n| **Mag** | `WanTransformerInferMagCaching` | Magnitude-based caching | Adaptive quality preservation |\n| **TaylorSeer** | `WanTransformerInferTaylorCaching` | Taylor expansion prediction | Predictive caching |\n| **Ada** | `WanTransformerInferAdaCaching` | Adaptive caching | Dynamic cache policy |\n| **Custom** | `WanTransformerInferCustomCaching` | User-defined policy | Experimental tuning |\n| **FirstBlock** | `WanTransformerInferFirstBlock` | Cache first blocks only | Fast approximation |\n| **DualBlock** | `WanTransformerInferDualBlock` | Two-tier block caching | Balanced approach |\n| **DynamicBlock** | `WanTransformerInferDynamicBlock` | Runtime block selection | Adaptive per-step |\n\nSources: [lightx2v/models/networks/wan/model.py:60-79]()\n\n### Configuration\n\nFeature caching is configured through the `feature_caching` parameter:\n\n```python\nconfig = {\n    \"feature_caching\": \"Tea\",  # Choose strategy\n    # ... other config\n}\n```\n\nThe strategy is selected during model initialization in `WanModel._init_infer_class()`:\n\n```python\nif self.config[\"feature_caching\"] == \"NoCaching\":\n    self.transformer_infer_class = WanTransformerInfer\nelif self.config[\"feature_caching\"] == \"Tea\":\n    self.transformer_infer_class = WanTransformerInferTeaCaching\n# ... etc\n```\n\nSources: [lightx2v/models/networks/wan/model.py:60-79]()\n\n### Scheduler Integration\n\nThe `WanScheduler` maintains caching state through `caching_records_2`:\n\n```python\nself.caching_records_2 = [True] * self.config[\"infer_steps\"]\n```\n\nThis tracks which denoising steps should compute fresh features vs reuse cached ones.\n\nSources: [lightx2v/models/schedulers/wan/scheduler.py:28]()\n\n## VAE Streaming Decode\n\n### Streaming Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Input Latents\"\n        Latents[\"Latents Tensor\u003cbr/\u003e[C, T, H, W]\u003cbr/\u003eT = temporal frames\"]\n    end\n    \n    subgraph \"Streaming Decode Flow\"\n        DecodeStream[\"decode_stream(latents)\"]\n        ChunkLoop[\"For each temporal chunk\"]\n        \n        subgraph \"Per-Chunk Processing\"\n            FeatCache[\"feat_cache dictionary\u003cbr/\u003eStores inter-chunk state\"]\n            FeatIdx[\"feat_idx counter\u003cbr/\u003eTracks cache position\"]\n            \n            ResampleCache[\"Resample.forward()\u003cbr/\u003ewith feat_cache\"]\n            ResBlockCache[\"ResidualBlock.forward()\u003cbr/\u003ewith feat_cache\"]\n            \n            CacheUpdate[\"Update feat_cache\u003cbr/\u003ewith boundary frames\"]\n        end\n        \n        YieldChunk[\"Yield decoded chunk\u003cbr/\u003e[C, chunk_T, H_out, W_out]\"]\n    end\n    \n    subgraph \"Output Stream\"\n        VideoStream[\"Stream of frame chunks\u003cbr/\u003eProcessed incrementally\"]\n    end\n    \n    Latents --\u003e DecodeStream\n    DecodeStream --\u003e ChunkLoop\n    ChunkLoop --\u003e FeatCache\n    ChunkLoop --\u003e FeatIdx\n    \n    FeatCache --\u003e ResampleCache\n    FeatCache --\u003e ResBlockCache\n    \n    ResampleCache --\u003e CacheUpdate\n    ResBlockCache --\u003e CacheUpdate\n    CacheUpdate --\u003e YieldChunk\n    \n    YieldChunk --\u003e VideoStream\n    YieldChunk -.next chunk.-\u003e ChunkLoop\n```\n\n**Streaming Decode Process**\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:285-348](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py:285-348]()\n\n### Temporal Caching Mechanism\n\nThe VAE decoder uses a temporal cache to maintain continuity across chunks:\n\n**Key Constants and Variables**:\n- `CACHE_T = 2`: Number of frames cached at chunk boundaries\n- `feat_cache`: Dictionary storing cached features per layer\n- `feat_idx`: Counter tracking cache position during layer traversal\n\n```python\nCACHE_T = 2  # Cache last 2 frames between chunks\n```\n\n**Cache Flow in Layers**:\n\n1. **Resample Layer** (upsampling/downsampling):\n   - Caches boundary frames to maintain temporal consistency\n   - Handles both spatial upsampling (upsample3d) and downsampling (downsample3d)\n   \n2. **ResidualBlock**:\n   - Each `CausalConv3d` layer stores `CACHE_T` frames\n   - Concatenates cached frames with current chunk input\n   - Updates cache with new boundary frames\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:21-22, 122-176, 220-241](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py:19-20, 119-175, 220-241]()\n\n### Streaming Implementation Details\n\n#### VAE Decoder Streaming Method\n\nThe `decode_stream()` method in VAE classes yields chunks progressively:\n\n```mermaid\ngraph LR\n    InitCache[\"1. Initialize\u003cbr/\u003efeat_cache = {}\"]\n    PrepareChunks[\"2. Prepare\u003cbr/\u003etemporal chunks\"]\n    ProcessFirst[\"3. Process\u003cbr/\u003efirst chunk\"]\n    ProcessMiddle[\"4. Process\u003cbr/\u003emiddle chunks\"]\n    ProcessLast[\"5. Process\u003cbr/\u003elast chunk\"]\n    \n    InitCache --\u003e PrepareChunks\n    PrepareChunks --\u003e ProcessFirst\n    ProcessFirst --\u003e ProcessMiddle\n    ProcessMiddle --\u003e ProcessLast\n    \n    ProcessFirst -.yield.-\u003e Out1[\"Chunk 1\"]\n    ProcessMiddle -.yield.-\u003e Out2[\"Chunk 2...N-1\"]\n    ProcessLast -.yield.-\u003e Out3[\"Chunk N\"]\n```\n\n**Decode Stream Signature**:\n```python\ndef decode_stream(\n    self, \n    latents: torch.Tensor,\n    is_init_image: bool = True\n) -\u003e Iterator[torch.Tensor]:\n    \"\"\"\n    Yields decoded video chunks progressively\n    \n    Args:\n        latents: Latent tensor [C, T, H, W]\n        is_init_image: Whether first frame is initialization\n        \n    Yields:\n        Decoded chunks [C, chunk_T, H_out, W_out]\n    \"\"\"\n```\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:285-348](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py:285-348]()\n\n#### Runner Integration\n\nRunners invoke streaming decode through `run_vae_decoder_stream()`:\n\n```python\ndef run_vae_decoder_stream(self, latents):\n    \"\"\"Stream VAE decoder output\"\"\"\n    if self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n        self.vae_decoder = self.load_vae_decoder()\n    \n    for frame_segment in self.vae_decoder.decode_stream(latents.to(GET_DTYPE())):\n        yield frame_segment\n    \n    if self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n        del self.vae_decoder\n        torch_device_module.empty_cache()\n        gc.collect()\n```\n\nSources: [lightx2v/models/runners/default_runner.py:404-415]()\n\n### Cached Decode with Flags\n\nFor more precise control, `WanAudioRunner` provides `run_vae_cached_decoder_withflag()`:\n\n```python\ndef run_vae_cached_decoder_withflag(\n    self, \n    latents, \n    is_first: bool, \n    is_last: bool\n):\n    \"\"\"\n    Decode with explicit first/last chunk flags\n    \n    Args:\n        latents: Latent tensor\n        is_first: Is this the first chunk?\n        is_last: Is this the last chunk?\n    \"\"\"\n    if self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n        self.vae_decoder = self.load_vae_decoder()\n    \n    images = self.vae_decoder.cached_decode_withflag(\n        latents.to(GET_DTYPE()), \n        is_first, \n        is_last\n    )\n    \n    if self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n        del self.vae_decoder\n        torch_device_module.empty_cache()\n        gc.collect()\n    \n    return images\n```\n\nThis enables fine-grained control over cache initialization and finalization.\n\nSources: [lightx2v/models/runners/wan/wan_audio_runner.py:798-807]()\n\n## Audio-to-Video Streaming Pipeline\n\n### Real-Time Generation Architecture\n\nThe `WanAudioRunner` implements end-to-end streaming for audio-to-video generation:\n\n```mermaid\ngraph TB\n    subgraph \"Audio Input Stream\"\n        VAController[\"VAController\u003cbr/\u003eAudio reader\"]\n        AudioSegment[\"get_audio_segment()\u003cbr/\u003eReturns audio chunk\"]\n    end\n    \n    subgraph \"Generation Loop\"\n        InitSegment[\"init_run_segment()\u003cbr/\u003eEncode audio chunk\"]\n        RunSegment[\"run_segment()\u003cbr/\u003eDiT inference\"]\n        \n        StreamDecision{\"use_stream_vae?\"}\n        \n        EndSegmentStream[\"end_run_segment_stream()\u003cbr/\u003eStream processing\"]\n        EndSegmentBatch[\"end_run_segment()\u003cbr/\u003eBatch processing\"]\n        \n        UpdatePrev[\"Update prev_video\u003cbr/\u003efor next segment\"]\n    end\n    \n    subgraph \"Streaming Output\"\n        StreamChunks[\"For chunk in decode_stream()\"]\n        PublishStream[\"va_controller.pub_livestream()\u003cbr/\u003ePublish frame chunk\"]\n    end\n    \n    VAController --\u003e AudioSegment\n    AudioSegment --\u003e InitSegment\n    InitSegment --\u003e RunSegment\n    RunSegment --\u003e StreamDecision\n    \n    StreamDecision --\u003e|\"Yes\"| EndSegmentStream\n    StreamDecision --\u003e|\"No\"| EndSegmentBatch\n    \n    EndSegmentStream --\u003e StreamChunks\n    StreamChunks --\u003e PublishStream\n    PublishStream --\u003e UpdatePrev\n    \n    EndSegmentBatch --\u003e UpdatePrev\n    UpdatePrev -.next segment.-\u003e AudioSegment\n```\n\n**Streaming Generation Main Loop**\n\nSources: [lightx2v/models/runners/wan/wan_audio_runner.py:661-732]()\n\n### Streaming Segment Processing\n\nThe `end_run_segment_stream()` method processes VAE output incrementally:\n\n```python\ndef end_run_segment_stream(self, latents, valid_duration=1e9):\n    \"\"\"\n    Process VAE decoder output as stream\n    \n    Args:\n        latents: Generated latents from DiT\n        valid_duration: Valid duration in seconds for timing control\n    \"\"\"\n    valid_length = self.segment.end_frame - self.segment.start_frame\n    frame_segments = []\n    frame_idx = 0\n    \n    # Stream decode: frame_segment is 1*C*1*H*W or 1*C*4*H*W\n    for origin_seg in self.run_vae_decoder_stream(latents):\n        origin_seg = torch.clamp(origin_seg, -1, 1).to(torch.float)\n        valid_T = min(valid_length - frame_idx, origin_seg.shape[2])\n        \n        # Convert to ComfyUI format\n        video_seg = wan_vae_to_comfy(origin_seg[:, :, :valid_T].cpu())\n        \n        # Extract corresponding audio\n        audio_start = frame_idx * self._audio_processor.audio_frame_rate\n        audio_end = (frame_idx + valid_T) * self._audio_processor.audio_frame_rate\n        audio_seg = self.segment.audio_array[:, audio_start:audio_end].sum(dim=0)\n        \n        # Publish to live stream\n        if self.va_controller.recorder is not None:\n            self.va_controller.pub_livestream(\n                video_seg, \n                audio_seg, \n                origin_seg[:, :, :valid_T], \n                valid_duration=valid_duration\n            )\n        \n        frame_segments.append(origin_seg)\n        frame_idx += valid_T\n        cur_duration = valid_T / self.config.get(\"fps\", 16)\n        valid_duration = max(valid_duration - cur_duration, 0)\n        \n        del video_seg, audio_seg\n    \n    # Concatenate for prev_video context\n    self.prev_video = torch.cat(frame_segments, dim=2)\n    torch.cuda.empty_cache()\n```\n\n**Key Features**:\n- Processes each decoded chunk immediately\n- Synchronizes audio with video chunks\n- Publishes to live stream without waiting for full segment\n- Maintains `prev_video` for temporal consistency\n\nSources: [lightx2v/models/runners/wan/wan_audio_runner.py:632-660]()\n\n### Configuration Options\n\nEnable streaming through configuration:\n\n```python\nconfig = {\n    \"use_stream_vae\": True,  # Enable streaming decode\n    \"target_video_length\": 81,  # Total frames per segment\n    \"fps\": 16,  # Frame rate for timing\n    # ... other config\n}\n```\n\nSources: [lightx2v/models/runners/wan/wan_audio_runner.py:714-719]()\n\n## Performance Trade-offs\n\n### Feature Caching Trade-offs\n\n| Aspect | Aggressive Caching (e.g., FirstBlock) | Balanced (e.g., Tea) | Minimal (NoCaching) |\n|--------|---------------------------------------|----------------------|---------------------|\n| **Speed** |  2-3x faster |  1.5-2x faster |  Baseline |\n| **Quality** |  Some artifacts possible |  Near-identical |  Perfect |\n| **Memory** |  +Cache overhead |  +Cache overhead |  Baseline |\n| **Use Case** | Preview, draft | Production | Quality-critical |\n\n### Streaming Decode Trade-offs\n\n| Aspect | With Streaming | Without Streaming |\n|--------|---------------|-------------------|\n| **Peak Memory** |  ~Chunk size |  ~Full video |\n| **Latency** |  First chunk immediately |  Wait for complete decode |\n| **Throughput** |  Similar overall time |  Similar overall time |\n| **Real-time** |  Enables live generation |  Batch only |\n| **Overhead** | +Cache management | None |\n\n**Memory Savings Example**:\n- Video: 100 frames, 1280720\n- Without streaming: ~3.3 GB peak for full decode\n- With streaming (chunks of 4 frames): ~132 MB peak per chunk\n- **Memory reduction: ~25x**\n\nSources: [lightx2v/models/video_encoders/hf/wan/vae.py:285-348]()\n\n## Implementation Code Map\n\n### Key Classes and Methods\n\n```mermaid\ngraph TB\n    subgraph \"Caching System\"\n        WanModel[\"WanModel\u003cbr/\u003e_init_infer_class()\"]\n        TeaCaching[\"WanTransformerInferTeaCaching\"]\n        MagCaching[\"WanTransformerInferMagCaching\"]\n        TaylorCaching[\"WanTransformerInferTaylorCaching\"]\n        CustomCaching[\"WanTransformerInferCustomCaching\"]\n    end\n    \n    subgraph \"Streaming System\"\n        VAE[\"WanVAE / Wan2_2_VAE\"]\n        DecodeStream[\"decode_stream()\"]\n        CachedDecodeFlag[\"cached_decode_withflag()\"]\n        \n        Resample[\"Resample.forward()\u003cbr/\u003efeat_cache handling\"]\n        ResBlock[\"ResidualBlock.forward()\u003cbr/\u003efeat_cache handling\"]\n    end\n    \n    subgraph \"Runner Integration\"\n        DefaultRunner[\"DefaultRunner\u003cbr/\u003erun_vae_decoder_stream()\"]\n        AudioRunner[\"WanAudioRunner\u003cbr/\u003eend_run_segment_stream()\"]\n        CachedRunner[\"WanAudioRunner\u003cbr/\u003erun_vae_cached_decoder_withflag()\"]\n    end\n    \n    subgraph \"Scheduler\"\n        WanScheduler[\"WanScheduler\u003cbr/\u003ecaching_records_2\"]\n    end\n    \n    WanModel --\u003e TeaCaching\n    WanModel --\u003e MagCaching\n    WanModel --\u003e TaylorCaching\n    WanModel --\u003e CustomCaching\n    \n    VAE --\u003e DecodeStream\n    VAE --\u003e CachedDecodeFlag\n    DecodeStream --\u003e Resample\n    DecodeStream --\u003e ResBlock\n    \n    DefaultRunner --\u003e DecodeStream\n    AudioRunner --\u003e DecodeStream\n    CachedRunner --\u003e CachedDecodeFlag\n    \n    TeaCaching -.uses.-\u003e WanScheduler\n```\n\n**Code Entity Reference Map**\n\nSources: [lightx2v/models/networks/wan/model.py:56-84](), [lightx2v/models/video_encoders/hf/wan/vae.py:21-348](), [lightx2v/models/runners/default_runner.py:404-415](), [lightx2v/models/runners/wan/wan_audio_runner.py:632-660, 798-807]()\n\n### File Organization\n\n| Component | File Path | Key Classes/Functions |\n|-----------|-----------|----------------------|\n| **Caching Strategy Selection** | `lightx2v/models/networks/wan/model.py` | `WanModel._init_infer_class()` |\n| **Caching Implementations** | `lightx2v/models/networks/wan/infer/feature_caching/` | `WanTransformerInferTeaCaching`, `WanTransformerInferMagCaching`, etc. |\n| **VAE Streaming** | `lightx2v/models/video_encoders/hf/wan/vae.py` | `WanVAE.decode_stream()`, `Resample`, `ResidualBlock` |\n| **VAE Streaming (2.2)** | `lightx2v/models/video_encoders/hf/wan/vae_2_2.py` | `Wan2_2_VAE.decode_stream()` |\n| **Runner Streaming** | `lightx2v/models/runners/default_runner.py` | `run_vae_decoder_stream()` |\n| **Audio Streaming** | `lightx2v/models/runners/wan/wan_audio_runner.py` | `end_run_segment_stream()`, `run_vae_cached_decoder_withflag()` |\n| **Scheduler State** | `lightx2v/models/schedulers/wan/scheduler.py` | `WanScheduler.caching_records_2` |\n\nSources: [lightx2v/models/networks/wan/model.py:1-10](), [lightx2v/models/video_encoders/hf/wan/vae.py:1-20](), [lightx2v/models/runners/default_runner.py:404-415](), [lightx2v/models/runners/wan/wan_audio_runner.py:1-30]()\n\n## Usage Examples\n\n### Enabling Feature Caching\n\n```python\nfrom lightx2v.utils.set_config import set_config\n\n# Configure with Tea caching strategy\nconfig = set_config({\n    \"model_cls\": \"wan2.1\",\n    \"feature_caching\": \"Tea\",  # Options: NoCaching, Tea, Mag, TaylorSeer, Ada, etc.\n    # ... other config\n})\n\nrunner = RUNNER_REGISTER[config[\"model_cls\"]](config)\nrunner.init_modules()\n```\n\n### Enabling Streaming Decode\n\n```python\n# Enable in configuration\nconfig = {\n    \"use_stream_vae\": True,\n    \"target_video_length\": 81,\n    \"fps\": 16,\n}\n\n# In runner, decode will automatically stream\nif config.get(\"use_stream_vae\", False):\n    frames = []\n    for frame_segment in runner.run_vae_decoder_stream(latents):\n        frames.append(frame_segment)\n        # Process each chunk immediately\n    gen_video = torch.cat(frames, dim=2)\nelse:\n    gen_video = runner.run_vae_decoder(latents)\n```\n\n### Audio-to-Video Streaming\n\n```python\n# Streaming is automatically enabled when VAController has a reader\n# In WanAudioRunner.run_main():\nif self.va_controller.reader is None:\n    # Standard fixed-segment processing\n    return super().run_main()\n\n# Otherwise, continuous streaming loop\nself.va_controller.start()\nwhile True:\n    audio_array, valid_duration = self.va_controller.reader.get_audio_segment()\n    \n    self.init_run_segment(segment_idx, audio_array)\n    latents = self.run_segment(segment_idx)\n    \n    if self.config.get(\"use_stream_vae\", False):\n        self.end_run_segment_stream(latents, valid_duration=valid_duration)\n    else:\n        self.gen_video = self.run_vae_decoder(latents)\n        self.end_run_segment(segment_idx, valid_duration=valid_duration)\n    \n    segment_idx += 1\n```\n\nSources: [lightx2v/infer.py:35-40](), [lightx2v/models/runners/default_runner.py:361-391](), [lightx2v/models/runners/wan/wan_audio_runner.py:661-732]()"])</script><script>self.__next_f.push([1,"3d:T5b29,"])</script><script>self.__next_f.push([1,"# Distributed and Parallel Inference\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json](configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json)\n- [docs/EN/source/getting_started/quickstart.md](docs/EN/source/getting_started/quickstart.md)\n- [docs/ZH_CN/source/getting_started/quickstart.md](docs/ZH_CN/source/getting_started/quickstart.md)\n- [lightx2v/__init__.py](lightx2v/__init__.py)\n- [lightx2v/common/modules/weight_module.py](lightx2v/common/modules/weight_module.py)\n- [lightx2v/common/ops/attn/utils/ring_comm.py](lightx2v/common/ops/attn/utils/ring_comm.py)\n- [lightx2v/common/ops/mm/triton_kernels.py](lightx2v/common/ops/mm/triton_kernels.py)\n- [lightx2v/pipeline.py](lightx2v/pipeline.py)\n- [lightx2v/server/__main__.py](lightx2v/server/__main__.py)\n- [lightx2v/server/config.py](lightx2v/server/config.py)\n- [lightx2v/server/schema.py](lightx2v/server/schema.py)\n- [lightx2v/server/services/distributed_utils.py](lightx2v/server/services/distributed_utils.py)\n- [lightx2v/server/services/generation/base.py](lightx2v/server/services/generation/base.py)\n- [lightx2v/server/services/generation/image.py](lightx2v/server/services/generation/image.py)\n- [lightx2v/server/services/inference/worker.py](lightx2v/server/services/inference/worker.py)\n- [pyproject.toml](pyproject.toml)\n- [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh](scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh)\n- [scripts/server/post_i2i_with_lora.py](scripts/server/post_i2i_with_lora.py)\n- [scripts/server/start_server_i2i_with_loradir.sh](scripts/server/start_server_i2i_with_loradir.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis page documents the distributed and parallel inference capabilities in LightX2V, which enable scaling inference across multiple GPUs to handle larger models, longer sequences, and higher throughput. The system supports multiple parallelism strategies that can be combined to optimize for different hardware configurations and workload characteristics.\n\nThis page covers:\n- Parallelism types (CFG, Sequence, Tensor, Pipeline)\n- Configuration and initialization\n- Communication patterns and distributed coordination\n- Multi-worker distributed server architecture\n\nFor memory optimization techniques like CPU offloading, see [Memory Management and CPU Offloading](#6.3). For attention operator implementations, see [Attention Operators and Sparse Patterns](#6.2).\n\n---\n\n## Parallelism Types Overview\n\nLightX2V implements four complementary parallelism strategies that can be combined to scale inference across multiple GPUs:\n\n```mermaid\ngraph TB\n    subgraph \"Parallelism Strategies\"\n        CFG[\"CFG Parallel\u003cbr/\u003ecfg_p_size\u003cbr/\u003eSplit conditional/unconditional\"]\n        SEQ[\"Sequence Parallel\u003cbr/\u003eseq_p_size\u003cbr/\u003eSplit sequence dimension\u003cbr/\u003eUlysses, Ring\"]\n        TENSOR[\"Tensor Parallel\u003cbr/\u003eSplit model weights\"]\n        PIPE[\"Pipeline Parallel\u003cbr/\u003eStage-based execution\"]\n    end\n    \n    subgraph \"Use Cases\"\n        CFG_USE[\"Reduce CFG memory\u003cbr/\u003e2x throughput for CFG\"]\n        SEQ_USE[\"Handle long sequences\u003cbr/\u003eVideo generation\"]\n        TENSOR_USE[\"Large model inference\u003cbr/\u003eSplit weights across GPUs\"]\n        PIPE_USE[\"Multi-stage models\u003cbr/\u003eLatency optimization\"]\n    end\n    \n    CFG --\u003e CFG_USE\n    SEQ --\u003e SEQ_USE\n    TENSOR --\u003e TENSOR_USE\n    PIPE --\u003e PIPE_USE\n```\n\n| Parallelism Type | Configuration Parameter | Primary Benefit | Communication Pattern |\n|------------------|------------------------|-----------------|----------------------|\n| **CFG Parallel** | `cfg_p_size` | Eliminates CFG memory overhead | Minimal (independent streams) |\n| **Sequence Parallel** | `seq_p_size`, `seq_p_attn_type` | Handles longer sequences | All-to-all (Ulysses), Ring (Ring) |\n| **Tensor Parallel** | Not directly exposed | Splits large models | All-reduce after each layer |\n| **Pipeline Parallel** | Model-specific | Reduces per-GPU memory | Point-to-point between stages |\n\n**Sources**: Diagram 5 from high-level architecture, [lightx2v/pipeline.py:392-397]()\n\n---\n\n## Configuration and Initialization\n\n### Enabling Parallel Inference via Pipeline API\n\nParallel inference is configured through the `enable_parallel` method of `LightX2VPipeline`:\n\n```python\npipe.enable_parallel(\n    cfg_p_size=1,        # CFG parallelism degree (1 = disabled)\n    seq_p_size=8,        # Sequence parallelism degree\n    seq_p_attn_type=\"ulysses\"  # \"ulysses\" or \"ring\"\n)\n```\n\n**Configuration Parameters:**\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `cfg_p_size` | int | 1 | Number of GPUs for CFG parallelism (must divide world size) |\n| `seq_p_size` | int | 1 | Number of GPUs for sequence parallelism (must divide world size) |\n| `seq_p_attn_type` | str | \"ulysses\" | Sequence parallel attention type: \"ulysses\" or \"ring\" |\n\n**Sources**: [lightx2v/pipeline.py:392-397]()\n\n### Initialization Flow\n\nWhen parallelism is enabled, the initialization follows this sequence:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Pipeline as LightX2VPipeline\n    participant Platform as PLATFORM_DEVICE_REGISTER\n    participant Config as set_parallel_config\n    participant Dist as torch.distributed\n    \n    User-\u003e\u003ePipeline: create_generator(config_json=...)\n    Pipeline-\u003e\u003ePipeline: enable_parallel(cfg_p_size, seq_p_size, ...)\n    Pipeline-\u003e\u003eConfig: set_config(self)\n    \n    alt config[\"parallel\"] is True\n        Pipeline-\u003e\u003ePlatform: get(\"cuda\").init_parallel_env()\n        Platform-\u003e\u003eDist: init_process_group(backend=\"nccl\")\n        Dist--\u003e\u003ePlatform: Process group created\n        Pipeline-\u003e\u003eConfig: set_parallel_config(config)\n        Config--\u003e\u003ePipeline: Device mesh configured\n    end\n    \n    Pipeline-\u003e\u003ePipeline: _init_runner(config)\n    Pipeline--\u003e\u003eUser: Runner initialized\n```\n\n**Sources**: [lightx2v/pipeline.py:175-183]()\n\n### Environment Setup with torchrun\n\nFor distributed inference, use `torchrun` to launch multiple processes:\n\n```bash\ntorchrun --nproc-per-node 8 -m lightx2v.infer \\\n    --model_cls seko_talk \\\n    --task s2v \\\n    --model_path /path/to/model \\\n    --config_json config.json\n```\n\n**Environment Variables Set by torchrun:**\n- `LOCAL_RANK`: GPU device index for this process (0-7 for 8 GPUs)\n- `WORLD_SIZE`: Total number of processes (8 in this example)\n- `RANK`: Global rank of this process\n- `MASTER_ADDR`: Address of rank 0 process\n- `MASTER_PORT`: Port for distributed communication\n\n**Sources**: [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh:1-22](), [lightx2v/server/services/inference/worker.py:18-19]()\n\n---\n\n## Sequence Parallelism\n\nSequence parallelism splits the sequence dimension (typically time for video) across multiple GPUs to handle longer sequences that wouldn't fit in a single GPU's memory.\n\n### Sequence Parallel Attention Types\n\nLightX2V supports two sequence parallelism strategies:\n\n#### Ulysses Attention (All-to-All Communication)\n\n```mermaid\ngraph LR\n    subgraph \"GPU 0\"\n        Q0[\"Q tokens 0-10\"]\n        K0[\"K tokens 0-10\"]\n        V0[\"V tokens 0-10\"]\n    end\n    \n    subgraph \"GPU 1\"\n        Q1[\"Q tokens 11-20\"]\n        K1[\"K tokens 11-20\"]\n        V1[\"V tokens 11-20\"]\n    end\n    \n    subgraph \"All-to-All Communication\"\n        A2A[\"Redistribute:\u003cbr/\u003eEach GPU gets full K,V\u003cbr/\u003efor its Q subset\"]\n    end\n    \n    subgraph \"GPU 0 Post-Comm\"\n        Q0P[\"Q tokens 0-10\"]\n        K0P[\"K full (0-20)\"]\n        V0P[\"V full (0-20)\"]\n    end\n    \n    subgraph \"GPU 1 Post-Comm\"\n        Q1P[\"Q tokens 11-20\"]\n        K1P[\"K full (0-20)\"]\n        V1P[\"V full (0-20)\"]\n    end\n    \n    Q0 --\u003e A2A\n    K0 --\u003e A2A\n    V0 --\u003e A2A\n    Q1 --\u003e A2A\n    K1 --\u003e A2A\n    V1 --\u003e A2A\n    \n    A2A --\u003e Q0P\n    A2A --\u003e K0P\n    A2A --\u003e V0P\n    A2A --\u003e Q1P\n    A2A --\u003e K1P\n    A2A --\u003e V1P\n```\n\n**Ulysses Characteristics:**\n- **Communication**: Single all-to-all collective at the start\n- **Computation**: Full attention within each GPU after redistribution\n- **Memory**: Each GPU needs memory for full K, V but only partial Q\n- **Best for**: Moderate sequence parallelism (2-8 GPUs)\n\n#### Ring Attention (Point-to-Point Communication)\n\nRing attention processes attention in chunks with peer-to-peer communication:\n\n```mermaid\ngraph LR\n    subgraph \"Iteration 1\"\n        GPU0_1[\"GPU 0\u003cbr/\u003eQ[0-10]  K[0-10], V[0-10]\"]\n        GPU1_1[\"GPU 1\u003cbr/\u003eQ[11-20]  K[11-20], V[11-20]\"]\n    end\n    \n    subgraph \"Iteration 2 (after ring send/recv)\"\n        GPU0_2[\"GPU 0\u003cbr/\u003eQ[0-10]  K[11-20], V[11-20]\"]\n        GPU1_2[\"GPU 1\u003cbr/\u003eQ[11-20]  K[0-10], V[0-10]\"]\n    end\n    \n    GPU0_1 --\u003e|\"send K,V to GPU1\u003cbr/\u003erecv K,V from GPU1\"| GPU0_2\n    GPU1_1 --\u003e|\"send K,V to GPU0\u003cbr/\u003erecv K,V from GPU0\"| GPU1_2\n```\n\n**Ring Characteristics:**\n- **Communication**: Multiple small point-to-point transfers\n- **Computation**: Attention computed incrementally across iterations\n- **Memory**: Only needs memory for local K, V chunks\n- **Best for**: Very long sequences or high sequence parallelism (8+ GPUs)\n\n**Sources**: [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:19-22](), [lightx2v/common/ops/attn/utils/ring_comm.py:1-47]()\n\n### RingComm Implementation\n\nThe `RingComm` class implements the ring communication pattern:\n\n```mermaid\nclassDiagram\n    class RingComm {\n        -ProcessGroup _process_group\n        -List _ops\n        -int rank\n        -int world_size\n        -int send_rank\n        -int recv_rank\n        -List _reqs\n        \n        +__init__(process_group)\n        +send_recv(to_send, recv_tensor) Tensor\n        +commit()\n        +wait()\n    }\n    \n    class DistributedManager {\n        +init_process_group() bool\n        +broadcast_task_data(task_data) Any\n        +barrier()\n    }\n    \n    RingComm ..\u003e DistributedManager : uses process group from\n```\n\n**Key Methods:**\n- `send_recv(to_send, recv_tensor)`: Registers send to next GPU and receive from previous GPU\n- `commit()`: Initiates all registered communication operations\n- `wait()`: Blocks until all communications complete\n\n**Ring Topology:**\n- Each GPU has `send_rank = (rank + 1) % world_size`\n- Each GPU has `recv_rank = (rank - 1) % world_size`\n- Forms a circular communication pattern: GPU0  GPU1  GPU2  ...  GPU0\n\n**Sources**: [lightx2v/common/ops/attn/utils/ring_comm.py:7-47]()\n\n---\n\n## CFG Parallelism\n\nClassifier-Free Guidance (CFG) requires running the model twice per step: once with the text condition and once without (unconditional). CFG parallelism eliminates this redundancy by running these two predictions on separate GPUs simultaneously.\n\n### CFG Parallel Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Single GPU (No CFG Parallel)\"\n        SEQ1[\"Step 1: Conditional forward\"]\n        SEQ2[\"Step 2: Unconditional forward\"]\n        SEQ3[\"Step 3: Combine predictions\"]\n    end\n    \n    subgraph \"Two GPUs (CFG Parallel, cfg_p_size=2)\"\n        direction LR\n        subgraph GPU0_CFG[\"GPU 0\"]\n            COND[\"Conditional forward\u003cbr/\u003e(with text prompt)\"]\n        end\n        \n        subgraph GPU1_CFG[\"GPU 1\"]\n            UNCOND[\"Unconditional forward\u003cbr/\u003e(empty prompt)\"]\n        end\n        \n        COND --\u003e COMBINE\n        UNCOND --\u003e COMBINE\n        COMBINE[\"Combine:\u003cbr/\u003eout = uncond + guidance_scale  (cond - uncond)\"]\n    end\n    \n    SEQ1 --\u003e SEQ2\n    SEQ2 --\u003e SEQ3\n```\n\n**Benefits:**\n- **2x throughput**: Both predictions run in parallel\n- **No additional memory**: Each GPU only stores one prediction path\n- **No communication overhead**: Predictions are independent until final combination\n\n**Requirements:**\n- `cfg_p_size` must evenly divide `world_size`\n- `guidance_scale \u003e 1.0` (CFG disabled if guidance_scale == 1.0)\n- Can be combined with sequence parallelism: e.g., 8 GPUs = 2 CFG  4 seq parallel\n\n**Sources**: Diagram 5 from high-level architecture, [lightx2v/pipeline.py:218-222]()\n\n---\n\n## Distributed Communication Patterns\n\n### DistributedManager Class\n\nThe `DistributedManager` handles all distributed coordination in the multi-worker server setup:\n\n```mermaid\nclassDiagram\n    class DistributedManager {\n        -bool is_initialized\n        -int rank\n        -int world_size\n        -str device\n        -ProcessGroup task_pg\n        -bool _shutting_down\n        +int CHUNK_SIZE = 1024*1024\n        \n        +init_process_group() bool\n        +cleanup(timeout)\n        +barrier()\n        +is_rank_zero() bool\n        +broadcast_task_data(task_data) Any\n        -_broadcast_byte_chunks(data_bytes)\n        -_receive_byte_chunks(total_length) bytes\n    }\n    \n    class TorchrunInferenceWorker {\n        -int rank\n        -int world_size\n        -Runner runner\n        -DistributedManager dist_manager\n        -bool processing\n        \n        +init(args) bool\n        +process_request(task_data) Dict\n        +worker_loop()\n        +cleanup()\n    }\n    \n    TorchrunInferenceWorker --\u003e DistributedManager : uses\n```\n\n**Sources**: [lightx2v/server/services/distributed_utils.py:11-157]()\n\n### Process Group Initialization\n\nThe manager initializes two process groups for different communication needs:\n\n```python\ndef init_process_group(self) -\u003e bool:\n    # Main process group (NCCL for GPU, Gloo for CPU)\n    backend = \"nccl\" if torch.cuda.is_available() else \"gloo\"\n    dist.init_process_group(backend=backend, init_method=\"env://\")\n    \n    # Task distribution group (Gloo with long timeout)\n    task_timeout = timedelta(days=30)\n    self.task_pg = dist.new_group(backend=\"gloo\", timeout=task_timeout)\n```\n\n**Why Two Process Groups?**\n- **Main group (NCCL)**: Fast GPU-to-GPU communication for model computations\n- **Task group (Gloo)**: CPU-based task distribution with long timeout to avoid timeouts during idle periods\n\n**Sources**: [lightx2v/server/services/distributed_utils.py:22-50]()\n\n### Task Broadcasting Protocol\n\nTasks are broadcast from rank 0 to all workers using a chunked protocol to handle large payloads:\n\n```mermaid\nsequenceDiagram\n    participant R0 as Rank 0 (Master)\n    participant R1 as Rank 1+ (Workers)\n    \n    alt Task Available\n        R0-\u003e\u003eR0: stop_signal = [0]\n        R0-\u003e\u003eR1: broadcast(stop_signal)\n        R0-\u003e\u003eR0: task_bytes = pickle.dumps(task_data)\n        R0-\u003e\u003eR0: task_length = len(task_bytes)\n        R0-\u003e\u003eR1: broadcast(task_length)\n        \n        loop For each 1MB chunk\n            R0-\u003e\u003eR0: chunk = task_bytes[offset:offset+1MB]\n            R0-\u003e\u003eR1: broadcast(chunk)\n        end\n        \n        R1-\u003e\u003eR1: task_data = pickle.loads(received_bytes)\n    else No Task (Stop Signal)\n        R0-\u003e\u003eR0: stop_signal = [1]\n        R0-\u003e\u003eR1: broadcast(stop_signal)\n        R1-\u003e\u003eR1: Return None (exit worker loop)\n    end\n```\n\n**Chunking Strategy:**\n- `CHUNK_SIZE = 1024 * 1024` (1MB chunks)\n- Prevents memory issues with large task payloads\n- Uses `torch.tensor` for communication (compatible with PyTorch distributed)\n\n**Sources**: [lightx2v/server/services/distributed_utils.py:87-157]()\n\n---\n\n## Distributed Server Architecture\n\n### TorchrunInferenceWorker\n\nThe server uses a multi-worker architecture where each GPU runs a separate worker process:\n\n```mermaid\ngraph TB\n    subgraph \"Rank 0 (Master)\"\n        API[\"FastAPI Server\u003cbr/\u003eReceives HTTP requests\"]\n        QUEUE[\"Task Queue\u003cbr/\u003eManages task lifecycle\"]\n        WORKER0[\"TorchrunInferenceWorker\u003cbr/\u003erank=0, device=cuda:0\"]\n    end\n    \n    subgraph \"Rank 1\"\n        WORKER1[\"TorchrunInferenceWorker\u003cbr/\u003erank=1, device=cuda:1\"]\n    end\n    \n    subgraph \"Rank N-1\"\n        WORKERN[\"TorchrunInferenceWorker\u003cbr/\u003erank=N-1, device=cuda:N-1\"]\n    end\n    \n    API --\u003e QUEUE\n    QUEUE --\u003e WORKER0\n    WORKER0 --\u003e|\"broadcast_task_data\"| WORKER1\n    WORKER0 --\u003e|\"broadcast_task_data\"| WORKERN\n    \n    WORKER0 --\u003e|\"Run inference\"| MODEL0[\"Model on GPU 0\"]\n    WORKER1 --\u003e|\"Run inference\"| MODEL1[\"Model on GPU 1\"]\n    WORKERN --\u003e|\"Run inference\"| MODELN[\"Model on GPU N-1\"]\n    \n    MODEL0 --\u003e|\"Distributed compute\"| MODEL1\n    MODEL1 --\u003e|\"Distributed compute\"| MODELN\n```\n\n**Sources**: [lightx2v/server/services/inference/worker.py:16-194]()\n\n### Worker Initialization\n\nEach worker initializes independently but coordinates through the distributed backend:\n\n```python\nclass TorchrunInferenceWorker:\n    def __init__(self):\n        self.rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n        self.world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n        self.runner = None\n        self.dist_manager = DistributedManager()\n        \n    def init(self, args) -\u003e bool:\n        # Initialize distributed environment\n        if self.world_size \u003e 1:\n            self.dist_manager.init_process_group()\n        \n        # Set parallel configuration\n        config = set_config(args)\n        if config[\"parallel\"]:\n            set_parallel_config(config)\n        \n        # Initialize runner (loads model weights)\n        self.runner = init_runner(config)\n```\n\n**Key Aspects:**\n- Each worker loads model weights independently (can use CPU offloading or lazy loading)\n- Rank 0 is the master that receives tasks and broadcasts to others\n- All workers participate in inference computation\n- Synchronization happens via `barrier()` after each task\n\n**Sources**: [lightx2v/server/services/inference/worker.py:16-65]()\n\n### Asynchronous Request Processing\n\nThe worker processes requests asynchronously with cancellation support:\n\n```mermaid\nstateDiagram-v2\n    [*] --\u003e Idle\n    Idle --\u003e BroadcastReceive : worker_loop()\n    BroadcastReceive --\u003e CheckStop : Receive task_data\n    CheckStop --\u003e Cleanup : task_data is None\n    CheckStop --\u003e ProcessTask : task_data valid\n    ProcessTask --\u003e UpdateConfig : Parse request\n    UpdateConfig --\u003e HandleLoRA : LoRA switching\n    HandleLoRA --\u003e RunInference : runner.run_pipeline()\n    RunInference --\u003e Barrier : Synchronize\n    Barrier --\u003e Idle : Continue loop\n    Cleanup --\u003e [*]\n```\n\n**Request Flow:**\n1. **Broadcast**: Rank 0 broadcasts task data to all workers\n2. **LoRA Handling**: Dynamic LoRA loading if `lora_name` specified\n3. **Configuration**: Update runner config with task parameters\n4. **Inference**: All workers participate in distributed inference\n5. **Barrier**: Synchronize before next task\n6. **Response**: Only rank 0 returns results to API\n\n**Sources**: [lightx2v/server/services/inference/worker.py:66-125]()\n\n---\n\n## Usage Examples\n\n### Example 1: Sequence Parallel for Long Video\n\nGenerate a 720p video with 8-way sequence parallelism:\n\n```python\nfrom lightx2v import LightX2VPipeline\n\npipe = LightX2VPipeline(\n    model_path=\"/path/to/Wan2.1-T2V-14B\",\n    model_cls=\"wan2.1\",\n    task=\"t2v\"\n)\n\npipe.create_generator(\n    attn_mode=\"flash_attn3\",\n    infer_steps=50,\n    height=720,\n    width=1280,\n    num_frames=241,  # Long sequence\n    guidance_scale=5.0,\n    config_json=\"config.json\"\n)\n\n# Enable 8-way sequence parallelism\npipe.enable_parallel(\n    cfg_p_size=1,\n    seq_p_size=8,\n    seq_p_attn_type=\"ulysses\"\n)\n\npipe.generate(\n    seed=42,\n    prompt=\"A cinematic shot of a city at sunset\",\n    negative_prompt=\"\",\n    save_result_path=\"output.mp4\"\n)\n```\n\n**Launch with torchrun:**\n```bash\ntorchrun --nproc-per-node 8 script.py\n```\n\n**Sources**: [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh:12-21](), [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:19-22]()\n\n### Example 2: Combined CFG and Sequence Parallelism\n\nUse 8 GPUs with 2-way CFG parallel and 4-way sequence parallel:\n\n```json\n{\n    \"parallel\": {\n        \"cfg_p_size\": 2,\n        \"seq_p_size\": 4,\n        \"seq_p_attn_type\": \"ulysses\"\n    },\n    \"sample_guide_scale\": 5.0,\n    \"enable_cfg\": true\n}\n```\n\n**GPU Allocation:**\n- GPUs 0-1: CFG group 0 (conditional and unconditional for sequence chunk 0)\n- GPUs 2-3: CFG group 1 (conditional and unconditional for sequence chunk 1)\n- GPUs 4-5: CFG group 2 (conditional and unconditional for sequence chunk 2)\n- GPUs 6-7: CFG group 3 (conditional and unconditional for sequence chunk 3)\n\n**Sources**: [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:19-22]()\n\n### Example 3: Distributed Server with LoRA Support\n\nStart a distributed inference server with dynamic LoRA loading:\n\n```bash\n#!/bin/bash\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\n\ntorchrun --nproc-per-node 4 -m lightx2v.server \\\n    --model_cls qwen_image \\\n    --task i2i \\\n    --model_path /path/to/model \\\n    --lora_dir /path/to/loras \\\n    --config_json config.json \\\n    --port 8000\n```\n\n**Submit task with LoRA:**\n```python\nimport requests\n\nresponse = requests.post(\"http://localhost:8000/v1/tasks/image/\", json={\n    \"prompt\": \"turn photo to anime style\",\n    \"image_path\": \"base64_encoded_image\",\n    \"lora_name\": \"anime_style.safetensors\",\n    \"lora_strength\": 1.0,\n    \"infer_steps\": 5,\n    \"seed\": 42\n})\n```\n\n**Dynamic LoRA Switching:**\n- LoRA weights loaded from `lora_dir/lora_name`\n- Applied at runtime via `runner.model._update_lora()`\n- Removed automatically when `lora_name=None`\n- Cached between requests with same LoRA\n\n**Sources**: [scripts/server/start_server_i2i_with_loradir.sh:1-23](), [scripts/server/post_i2i_with_lora.py:1-29](), [lightx2v/server/services/inference/worker.py:126-158]()\n\n---\n\n## Performance Considerations\n\n### Choosing Sequence Parallel Type\n\n| Sequence Length | GPUs | Recommended Type | Reason |\n|-----------------|------|------------------|---------|\n| \u003c 100 frames | 1-2 | No parallelism | Overhead exceeds benefit |\n| 100-200 frames | 2-4 | Ulysses | Single all-to-all is efficient |\n| 200-500 frames | 4-8 | Ulysses | Good balance of communication/compute |\n| \u003e 500 frames | 8+ | Ring | Lower memory footprint |\n\n### Communication Overhead\n\n**Ulysses (All-to-All):**\n- Communication volume: `O(seq_len * hidden_dim / seq_p_size)`\n- Single synchronization point per attention layer\n- Best with fast interconnects (NVLink, InfiniBand)\n\n**Ring (Point-to-Point):**\n- Communication volume: `O(seq_len * hidden_dim)`\n- Multiple smaller transfers during computation\n- More tolerant of slower interconnects\n- Can overlap communication with computation\n\n### Memory Scaling\n\nWith sequence parallelism on N GPUs:\n- **Activation memory**: Reduced by ~1/N (sequence dimension split)\n- **Weight memory**: Unchanged (replicated across all GPUs)\n- **KV cache**: Depends on attention type (Ulysses needs full, Ring needs partial)\n\n**Sources**: [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:19-22]()\n\n---\n\n## Integration with Other Optimizations\n\nDistributed inference can be combined with other optimization features:\n\n```json\n{\n    \"parallel\": {\n        \"seq_p_size\": 8,\n        \"seq_p_attn_type\": \"ulysses\"\n    },\n    \"cpu_offload\": true,\n    \"offload_granularity\": \"block\",\n    \"dit_quantized\": true,\n    \"dit_quant_scheme\": \"fp8-sgl\",\n    \"compile\": true\n}\n```\n\n**Compatible Optimizations:**\n- **Quantization** ([#6.1](#6.1)): Reduces per-GPU memory, enables larger batch sizes\n- **CPU Offloading** ([#6.3](#6.3)): Offload inactive weights during parallel computation\n- **JIT Compilation** ([#6.7](#6.7)): Compile distributed kernels for faster execution\n- **Feature Caching** ([#6.4](#6.4)): Cache intermediate features across parallel workers\n\n**Incompatible Combinations:**\n- CFG parallel with `guidance_scale=1.0` (CFG disabled)\n- Ring attention with certain fused kernels (may require fallback)\n\n**Sources**: [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:1-74]()"])</script><script>self.__next_f.push([1,"3e:T637c,"])</script><script>self.__next_f.push([1,"# LoRA Dynamic Application\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/common/modules/weight_module.py](lightx2v/common/modules/weight_module.py)\n- [lightx2v/models/networks/hunyuan_video/model.py](lightx2v/models/networks/hunyuan_video/model.py)\n- [lightx2v/models/networks/longcat_image/model.py](lightx2v/models/networks/longcat_image/model.py)\n- [lightx2v/models/networks/lora_adapter.py](lightx2v/models/networks/lora_adapter.py)\n- [lightx2v/models/networks/ltx2/model.py](lightx2v/models/networks/ltx2/model.py)\n- [lightx2v/models/networks/wan/animate_model.py](lightx2v/models/networks/wan/animate_model.py)\n- [lightx2v/models/runners/longcat_image/longcat_image_runner.py](lightx2v/models/runners/longcat_image/longcat_image_runner.py)\n- [lightx2v/models/runners/ltx2/ltx2_runner.py](lightx2v/models/runners/ltx2/ltx2_runner.py)\n- [lightx2v/pipeline.py](lightx2v/pipeline.py)\n- [lightx2v/server/__main__.py](lightx2v/server/__main__.py)\n- [lightx2v/server/config.py](lightx2v/server/config.py)\n- [lightx2v/server/schema.py](lightx2v/server/schema.py)\n- [lightx2v/server/services/distributed_utils.py](lightx2v/server/services/distributed_utils.py)\n- [lightx2v/server/services/generation/base.py](lightx2v/server/services/generation/base.py)\n- [lightx2v/server/services/generation/image.py](lightx2v/server/services/generation/image.py)\n- [lightx2v/server/services/inference/worker.py](lightx2v/server/services/inference/worker.py)\n- [scripts/server/post_i2i_with_lora.py](scripts/server/post_i2i_with_lora.py)\n- [scripts/server/start_server_i2i_with_loradir.sh](scripts/server/start_server_i2i_with_loradir.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the LoRA (Low-Rank Adaptation) dynamic application system in LightX2V, which enables runtime switching of LoRA weights without reloading the base model. This allows model customization on a per-request basis in production deployments and interactive experimentation during development.\n\nFor information about static LoRA merging and quantization workflows, see [Model Conversion and Quantization Tools](#8.1). For details on the weight management infrastructure that enables LoRA operations, see [Weight Management System](#4.4).\n\n**Key capabilities:**\n- Enable/disable LoRA dynamically at runtime\n- Switch between different LoRA weights without model reload\n- Adjust LoRA strength per request\n- Support multiple LoRA formats automatically\n- Combine LoRA with quantization and offloading\n\n---\n\n## LoRA Configuration Format\n\nLoRA configurations are specified as a list of dictionaries containing file paths, strengths, and optional alpha values. The system supports multiple concurrent LoRA weights that are applied sequentially.\n\n**Configuration Structure:**\n\n| Field | Type | Description | Default |\n|-------|------|-------------|---------|\n| `path` | str | Path to LoRA `.safetensors` file | Required |\n| `strength` | float | Multiplier for LoRA delta weights | 1.0 |\n| `alpha` | float | LoRA scaling factor (if not in file) | None |\n\n**Example configuration:**\n\n```python\nlora_configs = [\n    {\"path\": \"/models/lora/style_anime.safetensors\", \"strength\": 1.0},\n    {\"path\": \"/models/lora/detail_boost.safetensors\", \"strength\": 0.5, \"alpha\": 32}\n]\n```\n\nSources: [lightx2v/pipeline.py:354-356]()\n\n---\n\n## Pipeline LoRA Initialization\n\n### Enabling LoRA in Python API\n\nThe `LightX2VPipeline` provides two methods for LoRA configuration: `enable_lora()` for initial setup and `switch_lora()` for runtime changes.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Pipeline as LightX2VPipeline\n    participant Runner as DefaultRunner\n    participant Model as WeightModule\n    \n    User-\u003e\u003ePipeline: enable_lora(lora_configs, lora_dynamic_apply=True)\n    Pipeline-\u003e\u003ePipeline: self.lora_configs = lora_configs\n    Pipeline-\u003e\u003ePipeline: self.lora_dynamic_apply = True\n    \n    User-\u003e\u003ePipeline: create_generator()\n    Pipeline-\u003e\u003eRunner: _init_runner(config)\n    Runner-\u003e\u003eRunner: init_modules()\n    Runner-\u003e\u003eModel: register_lora(lora_weights, strength)\n    Model--\u003e\u003eRunner: LoRA registered\n    \n    Note over Pipeline,Model: Model now loaded with LoRA enabled\n    \n    User-\u003e\u003ePipeline: switch_lora(new_path, strength)\n    Pipeline-\u003e\u003eRunner: switch_lora(new_path, strength)\n    \n    alt LoRA path is empty\n        Runner-\u003e\u003eModel: _remove_lora()\n        Model--\u003e\u003eRunner: LoRA removed\n    else New LoRA path provided\n        Runner-\u003e\u003eModel: _update_lora(lora_path, strength)\n        Model--\u003e\u003eRunner: LoRA updated\n    end\n```\n\n**Key methods:**\n\n- **`enable_lora(lora_configs, lora_dynamic_apply=False)`** - Configures initial LoRA weights\n  - Sets `self.lora_configs` with list of LoRA configurations\n  - Sets `self.lora_dynamic_apply` flag to enable/disable runtime switching\n  - Must be called before `create_generator()`\n\n- **`switch_lora(lora_path, strength=1.0)`** - Changes LoRA at runtime\n  - Requires `lora_dynamic_apply=True` during initialization\n  - Empty path removes current LoRA\n  - Non-empty path loads new LoRA with specified strength\n  - Does not reload base model\n\nSources: [lightx2v/pipeline.py:354-366]()\n\n---\n\n## Dynamic LoRA Switching Mechanism\n\n### Switch LoRA Implementation\n\nThe dynamic switching mechanism operates through the runner's model reference, updating LoRA weights in-place without disrupting the inference pipeline.\n\n```mermaid\ngraph TB\n    subgraph \"Pipeline Layer\"\n        switch_lora[\"switch_lora(lora_path, strength)\"]\n    end\n    \n    subgraph \"Runner Layer\"\n        runner_switch[\"runner.switch_lora()\"]\n        check_flag{\"lora_dynamic_apply\u003cbr/\u003eenabled?\"}\n        get_model[\"self.model\"]\n    end\n    \n    subgraph \"Model Layer\"\n        has_update{\"hasattr(model,\u003cbr/\u003e'_update_lora')?\"}\n        update_lora[\"_update_lora(lora_path, strength)\"]\n        has_remove{\"hasattr(model,\u003cbr/\u003e'_remove_lora')?\"}\n        remove_lora[\"_remove_lora()\"]\n    end\n    \n    subgraph \"Weight Layer\"\n        load_weights[\"Load LoRA from safetensors\"]\n        apply_weights[\"Apply to WeightModule hierarchy\"]\n        clear_weights[\"Clear LoRA deltas from weights\"]\n    end\n    \n    switch_lora--\u003echeck_flag\n    check_flag--\u003e|No|error[\"Log error, return\"]\n    check_flag--\u003e|Yes|runner_switch\n    runner_switch--\u003eget_model\n    \n    get_model--\u003e|lora_path != ''|has_update\n    has_update--\u003e|Yes|update_lora\n    has_update--\u003e|No|warn1[\"Log warning\"]\n    \n    update_lora--\u003eload_weights\n    load_weights--\u003eapply_weights\n    \n    get_model--\u003e|lora_path == ''|has_remove\n    has_remove--\u003e|Yes|remove_lora\n    has_remove--\u003e|No|warn2[\"Log warning\"]\n    \n    remove_lora--\u003eclear_weights\n```\n\n**Switching workflow:**\n\n1. **Validation** - Check if `lora_dynamic_apply` flag is enabled ([lightx2v/pipeline.py:363-365]())\n2. **Path routing** - Empty string removes LoRA, non-empty loads new LoRA\n3. **Model method check** - Verify model supports `_update_lora()` or `_remove_lora()`\n4. **Weight operation** - Apply or remove LoRA deltas through `WeightModule` hierarchy\n\nSources: [lightx2v/pipeline.py:358-366]()\n\n---\n\n## Server Deployment with Per-Request LoRA\n\n### LoRA Directory Configuration\n\nThe HTTP server supports per-request LoRA selection from a designated directory, enabling multi-tenant deployments where different requests use different style adaptations.\n\n```mermaid\ngraph LR\n    subgraph \"Server Startup\"\n        args[\"--lora_dir /path/to/loras/\"]\n        worker_init[\"TorchrunInferenceWorker.init()\"]\n        set_dir[\"self.lora_dir = Path(lora_dir)\"]\n    end\n    \n    subgraph \"Request Processing\"\n        request[\"POST /v1/tasks/video/\"]\n        task_data[\"task_data = {\u003cbr/\u003elora_name: 'style.safetensors',\u003cbr/\u003elora_strength: 0.8\u003cbr/\u003e}\"]\n        process[\"process_request(task_data)\"]\n    end\n    \n    subgraph \"LoRA Application\"\n        extract[\"lora_name = task_data.pop('lora_name')\"]\n        switch[\"switch_lora(lora_name, lora_strength)\"]\n        resolve[\"lora_path = lora_dir / lora_name\"]\n        update[\"runner.model._update_lora(lora_path, strength)\"]\n    end\n    \n    args--\u003eworker_init\n    worker_init--\u003eset_dir\n    \n    request--\u003etask_data\n    task_data--\u003eprocess\n    process--\u003eextract\n    extract--\u003eswitch\n    switch--\u003eresolve\n    resolve--\u003eupdate\n```\n\n**Server startup with LoRA directory:**\n\n```bash\npython -m lightx2v.server \\\n  --model_cls qwen_image \\\n  --task i2i \\\n  --model_path /models/qwen_image/ \\\n  --lora_dir /models/loras/ \\\n  --port 8000\n```\n\n**Per-request LoRA specification:**\n\n```python\n{\n    \"prompt\": \"turn the style to anime\",\n    \"image_path\": \"base64_encoded_image\",\n    \"lora_name\": \"anime_style.safetensors\",  # Relative to lora_dir\n    \"lora_strength\": 1.0\n}\n```\n\nSources: [lightx2v/server/services/inference/worker.py:23-24,38-45,74-79](), [scripts/server/start_server_i2i_with_loradir.sh:1-22](), [scripts/server/post_i2i_with_lora.py:14-28]()\n\n---\n\n## TorchrunInferenceWorker LoRA Management\n\n### Worker LoRA Switching Logic\n\nThe `TorchrunInferenceWorker` tracks the current LoRA state and only updates weights when necessary, optimizing performance for consecutive requests with the same LoRA.\n\n```mermaid\nflowchart TD\n    start[\"switch_lora(lora_name, lora_strength)\"]\n    \n    check_none{\"lora_name\u003cbr/\u003eis None?\"}\n    has_current{\"current_lora_name\u003cbr/\u003eis not None?\"}\n    remove[\"runner.model._remove_lora()\"]\n    clear_state[\"current_lora_name = None\u003cbr/\u003edel current_lora_strength\"]\n    \n    check_changed{\"lora_name != current_lora_name\u003cbr/\u003eOR\u003cbr/\u003estrength != current_strength?\"}\n    resolve[\"lora_path = _lora_path(lora_name)\"]\n    check_exists{\"lora_path\u003cbr/\u003eexists?\"}\n    \n    has_method{\"hasattr(model,\u003cbr/\u003e'_update_lora')?\"}\n    update[\"model._update_lora(lora_path, strength)\"]\n    update_state[\"current_lora_name = lora_name\u003cbr/\u003ecurrent_lora_strength = strength\"]\n    \n    warn_missing[\"Log warning:\u003cbr/\u003eLoRA file not found\"]\n    warn_unsupported[\"Log warning:\u003cbr/\u003eModel doesn't support LoRA\"]\n    \n    done[\"return\"]\n    \n    start--\u003echeck_none\n    check_none--\u003e|Yes|has_current\n    check_none--\u003e|No|check_changed\n    \n    has_current--\u003e|Yes|remove\n    has_current--\u003e|No|done\n    remove--\u003eclear_state\n    clear_state--\u003edone\n    \n    check_changed--\u003e|Yes|resolve\n    check_changed--\u003e|No|done\n    \n    resolve--\u003echeck_exists\n    check_exists--\u003e|Yes|has_method\n    check_exists--\u003e|No|warn_missing\n    warn_missing--\u003edone\n    \n    has_method--\u003e|Yes|update\n    has_method--\u003e|No|warn_unsupported\n    warn_unsupported--\u003edone\n    \n    update--\u003eupdate_state\n    update_state--\u003edone\n```\n\n**State tracking fields:**\n\n| Field | Purpose |\n|-------|---------|\n| `self.current_lora_name` | Currently loaded LoRA filename |\n| `self.current_lora_strength` | Current LoRA strength multiplier |\n| `self.lora_dir` | Base directory for LoRA files |\n\n**Optimization:** Skip re-loading if the requested LoRA and strength match the current state ([lightx2v/server/services/inference/worker.py:140-141]()).\n\nSources: [lightx2v/server/services/inference/worker.py:126-157]()\n\n---\n\n## Weight Module LoRA Operations\n\n### WeightModule LoRA Methods\n\nThe `WeightModule` class provides three core LoRA operations that propagate through the module hierarchy to all parameters and sub-modules.\n\n```mermaid\nclassDiagram\n    class WeightModule {\n        -dict _modules\n        -dict _parameters\n        +register_lora(weight_dict, strength)\n        +update_lora(weight_dict, strength)\n        +remove_lora()\n    }\n    \n    class MMWeight {\n        +register_lora(weight_dict, strength)\n        +update_lora(weight_dict, strength)\n        +remove_lora()\n    }\n    \n    class WeightModuleList {\n        -list _list\n        +register_lora(weight_dict, strength)\n        +update_lora(weight_dict, strength)\n        +remove_lora()\n    }\n    \n    WeightModule \u003c|-- WeightModuleList\n    WeightModule --\u003e MMWeight : contains\n    \n    note for WeightModule \"Recursively calls LoRA methods\\non all _modules and _parameters\"\n```\n\n**Method semantics:**\n\n| Method | Purpose | When Used |\n|--------|---------|-----------|\n| `register_lora(weight_dict, strength)` | Initial LoRA registration during model load | Called once at initialization |\n| `update_lora(weight_dict, strength)` | Replace existing LoRA with new weights | Called by `switch_lora()` |\n| `remove_lora()` | Remove LoRA deltas, revert to base weights | Called when disabling LoRA |\n\n**Recursive propagation example:**\n\n```python\ndef update_lora(self, weight_dict, strength):\n    # Propagate to all sub-modules\n    for _, module in self._modules.items():\n        if hasattr(module, \"update_lora\"):\n            module.update_lora(weight_dict, strength)\n    \n    # Propagate to all parameters\n    for _, parameter in self._parameters.items():\n        if hasattr(parameter, \"update_lora\"):\n            parameter.update_lora(weight_dict, strength)\n```\n\nSources: [lightx2v/common/modules/weight_module.py:35-60]()\n\n---\n\n## LoRA Loading and Application\n\n### LoRALoader Implementation\n\nThe `LoRALoader` class handles the complexity of matching LoRA keys to model weights, supporting multiple naming conventions and applying rank decomposition.\n\n```mermaid\ngraph TB\n    subgraph \"LoRA File Format\"\n        lora_file[\"LoRA .safetensors\"]\n        lora_up[\"key.lora_up.weight\u003cbr/\u003e[out_features, rank]\"]\n        lora_down[\"key.lora_down.weight\u003cbr/\u003e[rank, in_features]\"]\n        lora_alpha[\"key.alpha\u003cbr/\u003e[scalar]\"]\n    end\n    \n    subgraph \"LoRALoader.apply_lora()\"\n        load[\"Load LoRA tensors from file\"]\n        detect[\"Detect LoRA format:\u003cbr/\u003estandard/diffusers/mochi/qwen/etc\"]\n        extract[\"Extract up/down/alpha tensors\"]\n        \n        map_keys[\"Apply key_mapping_rules\u003cbr/\u003eif provided\"]\n        \n        match[\"Match LoRA keys to model weights\"]\n        \n        compute[\"delta = (up @ down) * scale\"]\n        scale_calc[\"scale = (alpha / rank) * strength\"]\n        \n        apply[\"weight += delta\"]\n    end\n    \n    subgraph \"Model Weights\"\n        base_weight[\"model_weight\u003cbr/\u003e[out_features, in_features]\"]\n        updated_weight[\"updated_weight\u003cbr/\u003e= base + delta\"]\n    end\n    \n    lora_file--\u003eload\n    load--\u003edetect\n    detect--\u003eextract\n    extract--\u003elora_up\n    extract--\u003elora_down\n    extract--\u003elora_alpha\n    \n    lora_up--\u003emap_keys\n    lora_down--\u003emap_keys\n    map_keys--\u003ematch\n    \n    match--\u003ecompute\n    lora_alpha--\u003escale_calc\n    scale_calc--\u003ecompute\n    \n    base_weight--\u003eapply\n    compute--\u003eapply\n    apply--\u003eupdated_weight\n```\n\n**LoRA delta computation:**\n\nFor a layer with base weight **W**, LoRA adds a low-rank update:\n\n```\ndelta = (LoRA_up @ LoRA_down) * scale\nscale = (alpha / rank) * strength\n\nW_new = W + delta\n```\n\nWhere:\n- `LoRA_up`: [out_features, rank]\n- `LoRA_down`: [rank, in_features]  \n- `alpha`: Typically 8, 16, or 32\n- `rank`: Low-rank dimension (typically 4-128)\n- `strength`: User-specified multiplier (0.0-2.0)\n\nSources: [tools/convert/converter.py:433-459]()\n\n---\n\n## Supported LoRA Formats\n\n### Format Detection and Key Mapping\n\nThe system automatically detects and handles 7 different LoRA naming conventions, plus 3 diff-based formats.\n\n**LoRA Format Table:**\n\n| Format | Up Key Pattern | Down Key Pattern | Alpha Key | Usage |\n|--------|----------------|------------------|-----------|-------|\n| Standard | `{key}.lora_up.weight` | `{key}.lora_down.weight` | `{key}.alpha` | LightX2V native |\n| Diffusers | `{key}_lora.up.weight` | `{key}_lora.down.weight` | `{key}_lora.alpha` | HuggingFace Diffusers |\n| Diffusers V2 | `{key}.lora_B.weight` | `{key}.lora_A.weight` | `{key}.alpha` | Alternative format |\n| Diffusers V3 | `{key}.lora.up.weight` | `{key}.lora.down.weight` | `{key}.lora.alpha` | Newer Diffusers |\n| Mochi | `{key}.lora_B` | `{key}.lora_A` | None | No `.weight` suffix |\n| Transformers | `{key}.lora_linear_layer.up.weight` | `{key}.lora_linear_layer.down.weight` | `{key}.lora_linear_layer.alpha` | Transformers library |\n| Qwen | `{key}.lora_B.default.weight` | `{key}.lora_A.default.weight` | `{key}.lora_A.default.alpha` | Qwen models |\n\n**Diff Format Table:**\n\n| Format | Key Pattern | Usage |\n|--------|-------------|-------|\n| Weight Diff | `{key}.diff` | Direct weight delta |\n| Bias Diff | `{key}.diff_b` | Bias delta |\n| Modulation Diff | `{key}.diff_m` | Modulation parameter delta |\n\n**Format detection logic:**\n\nThe loader iterates through format patterns, attempting to find matching up/down pairs. The first successful match determines the format.\n\nSources: [tools/convert/readme.md:83-99]()\n\n---\n\n## LoRA with Key Conversion\n\n### LightX2V  Diffusers Conversion\n\nWhen converting models between architectures, LoRA keys may need transformation to match the target format. The converter supports three modes for handling this.\n\n```mermaid\ngraph TB\n    subgraph \"Conversion Modes\"\n        auto[\"Mode: auto\u003cbr/\u003e(default)\"]\n        same[\"Mode: same\"]\n        convert[\"Mode: convert\"]\n    end\n    \n    subgraph \"Auto Mode Behavior\"\n        try_convert[\"Try with key conversion first\"]\n        check_match{\"LoRA keys\u003cbr/\u003ematch model?\"}\n        fallback[\"Fall back to original keys\"]\n    end\n    \n    subgraph \"Key Mapping Rules\"\n        rules[\"get_key_mapping_rules(direction, model_type)\"]\n        regex[\"Regex patterns:\u003cbr/\u003eblocks\\.(\\d+)\\.self_attn\\.q\\.\u003cbr/\u003e\u003cbr/\u003eblocks.\\1.attn1.to_q.\"]\n    end\n    \n    subgraph \"LoRALoader Application\"\n        loader[\"LoRALoader(key_mapping_rules)\"]\n        apply[\"apply_lora(weight_dict, lora_weights)\"]\n    end\n    \n    auto--\u003etry_convert\n    try_convert--\u003echeck_match\n    check_match--\u003e|No|fallback\n    \n    convert--\u003erules\n    rules--\u003eregex\n    regex--\u003eloader\n    \n    same--\u003eapply\n    \n    loader--\u003eapply\n    fallback--\u003eapply\n```\n\n**Example: Converting LoRA during model merge**\n\n```bash\n# Merge LoRA with key conversion (Diffusers  LightX2V)\npython converter.py \\\n    --source /path/to/model_diffusers/ \\\n    --output /path/to/model_lightx2v/ \\\n    --direction backward \\\n    --lora_path /path/to/lora.safetensors \\\n    --lora_key_convert convert \\\n    --single_file\n```\n\n**Conversion rules excerpt:**\n\n```python\n# blocks.0.self_attn.q.weight  blocks.0.attn1.to_q.weight\n{\n    \"forward\": (r\"blocks\\.(\\d+)\\.self_attn\\.q\\.\", r\"blocks.\\1.attn1.to_q.\"),\n    \"backward\": (r\"blocks\\.(\\d+)\\.attn1\\.to_q\\.\", r\"blocks.\\1.self_attn.q.\"),\n}\n```\n\nSources: [tools/convert/converter.py:564-582](), [tools/convert/readme.md:364-395]()\n\n---\n\n## Static LoRA Merging\n\n### Offline LoRA Integration\n\nFor deployment scenarios that don't require dynamic switching, LoRA weights can be permanently merged into the base model using the conversion tool.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Converter as converter.py\n    participant Loader as LoRALoader\n    participant Quantizer as quantize_model()\n    \n    User-\u003e\u003eConverter: converter.py\u003cbr/\u003e--source base_model/\u003cbr/\u003e--lora_path lora.safetensors\u003cbr/\u003e--quantized --linear_type fp8\n    \n    Converter-\u003e\u003eConverter: Load base model weights\n    Converter-\u003e\u003eConverter: Apply key conversion (if --direction)\n    \n    Converter-\u003e\u003eLoader: load_loras(lora_path, weight_dict, alpha, strength)\n    Loader-\u003e\u003eLoader: Load LoRA from safetensors\n    Loader-\u003e\u003eLoader: Detect LoRA format\n    Loader-\u003e\u003eLoader: apply_lora(weight_dict, lora_weights)\n    \n    loop For each LoRA weight\n        Loader-\u003e\u003eLoader: Compute delta = (up @ down) * scale\n        Loader-\u003e\u003eLoader: weight_dict[key] += delta\n    end\n    \n    Loader--\u003e\u003eConverter: Modified weight_dict\n    \n    opt If quantization enabled\n        Converter-\u003e\u003eQuantizer: quantize_model(weight_dict, linear_type=fp8)\n        Quantizer--\u003e\u003eConverter: Quantized weights + scales\n    end\n    \n    Converter-\u003e\u003eConverter: Save to output path\n```\n\n**Multi-LoRA merging:**\n\n```bash\n# Merge two LoRAs with different strengths\npython converter.py \\\n    --source /path/to/base_model/ \\\n    --output /path/to/merged_model/ \\\n    --lora_path lora1.safetensors lora2.safetensors \\\n    --lora_strength 1.0 0.8 \\\n    --single_file\n```\n\n**LoRA + Quantization workflow:**\n\n```bash\n# Merge LoRA, then quantize to FP8\npython converter.py \\\n    --source /path/to/base_model/ \\\n    --output /path/to/merged_fp8/ \\\n    --lora_path style.safetensors \\\n    --lora_strength 1.0 \\\n    --quantized \\\n    --linear_type fp8 \\\n    --single_file\n```\n\nSources: [tools/convert/converter.py:548-582](), [tools/convert/readme.md:283-361]()\n\n---\n\n## LoRA Configuration in Server Schema\n\n### Request Schema Fields\n\nThe HTTP server request schemas include dedicated fields for per-request LoRA specification, integrated into the base request schema used by all task types.\n\n**BaseTaskRequest LoRA Fields:**\n\n```python\nclass BaseTaskRequest(BaseModel):\n    # ... other fields ...\n    lora_name: Optional[str] = Field(\n        None, \n        description=\"LoRA filename to load from lora_dir, None to disable LoRA\"\n    )\n    lora_strength: float = Field(\n        1.0, \n        description=\"LoRA strength\"\n    )\n```\n\n**Field semantics:**\n\n| Field | Type | Behavior | Default |\n|-------|------|----------|---------|\n| `lora_name` | Optional[str] | Filename relative to `lora_dir` | None (no LoRA) |\n| `lora_strength` | float | Strength multiplier (0.0-2.0 typical) | 1.0 |\n\n**Request processing flow:**\n\n1. **Extract fields** - `lora_name` and `lora_strength` popped from `task_data` ([lightx2v/server/services/inference/worker.py:75-76]())\n2. **Validate directory** - Check if `self.lora_dir` is configured\n3. **Switch LoRA** - Call `self.switch_lora(lora_name, lora_strength)` before inference\n4. **Execute task** - Run inference with applied LoRA\n\nSources: [lightx2v/server/schema.py:28-29](), [lightx2v/server/services/inference/worker.py:74-79]()\n\n---\n\n## Performance Considerations\n\n### LoRA Switching Overhead\n\nDynamic LoRA switching introduces minimal overhead compared to full model reloading, making it suitable for production deployments.\n\n**Overhead Analysis:**\n\n| Operation | Time Complexity | Memory |\n|-----------|----------------|---------|\n| Load LoRA from disk | O(rank  parameters) | ~10-50MB per LoRA |\n| Compute delta (up @ down) | O(rank  in  out) | Temporary allocation |\n| Apply delta to weights | O(parameters) | In-place update |\n| Full model reload | O(all parameters) | ~14GB for base model |\n\n**Optimization strategies:**\n\n1. **State caching** - Worker tracks current LoRA to avoid redundant loads ([lightx2v/server/services/inference/worker.py:139-141]())\n2. **Lazy loading** - LoRA loaded only when needed, not at startup\n3. **In-place updates** - Delta applied directly to existing tensors, no copy\n4. **Quantized LoRA** - Audio adapter supports FP8 LoRA for reduced memory ([tools/convert/quant_adapter.py:1-80]())\n\n**Typical latencies (RTX 4090):**\n\n- LoRA switch (first time): ~50-100ms\n- LoRA switch (cached name/strength): ~0ms  \n- Full model reload: ~2-5 seconds\n\nSources: [lightx2v/server/services/inference/worker.py:126-157](), [tools/convert/quant_adapter.py:50-76]()\n\n---\n\n## Example Workflows\n\n### Workflow 1: Python API with Dynamic Switching\n\n```python\nfrom lightx2v.pipeline import LightX2VPipeline\n\n# Initialize pipeline with LoRA support\npipeline = LightX2VPipeline(\n    task=\"i2v\",\n    model_path=\"/models/wan2.1/\",\n    model_cls=\"wan2.1\"\n)\n\n# Enable LoRA with dynamic switching\npipeline.enable_lora(\n    lora_configs=[\n        {\"path\": \"/models/lora/style1.safetensors\", \"strength\": 1.0}\n    ],\n    lora_dynamic_apply=True\n)\n\npipeline.create_generator()\n\n# Generate with first LoRA\npipeline.generate(\n    seed=42,\n    prompt=\"a beautiful landscape\",\n    negative_prompt=\"\",\n    save_result_path=\"output1.mp4\"\n)\n\n# Switch to different LoRA without reloading model\npipeline.switch_lora(\"/models/lora/style2.safetensors\", strength=0.8)\n\npipeline.generate(\n    seed=42,\n    prompt=\"a beautiful landscape\",\n    negative_prompt=\"\",\n    save_result_path=\"output2.mp4\"\n)\n\n# Remove LoRA\npipeline.switch_lora(\"\", strength=1.0)\n```\n\nSources: [lightx2v/pipeline.py:354-366]()\n\n---\n\n### Workflow 2: Server Deployment with LoRA Directory\n\n**Server startup:**\n\n```bash\ntorchrun --nproc_per_node=1 \\\n    -m lightx2v.server \\\n    --model_cls qwen_image \\\n    --task i2i \\\n    --model_path /models/qwen_image/ \\\n    --lora_dir /models/loras/ \\\n    --config_json configs/qwen_image_i2i.json \\\n    --port 8000\n```\n\n**Client request:**\n\n```python\nimport requests\n\nresponse = requests.post(\n    \"http://localhost:8000/v1/tasks/image/\",\n    json={\n        \"prompt\": \"turn this photo into anime style\",\n        \"image_path\": \"data:image/png;base64,...\",\n        \"lora_name\": \"anime_style.safetensors\",\n        \"lora_strength\": 1.0,\n        \"infer_steps\": 5\n    }\n)\n\nprint(response.json())\n# {\"task_id\": \"...\", \"task_status\": \"completed\", \"save_result_path\": \"output.png\"}\n```\n\nSources: [scripts/server/start_server_i2i_with_loradir.sh:1-22](), [scripts/server/post_i2i_with_lora.py:14-28]()\n\n---\n\n### Workflow 3: Offline LoRA Merging with Quantization\n\n```bash\n# Merge multiple LoRAs and quantize to FP8\npython tools/convert/converter.py \\\n    --source /models/wan2.1-i2v-14b-480p/ \\\n    --output /models/wan2.1-i2v-14b-480p-lora-fp8/ \\\n    --output_ext .safetensors \\\n    --output_name merged_model \\\n    --model_type wan_dit \\\n    --lora_path /models/loras/style.safetensors /models/loras/detail.safetensors \\\n    --lora_strength 1.0 0.5 \\\n    --quantized \\\n    --linear_type fp8 \\\n    --non_linear_dtype torch.bfloat16 \\\n    --single_file\n```\n\n**Output:** Single `.safetensors` file with merged LoRA and FP8 quantization, ready for deployment.\n\nSources: [tools/convert/readme.md:315-328]()"])</script><script>self.__next_f.push([1,"3f:T3e7d,"])</script><script>self.__next_f.push([1,"# JIT Compilation and Shape Caching\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json](configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json)\n- [docs/EN/source/getting_started/quickstart.md](docs/EN/source/getting_started/quickstart.md)\n- [docs/ZH_CN/source/getting_started/quickstart.md](docs/ZH_CN/source/getting_started/quickstart.md)\n- [lightx2v/__init__.py](lightx2v/__init__.py)\n- [lightx2v/common/ops/attn/utils/ring_comm.py](lightx2v/common/ops/attn/utils/ring_comm.py)\n- [lightx2v/common/ops/mm/triton_kernels.py](lightx2v/common/ops/mm/triton_kernels.py)\n- [pyproject.toml](pyproject.toml)\n- [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh](scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the Just-In-Time (JIT) compilation system and shape-specific kernel caching in LightX2V. These features use PyTorch's `torch.compile` to generate optimized CUDA kernels for different input resolutions, significantly improving inference performance after an initial compilation overhead.\n\nFor general performance optimization strategies, see [Performance Optimization](#6). For memory management features, see [Memory Management and CPU Offloading](#6.3).\n\n---\n\n## Overview\n\nLightX2V uses PyTorch 2.x's `torch.compile` feature to apply graph-level optimizations and generate specialized CUDA kernels for model inference. Because diffusion models process different spatial resolutions and the compiled kernels are shape-dependent, the system maintains a cache of compiled kernels for each unique resolution configuration.\n\nThe compilation system provides two primary benefits:\n1. **Kernel Fusion**: Multiple operations are fused into single optimized kernels, reducing memory bandwidth and kernel launch overhead\n2. **Shape Specialization**: Generated code is optimized for specific tensor shapes, eliminating dynamic shape checks and enabling better compiler optimizations\n\n**Key Trade-off**: First-time compilation for each shape incurs a significant overhead (30-60 seconds per resolution), but subsequent inferences at the same resolution are 10-20% faster.\n\nSources: [pyproject.toml:33-70](), high-level system diagrams\n\n---\n\n## Configuration Parameters\n\nThe JIT compilation system is controlled through two configuration parameters, typically specified in JSON configuration files:\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `compile` | boolean | Enable/disable torch.compile for model inference |\n| `compile_shapes` | list of [height, width] | Pre-defined resolutions to compile kernels for |\n\n### Example Configuration\n\n```json\n{\n    \"compile\": true,\n    \"compile_shapes\": [\n        [480, 832],\n        [544, 960],\n        [720, 1280],\n        [832, 480],\n        [960, 544],\n        [1280, 720],\n        [480, 480],\n        [576, 576],\n        [704, 704],\n        [960, 960]\n    ]\n}\n```\n\nThe `compile_shapes` list defines the set of resolutions for which kernels will be pre-compiled or cached. Common resolutions include:\n- **16:9 landscape**: 480832, 7201280\n- **16:9 portrait**: 832480, 1280720  \n- **Square**: 480480, 576576, 704704, 960960\n\nSources: [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:31-73]()\n\n---\n\n## Shape Caching Mechanism\n\n```mermaid\nflowchart TB\n    Start[\"Inference Request\u003cbr/\u003e(H, W, F)\"]\n    CheckCompile{\"compile\u003cbr/\u003eenabled?\"}\n    NormalPath[\"Standard Eager Mode\u003cbr/\u003eInference\"]\n    CheckCache{\"Shape in\u003cbr/\u003ecache?\"}\n    CompileNew[\"torch.compile()\u003cbr/\u003eGenerate optimized kernels\u003cbr/\u003e(30-60s overhead)\"]\n    StoreCache[\"Store compiled graph\u003cbr/\u003ein shape cache\"]\n    LoadCache[\"Load cached\u003cbr/\u003ecompiled graph\"]\n    Execute[\"Execute optimized\u003cbr/\u003einference\"]\n    End[\"Return output\"]\n    \n    Start --\u003e CheckCompile\n    CheckCompile --\u003e|\"false\"| NormalPath\n    CheckCompile --\u003e|\"true\"| CheckCache\n    CheckCache --\u003e|\"not found\"| CompileNew\n    CompileNew --\u003e StoreCache\n    StoreCache --\u003e Execute\n    CheckCache --\u003e|\"found\"| LoadCache\n    LoadCache --\u003e Execute\n    NormalPath --\u003e End\n    Execute --\u003e End\n    \n    style CompileNew fill:#ffcccc\n    style Execute fill:#ccffcc\n```\n\n**Diagram: Shape-Based Compilation Flow**\n\n### Cache Key Structure\n\nThe cache key is determined by the spatial dimensions of the latent tensor:\n- **Height** (H): Latent height after VAE encoding\n- **Width** (W): Latent width after VAE encoding\n- **Frames** (F): Number of video frames (for video models)\n\nFor a 7201280 video with 81 frames and 88 spatial compression:\n- Latent shape: (B, C, 81, 90, 160)\n- Cache key: (90, 160) - spatial dimensions only\n\n### First-Time Compilation Overhead\n\nWhen a new shape is encountered:\n\n1. **Graph Tracing**: PyTorch traces the model execution to build a computation graph\n2. **Optimization**: The Inductor backend applies graph-level optimizations:\n   - Operator fusion (e.g., matmul + bias + activation)\n   - Memory layout optimization\n   - Constant folding\n3. **Code Generation**: Triton/CUDA code is generated for each operation\n4. **Compilation**: Generated code is compiled to PTX and machine code\n5. **Caching**: Compiled kernels are stored for future use\n\n**Timeline**: This process takes 30-60 seconds per unique shape, occurring only once per resolution.\n\nSources: [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:31-73]()\n\n---\n\n## Integration with Model Components\n\n```mermaid\ngraph TB\n    subgraph \"Compilation Scope\"\n        DitForward[\"DIT Transformer Forward Pass\"]\n        AttnBlocks[\"Attention Blocks\u003cbr/\u003eSelf/Cross Attention\"]\n        FFN[\"Feed-Forward Networks\"]\n        Modulation[\"Modulation/AdaLN\"]\n    end\n    \n    subgraph \"Not Compiled\"\n        TextEnc[\"Text Encoders\u003cbr/\u003e(T5, CLIP)\"]\n        VAEEnc[\"VAE Encoder\"]\n        VAEDec[\"VAE Decoder\"]\n        Scheduler[\"Scheduler\u003cbr/\u003e(timestep logic)\"]\n    end\n    \n    CompileWrapper[\"torch.compile wrapper\"]\n    \n    CompileWrapper -.applies to.-\u003e DitForward\n    DitForward --\u003e AttnBlocks\n    DitForward --\u003e FFN\n    DitForward --\u003e Modulation\n    \n    TextEnc -.feeds into.-\u003e DitForward\n    Scheduler -.controls.-\u003e DitForward\n    DitForward -.latents to.-\u003e VAEDec\n    \n    style DitForward fill:#ccffcc\n    style AttnBlocks fill:#ccffcc\n    style FFN fill:#ccffcc\n    style Modulation fill:#ccffcc\n```\n\n**Diagram: Compilation Scope in Inference Pipeline**\n\n### Compiled Components\n\nThe compilation primarily targets the transformer forward pass, which is the most computationally intensive component:\n\n- **DiT Transformer Blocks**: Self-attention, cross-attention, FFN, and modulation layers\n- **Attention Operators**: Flash Attention, Sage Attention, Radial Attention (when applicable)\n- **Quantized Operations**: INT8/FP8 matmul kernels benefit from fusion with activation functions\n\n### Excluded Components\n\nCertain components are excluded from compilation to avoid overhead or incompatibilities:\n\n- **Text Encoders**: Run once per prompt, compilation overhead not justified\n- **VAE Encoder/Decoder**: Often use different resolutions than latent processing\n- **Scheduler Logic**: Contains Python control flow that doesn't benefit from compilation\n- **Dynamic Operations**: Operations with data-dependent shapes or control flow\n\nSources: [lightx2v/common/ops/mm/triton_kernels.py:90-269]()\n\n---\n\n## Performance Characteristics\n\n### Compilation Time vs Inference Speedup\n\n| Resolution | First Compilation | Subsequent Inference | Speedup |\n|------------|-------------------|---------------------|---------|\n| 480832 | ~35s | 10-15% faster | 1.10-1.15 |\n| 7201280 | ~50s | 15-20% faster | 1.15-1.20 |\n| 960960 | ~45s | 12-18% faster | 1.12-1.18 |\n\n**Speedup Factors**:\n- Kernel fusion reduces memory traffic by 20-30%\n- Specialized code eliminates runtime shape checks\n- Better instruction scheduling and register allocation\n- Combined with quantization: up to 25 total speedup\n\n### Memory Overhead\n\nCompiled kernels require additional GPU memory:\n- **Compilation artifacts**: ~200-500 MB per shape\n- **Kernel cache**: Persistent across runs if disk caching is enabled\n- **Trade-off**: 3-5% memory overhead for 10-20% speed improvement\n\nSources: [docs/EN/source/getting_started/quickstart.md:1-369]()\n\n---\n\n## Implementation with Quantization\n\nJIT compilation works synergistically with quantization to achieve maximum performance:\n\n```mermaid\ngraph LR\n    Input[\"Input Tensor\u003cbr/\u003e(FP16/BF16)\"]\n    Quant[\"Quantize\u003cbr/\u003e(INT8/FP8)\"]\n    MatMul[\"Quantized MatMul\"]\n    Dequant[\"Dequantize\u003cbr/\u003e+ Scale\"]\n    Bias[\"Add Bias\"]\n    Act[\"Activation\u003cbr/\u003e(GELU)\"]\n    Output[\"Output Tensor\"]\n    \n    Input --\u003e Quant\n    Quant --\u003e MatMul\n    MatMul --\u003e Dequant\n    Dequant --\u003e Bias\n    Bias --\u003e Act\n    Act --\u003e Output\n    \n    FusedKernel[\"Fused Kernel\u003cbr/\u003e(torch.compile)\"]\n    FusedKernel -.fuses.-\u003e Quant\n    FusedKernel -.fuses.-\u003e MatMul\n    FusedKernel -.fuses.-\u003e Dequant\n    FusedKernel -.fuses.-\u003e Bias\n    FusedKernel -.fuses.-\u003e Act\n```\n\n**Diagram: Kernel Fusion with Quantization**\n\n### Triton Kernel Compilation\n\nThe custom Triton kernels for quantized operations include autotune decorators that work with `torch.compile`:\n\n**INT8 GEMM with Bias Fusion**:\n- Base operation: `int8_gemm_bias_triton(a, b, bias, a_scales, b_scales, fuse_gelu=True)`\n- Compiled version: Fuses quantization, matmul, scaling, bias addition, and GELU activation\n- Performance: 2-3 faster than unfused operations\n\n**Autotune Configuration**:\n```python\n@autotune(\n    configs=[\n        Config({\"BLOCK_M\": 256, \"BLOCK_N\": 128, \"BLOCK_K\": 128}, num_stages=3, num_warps=8),\n        Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 128}, num_stages=4, num_warps=8),\n        # Additional configurations...\n    ],\n    key=[\"M\", \"N\", \"K\"]\n)\n```\n\nThe autotune decorator tests multiple block size configurations and selects the fastest for each shape, which is then cached alongside the compiled graph.\n\nSources: [lightx2v/common/ops/mm/triton_kernels.py:90-269]()\n\n---\n\n## Usage in Distributed Inference\n\nWhen using distributed inference with sequence parallelism or tensor parallelism, compilation applies to each rank independently:\n\n```mermaid\ngraph TB\n    subgraph \"Rank 0\"\n        Compile0[\"torch.compile\u003cbr/\u003efor local shape\"]\n        Cache0[\"Shape cache\u003cbr/\u003e(rank 0)\"]\n        Execute0[\"Execute compiled\u003cbr/\u003ekernels\"]\n    end\n    \n    subgraph \"Rank 1\"\n        Compile1[\"torch.compile\u003cbr/\u003efor local shape\"]\n        Cache1[\"Shape cache\u003cbr/\u003e(rank 1)\"]\n        Execute1[\"Execute compiled\u003cbr/\u003ekernels\"]\n    end\n    \n    subgraph \"Rank N\"\n        CompileN[\"torch.compile\u003cbr/\u003efor local shape\"]\n        CacheN[\"Shape cache\u003cbr/\u003e(rank N)\"]\n        ExecuteN[\"Execute compiled\u003cbr/\u003ekernels\"]\n    end\n    \n    DistConfig[\"Distributed Config\u003cbr/\u003eseq_p_size=8\"]\n    \n    DistConfig --\u003e Compile0\n    DistConfig --\u003e Compile1\n    DistConfig --\u003e CompileN\n    \n    Compile0 --\u003e Cache0\n    Compile1 --\u003e Cache1\n    CompileN --\u003e CacheN\n    \n    Cache0 --\u003e Execute0\n    Cache1 --\u003e Execute1\n    CacheN --\u003e ExecuteN\n```\n\n**Diagram: Compilation in Distributed Setting**\n\n### Per-Rank Shapes\n\nWith **sequence parallelism** (Ulysses or Ring attention):\n- Sequence dimension is split across ranks\n- Each rank processes a different sequence length\n- Separate compilation for each rank's local shape\n\nExample with `seq_p_size=8` for 720128081 video:\n- Rank 0: processes frames [0, 10]  shape (B, C, 11, 90, 160)\n- Rank 1: processes frames [11, 20]  shape (B, C, 10, 90, 160)\n- Different shapes require separate compiled kernels\n\n### Compilation Overhead in Distributed Setting\n\n**First run**: All ranks compile simultaneously (total wall time  single-rank compilation time)  \n**Subsequent runs**: All ranks use cached kernels (no additional overhead)\n\nSources: [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh:1-22](), [lightx2v/common/ops/attn/utils/ring_comm.py:1-47]()\n\n---\n\n## Best Practices\n\n### Pre-Warming the Cache\n\nTo avoid compilation overhead during production inference:\n\n1. **Pre-compile common resolutions**: Run dummy inferences for all resolutions in `compile_shapes`\n2. **Save compiled cache**: PyTorch caches compiled kernels in `~/.triton/cache` and `torch_compile_cache/`\n3. **Deploy with cache**: Copy cache directories to production environment\n\n### Shape Selection Strategy\n\n**Recommended configurations**:\n\n| Use Case | Suggested Shapes | Rationale |\n|----------|------------------|-----------|\n| Web service | 3-5 common resolutions | Minimize cache size, cover 80% of requests |\n| Batch processing | All possible resolutions | One-time compilation cost amortized over many runs |\n| Research/development | Disable compilation | Faster iteration, avoid cache invalidation |\n\n### When to Disable Compilation\n\nDisable `compile: false` when:\n- **Rapid prototyping**: Model code changes frequently, invalidating cache\n- **Single-run inference**: Compilation overhead exceeds total inference time\n- **Memory constrained**: Cannot afford 200-500 MB overhead per shape\n- **Debugging**: Compilation hides stack traces and makes debugging harder\n\nSources: [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:1-74]()\n\n---\n\n## Configuration File Reference\n\n### Complete Example with Compilation\n\n```json\n{\n    \"compile\": true,\n    \"compile_shapes\": [\n        [480, 832],   // 16:9 landscape low-res\n        [720, 1280],  // 16:9 landscape high-res\n        [832, 480],   // 16:9 portrait low-res\n        [1280, 720],  // 16:9 portrait high-res\n        [576, 576],   // Square mid-res\n        [960, 960]    // Square high-res\n    ],\n    \n    // Other optimizations that work well with compilation\n    \"dit_quantized\": true,\n    \"dit_quant_scheme\": \"fp8-sgl\",\n    \"self_attn_1_type\": \"nbhd_attn\",\n    \"cross_attn_1_type\": \"flash_attn3\"\n}\n```\n\n### Integration with Other Features\n\n**Compatible optimizations**:\n-  Quantization (INT8, FP8) - excellent synergy\n-  Flash Attention / Sage Attention - compiled together\n-  CPU offloading - compilation happens on GPU\n-  Feature caching - orthogonal features\n-  Distributed inference - per-rank compilation\n\n**Incompatible features**:\n-  Dynamic LoRA switching - invalidates compiled graph\n-  Variable-length generation - creates too many shapes\n-  Dynamic attention masks - shape-dependent control flow\n\nSources: [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:1-74]()\n\n---\n\n## Performance Monitoring\n\n### Compilation Indicators\n\nDuring first-time inference with new shapes, expect to see:\n```\n[Torch Compile] Compiling forward pass for shape (B, C, 81, 90, 160)\n[Torch Compile] Compilation complete in 42.3s\n```\n\n### Measuring Speedup\n\nTo measure compilation benefits:\n\n1. **Baseline**: Run inference with `compile: false`\n2. **First run**: Enable compilation, measure total time (includes compilation)\n3. **Subsequent runs**: Measure inference time with cached kernels\n4. **Calculate speedup**: `speedup = baseline_time / cached_inference_time`\n\nExpected results:\n- First run: 30-60s slower (compilation overhead)\n- Subsequent runs: 10-20% faster\n- Break-even: After 3-6 inferences at the same resolution\n\nSources: [pyproject.toml:1-116]()\n\n---\n\n## Summary\n\nJIT compilation with shape caching provides:\n- **10-20% inference speedup** for repeated resolutions\n- **Kernel fusion** reduces memory bandwidth by 20-30%\n- **Shape specialization** eliminates runtime overhead\n- **One-time cost** of 30-60s compilation per resolution\n- **Production-ready** with proper cache management\n\nThe system is most effective when:\n- Serving a fixed set of resolutions in production\n- Running batch inference on large datasets\n- Combined with quantization for maximum performance\n- Cache can be pre-warmed and reused across deployments\n\nSources: [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json:1-74](), [lightx2v/common/ops/mm/triton_kernels.py:1-744]()"])</script><script>self.__next_f.push([1,"40:Taba9,"])</script><script>self.__next_f.push([1,"# Advanced Architecture Topics\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n\n\u003c/details\u003e\n\n\n\nThis page provides detailed technical documentation of advanced architectural components in LightX2V that enable efficient video generation. Topics covered include the internal structure of transformer blocks, rotary position embeddings (RoPE), specialized audio conditioning mechanisms, custom kernel implementations, distributed VAE processing, and dynamic weight loading strategies.\n\nFor high-level architecture concepts, see [System Architecture Overview](#1.2). For runner-specific implementations, see [Model Runners and Tasks](#5). For optimization techniques configuration, see [Performance Optimization](#6).\n\n---\n\n## Transformer Block Architecture\n\nThe WAN transformer architecture implements a specialized DiT (Diffusion Transformer) block structure with three sequential compute phases and adaptive layer normalization (AdaLN) modulation.\n\n### Block Structure\n\nEach transformer block (`WanTransformerAttentionBlock`) consists of three compute phases executed sequentially:\n\n1. **Phase 0: Self-Attention** - Spatial-temporal attention across video tokens\n2. **Phase 1: Cross-Attention** - Conditioning on text and image embeddings  \n3. **Phase 2: Feed-Forward Network** - Position-wise MLP with gated residuals\n\n```mermaid\ngraph TB\n    Input[\"Input Tensor x\u003cbr/\u003e[seq_len, dim]\"]\n    Embed[\"Time Embedding embed0\u003cbr/\u003e[1, 6*dim]\"]\n    \n    subgraph Phase0[\"Phase 0: Self-Attention\"]\n        Mod0[\"modulation.tensor\u003cbr/\u003e+ embed0  6 chunks\"]\n        Shift0[\"shift_msa, scale_msa\"]\n        Gate0[\"gate_msa\"]\n        Norm1[\"norm1\u003cbr/\u003eLayerNorm\"]\n        Modulate1[\"modulate(norm1_out,\u003cbr/\u003escale_msa, shift_msa)\"]\n        QKV[\"Q/K/V projections\u003cbr/\u003eself_attn_q/k/v\"]\n        RoPE[\"apply_rope_func\u003cbr/\u003e(q, k, cos_sin)\"]\n        SelfAttn[\"self_attn_1.apply\u003cbr/\u003eFlash/Sage Attention\"]\n        ProjO[\"self_attn_o\u003cbr/\u003eOutput projection\"]\n        Gate1[\"y_out * gate_msa\"]\n    end\n    \n    subgraph Phase1[\"Phase 1: Cross-Attention\"]\n        Norm3[\"norm3\u003cbr/\u003eLayerNorm\"]\n        CQ[\"cross_attn_q\u003cbr/\u003eQuery from x\"]\n        CKV[\"cross_attn_k/v\u003cbr/\u003eKey/Value from context\"]\n        CrossAttn[\"cross_attn_1.apply\u003cbr/\u003eText attention\"]\n        ImgKV[\"cross_attn_k_img/v_img\u003cbr/\u003eImage context\"]\n        ImgAttn[\"cross_attn_2.apply\u003cbr/\u003eImage attention\"]\n        Add[\"attn_out + img_attn_out\"]\n        ProjCO[\"cross_attn_o\u003cbr/\u003eOutput projection\"]\n        AddRes[\"x + attn_out\"]\n    end\n    \n    subgraph Phase2[\"Phase 2: Feed-Forward\"]\n        CShift[\"c_shift_msa, c_scale_msa\"]\n        CGate[\"c_gate_msa\"]\n        Norm2[\"norm2\u003cbr/\u003eLayerNorm\"]\n        Modulate2[\"modulate(norm2_out,\u003cbr/\u003ec_scale_msa, c_shift_msa)\"]\n        FFN0[\"ffn_0\u003cbr/\u003eLinear expansion\"]\n        GELU[\"GELU activation\"]\n        FFN2[\"ffn_2\u003cbr/\u003eLinear projection\"]\n        Gate2[\"y * c_gate_msa\"]\n        FinalAdd[\"x + gated_y\"]\n    end\n    \n    Output[\"Output x\u003cbr/\u003e[seq_len, dim]\"]\n    \n    Input --\u003e Phase0\n    Embed --\u003e Mod0\n    Mod0 --\u003e Shift0\n    Mod0 --\u003e Gate0\n    Mod0 --\u003e CShift\n    Mod0 --\u003e CGate\n    \n    Input --\u003e Norm1\n    Norm1 --\u003e Modulate1\n    Modulate1 --\u003e QKV\n    QKV --\u003e RoPE\n    RoPE --\u003e SelfAttn\n    SelfAttn --\u003e ProjO\n    ProjO --\u003e Gate1\n    \n    Gate1 --\u003e Phase1\n    Gate1 --\u003e Norm3\n    Norm3 --\u003e CQ\n    CQ --\u003e CrossAttn\n    CKV --\u003e CrossAttn\n    ImgKV --\u003e ImgAttn\n    CQ --\u003e ImgAttn\n    CrossAttn --\u003e Add\n    ImgAttn --\u003e Add\n    Add --\u003e ProjCO\n    ProjCO --\u003e AddRes\n    \n    AddRes --\u003e Phase2\n    AddRes --\u003e Norm2\n    Norm2 --\u003e Modulate2\n    Modulate2 --\u003e FFN0\n    FFN0 --\u003e GELU\n    GELU --\u003e FFN2\n    FFN2 --\u003e Gate2\n    Gate2 --\u003e FinalAdd\n    \n    FinalAdd --\u003e Output\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:130-153](), [lightx2v/models/networks/wan/weights/transformer_weights.py:145-216]()\n\n### AdaLN Modulation Mechanism\n\nThe Adaptive Layer Normalization (AdaLN) uses 6 learned parameters per block to modulate activations based on timestep embeddings:\n\n| Parameter | Phase | Application | Purpose |\n|-----------|-------|-------------|---------|\n| `shift_msa` | Phase 0 | Self-Attention | Shift normalized features |\n| `scale_msa` | Phase 0 | Self-Attention | Scale normalized features |\n| `gate_msa` | Phase 0 | Self-Attention | Gate residual connection |\n| `c_shift_msa` | Phase 2 | Feed-Forward | Shift normalized features |\n| `c_scale_msa` | Phase 2 | Feed-Forward | Scale normalized features |\n| `c_gate_msa` | Phase 2 | Feed-Forward | Gate residual connection |\n\nThe modulation function applies: `modulate(x, scale, shift) = x * (1 + scale) + shift`\n\n**Implementation variants:**\n- **Triton kernel** (`fuse_scale_shift_kernel`): Fused GPU kernel for performance\n- **Smooth modulation**: Pre-computed `smooth_norm1_weight` and `smooth_norm1_bias` for fused operation\n- **PyTorch native**: Standard tensor operations for compatibility\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:13-14](), [lightx2v/models/networks/wan/infer/transformer_infer.py:155-167](), [lightx2v/models/networks/wan/infer/transformer_infer.py:171-182]()\n\n### Weight Organization\n\nTransformer weights are organized hierarchically for efficient offloading and quantization:\n\n```mermaid\ngraph TB\n    TransWeights[\"WanTransformerWeights\"]\n    \n    subgraph BlockList[\"blocks: WeightModuleList[num_layers]\"]\n        Block0[\"Block 0\"]\n        Block1[\"Block 1\"]\n        BlockN[\"Block N-1\"]\n    end\n    \n    subgraph NonBlock[\"Non-block weights\"]\n        Norm[\"norm: LayerNorm\"]\n        Head[\"head: MMWeight\"]\n        HeadMod[\"head_modulation: Tensor\"]\n    end\n    \n    subgraph OffloadBuffers[\"CPU Offload Buffers (if enabled)\"]\n        CudaBuf[\"offload_block_cuda_buffers\u003cbr/\u003e2 blocks on GPU\"]\n        CpuBuf[\"offload_block_cpu_buffers\u003cbr/\u003ePinned memory\"]\n        PhaseBuf[\"offload_phase_cuda_buffers\u003cbr/\u003eSingle phase granularity\"]\n    end\n    \n    TransWeights --\u003e BlockList\n    TransWeights --\u003e NonBlock\n    TransWeights --\u003e OffloadBuffers\n    \n    Block0 --\u003e Phase0_0[\"Phase 0: WanSelfAttention\"]\n    Block0 --\u003e Phase1_0[\"Phase 1: WanCrossAttention\"]\n    Block0 --\u003e Phase2_0[\"Phase 2: WanFFN\"]\n    \n    Phase0_0 --\u003e Modulation0[\"modulation: Tensor[1,6,dim]\"]\n    Phase0_0 --\u003e SelfAttnWeights[\"norm1, self_attn_q/k/v\u003cbr/\u003eself_attn_norm_q/k\u003cbr/\u003eself_attn_1, self_attn_o\"]\n    \n    Phase1_0 --\u003e CrossAttnWeights[\"norm3, cross_attn_q/k/v\u003cbr/\u003ecross_attn_norm_q/k\u003cbr/\u003ecross_attn_k_img/v_img\u003cbr/\u003ecross_attn_norm_k_img\u003cbr/\u003ecross_attn_1/2, cross_attn_o\"]\n    \n    Phase2_0 --\u003e FFNWeights[\"norm2, ffn_0, ffn_2\"]\n```\n\n**Sources:** [lightx2v/models/networks/wan/weights/transformer_weights.py:11-42](), [lightx2v/models/networks/wan/weights/transformer_weights.py:145-216]()\n\n### Precision Management\n\nThe transformer uses mixed precision with sensitive layer handling:\n\n- **Inference dtype** (`infer_dtype`): `bfloat16` or `float16` for most operations\n- **Sensitive layer dtype** (`sensitive_layer_dtype`): `float32` for numerically unstable operations\n- **Sensitive layers**: `norm`, `modulation`, `time_embedding`, projections\n\nAutomatic conversion occurs at layer boundaries to maintain numerical stability while maximizing throughput.\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:60-61](), [lightx2v/models/networks/wan/infer/transformer_infer.py:111-115](), [lightx2v/models/networks/wan/infer/transformer_infer.py:175-185]()\n\n---\n\n## Rotary Position Embeddings (RoPE)\n\nLightX2V implements RoPE for spatial-temporal position encoding in video generation, with multiple backend implementations optimized for different hardware.\n\n### RoPE Mathematical Foundation\n\nRoPE applies rotary transformations to query and key vectors using complex-valued rotations:\n\n```\nfreqs = exp(i * _j) where _j = position * 10000^(-2j/d)\nq_rotated = q * freqs\nk_rotated = k * freqs\n```\n\nFor 3D video data (THW), frequencies are split across temporal and spatial dimensions.\n\n### Frequency Computation\n\n```mermaid\ngraph TB\n    HeadDim[\"head_dim = dim / num_heads\u003cbr/\u003ee.g., 128\"]\n    \n    subgraph FreqSplit[\"Frequency Dimension Split\"]\n        TDim[\"Temporal: head_dim//2 - 2*(head_dim//6)\"]\n        HDim[\"Height: head_dim//6\"]\n        WDim[\"Width: head_dim//6\"]\n    end\n    \n    subgraph FreqGen[\"Frequency Generation\"]\n        BaseFreq[\"Base freqs:\u003cbr/\u003erope_params(1024, dim, theta=10000)\"]\n        SplitFreq[\"Split into [t_freqs, h_freqs, w_freqs]\"]\n    end\n    \n    subgraph GridCompute[\"Grid-based Computation\"]\n        GridSize[\"grid_sizes = (f, h, w)\u003cbr/\u003eLatent dimensions\"]\n        Expand[\"Expand frequencies:\u003cbr/\u003efreqs[0][:f].expand(f,h,w,-1)\u003cbr/\u003efreqs[1][:h].expand(f,h,w,-1)\u003cbr/\u003efreqs[2][:w].expand(f,h,w,-1)\"]\n        Concat[\"Concatenate along feature dim\u003cbr/\u003eReshape to [seq_len, 1, head_dim//2]\"]\n    end\n    \n    subgraph NegativeRoPE[\"Negative Temporal RoPE\"]\n        RefFrame[\"Reference frame detection:\u003cbr/\u003ei2v/s2v tasks\"]\n        NegFreq[\"Negative frequencies\u003cbr/\u003efor reference frames\"]\n    end\n    \n    HeadDim --\u003e FreqSplit\n    FreqSplit --\u003e FreqGen\n    FreqGen --\u003e GridCompute\n    GridCompute --\u003e NegativeRoPE\n    GridSize --\u003e Expand\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/pre_infer.py:32-50](), [lightx2v/models/networks/wan/infer/pre_infer.py:55-94](), [lightx2v/models/networks/wan/infer/utils.py:127-140]()\n\n### Implementation Backends\n\nLightX2V provides four RoPE implementations selected via `rope_type` configuration:\n\n| Backend | Implementation | Use Case | File |\n|---------|---------------|----------|------|\n| `flashinfer` | `apply_rope_with_cos_sin_cache_inplace` | Default, fastest | [lightx2v/models/networks/wan/infer/utils.py:101-124]() |\n| `torch` | Complex tensor view + multiplication | CPU/compatibility | [lightx2v/models/networks/wan/infer/utils.py:12-28]() |\n| `torch_naive` | Explicit cos/sin rotation | Debugging | [lightx2v/models/networks/wan/infer/utils.py:31-68]() |\n| Platform-specific | Registry-based (`ROPE_REGISTER`) | Custom hardware | [lightx2v/models/networks/wan/infer/transformer_infer.py:42-57]() |\n\n**FlashInfer Implementation** (default):\n```python\n# Reshape to [L, H*D] for efficient inplace operation\nquery = xq.reshape(L, H * D).contiguous()\nkey = xk.reshape(L, H * D).contiguous()\n\n# Generate position indices\npositions = torch.arange(L, device=\"cpu\").to(xq.device, non_blocking=True)\n\n# Apply rotation inplace\napply_rope_with_cos_sin_cache_inplace(\n    positions=positions,\n    query=query,\n    key=key,\n    head_size=D,\n    cos_sin_cache=cos_sin_cache,\n    is_neox=False,\n)\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/utils.py:101-124](), [lightx2v/models/networks/wan/infer/transformer_infer.py:36-58]()\n\n### Chunked RoPE for Memory Efficiency\n\nFor long sequences, chunked processing prevents OOM errors:\n\n```python\ndef apply_wan_rope_with_chunk(xq, xk, cos_sin_cache, chunk_size, rope_func):\n    seq_len = cos_sin_cache.size(0)\n    x_q = torch.empty_like(xq)\n    x_k = torch.empty_like(xk)\n    \n    for start in range(0, seq_len, chunk_size):\n        end = min(start + chunk_size, seq_len)\n        xq_chunk = xq[start:end]\n        xk_chunk = xk[start:end]\n        cos_sin_chunk = cos_sin_cache[start:end]\n        xq_chunk_out, xk_chunk_out = rope_func(xq_chunk, xk_chunk, cos_sin_chunk)\n        x_q[start:end].copy_(xq_chunk_out, non_blocking=True)\n        x_k[start:end].copy_(xk_chunk_out, non_blocking=True)\n```\n\nConfigured via `rope_chunk=True` and `rope_chunk_size` (default 100).\n\n**Sources:** [lightx2v/models/networks/wan/infer/utils.py:71-98](), [lightx2v/models/networks/wan/infer/transformer_infer.py:55-58]()\n\n### Sequence Parallelism Integration\n\nWhen sequence parallelism is enabled, RoPE frequencies are distributed across ranks:\n\n```python\ndef compute_freqs_dist(s, c, grid_sizes, freqs, seq_p_group):\n    world_size = dist.get_world_size(seq_p_group)\n    cur_rank = dist.get_rank(seq_p_group)\n    \n    # Compute full frequency grid\n    freqs_i = compute_freqs(c, grid_sizes, freqs)\n    \n    # Pad to multiple of world_size * frame_count\n    freqs_i = pad_freqs(freqs_i, s * world_size)\n    \n    # Each rank gets a slice\n    s_per_rank = s\n    freqs_i_rank = freqs_i[(cur_rank * s_per_rank):((cur_rank + 1) * s_per_rank)]\n    return freqs_i_rank\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/utils.py:143-161](), [lightx2v/models/networks/wan/infer/pre_infer.py:74-83]()\n\n---\n\n## Audio Adapter and Cross-Attention\n\nThe audio adapter enables audio-driven video generation (talking head synthesis) through a specialized perceiver-based architecture that projects audio features into the transformer's latent space.\n\n### Audio Adapter Architecture\n\n```mermaid\ngraph TB\n    subgraph AudioInput[\"Audio Input Processing\"]\n        RawAudio[\"Raw Audio\u003cbr/\u003e[N_person, audio_len]\u003cbr/\u003e16kHz sample rate\"]\n        AudioSeg[\"AudioSegment\u003cbr/\u003e5-frame overlap\u003cbr/\u003esegment_audio()\"]\n        Encoder[\"SekoAudioEncoderModel\u003cbr/\u003eTencentGameMate Hubert\"]\n        AudioFeat[\"Audio Features\u003cbr/\u003e[N_person, T, 1024]\"]\n    end\n    \n    subgraph Adapter[\"AudioAdapter Module\"]\n        TimeEmb[\"Time Embedding\u003cbr/\u003esinusoidal_embedding_1d\"]\n        AudioProj[\"AudioProjection MLP\u003cbr/\u003e(1024, 1024, 32*1024)\u003cbr/\u003e3-layer with LayerNorm\"]\n        QueryTokens[\"Learnable Query Tokens\u003cbr/\u003en_query_tokens = 128\"]\n        \n        subgraph Perceiver[\"Perceiver Cross-Attention\"]\n            PerceiverLayers[\"4 Transformer Layers\u003cbr/\u003eprojection_transformer_layers\"]\n            CrossAttn[\"Cross-Attention:\u003cbr/\u003eQ: query_tokens\u003cbr/\u003eK,V: audio_proj(features)\"]\n            SelfAttn[\"Self-Attention:\u003cbr/\u003eQ,K,V: query_tokens\"]\n        end\n        \n        Hidden[\"hidden_states_aligned\u003cbr/\u003e[N_person, T, 128, 1024]\"]\n    end\n    \n    subgraph Integration[\"Integration with DiT\"]\n        PersonMask[\"person_mask_latens\u003cbr/\u003e[N_person, 1, H/2, W/2]\u003cbr/\u003eOptional for multi-person\"]\n        TimeAlign[\"Time Embedding Alignment\u003cbr/\u003eaudio_adapter_t_emb\"]\n        BlockAttn[\"Per-Block Audio Cross-Attention\u003cbr/\u003eaudio_attn in compute_phases\"]\n        Modulate[\"Modulated Integration\u003cbr/\u003eaudio_out * audio_gate\"]\n    end\n    \n    RawAudio --\u003e AudioSeg\n    AudioSeg --\u003e Encoder\n    Encoder --\u003e AudioFeat\n    \n    AudioFeat --\u003e AudioProj\n    TimeEmb --\u003e AudioProj\n    AudioProj --\u003e Perceiver\n    QueryTokens --\u003e CrossAttn\n    CrossAttn --\u003e SelfAttn\n    SelfAttn --\u003e Hidden\n    \n    Hidden --\u003e Integration\n    PersonMask --\u003e BlockAttn\n    TimeAlign --\u003e BlockAttn\n    BlockAttn --\u003e Modulate\n```\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:174-251](), [lightx2v/models/runners/wan/wan_audio_runner.py:751-781](), [lightx2v/models/input_encoders/hf/seko_audio/audio_adapter.py]()\n\n### Audio Segmentation with Overlap\n\nLong audio is segmented with 5-frame overlap to ensure temporal continuity:\n\n```python\ndef segment_audio(self, audio_array, expected_frames, max_num_frames, prev_frame_length=5):\n    segments = []\n    segments_idx = self.init_segments_idx(expected_frames, max_num_frames, prev_frame_length)\n    \n    for idx, (start_idx, end_idx) in enumerate(segments_idx):\n        audio_start, audio_end = self.get_audio_range(start_idx, end_idx)\n        audio_array = audio_array_ori[:, audio_start:audio_end]\n        \n        if idx \u003c len(segments_idx) - 1:\n            end_idx = segments_idx[idx + 1][0]  # Overlap with next segment\n        \n        segments.append(AudioSegment(audio_array, start_idx, end_idx))\n```\n\nThe overlap ensures smooth transitions between segments during video generation.\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:209-235](), [lightx2v/models/runners/wan/wan_audio_runner.py:237-251]()\n\n### Multi-Person Audio Support\n\nFor multi-person talking head generation, the system supports:\n\n1. **Multiple audio tracks**: One per person, loaded as `[N_person, max_len]` tensor\n2. **Person masks**: Binary masks `[N_person, 1, H/16, W/16]` indicating each person's spatial region\n3. **Parallel audio encoding**: Each person's audio encoded independently\n4. **Masked integration**: Audio features masked by person regions during attention\n\n```python\n# Load multi-person audio\naudio_arrays = []\nfor audio_path in audio_paths:\n    audio_array = self.load_audio(audio_path)\n    audio_arrays.append(audio_array)\n\n# Pad to max length\npadded = torch.zeros(num_files, max_len, dtype=torch.float32)\nfor i, arr in enumerate(audio_arrays):\n    padded[i, :length] = arr\n\n# Load corresponding masks\nmask_latents = [self.process_single_mask(mask_file) for mask_file in mask_files]\nmask_latents = torch.cat(mask_latents, dim=0)  # [N_person, 1, H/16, W/16]\n```\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:187-203](), [lightx2v/models/runners/wan/wan_audio_runner.py:330-345](), [lightx2v/models/runners/wan/wan_audio_runner.py:347-369]()\n\n### AudioProjection MLP\n\nThe audio projection network transforms audio encoder outputs to the transformer's feature space:\n\n```python\nclass AudioProjection:\n    def __init__(self, audio_feature_dim=1024, time_freq_dim=256, mlp_dims=(1024, 1024, 32*1024)):\n        # 3-layer MLP with LayerNorm\n        self.layers = [\n            Linear(audio_feature_dim + time_freq_dim, mlp_dims[0]),\n            LayerNorm(mlp_dims[0]),\n            GELU(),\n            Linear(mlp_dims[0], mlp_dims[1]),\n            LayerNorm(mlp_dims[1]),\n            GELU(),\n            Linear(mlp_dims[1], mlp_dims[2]),\n        ]\n```\n\nTime embeddings are concatenated with audio features before projection to provide temporal context.\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:763-775]()\n\n### Perceiver Cross-Attention Mechanism\n\nThe perceiver architecture compresses audio features through learnable query tokens:\n\n1. **Query tokens**: 128 learnable embeddings initialized randomly\n2. **Cross-attention**: Queries attend to projected audio features (K, V from audio)\n3. **Self-attention**: Queries attend to each other for feature refinement\n4. **4 transformer layers**: Repeated cross+self attention for feature extraction\n5. **Output**: `hidden_states_aligned` with shape `[N_person, T, 128, 1024]`\n\nEach query token learns to extract specific audio characteristics (phonemes, prosody, etc.) that drive facial animation.\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:569-576]()\n\n### Frame Preprocessing for Previous Frames\n\nThe system adds noise and masking to previous frames to prevent overfitting:\n\n```python\nclass FramePreprocessorTorchVersion:\n    def add_noise(self, frames, generator):\n        sigma = torch.normal(mean=-3.0, std=0.5, size=(bs,), device=device, generator=generator)\n        sigma = torch.exp(sigma)\n        noise = torch.randn(*shape, device=device, generator=generator) * sigma\n        return frames + noise\n    \n    def add_mask(self, frames, generator):\n        mask = torch.rand(h, w, device=device, generator=generator) \u003e 0.1\n        return frames * mask\n```\n\nThis prevents the model from simply copying previous frames and encourages audio-driven motion.\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:131-171]()\n\n---\n\n## Triton Kernel Implementation\n\nLightX2V implements custom Triton kernels for performance-critical operations, particularly quantized matrix multiplication and modulation.\n\n### Quantized GEMM Kernels\n\nThe INT8 and FP8 quantized GEMM implementations use Triton for fused dequantization and matrix multiplication:\n\n```mermaid\ngraph TB\n    subgraph INT8Flow[\"INT8 GEMM Pipeline\"]\n        ActInt8[\"Activation [M, K]\u003cbr/\u003edtype: float\"]\n        QuantAct[\"int8_quantize_triton\u003cbr/\u003ePer-token absmax\"]\n        ActQuant[\"Quantized Activation\u003cbr/\u003e[M, K] int8\u003cbr/\u003e+ scale [M, 1]\"]\n        WeightInt8[\"Weight [N, K]\u003cbr/\u003eint8 pre-quantized\u003cbr/\u003e+ scale [N, 1]\"]\n        GemmInt8[\"int8_gemm_triton or\u003cbr/\u003eint8_gemm_bias_triton\"]\n        Output[\"Output [M, N]\u003cbr/\u003edtype: float\"]\n    end\n    \n    subgraph FP8Flow[\"FP8 GEMM Pipeline\"]\n        ActFp8[\"Activation [M, K]\u003cbr/\u003edtype: float/bf16\"]\n        QuantFp8[\"fp8_quantize_triton\u003cbr/\u003eFP8_MAX_VAL clamping\"]\n        ActFp8Q[\"Quantized Activation\u003cbr/\u003e[M, K] float8_e4m3fn\u003cbr/\u003e+ scale [M, 1]\"]\n        WeightFp8[\"Weight [N, K]\u003cbr/\u003efloat8_e4m3fn\u003cbr/\u003e+ scale [N, 1]\"]\n        GemmFp8[\"fp8_gemm_triton or\u003cbr/\u003efp8_gemm_bias_triton\"]\n        OutputFp8[\"Output [M, N]\u003cbr/\u003edtype: float\"]\n    end\n    \n    subgraph TritonOpt[\"Triton Optimizations\"]\n        BlockTiling[\"Block Tiling\u003cbr/\u003eBLOCK_M  BLOCK_N  BLOCK_K\"]\n        SharedMem[\"Shared Memory\u003cbr/\u003eA_shared, B_shared\"]\n        FusedDequant[\"Fused Dequantization\u003cbr/\u003escale_a * scale_b * accumulator\"]\n        BiasAdd[\"Optional Bias Addition\u003cbr/\u003eFused in kernel\"]\n    end\n    \n    ActInt8 --\u003e QuantAct\n    QuantAct --\u003e ActQuant\n    ActQuant --\u003e GemmInt8\n    WeightInt8 --\u003e GemmInt8\n    GemmInt8 --\u003e Output\n    \n    ActFp8 --\u003e QuantFp8\n    QuantFp8 --\u003e ActFp8Q\n    ActFp8Q --\u003e GemmFp8\n    WeightFp8 --\u003e GemmFp8\n    GemmFp8 --\u003e OutputFp8\n    \n    GemmInt8 -.uses.-\u003e TritonOpt\n    GemmFp8 -.uses.-\u003e TritonOpt\n```\n\n**Sources:** [lightx2v/common/ops/mm/triton_kernels.py](), [lightx2v/common/ops/mm/mm_weight.py:9-16]()\n\n### INT8 Quantization Kernel\n\nThe INT8 quantization uses per-token absolute maximum scaling:\n\n```python\n@triton.jit\ndef int8_quantize_triton(x_ptr, scale_ptr, out_ptr, M, K, BLOCK_SIZE: tl.constexpr):\n    # Per-token quantization\n    pid = tl.program_id(0)\n    row_start = pid * K\n    \n    # Load token\n    k_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = k_offsets \u003c K\n    x = tl.load(x_ptr + row_start + k_offsets, mask=mask)\n    \n    # Compute scale: max(abs(x)) / 127\n    abs_x = tl.abs(x)\n    max_val = tl.max(abs_x)\n    scale = max_val / 127.0\n    \n    # Quantize: round(x / scale)\n    quantized = tl.libdevice.round(x / scale)\n    quantized = quantized.to(tl.int8)\n    \n    # Store\n    tl.store(out_ptr + row_start + k_offsets, quantized, mask=mask)\n    tl.store(scale_ptr + pid, scale)\n```\n\n**Key features:**\n- **Per-token scaling**: Each token (sequence element) gets its own scale factor\n- **Symmetric quantization**: Range [-127, 127] for INT8\n- **Fused operations**: Scale computation and quantization in single kernel\n\n**Sources:** [lightx2v/common/ops/mm/mm_weight.py:598-645]()\n\n### FP8 Quantization with Clamping\n\nFP8 quantization uses hardware-native float8_e4m3fn format with clamping:\n\n```python\nFP8_MAX_VAL = 448.0  # Max representable value in E4M3\n\n@triton.jit\ndef fp8_quantize_triton(x_ptr, scale_ptr, out_ptr, M, K, BLOCK_SIZE: tl.constexpr):\n    # Load token\n    x = tl.load(x_ptr + row_start + k_offsets, mask=mask)\n    \n    # Compute scale: max(abs(x)) / FP8_MAX_VAL\n    abs_x = tl.abs(x)\n    max_val = tl.max(abs_x)\n    scale = max_val / FP8_MAX_VAL\n    \n    # Quantize with clamping\n    scaled = x / scale\n    clamped = tl.clamp(scaled, -FP8_MAX_VAL, FP8_MAX_VAL)\n    quantized = clamped.to(tl.float8e4m3fn)\n    \n    tl.store(out_ptr + row_start + k_offsets, quantized, mask=mask)\n    tl.store(scale_ptr + pid, scale)\n```\n\n**Sources:** [lightx2v/common/ops/mm/mm_weight.py:647-690]()\n\n### Fused Scale-Shift Kernel\n\nThe modulation operation is fused into a single Triton kernel:\n\n```python\n@triton.jit\ndef fuse_scale_shift_kernel(x_ptr, scale_ptr, shift_ptr, out_ptr, N, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets \u003c N\n    \n    # Load inputs\n    x = tl.load(x_ptr + offsets, mask=mask)\n    scale = tl.load(scale_ptr + offsets, mask=mask)\n    shift = tl.load(shift_ptr + offsets, mask=mask)\n    \n    # Fused computation: x * (1 + scale) + shift\n    out = x * (1.0 + scale) + shift\n    \n    tl.store(out_ptr + offsets, out, mask=mask)\n```\n\nThis eliminates multiple kernel launches and memory reads/writes for the common AdaLN modulation pattern.\n\n**Sources:** [lightx2v/models/networks/wan/infer/triton_ops.py](), [lightx2v/models/networks/wan/infer/transformer_infer.py:32-35]()\n\n### GEMM Kernel Block Tiling\n\nThe GEMM kernels use hierarchical tiling for optimal cache utilization:\n\n| Tiling Level | Size | Purpose |\n|--------------|------|---------|\n| `BLOCK_M` | 128 | Output rows per thread block |\n| `BLOCK_N` | 128 | Output columns per thread block |\n| `BLOCK_K` | 32 | Reduction dimension per iteration |\n| Warp tiling | 16x16 | Per-warp computation tile |\n\n**Computation flow:**\n1. Load `BLOCK_M  BLOCK_K` of A into shared memory\n2. Load `BLOCK_K  BLOCK_N` of B into shared memory\n3. Each warp computes partial products using register tiles\n4. Accumulate across K dimension\n5. Apply scaling factors: `output = scale_a * scale_b * accumulator`\n6. Optionally add bias\n7. Store output block\n\n**Sources:** [lightx2v/common/ops/mm/triton_kernels.py]()\n\n### Backend Selection Strategy\n\nLightX2V selects GEMM backends based on configuration:\n\n```python\n# INT8 backends (in priority order):\n# 1. \"int8-triton\": Custom Triton kernel\n# 2. \"int8-vllm\": VLLM optimized kernel\n# 3. \"int8-torchao\": TorchAO native implementation\n# 4. \"int8-sgl\": SGLang kernel\n# 5. \"int8-q8f\": Q8 kernels (Ada architecture)\n\n# FP8 backends:\n# 1. \"fp8-triton\": Custom Triton kernel\n# 2. \"fp8-vllm\": VLLM FP8 support\n# 3. \"fp8-sgl\": SGLang FP8 kernel\n# 4. \"fp8-torchao\": TorchAO FP8\n```\n\nThe backend is selected via the `quant_scheme` configuration parameter.\n\n**Sources:** [lightx2v/common/ops/mm/mm_weight.py:252-329]()\n\n---\n\n## VAE Tiling and Distributed Processing\n\nThe VAE encoder/decoder implements advanced tiling and distributed processing strategies to handle high-resolution videos efficiently.\n\n### VAE Tiling Strategy\n\nTiling enables processing images larger than GPU memory by dividing them into overlapping tiles with blending:\n\n```mermaid\ngraph TB\n    subgraph Input[\"Input Video [B, C, T, H, W]\"]\n        FullVideo[\"Full Resolution Video\u003cbr/\u003ee.g., 128072081 frames\"]\n    end\n    \n    subgraph TileGen[\"Tile Generation\"]\n        TileSize[\"tile_size = 64\u003cbr/\u003etile_stride = 32\"]\n        NumTiles[\"num_tiles_h = ceil(H/stride)\u003cbr/\u003enum_tiles_w = ceil(W/stride)\"]\n        TileGrid[\"Tile Grid with Overlap:\u003cbr/\u003eoverlap = tile_size - tile_stride\"]\n    end\n    \n    subgraph Processing[\"Per-Tile Processing\"]\n        ExtractTile[\"Extract tile with padding:\u003cbr/\u003e[B, C, T, 64, 64]\"]\n        VAEProcess[\"VAE encode/decode\"]\n        TileOutput[\"Tile output\u003cbr/\u003e[B, C_out, T, 64, 64]\"]\n    end\n    \n    subgraph Blending[\"Tile Blending\"]\n        BlendV[\"blend_v: Vertical blend mask\u003cbr/\u003eLinear fade in overlap region\"]\n        BlendH[\"blend_h: Horizontal blend mask\u003cbr/\u003eLinear fade in overlap region\"]\n        WeightedSum[\"Weighted sum of overlapping tiles\u003cbr/\u003eoutput += tile * blend_v * blend_h\"]\n    end\n    \n    subgraph Output[\"Output Video\"]\n        Reconstructed[\"Reconstructed Video\u003cbr/\u003e[B, C_out, T, H, W]\u003cbr/\u003eSeamless result\"]\n    end\n    \n    FullVideo --\u003e TileGen\n    TileGen --\u003e Processing\n    Processing --\u003e Blending\n    Blending --\u003e Output\n    \n    TileSize --\u003e NumTiles\n    NumTiles --\u003e ExtractTile\n    ExtractTile --\u003e VAEProcess\n    VAEProcess --\u003e TileOutput\n    TileOutput --\u003e BlendV\n    TileOutput --\u003e BlendH\n    BlendV --\u003e WeightedSum\n    BlendH --\u003e WeightedSum\n    WeightedSum --\u003e Reconstructed\n```\n\n**Sources:** [lightx2v/models/video_encoders/hf/wan/vae.py](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py]()\n\n### Blend Mask Generation\n\nThe blend masks create smooth transitions in overlap regions:\n\n```python\ndef get_blend_mask(tile_size, stride, device):\n    \"\"\"Generate 1D blend mask for tile blending\"\"\"\n    overlap = tile_size - stride\n    \n    # Linear ramp from 0 to 1 in overlap region\n    mask = torch.ones(tile_size, device=device)\n    \n    # Left overlap: ramp up from 0 to 1\n    if overlap \u003e 0:\n        ramp = torch.linspace(0, 1, overlap, device=device)\n        mask[:overlap] = ramp\n    \n    # Right overlap: ramp down from 1 to 0\n    if overlap \u003e 0:\n        ramp = torch.linspace(1, 0, overlap, device=device)\n        mask[-overlap:] = ramp\n    \n    return mask\n\n# Create 2D blend mask\nblend_v = get_blend_mask(tile_h, stride_h, device).view(-1, 1)  # [tile_h, 1]\nblend_h = get_blend_mask(tile_w, stride_w, device).view(1, -1)  # [1, tile_w]\nblend_2d = blend_v * blend_h  # [tile_h, tile_w]\n```\n\n**Key properties:**\n- **Overlap blending**: Prevents seam artifacts at tile boundaries\n- **Linear interpolation**: Simple and effective for most cases\n- **2D separability**: Vertical and horizontal masks multiplied for efficiency\n\n**Sources:** [lightx2v/models/video_encoders/hf/wan/vae.py]()\n\n### 2D Grid Distributed Encoding\n\nFor very large videos, VAE processing is distributed across multiple GPUs in a 2D grid:\n\n```mermaid\ngraph TB\n    subgraph InputDist[\"Distributed Input\"]\n        FullImage[\"Full Image [B, C, H, W]\u003cbr/\u003ee.g., 19201080\"]\n        GridConfig[\"2D Grid: world_size_h  world_size_w\u003cbr/\u003ee.g., 24 = 8 GPUs\"]\n    end\n    \n    subgraph GridSplit[\"Grid Splitting\"]\n        CalcGrid[\"_calculate_2d_grid:\u003cbr/\u003eFactorize world_size\u003cbr/\u003ePrefer balanced grids\"]\n        AdjustDim[\"Adjust H, W to be divisible\u003cbr/\u003eMinimal padding\"]\n        SplitH[\"Split H into world_size_h chunks\"]\n        SplitW[\"Split W into world_size_w chunks\"]\n    end\n    \n    subgraph RankAssign[\"Rank Assignment\"]\n        Rank[\"Each rank gets:\u003cbr/\u003eH_chunk = H / world_size_h\u003cbr/\u003eW_chunk = W / world_size_w\"]\n        LocalTile[\"Local tile:\u003cbr/\u003e[B, C, H_chunk, W_chunk]\"]\n    end\n    \n    subgraph Process[\"Processing\"]\n        VAEEncode[\"VAE.encode() on local tile\"]\n        LocalLatent[\"Local latent:\u003cbr/\u003e[B, C_latent, H_chunk/8, W_chunk/8]\"]\n    end\n    \n    subgraph Gather[\"All-Gather\"]\n        Comm[\"dist.all_gather()\"]\n        Reconstruct[\"Reconstruct full latent:\u003cbr/\u003e[B, C_latent, H/8, W/8]\"]\n    end\n    \n    FullImage --\u003e GridConfig\n    GridConfig --\u003e CalcGrid\n    CalcGrid --\u003e AdjustDim\n    AdjustDim --\u003e SplitH\n    AdjustDim --\u003e SplitW\n    SplitH --\u003e Rank\n    SplitW --\u003e Rank\n    Rank --\u003e LocalTile\n    LocalTile --\u003e VAEEncode\n    VAEEncode --\u003e LocalLatent\n    LocalLatent --\u003e Comm\n    Comm --\u003e Reconstruct\n```\n\n**Grid selection algorithm:**\n\n```python\ndef _calculate_2d_grid(height, width, world_size):\n    \"\"\"Find optimal 2D grid factorization\"\"\"\n    # Prefer balanced grids: 24 over 18\n    priority_grids = []\n    if world_size == 8:\n        priority_grids = [(2, 4), (4, 2), (1, 8), (8, 1)]\n    elif world_size == 4:\n        priority_grids = [(2, 2), (1, 4), (4, 1)]\n    \n    # Try to find grid with no padding\n    for grid_h, grid_w in priority_grids:\n        if height % grid_h == 0 and width % grid_w == 0:\n            return grid_h, grid_w\n    \n    # Find grid with minimal padding\n    min_padding = float('inf')\n    best_grid = (1, world_size)\n    for grid_h, grid_w in priority_grids:\n        pad_h = (grid_h - (height % grid_h)) % grid_h\n        pad_w = (grid_w - (width % grid_w)) % grid_w\n        total_padding = pad_h + pad_w\n        if total_padding \u003c min_padding:\n            min_padding = total_padding\n            best_grid = (grid_h, grid_w)\n    \n    return best_grid\n```\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:302-352](), [lightx2v/models/video_encoders/hf/wan/vae.py]()\n\n### Streaming VAE Decoding\n\nFor progressive output, the VAE decoder can stream results frame-by-frame:\n\n```python\ndef decode_stream(self, latents):\n    \"\"\"Stream VAE decoding for progressive output\"\"\"\n    # latents: [C, T_latent, H_latent, W_latent]\n    # Output: yields [C, T_chunk, H, W] chunks\n    \n    # Decode in temporal chunks\n    chunk_size = 4  # Decode 4 latent frames at a time\n    \n    for start in range(0, latents.shape[1], chunk_size):\n        end = min(start + chunk_size, latents.shape[1])\n        latent_chunk = latents[:, start:end]\n        \n        # Add temporal padding for causal convolution\n        if start \u003e 0:\n            latent_chunk = torch.cat([\n                latents[:, start-1:start],  # Previous frame for context\n                latent_chunk\n            ], dim=1)\n        \n        # Decode chunk\n        decoded_chunk = self.decoder(latent_chunk)\n        \n        # Remove padding frame if added\n        if start \u003e 0:\n            decoded_chunk = decoded_chunk[:, 1:]\n        \n        yield decoded_chunk\n```\n\n**Benefits:**\n- **Progressive rendering**: Show results before full completion\n- **Memory efficiency**: Only one chunk in memory at a time\n- **Causal convolution cache**: Maintains temporal consistency across chunks\n\n**Sources:** [lightx2v/models/runners/default_runner.py:404-415](), [lightx2v/models/runners/wan/wan_audio_runner.py:633-659]()\n\n### Causal Convolution Caching\n\nThe VAE uses causal convolutions with caching to maintain temporal coherence:\n\n```python\nclass CausalConv3d:\n    def __init__(self, in_channels, out_channels, kernel_size):\n        self.cache = None  # Store previous frames\n        self.temporal_padding = kernel_size[0] - 1\n    \n    def forward(self, x):\n        # x: [B, C, T, H, W]\n        \n        # Add cached frames from previous chunks\n        if self.cache is not None:\n            x = torch.cat([self.cache, x], dim=2)\n        \n        # Apply convolution\n        output = self.conv3d(x)\n        \n        # Update cache with last frames\n        self.cache = x[:, :, -(self.temporal_padding):].clone()\n        \n        return output\n```\n\nThe cache (`CACHE_T=2`) stores the last 2 frames to provide context for the next chunk.\n\n**Sources:** [lightx2v/models/video_encoders/hf/wan/vae.py](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py]()\n\n---\n\n## Lazy Loading and Async Weight Streaming\n\nLightX2V implements sophisticated lazy loading and asynchronous weight streaming to enable inference with minimal VRAM by loading weights on-demand from disk.\n\n### Weight Hierarchy and Buffer Organization\n\n```mermaid\ngraph TB\n    subgraph Storage[\"Storage Hierarchy\"]\n        Disk[\"Disk Storage\u003cbr/\u003eSafeTensors files\u003cbr/\u003eBy block: blocks.0.weight,\u003cbr/\u003eblocks.1.weight, ...\"]\n        PinnedCPU[\"Pinned CPU Memory\u003cbr/\u003eFast host-to-device transfer\u003cbr/\u003eoffload_block_cpu_buffers[2]\"]\n        GPUBuf[\"GPU Buffers\u003cbr/\u003e2 blocks active\u003cbr/\u003eoffload_block_cuda_buffers[2]\"]\n        GPUActive[\"Active GPU Memory\u003cbr/\u003eCurrently executing block\"]\n    end\n    \n    subgraph LazyLoad[\"Lazy Loading Strategy\"]\n        OnDemand[\"On-Demand Loading\u003cbr/\u003eLoad block only when needed\"]\n        Prefetch[\"Async Prefetch\u003cbr/\u003eLoad next block while computing\"]\n        Evict[\"Eviction Strategy\u003cbr/\u003eMove old blocks to CPU\"]\n    end\n    \n    subgraph Granularity[\"Offload Granularity\"]\n        Model[\"model: Load entire model\u003cbr/\u003eper inference\"]\n        Block[\"block: Load 2 blocks\u003cbr/\u003ein rotation (default)\"]\n        Phase[\"phase: Load single phase\u003cbr/\u003ewithin block\"]\n    end\n    \n    Disk --\u003e PinnedCPU\n    PinnedCPU --\u003e GPUBuf\n    GPUBuf --\u003e GPUActive\n    \n    OnDemand --\u003e Block\n    OnDemand --\u003e Phase\n    Prefetch --\u003e Block\n    \n    Model -.used for.-\u003e LazyLoad\n    Block -.default.-\u003e LazyLoad\n    Phase -.finest.-\u003e LazyLoad\n```\n\n**Sources:** [lightx2v/models/networks/wan/weights/transformer_weights.py:55-133](), [lightx2v/common/ops/utils.py]()\n\n### Block-Level Offloading\n\nBlock-level offloading maintains 2 GPU buffers and rotates blocks through them:\n\n```python\nclass WeightAsyncStreamManager:\n    def __init__(self, blocks, offload_block_cuda_buffers, offload_block_cpu_buffers):\n        self.blocks = blocks  # All blocks on CPU\n        self.cuda_buffers = offload_block_cuda_buffers  # 2 GPU buffers\n        self.cpu_buffers = offload_block_cpu_buffers  # 2 CPU pinned buffers (if lazy_load)\n        self.current_idx = 0  # Current buffer index (0 or 1)\n        self.executor = ThreadPoolExecutor(max_workers=2)  # Async loading\n    \n    def prepare_block(self, block_idx):\n        \"\"\"Load block to GPU buffer asynchronously\"\"\"\n        buffer_idx = self.current_idx\n        \n        if self.lazy_load:\n            # Load from disk to CPU pinned buffer\n            self.cpu_buffers[buffer_idx].load_state_dict_from_disk(block_idx)\n            # Copy CPU pinned  GPU buffer\n            self.cuda_buffers[buffer_idx].load_state_dict(\n                self.cpu_buffers[buffer_idx].state_dict(), \n                block_index=None\n            )\n        else:\n            # Copy CPU  GPU buffer\n            self.cuda_buffers[buffer_idx].load_state_dict(\n                self.blocks[block_idx].state_dict(), \n                block_index=None\n            )\n        \n        # Move to GPU\n        self.cuda_buffers[buffer_idx].to_cuda(non_blocking=True)\n        \n        return self.cuda_buffers[buffer_idx]\n    \n    def advance(self):\n        \"\"\"Switch to next buffer\"\"\"\n        self.current_idx = 1 - self.current_idx\n```\n\n**Key optimizations:**\n- **Non-blocking transfers**: CPUGPU copies overlap with computation\n- **Pinned memory**: Faster host-to-device bandwidth\n- **Double buffering**: Prepare next block while computing current\n\n**Sources:** [lightx2v/common/ops/utils.py]()\n\n### Lazy Loading from Disk\n\nWhen `lazy_load=True`, weights are loaded from SafeTensors files on-demand:\n\n```python\ndef load_state_dict_from_disk(self, block_index, adapter_block_index=None):\n    \"\"\"Load weights from SafeTensors file\"\"\"\n    # Resolve block-specific key names\n    self.weight_name = resolve_block_name(self.weight_name, block_index)\n    self.bias_name = resolve_block_name(self.bias_name, block_index) if self.bias_name else None\n    \n    # Determine which file contains this block\n    lazy_load_file_path = get_lazy_load_file_path(self.lazy_load_file, self.weight_name)\n    \n    # Load directly to pinned CPU buffer\n    with safe_open(lazy_load_file_path, framework=\"pt\", device=\"cpu\") as f:\n        weight_tensor = f.get_tensor(self.weight_name).t()  # Transpose\n        self.pin_weight.copy_(weight_tensor)  # Copy to pinned buffer\n        \n        if self.bias_name is not None:\n            bias_tensor = f.get_tensor(self.bias_name)\n            self.pin_bias.copy_(bias_tensor)\n```\n\n**File organization strategies:**\n- **Single file**: All weights in one SafeTensors file\n- **Chunked**: Weights split across multiple files by layer range\n- **Per-block**: Each block in separate file for maximum parallelism\n\n**Sources:** [lightx2v/common/ops/mm/mm_weight.py:311-327]()\n\n### Prefetching Strategy\n\nAsynchronous prefetching loads the next block while the current block computes:\n\n```python\ndef infer_with_prefetch(blocks, x, pre_infer_out):\n    \"\"\"Inference with async prefetching\"\"\"\n    \n    # Warm up: Load first block\n    current_block = offload_manager.prepare_block(0)\n    \n    for block_idx in range(len(blocks)):\n        # Start prefetching next block (async)\n        if block_idx + 1 \u003c len(blocks):\n            next_future = executor.submit(offload_manager.prepare_block, block_idx + 1)\n        \n        # Compute current block\n        x = infer_block(current_block, x, pre_infer_out)\n        \n        # Switch to next buffer\n        offload_manager.advance()\n        \n        # Wait for prefetch to complete\n        if block_idx + 1 \u003c len(blocks):\n            current_block = next_future.result()\n    \n    return x\n```\n\n**Timeline visualization:**\n\n```\nBlock 0: [========COMPUTE========]\nBlock 1:          [==LOAD==][========COMPUTE========]\nBlock 2:                     [==LOAD==][========COMPUTE========]\n```\n\nLoad operations overlap with compute, hiding I/O latency.\n\n**Sources:** [lightx2v/models/networks/wan/infer/offload/transformer_infer.py]()\n\n### Phase-Level Offloading\n\nPhase-level offloading provides the finest granularity, loading each phase (self-attn, cross-attn, FFN) separately:\n\n```python\n# Phase offload structure\noffload_phase_cuda_buffers = [\n    Phase0(self_attn weights),\n    Phase1(cross_attn weights),\n    Phase2(ffn weights)\n]\n\ndef infer_block_phase_offload(block_idx, x, pre_infer_out):\n    \"\"\"Infer single block with phase-level offload\"\"\"\n    \n    # Load phase 0 weights\n    phase0_cuda = load_phase_to_cuda(block_idx, phase=0)\n    x = infer_self_attn(phase0_cuda, x, pre_infer_out)\n    unload_phase_to_cpu(phase0_cuda)\n    \n    # Load phase 1 weights\n    phase1_cuda = load_phase_to_cuda(block_idx, phase=1)\n    x = infer_cross_attn(phase1_cuda, x, pre_infer_out)\n    unload_phase_to_cpu(phase1_cuda)\n    \n    # Load phase 2 weights\n    phase2_cuda = load_phase_to_cuda(block_idx, phase=2)\n    x = infer_ffn(phase2_cuda, x, pre_infer_out)\n    unload_phase_to_cpu(phase2_cuda)\n    \n    return x\n```\n\n**Trade-offs:**\n- **Memory**: Minimal GPU memory (only one phase at a time)\n- **Performance**: ~50% slower than block-level due to more frequent transfers\n- **Use case**: Extremely memory-constrained environments (8GB VRAM)\n\n**Sources:** [lightx2v/models/networks/wan/weights/transformer_weights.py:99-132]()\n\n### Memory Strategy Comparison\n\n| Strategy | VRAM Usage | Speed | Configuration |\n|----------|------------|-------|---------------|\n| No offload | 100% (all blocks) | 1.0 (baseline) | `cpu_offload=False` |\n| Model offload | ~20% (single forward pass) | 2.0 slower | `offload_granularity=\"model\"` |\n| Block offload | ~10% (2 blocks) | 1.3 slower | `offload_granularity=\"block\"` (default) |\n| Phase offload | ~5% (1 phase) | 1.5 slower | `offload_granularity=\"phase\"` |\n| Lazy load | ~5% + disk I/O | 2.0 slower | `lazy_load=True` + block/phase offload |\n\n**Sources:** [lightx2v/models/networks/wan/model.py:161-199]()\n\n### Warm-Up and Buffer Initialization\n\nBefore inference, buffers are warmed up to avoid first-iteration latency:\n\n```python\ndef warm_up_cpu_buffers(self):\n    \"\"\"Pre-allocate and initialize CPU buffers\"\"\"\n    # Load first 2 blocks to CPU buffers\n    for i in range(min(2, len(self.blocks))):\n        self.cpu_buffers[i].load_state_dict_from_disk(block_index=i)\n    \n    # Pin memory for fast transfer\n    for buf in self.cpu_buffers:\n        buf.pin_memory()\n```\n\nThis eliminates cold-start delays on the first inference step.\n\n**Sources:** [lightx2v/common/ops/utils.py]()\n\n---\n\n**Page Sources:**\n- [lightx2v/models/networks/wan/infer/transformer_infer.py]()\n- [lightx2v/models/networks/wan/weights/transformer_weights.py]()\n- [lightx2v/models/networks/wan/infer/utils.py]()\n- [lightx2v/models/networks/wan/infer/pre_infer.py]()\n- [lightx2v/models/runners/wan/wan_audio_runner.py]()\n- [lightx2v/models/networks/wan/audio_model.py]()\n- [lightx2v/common/ops/mm/triton_kernels.py]()\n- [lightx2v/common/ops/mm/mm_weight.py]()\n- [lightx2v/models/video_encoders/hf/wan/vae.py]()\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py]()\n- [lightx2v/models/runners/wan/wan_runner.py]()\n- [lightx2v/common/ops/utils.py]()\n- [lightx2v/models/networks/wan/model.py]()"])</script><script>self.__next_f.push([1,"41:T83d6,"])</script><script>self.__next_f.push([1,"# Transformer Block Architecture\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py](lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py)\n- [lightx2v/models/networks/qwen_image/infer/pre_infer.py](lightx2v/models/networks/qwen_image/infer/pre_infer.py)\n- [lightx2v/models/networks/qwen_image/infer/transformer_infer.py](lightx2v/models/networks/qwen_image/infer/transformer_infer.py)\n- [lightx2v/models/networks/qwen_image/model.py](lightx2v/models/networks/qwen_image/model.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/qwen_image/qwen_image_runner.py](lightx2v/models/runners/qwen_image/qwen_image_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/schedulers/qwen_image/scheduler.py](lightx2v/models/schedulers/qwen_image/scheduler.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/qwen_image/vae.py](lightx2v/models/video_encoders/hf/qwen_image/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the internal architecture of transformer blocks used in LightX2V's DiT (Diffusion Transformer) models. It covers the weight organization, compute phases, attention mechanisms, and modulation strategies within individual transformer blocks. For information about the overall three-stage inference pipeline (pre_infer, transformer_infer, post_infer), see [4.2](#4.2). For attention operator implementations and sparse attention patterns, see [6.2](#6.2).\n\nThe framework supports two primary transformer architectures:\n- **WAN DiT**: Used in WAN 2.1/2.2 models for video generation\n- **Qwen Image DiT**: Used in Qwen-based image generation models\n\nBoth architectures share common design principles but differ in their conditioning mechanisms and sequence organization.\n\n---\n\n## Three-Tier Weight Organization\n\nTransformer models are organized into three hierarchical weight structures that separate concerns and enable efficient memory management:\n\n```mermaid\ngraph TB\n    subgraph \"Model Structure\"\n        PreWeights[\"PreWeights\u003cbr/\u003eInput projection\u003cbr/\u003eTime embeddings\"]\n        TransformerWeights[\"TransformerWeights\u003cbr/\u003eN transformer blocks\"]\n        PostWeights[\"PostWeights\u003cbr/\u003eOutput projection\u003cbr/\u003eNormalization\"]\n    end\n    \n    subgraph \"TransformerWeights Detail\"\n        BlocksList[\"WeightModuleList[blocks]\u003cbr/\u003e40-60 blocks\"]\n        OffloadBuffers[\"Offload Buffers\u003cbr/\u003e(optional CPU buffers)\"]\n        NonBlockWeights[\"Non-block weights\u003cbr/\u003enorm, head, head_modulation\"]\n    end\n    \n    PreWeights --\u003e TransformerWeights\n    TransformerWeights --\u003e PostWeights\n    TransformerWeights --\u003e BlocksList\n    TransformerWeights --\u003e OffloadBuffers\n    TransformerWeights --\u003e NonBlockWeights\n    \n    subgraph \"Single Block Structure\"\n        AttentionBlock[\"WanTransformerAttentionBlock\u003cbr/\u003eor\u003cbr/\u003eQwenImageTransformerAttentionBlock\"]\n        ComputePhases[\"compute_phases\u003cbr/\u003eWeightModuleList\"]\n    end\n    \n    BlocksList --\u003e AttentionBlock\n    AttentionBlock --\u003e ComputePhases\n```\n\n**Sources:**\n- [lightx2v/models/networks/wan/weights/transformer_weights.py:11-54]()\n- [lightx2v/models/networks/qwen_image/weights/transformer_weights.py:10-35]()\n- [lightx2v/models/networks/qwen_image/model.py:22-26]()\n\n---\n\n## Transformer Block Compute Phases\n\nEach transformer block consists of sequential compute phases implemented as a `WeightModuleList`. The phases encapsulate weights and are executed in order during inference.\n\n### WAN Block Structure\n\n```mermaid\ngraph LR\n    Block[\"WanTransformerAttentionBlock\u003cbr/\u003eblock_index=i\"]\n    \n    subgraph \"compute_phases[3]\"\n        Phase0[\"0: WanSelfAttention\u003cbr/\u003eQ,K,V projections\u003cbr/\u003eRMS norms\u003cbr/\u003eAttention operator\"]\n        Phase1[\"1: WanCrossAttention\u003cbr/\u003eText/Image conditioning\u003cbr/\u003eK,V projections\u003cbr/\u003eAttention operator\"]\n        Phase2[\"2: WanFFN\u003cbr/\u003eFeed-forward network\u003cbr/\u003eGELU activation\u003cbr/\u003e2-layer MLP\"]\n    end\n    \n    Block --\u003e Phase0\n    Phase0 --\u003e Phase1\n    Phase1 --\u003e Phase2\n```\n\n### Qwen Image Block Structure\n\n```mermaid\ngraph LR\n    Block[\"QwenImageTransformerAttentionBlock\u003cbr/\u003eblock_index=i\"]\n    \n    subgraph \"compute_phases[4]\"\n        Phase0[\"0: QwenImageImgAttention\u003cbr/\u003eImage stream Q,K,V\u003cbr/\u003eRMS norms\u003cbr/\u003eRoPE application\"]\n        Phase1[\"1: QwenImageTxtAttention\u003cbr/\u003eText stream Q,K,V\u003cbr/\u003eContext projections\u003cbr/\u003eRMS norms\"]\n        Phase2[\"2: QwenImageCrossAttention\u003cbr/\u003eJoint attention\u003cbr/\u003eConcatenate streams\u003cbr/\u003eOutput projections\"]\n        Phase3[\"3: QwenImageFFN\u003cbr/\u003eDual-stream FFN\u003cbr/\u003eSeparate img/txt MLPs\"]\n    end\n    \n    Block --\u003e Phase0\n    Phase0 --\u003e Phase1\n    Phase1 --\u003e Phase2\n    Phase2 --\u003e Phase3\n```\n\n**Key Differences:**\n- WAN uses 3 phases with unified processing\n- Qwen Image uses 4 phases with dual-stream (image/text) processing\n- Qwen Image applies rotary position embeddings (RoPE) in attention phases\n\n**Sources:**\n- [lightx2v/models/networks/wan/weights/transformer_weights.py:145-216]()\n- [lightx2v/models/networks/qwen_image/weights/transformer_weights.py:108-186]()\n\n---\n\n## Self-Attention Weight Components\n\nSelf-attention phases contain Query, Key, Value projection weights, normalization parameters, and attention operator registration.\n\n### WAN Self-Attention Weights\n\n```mermaid\ngraph TB\n    SelfAttn[\"WanSelfAttention\"]\n    \n    subgraph \"Modulation \u0026 Norm\"\n        Modulation[\"modulation\u003cbr/\u003eTENSOR_REGISTER\"]\n        Norm1[\"norm1\u003cbr/\u003eLN_WEIGHT_REGISTER\"]\n    end\n    \n    subgraph \"QKV Projections\"\n        Q[\"self_attn_q\u003cbr/\u003eMM_WEIGHT_REGISTER[mm_type]\u003cbr/\u003eweight + bias\"]\n        K[\"self_attn_k\u003cbr/\u003eMM_WEIGHT_REGISTER[mm_type]\u003cbr/\u003eweight + bias\"]\n        V[\"self_attn_v\u003cbr/\u003eMM_WEIGHT_REGISTER[mm_type]\u003cbr/\u003eweight + bias\"]\n        O[\"self_attn_o\u003cbr/\u003eOutput projection\u003cbr/\u003eweight + bias\"]\n    end\n    \n    subgraph \"Normalization\"\n        NormQ[\"self_attn_norm_q\u003cbr/\u003eRMS_WEIGHT_REGISTER[rms_type]\"]\n        NormK[\"self_attn_norm_k\u003cbr/\u003eRMS_WEIGHT_REGISTER[rms_type]\"]\n    end\n    \n    subgraph \"Attention Operator\"\n        AttnOp[\"self_attn_1\u003cbr/\u003eATTN_WEIGHT_REGISTER[self_attn_1_type]\u003cbr/\u003ee.g., flash_attn3, sage_attn, svg_attn\"]\n    end\n    \n    SelfAttn --\u003e Modulation\n    SelfAttn --\u003e Norm1\n    SelfAttn --\u003e Q\n    SelfAttn --\u003e K\n    SelfAttn --\u003e V\n    SelfAttn --\u003e O\n    SelfAttn --\u003e NormQ\n    SelfAttn --\u003e NormK\n    SelfAttn --\u003e AttnOp\n```\n\nThe `mm_type` parameter determines quantization scheme (e.g., `\"Default\"`, `\"nvfp4\"`, `\"int8\"`), while `rms_type` selects the RMS normalization implementation (e.g., `\"sgl-kernel\"`, `\"self_forcing\"`).\n\n**Sources:**\n- [lightx2v/models/networks/wan/weights/transformer_weights.py:218-418]()\n- [lightx2v/models/networks/qwen_image/weights/transformer_weights.py:189-308]()\n\n---\n\n## Cross-Attention Weight Components\n\nCross-attention integrates conditioning information (text prompts, reference images, audio features) into the latent representation.\n\n### WAN Cross-Attention Structure\n\n```mermaid\ngraph TB\n    CrossAttn[\"WanCrossAttention\"]\n    \n    subgraph \"Text Conditioning Path\"\n        Norm3[\"norm3\u003cbr/\u003eLayer normalization\"]\n        CrossQ[\"cross_attn_q\u003cbr/\u003eQuery from latents\"]\n        CrossK[\"cross_attn_k\u003cbr/\u003eKey from text\"]\n        CrossV[\"cross_attn_v\u003cbr/\u003eValue from text\"]\n        CrossO[\"cross_attn_o\u003cbr/\u003eOutput projection\"]\n    end\n    \n    subgraph \"Image Conditioning Path (Optional)\"\n        CrossKImg[\"cross_attn_k_img\u003cbr/\u003eKey from image encoder\"]\n        CrossVImg[\"cross_attn_v_img\u003cbr/\u003eValue from image encoder\"]\n        NormKImg[\"cross_attn_norm_k_img\u003cbr/\u003eRMS normalization\"]\n        CrossAttn2[\"cross_attn_2\u003cbr/\u003eSecondary attention operator\"]\n    end\n    \n    subgraph \"Attention Operators\"\n        CrossAttn1[\"cross_attn_1\u003cbr/\u003eATTN_WEIGHT_REGISTER[cross_attn_1_type]\"]\n    end\n    \n    CrossAttn --\u003e Norm3\n    CrossAttn --\u003e CrossQ\n    CrossAttn --\u003e CrossK\n    CrossAttn --\u003e CrossV\n    CrossAttn --\u003e CrossO\n    CrossAttn --\u003e CrossAttn1\n    \n    CrossAttn --\u003e CrossKImg\n    CrossAttn --\u003e CrossVImg\n    CrossAttn --\u003e NormKImg\n    CrossAttn --\u003e CrossAttn2\n```\n\n**Image conditioning is used when:**\n- `task` is `\"i2v\"`, `\"flf2v\"`, `\"animate\"`, or `\"s2v\"`\n- `use_image_encoder` is `True`\n- `model_cls` is not `\"wan2.1_sf_mtxg2\"`\n\n**Sources:**\n- [lightx2v/models/networks/wan/weights/transformer_weights.py:420-579]()\n- [lightx2v/models/networks/qwen_image/weights/transformer_weights.py:431-502]()\n\n---\n\n## Feed-Forward Network (FFN) Structure\n\nFFN applies non-linear transformations after attention operations, typically using a two-layer MLP with GELU activation.\n\n```mermaid\ngraph LR\n    Input[\"Input\u003cbr/\u003ehidden_states\"]\n    \n    subgraph \"WanFFN\"\n        Norm2[\"norm2\u003cbr/\u003eLayer norm\"]\n        FFN0[\"ffn_0\u003cbr/\u003eExpand dimension\u003cbr/\u003eMM_WEIGHT_REGISTER\"]\n        GELU[\"GELU activation\u003cbr/\u003etanh approximation\"]\n        FFN2[\"ffn_2\u003cbr/\u003eProject back\u003cbr/\u003eMM_WEIGHT_REGISTER\"]\n    end\n    \n    subgraph \"Optional Smooth Quantization\"\n        SmoothNorm2Weight[\"smooth_norm2_weight\"]\n        SmoothNorm2Bias[\"smooth_norm2_bias\"]\n    end\n    \n    Input --\u003e Norm2\n    Norm2 --\u003e FFN0\n    FFN0 --\u003e GELU\n    GELU --\u003e FFN2\n    \n    Note[\"Advanced PTQ quantization\u003cbr/\u003eadds smoothing parameters\"]\n    SmoothNorm2Weight -.-\u003e Note\n    SmoothNorm2Bias -.-\u003e Note\n```\n\n**Configuration:**\n- Quantization controlled by `quant_method` (e.g., `\"advanced_ptq\"`)\n- FFN expansion ratio typically 4x hidden dimension\n- GELU uses `tanh` approximation for efficiency\n\n**Sources:**\n- [lightx2v/models/networks/wan/weights/transformer_weights.py:581-657]()\n- [lightx2v/models/networks/qwen_image/infer/transformer_infer.py:246-276]()\n\n---\n\n## Attention Operator Registry and Selection\n\nLightX2V uses a registry pattern to dynamically select attention implementations based on configuration. This enables swapping between different operators without code changes.\n\n### Operator Registration System\n\n```mermaid\ngraph TB\n    Registry[\"ATTN_WEIGHT_REGISTER\u003cbr/\u003elightx2v/utils/registry_factory.py\"]\n    \n    subgraph \"Flash Attention Family\"\n        Flash2[\"flash_attn2\u003cbr/\u003eFlashAttn2Weight\"]\n        Flash3[\"flash_attn3\u003cbr/\u003eFlashAttn3Weight\"]\n    end\n    \n    subgraph \"Sage Attention\"\n        Sage2[\"sage_attn2\u003cbr/\u003eSageAttn2Weight\"]\n        Sage3[\"sage_attn3\u003cbr/\u003eSageAttn3Weight\"]\n    end\n    \n    subgraph \"Sparse Attention\"\n        SVG[\"svg_attn\u003cbr/\u003eSvgAttnWeight\"]\n        SLA[\"sla_attn\u003cbr/\u003eSlaAttnWeight\"]\n        Nbhd[\"nbhd_attn\u003cbr/\u003eNbhdAttnWeight\"]\n        GeneralSparse[\"general_sparse_attn\u003cbr/\u003eGeneralSparseAttnWeight\"]\n    end\n    \n    subgraph \"Parallel Attention\"\n        Ulysses[\"ulysses\u003cbr/\u003eUlyssesAttnWeight\"]\n        Ring[\"ring\u003cbr/\u003eRingAttnWeight\"]\n    end\n    \n    Registry --\u003e Flash2\n    Registry --\u003e Flash3\n    Registry --\u003e Sage2\n    Registry --\u003e Sage3\n    Registry --\u003e SVG\n    Registry --\u003e SLA\n    Registry --\u003e Nbhd\n    Registry --\u003e GeneralSparse\n    Registry --\u003e Ulysses\n    Registry --\u003e Ring\n```\n\n### Selection in Configuration\n\nAttention operators are selected via JSON configuration:\n\n```json\n{\n    \"self_attn_1_type\": \"flash_attn3\",\n    \"cross_attn_1_type\": \"flash_attn3\",\n    \"cross_attn_2_type\": \"flash_attn3\"\n}\n```\n\nFor sparse attention with pluggable components:\n\n```json\n{\n    \"self_attn_1_type\": \"general_sparse_attn\",\n    \"general_sparse_attn_setting\": {\n        \"sparse_mask_generator\": \"svg_mask_generator\",\n        \"sparse_operator\": \"magi_operator\",\n        \"sparse_setting\": {\n            \"svg_sparsity\": 0.8\n        }\n    }\n}\n```\n\n**Sources:**\n- [lightx2v/utils/registry_factory.py:59-78]()\n- [lightx2v/common/ops/attn/__init__.py:1-16]()\n- [lightx2v/models/networks/wan/weights/transformer_weights.py:341-388]()\n- [configs/attentions/wan_i2v_svg_magi_480p.json:1-23]()\n\n---\n\n## Adaptive Layer Normalization (AdaLN) Modulation\n\nBoth architectures use adaptive modulation to inject timestep and conditioning information into the transformer blocks. This follows the AdaLN design from DiT papers.\n\n### WAN Modulation Implementation\n\nWAN models compute modulation parameters once per block via the `pre_process` method:\n\n```mermaid\ngraph TB\n    Input[\"pre_infer_out.embed0\u003cbr/\u003eshape: [1, 6*hidden_dim]\"]\n    \n    subgraph \"Pre-Process Method\"\n        AddMod[\"modulation.tensor + embed0\"]\n        Chunk[\"chunk(6, dim=1)\"]\n        Components[\"shift_msa, scale_msa, gate_msa\u003cbr/\u003ec_shift_msa, c_scale_msa, c_gate_msa\"]\n    end\n    \n    subgraph \"Self-Attention Modulation\"\n        Norm1[\"norm1.apply(x)\"]\n        ModFunc[\"modulate_func(norm1_out, scale_msa, shift_msa)\u003cbr/\u003e x * (1 + scale) + shift\"]\n        GateSA[\"y_out * gate_msa\"]\n    end\n    \n    subgraph \"FFN Modulation\"\n        Norm2[\"norm2.apply(x)\"]\n        ModFunc2[\"modulate_func(norm2_out, c_scale_msa, c_shift_msa)\"]\n        GateFFN[\"ffn_out * c_gate_msa\"]\n    end\n    \n    Input --\u003e AddMod\n    AddMod --\u003e Chunk\n    Chunk --\u003e Components\n    Components --\u003e Norm1\n    Components --\u003e Norm2\n    Norm1 --\u003e ModFunc\n    Norm2 --\u003e ModFunc2\n    ModFunc --\u003e GateSA\n    ModFunc2 --\u003e GateFFN\n```\n\n**Modulation Function Selection:**\nThe `modulate_func` can use two implementations based on `config[\"modulate_type\"]`:\n- `\"triton\"`: Uses `fuse_scale_shift_kernel` (faster, fused kernel)\n- `\"default\"`: Uses `modulate(x, scale, shift)` (pure PyTorch)\n\n**Block-level modulation tensor structure:**\n```\nmodulation.tensor: [1, 6*hidden_dim]\n [0:hidden_dim]     shift_msa     (self-attention shift)\n [1:hidden_dim]     scale_msa     (self-attention scale)\n [2:hidden_dim]     gate_msa      (self-attention gate)\n [3:hidden_dim]     c_shift_msa   (FFN shift)\n [4:hidden_dim]     c_scale_msa   (FFN scale)\n [5:hidden_dim]     c_gate_msa    (FFN gate)\n```\n\n**Smooth Quantization Support:**\nAdvanced PTQ quantization adds smoothing parameters:\n```\nsmooth_norm1_weight = (1 + scale_msa) * phase.smooth_norm1_weight.tensor\nsmooth_norm1_bias = shift_msa * phase.smooth_norm1_bias.tensor\n```\n\n**Sources:**\n- [lightx2v/models/networks/wan/infer/transformer_infer.py:158-170]()\n- [lightx2v/models/networks/wan/infer/transformer_infer.py:17-18]()\n- [lightx2v/models/networks/wan/infer/transformer_infer.py:32-35]()\n- [lightx2v/models/networks/wan/infer/triton_ops.py:1-50]()\n\n### Qwen Image Dual-Stream Modulation\n\nQwen Image models maintain separate modulation for image and text streams, computed in `QwenImagePreInfer`:\n\n```mermaid\ngraph TB\n    subgraph \"Pre-Infer Modulation Setup\"\n        TembImg[\"temb_img = timestep_embedding(t)\u003cbr/\u003e+ img_cond_embedding\"]\n        TembTxt[\"temb_txt = timestep_embedding(t)\u003cbr/\u003e+ txt_cond_embedding\"]\n        SiLU[\"F.silu activation\"]\n    end\n    \n    subgraph \"Image Stream Modulation\"\n        ImgMod[\"img_mod.apply(temb_img_silu)\u003cbr/\u003e [1, num_layers, 6*hidden_dim]\"]\n        ImgSplit[\"For each layer:\u003cbr/\u003eimg_mod1 = mod[:, :, :3*dim]\u003cbr/\u003eimg_mod2 = mod[:, :, 3*dim:]\"]\n    end\n    \n    subgraph \"Text Stream Modulation\"\n        TxtMod[\"txt_mod.apply(temb_txt_silu)\u003cbr/\u003e [1, num_layers, 6*hidden_dim]\"]\n        TxtSplit[\"For each layer:\u003cbr/\u003etxt_mod1 = mod[:, :, :3*dim]\u003cbr/\u003etxt_mod2 = mod[:, :, 3*dim:]\"]\n    end\n    \n    subgraph \"Layer Selection (Layered Mode)\"\n        ModIndex[\"modulate_index\u003cbr/\u003e[batch, seq_len]\u003cbr/\u003eValues: 0 or 1\"]\n        Select[\"fuse_scale_shift_gate_select01_kernel\u003cbr/\u003eSelects between two mod sets\"]\n    end\n    \n    TembImg --\u003e SiLU\n    TembTxt --\u003e SiLU\n    SiLU --\u003e ImgMod\n    SiLU --\u003e TxtMod\n    ImgMod --\u003e ImgSplit\n    TxtMod --\u003e TxtSplit\n    ImgSplit --\u003e ModIndex\n    TxtSplit --\u003e ModIndex\n    ModIndex --\u003e Select\n```\n\n**Modulation Application:**\nEach modulation output contains 6 parameters:\n```\nimg_mod1: shift1_img, scale1_img, gate1_img  (for img_norm1)\nimg_mod2: shift2_img, scale2_img, gate2_img  (for img_norm2)\ntxt_mod1: shift1_txt, scale1_txt, gate1_txt  (for txt_norm1)\ntxt_mod2: shift2_txt, scale2_txt, gate2_txt  (for txt_norm2)\n```\n\n**Zero Text Conditioning:**\nWhen `zero_cond_t=True`, text modulation is zeroed for ablation studies:\n```python\ntxt_mod1 = torch.zeros_like(txt_mod1)\ntxt_mod2 = torch.zeros_like(txt_mod2)\n```\n\n**Sources:**\n- [lightx2v/models/networks/qwen_image/infer/pre_infer.py:1-63]()\n- [lightx2v/models/networks/qwen_image/infer/transformer_infer.py:56-94]()\n- [lightx2v/models/networks/qwen_image/infer/transformer_infer.py:96-123]()\n- [lightx2v/models/networks/qwen_image/infer/triton_ops.py:1-100]()\n\n---\n\n## Block Inference Execution Flow\n\nThis section maps the weight components to their execution during inference, showing how data flows through a single transformer block. The execution follows the `infer_block` method in transformer inference classes.\n\n### WAN Block Inference Code Flow\n\nThe `WanTransformerInfer.infer_block` method orchestrates the execution:\n\n```mermaid\ngraph TB\n    Start[\"infer_block(block, x, pre_infer_out)\"]\n    \n    subgraph \"Pre-Processing\"\n        PreProc[\"pre_process()\u003cbr/\u003emodulation.tensor + embed0\u003cbr/\u003e 6 chunks\"]\n        Chunks[\"shift_msa, scale_msa, gate_msa\u003cbr/\u003ec_shift_msa, c_scale_msa, c_gate_msa\"]\n    end\n    \n    subgraph \"Phase 0: Self-Attention\"\n        SelfAttn[\"infer_self_attn()\u003cbr/\u003ephase=block.compute_phases[0]\"]\n        Norm1[\"norm1.apply(x)\"]\n        Modulate1[\"modulate_func(norm1_out, scale_msa, shift_msa)\"]\n        QKV[\"Q=self_attn_q.apply()\u003cbr/\u003eK=self_attn_k.apply()\u003cbr/\u003eV=self_attn_v.apply()\"]\n        NormQK[\"self_attn_norm_q.apply(Q)\u003cbr/\u003eself_attn_norm_k.apply(K)\"]\n        Rope[\"apply_rope_func(q, k, cos_sin)\"]\n        Attn1[\"self_attn_1.apply(q, k, v)\u003cbr/\u003eFlashAttn/SageAttn/etc\"]\n        ProjO[\"self_attn_o.apply(attn_out)\"]\n    end\n    \n    subgraph \"Phase 1: Cross-Attention\"\n        CrossAttn[\"infer_cross_attn()\u003cbr/\u003ephase=block.compute_phases[1]\"]\n        Add1[\"x = x + y_out * gate_msa\"]\n        Norm3[\"norm3.apply(x)\"]\n        CrossQKV[\"Q=cross_attn_q.apply(norm3_out)\u003cbr/\u003eK=cross_attn_k.apply(context)\u003cbr/\u003eV=cross_attn_v.apply(context)\"]\n        CrossAttn1[\"cross_attn_1.apply(q, k, v)\"]\n        ImgAttn[\"Optional: cross_attn_2 for image\"]\n        CrossO[\"cross_attn_o.apply(attn_out)\"]\n    end\n    \n    subgraph \"Phase 2: FFN\"\n        FFNProc[\"infer_ffn()\u003cbr/\u003ephase=block.compute_phases[2]\"]\n        Add2[\"x = x + attn_out\"]\n        Norm2[\"norm2.apply(x)\"]\n        Modulate2[\"modulate_func(norm2_out, c_scale_msa, c_shift_msa)\"]\n        FFN0[\"ffn_0.apply(norm2_out)\"]\n        GELU[\"F.gelu(y, approximate='tanh')\"]\n        FFN2[\"ffn_2.apply(y)\"]\n    end\n    \n    subgraph \"Post-Processing\"\n        PostProc[\"post_process()\"]\n        Add3[\"x = x + y * c_gate_msa\"]\n    end\n    \n    Start --\u003e PreProc\n    PreProc --\u003e Chunks\n    Chunks --\u003e SelfAttn\n    SelfAttn --\u003e Norm1\n    Norm1 --\u003e Modulate1\n    Modulate1 --\u003e QKV\n    QKV --\u003e NormQK\n    NormQK --\u003e Rope\n    Rope --\u003e Attn1\n    Attn1 --\u003e ProjO\n    ProjO --\u003e CrossAttn\n    CrossAttn --\u003e Add1\n    Add1 --\u003e Norm3\n    Norm3 --\u003e CrossQKV\n    CrossQKV --\u003e CrossAttn1\n    CrossAttn1 --\u003e ImgAttn\n    ImgAttn --\u003e CrossO\n    CrossO --\u003e FFNProc\n    FFNProc --\u003e Add2\n    Add2 --\u003e Norm2\n    Norm2 --\u003e Modulate2\n    Modulate2 --\u003e FFN0\n    FFN0 --\u003e GELU\n    GELU --\u003e FFN2\n    FFN2 --\u003e PostProc\n    PostProc --\u003e Add3\n    \n    Output[\"Return x\"]\n    Add3 --\u003e Output\n```\n\n**Key Implementation Details:**\n- `pre_process` splits modulation tensor into 6 components for attention and FFN\n- `modulate_func` can use Triton kernel (`fuse_scale_shift_kernel`) or PyTorch ops\n- RoPE application via `apply_rope_func` (FlashInfer, Torch, or Torch naive)\n- Residual connections use in-place addition when dtype matches\n- Optional `clean_cuda_cache` after each phase for memory management\n\n**Sources:**\n- [lightx2v/models/networks/wan/infer/transformer_infer.py:133-156]()\n- [lightx2v/models/networks/wan/infer/transformer_infer.py:158-170]()\n- [lightx2v/models/networks/wan/infer/transformer_infer.py:172-247]()\n- [lightx2v/models/networks/wan/infer/transformer_infer.py:249-323]()\n- [lightx2v/models/networks/wan/infer/transformer_infer.py:325-357]()\n- [lightx2v/models/networks/wan/infer/transformer_infer.py:359-368]()\n\n### Qwen Image Block Inference (Dual Stream)\n\nQwen Image maintains separate image and text streams throughout the transformer, converging only during joint attention:\n\n```mermaid\ngraph TB\n    Start[\"infer_block(block_idx, img_x, txt_x)\"]\n    \n    subgraph \"Phase 0: Image Stream QKV\"\n        ImgQKV[\"infer_img_qkv()\u003cbr/\u003ephase=compute_phases[0]\"]\n        ImgMod[\"img_mod1 modulation\"]\n        ImgNorm[\"img_norm1.apply(img_x)\"]\n        ImgProj[\"img_q_proj, img_k_proj, img_v_proj\"]\n        ImgRoPE[\"apply_qwen_rope(img_q, img_k, img_freqs)\"]\n    end\n    \n    subgraph \"Phase 1: Text Stream QKV\"\n        TxtQKV[\"infer_txt_qkv()\u003cbr/\u003ephase=compute_phases[1]\"]\n        TxtMod[\"txt_mod1 modulation\"]\n        TxtNorm[\"txt_norm1.apply(txt_x)\"]\n        TxtProj[\"txt_q_proj, txt_k_proj, txt_v_proj\"]\n        TxtRoPE[\"apply_qwen_rope(txt_q, txt_k, txt_freqs)\"]\n    end\n    \n    subgraph \"Phase 2: Joint Attention\"\n        JointAttn[\"infer_joint_attention()\u003cbr/\u003ephase=compute_phases[2]\"]\n        Concat[\"Concatenate streams\u003cbr/\u003eq = [txt_q; img_q]\u003cbr/\u003ek = [txt_k; img_k]\u003cbr/\u003ev = [txt_v; img_v]\"]\n        AttnOp[\"joint_attn.apply(q, k, v)\u003cbr/\u003ecu_seqlens tracking\"]\n        Split[\"Split output\u003cbr/\u003etxt_attn_out, img_attn_out\"]\n        ProjOut[\"txt_o_proj, img_o_proj\"]\n        Gate[\"txt_x += gate1 * txt_out\u003cbr/\u003eimg_x += gate1 * img_out\"]\n    end\n    \n    subgraph \"Phase 3: Dual FFN\"\n        DualFFN[\"infer_ffn()\u003cbr/\u003ephase=compute_phases[3]\"]\n        ImgFFN[\"img_norm2, img_mod2\u003cbr/\u003eimg_mlp_0, img_mlp_2\"]\n        TxtFFN[\"txt_norm2, txt_mod2\u003cbr/\u003etxt_mlp_0, txt_mlp_2\"]\n        ImgGate[\"img_x += gate2 * img_ffn_out\"]\n        TxtGate[\"txt_x += gate2 * txt_ffn_out\"]\n    end\n    \n    Start --\u003e ImgQKV\n    Start --\u003e TxtQKV\n    \n    ImgQKV --\u003e ImgMod\n    ImgMod --\u003e ImgNorm\n    ImgNorm --\u003e ImgProj\n    ImgProj --\u003e ImgRoPE\n    \n    TxtQKV --\u003e TxtMod\n    TxtMod --\u003e TxtNorm\n    TxtNorm --\u003e TxtProj\n    TxtProj --\u003e TxtRoPE\n    \n    ImgRoPE --\u003e JointAttn\n    TxtRoPE --\u003e JointAttn\n    \n    JointAttn --\u003e Concat\n    Concat --\u003e AttnOp\n    AttnOp --\u003e Split\n    Split --\u003e ProjOut\n    ProjOut --\u003e Gate\n    \n    Gate --\u003e DualFFN\n    DualFFN --\u003e ImgFFN\n    DualFFN --\u003e TxtFFN\n    ImgFFN --\u003e ImgGate\n    TxtFFN --\u003e TxtGate\n    \n    Output[\"Return img_x, txt_x\"]\n    ImgGate --\u003e Output\n    TxtGate --\u003e Output\n```\n\n**Implementation Notes:**\n- Image and text streams have separate modulation parameters (`img_mod1`, `txt_mod1`, `img_mod2`, `txt_mod2`)\n- RoPE applied independently to each stream with different frequency tables\n- `cu_seqlens_q` and `cu_seqlens_k` track concatenated sequence boundaries for varlen attention\n- Layered mode uses `modulate_index` to select between two parameter sets per layer\n- `zero_cond_t` option can zero out text modulation for ablation studies\n\n**Sources:**\n- [lightx2v/models/networks/qwen_image/infer/transformer_infer.py:278-367]()\n- [lightx2v/models/networks/qwen_image/infer/transformer_infer.py:184-244]()\n- [lightx2v/models/networks/qwen_image/infer/transformer_infer.py:125-182]()\n\n---\n\n## Audio Conditioning Integration (WAN Audio Models)\n\nWAN audio models extend the base transformer architecture to support audio-driven video generation, integrating audio features into the cross-attention mechanism.\n\n### Audio Adapter Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Audio Input Processing\"\n        AudioSeg[\"AudioSegment\u003cbr/\u003eaudio_array: [N, T]\"]\n        AudioEnc[\"SekoAudioEncoderModel\u003cbr/\u003eTencentGameMate-chinese-hubert-large\"]\n        AudioFeat[\"audio_features\u003cbr/\u003e[N, seq_len, 1024]\"]\n    end\n    \n    subgraph \"Audio Adapter Projection\"\n        AudioAdapter[\"AudioAdapter.forward_audio_proj()\"]\n        TimeFreq[\"time_freq projection\u003cbr/\u003e1024  256\"]\n        AttnLayers[\"projection_transformer_layers=4\u003cbr/\u003eSelf-attention blocks\"]\n        MLP[\"mlp_dims=(1024, 1024, 32*1024)\u003cbr/\u003eMulti-layer projection\"]\n        Output[\"audio_encoder_output\u003cbr/\u003e[N, seq_len, 128, 1024]\"]\n    end\n    \n    subgraph \"Integration into Transformer\"\n        CrossAttn[\"Cross-Attention Phase\"]\n        AudioProj[\"audio_context projection\u003cbr/\u003eInjected into K, V\"]\n        AttnOp[\"Joint attention with\u003cbr/\u003etext + audio context\"]\n    end\n    \n    AudioSeg --\u003e AudioEnc\n    AudioEnc --\u003e AudioFeat\n    AudioFeat --\u003e AudioAdapter\n    AudioAdapter --\u003e TimeFreq\n    TimeFreq --\u003e AttnLayers\n    AttnLayers --\u003e MLP\n    MLP --\u003e Output\n    Output --\u003e CrossAttn\n    CrossAttn --\u003e AudioProj\n    AudioProj --\u003e AttnOp\n```\n\n**Audio Adapter Configuration:**\n```python\nAudioAdapter(\n    attention_head_dim=dim // num_heads,\n    num_attention_heads=num_heads,\n    base_num_layers=num_layers,\n    interval=1,\n    audio_feature_dim=1024,\n    time_freq_dim=256,\n    projection_transformer_layers=4,\n    mlp_dims=(1024, 1024, 32*1024),\n    quantized=config.get(\"adapter_quantized\", False),\n    quant_scheme=config.get(\"adapter_quant_scheme\", None),\n    cpu_offload=audio_adapter_offload\n)\n```\n\n### Audio Transformer Weights\n\nWAN audio models use `WanAudioTransformerWeights` which extends base weights with audio-specific components:\n\n```mermaid\ngraph TB\n    BaseWeights[\"WanTransformerWeights\u003cbr/\u003eBase transformer structure\"]\n    \n    subgraph \"Audio Extensions\"\n        AudioCA[\"WanAudioCrossAttention\u003cbr/\u003eExtends WanCrossAttention\"]\n        AudioProj[\"audio_context_proj\u003cbr/\u003eProjects audio features\"]\n        AudioNorm[\"audio_context_norm\u003cbr/\u003eRMS normalization\"]\n    end\n    \n    subgraph \"Multi-Person Support\"\n        MaskProj[\"Optional: person_mask_proj\u003cbr/\u003eFor multi-speaker scenarios\"]\n    end\n    \n    BaseWeights --\u003e AudioCA\n    AudioCA --\u003e AudioProj\n    AudioCA --\u003e AudioNorm\n    AudioCA --\u003e MaskProj\n```\n\n**Audio Cross-Attention Integration:**\nIn the cross-attention phase, audio features are processed alongside text:\n```\ncontext_combined = concatenate([text_context, audio_context])\nQ = cross_attn_q(hidden_states)\nK = cross_attn_k(context_combined)\nV = cross_attn_v(context_combined)\n```\n\n**Temporal Consistency:**\nAudio models maintain temporal consistency across segments via `prev_latents`:\n```python\nprev_latents_dict = {\n    \"prev_latents\": encoded_prev_frames,\n    \"prev_mask\": temporal_mask,\n    \"prev_len\": number_of_previous_frames\n}\n```\n\n**Sources:**\n- [lightx2v/models/networks/wan/audio_model.py:19-127]()\n- [lightx2v/models/runners/wan/wan_audio_runner.py:753-790]()\n- [lightx2v/models/input_encoders/hf/seko_audio/audio_adapter.py]() (referenced)\n- [lightx2v/models/networks/wan/weights/audio/transformer_weights.py:1-90]()\n\n---\n\n## CPU Offloading and Memory Management\n\nTransformer blocks support granular CPU offloading to manage memory pressure when running large models on consumer hardware.\n\n### Offload Buffer Structure\n\n```mermaid\ngraph TB\n    TransWeights[\"TransformerWeights\"]\n    \n    subgraph \"Main Weights (CPU or CUDA)\"\n        Blocks[\"blocks: WeightModuleList\u003cbr/\u003e[0..N-1]\"]\n    end\n    \n    subgraph \"Offload Buffers (CUDA)\"\n        BlockBuffers[\"offload_block_cuda_buffers\u003cbr/\u003e2 block buffers\"]\n        PhaseBuffers[\"offload_phase_cuda_buffers\u003cbr/\u003e4 phase buffers\"]\n    end\n    \n    subgraph \"Lazy Load Buffers (CPU)\"\n        CPUBlockBuffers[\"offload_block_cpu_buffers\u003cbr/\u003e2 block buffers\"]\n        CPUPhaseBuffers[\"offload_phase_cpu_buffers\u003cbr/\u003e2 phase buffers\"]\n    end\n    \n    TransWeights --\u003e Blocks\n    TransWeights --\u003e BlockBuffers\n    TransWeights --\u003e PhaseBuffers\n    TransWeights --\u003e CPUBlockBuffers\n    TransWeights --\u003e CPUPhaseBuffers\n```\n\n### Offload Granularities\n\n| Granularity | Description | Buffer Size | Use Case |\n|------------|-------------|-------------|----------|\n| `\"model\"` | Entire model moved to GPU at start | Full model | Simple CPU offload |\n| `\"block\"` | One block on GPU at a time | 2 block buffers | Balanced memory/speed |\n| `\"phase\"` | One compute phase on GPU at a time | 4 phase buffers | Extreme memory saving |\n\n### Block-Level Offload Flow\n\n```mermaid\nsequenceDiagram\n    participant CPU as Main Blocks (CPU)\n    participant CUDA0 as CUDA Buffer 0\n    participant CUDA1 as CUDA Buffer 1\n    participant Compute as Compute Stream\n    \n    Note over CPU: Block 0 resident\n    CPU-\u003e\u003eCUDA0: Prefetch Block 0\n    CUDA0-\u003e\u003eCompute: Process Block 0\n    \n    Note over CPU: Block 1 prefetch starts\n    CPU-\u003e\u003eCUDA1: Prefetch Block 1 (async)\n    \n    Compute-\u003e\u003eCompute: Block 0 computation\n    \n    Note over CUDA0,CUDA1: Swap buffers\n    CUDA1-\u003e\u003eCompute: Process Block 1\n    CPU-\u003e\u003eCUDA0: Prefetch Block 2 (async)\n```\n\n**Key Features:**\n- **Double buffering:** Prefetch next block while current block computes\n- **Lazy loading:** Optional disk-to-CPU loading for ultra-low memory\n- **WeightAsyncStreamManager:** Manages async H2D transfers\n- **num_disk_workers:** Configurable parallel loading threads\n\n**Sources:**\n- [lightx2v/models/networks/wan/weights/transformer_weights.py:55-133]()\n- [lightx2v/models/networks/qwen_image/weights/transformer_weights.py:37-105]()\n- [lightx2v/models/networks/qwen_image/infer/offload/transformer_infer.py:12-144]()\n\n---\n\n## Weight Module Hierarchy\n\nAll weight components inherit from `WeightModule`, which provides unified interfaces for memory management, offloading, and LoRA integration.\n\n### Class Hierarchy\n\n```mermaid\ngraph TB\n    WeightModule[\"WeightModule\u003cbr/\u003e(base class)\"]\n    \n    subgraph \"Container Classes\"\n        WeightModuleList[\"WeightModuleList\u003cbr/\u003eSequential list of modules\"]\n        TransformerWeights[\"TransformerWeights\u003cbr/\u003eTop-level container\"]\n        AttentionBlock[\"AttentionBlock\u003cbr/\u003eSingle block container\"]\n    end\n    \n    subgraph \"Leaf Weight Classes\"\n        MMWeight[\"MM_WEIGHT_REGISTER\u003cbr/\u003eMatrix multiplication\u003cbr/\u003e(Linear layers)\"]\n        AttnWeight[\"ATTN_WEIGHT_REGISTER\u003cbr/\u003eAttention operators\"]\n        RMSWeight[\"RMS_WEIGHT_REGISTER\u003cbr/\u003eRMS normalization\"]\n        LNWeight[\"LN_WEIGHT_REGISTER\u003cbr/\u003eLayer normalization\"]\n        TensorWeight[\"TENSOR_REGISTER\u003cbr/\u003eGeneric tensors\"]\n    end\n    \n    WeightModule --\u003e WeightModuleList\n    WeightModule --\u003e TransformerWeights\n    WeightModule --\u003e AttentionBlock\n    WeightModule --\u003e MMWeight\n    WeightModule --\u003e AttnWeight\n    WeightModule --\u003e RMSWeight\n    WeightModule --\u003e LNWeight\n    WeightModule --\u003e TensorWeight\n```\n\n**Common Operations:**\n- `to_cuda()` / `to_cpu()`: Move weights between devices\n- `load(weight_dict)`: Initialize from checkpoint\n- `register_lora()`: Apply LoRA adapters (optional)\n- `apply(x)`: Execute forward pass\n\n**Sources:**\n- [lightx2v/common/modules/weight_module.py]() (referenced but not in provided files)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py:1-8]()\n- [lightx2v/models/networks/qwen_image/weights/transformer_weights.py:1-7]()\n\n---\n\n## Configuration Reference\n\nKey configuration parameters that control transformer block behavior:\n\n### Block Architecture Parameters\n\n| Parameter | Type | Description | Example |\n|-----------|------|-------------|---------|\n| `num_layers` | int | Number of transformer blocks | `40`, `60` |\n| `num_attention_heads` | int | Number of attention heads | `20`, `32` |\n| `dim` | int | Hidden dimension | `2560`, `3072` |\n| `self_attn_1_type` | string | Self-attention operator | `\"flash_attn3\"` |\n| `cross_attn_1_type` | string | Cross-attention operator | `\"flash_attn3\"` |\n| `rms_type` | string | RMS norm implementation | `\"sgl-kernel\"` |\n| `attnmap_frame_num` | int | Number of video frames (for sparse attention) | `13`, `81` |\n\n### Memory Management Parameters\n\n| Parameter | Type | Description | Example |\n|-----------|------|-------------|---------|\n| `cpu_offload` | bool | Enable CPU offloading | `true` |\n| `offload_granularity` | string | Offload level | `\"block\"`, `\"phase\"`, `\"model\"` |\n| `offload_ratio` | float | Fraction of blocks to offload | `1.0` |\n| `lazy_load` | bool | Enable disk-based lazy loading | `true` |\n| `num_disk_workers` | int | Parallel loading threads | `4` |\n\n### Quantization Parameters\n\n| Parameter | Type | Description | Example |\n|-----------|------|-------------|---------|\n| `dit_quantized` | bool | Use quantized weights | `true` |\n| `dit_quant_scheme` | string | Quantization method | `\"nvfp4\"`, `\"int8\"`, `\"fp8\"` |\n| `quant_method` | string | Advanced quantization | `\"advanced_ptq\"` |\n\n**Sources:**\n- [configs/attentions/wan_i2v_svg_magi_480p.json:1-23]()\n- [lightx2v/models/networks/wan/weights/transformer_weights.py:12-28]()\n- [lightx2v/models/networks/qwen_image/infer/offload/transformer_infer.py:15-32]()"])</script><script>self.__next_f.push([1,"42:T5081,"])</script><script>self.__next_f.push([1,"# Rotary Position Embeddings (RoPE)\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the Rotary Position Embeddings (RoPE) system in LightX2V, which provides positional information to transformer models during video and image generation. RoPE enables models to understand spatial and temporal relationships between tokens by applying rotation-based position encodings to query and key vectors in attention mechanisms.\n\nFor information about the broader transformer architecture, see [Transformer Block Architecture](#7.1). For attention mechanism details, see [Attention Mechanisms and Sparse Patterns](#6.2).\n\n**Key responsibilities:**\n- Compute and cache frequency parameters for positional encoding\n- Apply rotary embeddings to Q/K vectors in self-attention layers\n- Support 3D position encoding (frame, height, width) for video generation\n- Provide multiple implementation backends (torch, flashinfer, chunked)\n- Handle distributed processing with sequence parallelism\n\n---\n\n## RoPE Fundamentals\n\n### Mathematical Concept\n\nRoPE applies rotation transformations to query and key vectors based on their positions in a sequence. Instead of adding position information, it rotates the embedding vectors by position-dependent angles, which naturally encodes relative positions through the inner product of rotated vectors.\n\nFor position `m` and dimension pair `(i, i+1)`, RoPE applies:\n```\nq_m^(i,i+1) = R(_i * m) @ q_m^(i,i+1)\nk_n^(i,i+1) = R(_i * n) @ k_n^(i,i+1)\n```\n\nwhere `R()` is a 2D rotation matrix and `_i = 10000^(-2i/d)`.\n\nSources: [lightx2v/models/networks/wan/infer/utils.py:12-28](), [lightx2v/models/schedulers/wan/scheduler.py:42-49]()\n\n---\n\n## Implementation Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Scheduler Initialization\"\n        RopeParams[\"rope_params()\u003cbr/\u003emax_seq_len, dim, theta\"]\n        FreqsInit[\"freqs cache\u003cbr/\u003etorch.cat([f_freqs, h_freqs, w_freqs])\"]\n    end\n    \n    subgraph \"Frequency Computation\"\n        ComputeFreqs[\"compute_freqs()\u003cbr/\u003e3D grid  sequence\"]\n        ComputeFreqsDist[\"compute_freqs_dist()\u003cbr/\u003ewith seq_parallel\"]\n        ComputeFreqsCaus[\"compute_freqs_causvid()\u003cbr/\u003ecausal video\"]\n    end\n    \n    subgraph \"Implementation Selection\"\n        Registry[\"ROPE_REGISTER\u003cbr/\u003ePlatform-specific\"]\n        DefaultSelection[\"rope_type config\u003cbr/\u003e'flashinfer', 'torch', 'torch_naive'\"]\n        RopeWrapper[\"rope_wrapper()\u003cbr/\u003eUnified interface\"]\n    end\n    \n    subgraph \"Application Methods\"\n        FlashInfer[\"apply_wan_rope_with_flashinfer()\u003cbr/\u003eapply_rope_with_cos_sin_cache_inplace\"]\n        Torch[\"apply_wan_rope_with_torch()\u003cbr/\u003eComplex number rotation\"]\n        TorchNaive[\"apply_wan_rope_with_torch_naive()\u003cbr/\u003eManual rotation\"]\n        Chunk[\"apply_wan_rope_with_chunk()\u003cbr/\u003eMemory-efficient chunked\"]\n    end\n    \n    subgraph \"Transformer Integration\"\n        SelfAttn[\"WanTransformerInfer.infer_self_attn()\"]\n        QKProjection[\"q, k = proj(norm(x))\"]\n        ApplyRope[\"q, k = apply_rope_func(q, k, cos_sin)\"]\n        Attention[\"attention(q, k, v)\"]\n    end\n    \n    RopeParams --\u003e FreqsInit\n    FreqsInit --\u003e ComputeFreqs\n    FreqsInit --\u003e ComputeFreqsDist\n    FreqsInit --\u003e ComputeFreqsCaus\n    \n    Registry --\u003e RopeWrapper\n    DefaultSelection --\u003e RopeWrapper\n    \n    RopeWrapper --\u003e FlashInfer\n    RopeWrapper --\u003e Torch\n    RopeWrapper --\u003e TorchNaive\n    \n    Chunk --\u003e FlashInfer\n    Chunk --\u003e Torch\n    Chunk --\u003e TorchNaive\n    \n    ComputeFreqs --\u003e ApplyRope\n    ComputeFreqsDist --\u003e ApplyRope\n    ComputeFreqsCaus --\u003e ApplyRope\n    \n    FlashInfer --\u003e ApplyRope\n    Torch --\u003e ApplyRope\n    TorchNaive --\u003e ApplyRope\n    \n    SelfAttn --\u003e QKProjection\n    QKProjection --\u003e ApplyRope\n    ApplyRope --\u003e Attention\n```\n\n**Diagram: RoPE System Architecture** - Shows the complete flow from frequency initialization through implementation selection to application in transformer blocks.\n\nSources: [lightx2v/models/networks/wan/infer/transformer_infer.py:36-58](), [lightx2v/models/networks/wan/infer/utils.py:12-124](), [lightx2v/models/schedulers/wan/scheduler.py:33-49]()\n\n---\n\n## Implementation Variants\n\n### Registry-Based Selection\n\nRoPE implementations are selected through a registry pattern that allows platform-specific optimizations:\n\n```mermaid\ngraph LR\n    Config[\"config['rope_type']\"]\n    \n    subgraph \"Selection Logic\"\n        CheckRegistry[\"Check ROPE_REGISTER\"]\n        FallbackDict[\"Fallback to rope_funcs dict\"]\n    end\n    \n    subgraph \"Registry Implementations\"\n        PlatformRope[\"Platform-specific\u003cbr/\u003erope_instance.apply()\"]\n    end\n    \n    subgraph \"Default Implementations\"\n        FlashInfer[\"'flashinfer'\u003cbr/\u003eapply_wan_rope_with_flashinfer\"]\n        Torch[\"'torch'\u003cbr/\u003eapply_wan_rope_with_torch\"]\n        TorchNaive[\"'torch_naive'\u003cbr/\u003eapply_wan_rope_with_torch_naive\"]\n    end\n    \n    Config --\u003e CheckRegistry\n    CheckRegistry --\u003e|Found| PlatformRope\n    CheckRegistry --\u003e|Not found| FallbackDict\n    FallbackDict --\u003e FlashInfer\n    FallbackDict --\u003e Torch\n    FallbackDict --\u003e TorchNaive\n```\n\n**Diagram: RoPE Implementation Selection** - Dynamic selection of RoPE backend based on configuration and platform availability.\n\nSources: [lightx2v/models/networks/wan/infer/transformer_infer.py:36-58]()\n\n---\n\n### Implementation Comparison\n\n| Implementation | Method | Pros | Cons | Use Case |\n|----------------|--------|------|------|----------|\n| **flashinfer** | `apply_rope_with_cos_sin_cache_inplace` | Fastest, in-place | Requires flashinfer library | Default for production |\n| **torch** | Complex number operations | Clean, efficient | Requires complex dtype conversion | Standard PyTorch backend |\n| **torch_naive** | Manual rotation computation | Most compatible | Slower | Fallback/debugging |\n| **chunk** | Process in segments | Memory-efficient | Slight overhead | Large sequence lengths |\n\nSources: [lightx2v/models/networks/wan/infer/utils.py:12-124]()\n\n---\n\n### Implementation Details\n\n#### FlashInfer Implementation\n\nUses the `flashinfer` library's optimized kernel for in-place rotation:\n\n```python\n# Signature from code\napply_rope_with_cos_sin_cache_inplace(\n    positions=positions,      # [L] sequence positions\n    query=query,             # [L, H*D] flattened Q\n    key=key,                 # [L, H*D] flattened K\n    head_size=D,             # Dimension per head\n    cos_sin_cache=cos_sin_cache,  # Pre-computed frequencies\n    is_neox=False            # RoPE variant\n)\n```\n\nSources: [lightx2v/models/networks/wan/infer/utils.py:101-124]()\n\n#### Torch Implementation\n\nUses complex number representation for efficient rotation:\n\n```python\n# Convert to complex, apply rotation, convert back\nxq = torch.view_as_complex(xq.reshape(..., 2))\nxk = torch.view_as_complex(xk.reshape(..., 2))\nxq = torch.view_as_real(xq * cos_sin_cache).flatten(2)\nxk = torch.view_as_real(xk * cos_sin_cache).flatten(2)\n```\n\nSources: [lightx2v/models/networks/wan/infer/utils.py:12-28]()\n\n#### Chunked Processing\n\nProcesses large sequences in chunks to reduce memory usage:\n\n- Default chunk size: 100 tokens (configurable via `rope_chunk_size`)\n- Processes `[start:end]` slices independently\n- Uses `non_blocking=True` for async copy operations\n- Triggered by `rope_chunk=True` in config\n\nSources: [lightx2v/models/networks/wan/infer/utils.py:71-98](), [lightx2v/models/networks/wan/infer/transformer_infer.py:56-57]()\n\n---\n\n## 3D Position Encoding for Video\n\n### Frequency Decomposition\n\nFor video generation, RoPE encodes 3D positions (frame, height, width) by splitting the embedding dimension into three components:\n\n```mermaid\ngraph TB\n    subgraph \"Dimension Split\"\n        TotalDim[\"head_dim = dim // num_heads\"]\n        FrameDim[\"frame_dim = head_dim - 2*(head_dim//6)\"]\n        HeightDim[\"height_dim = head_dim // 6\"]\n        WidthDim[\"width_dim = head_dim // 6\"]\n    end\n    \n    subgraph \"Frequency Generation\"\n        FrameFreqs[\"freqs[0] = rope_params(1024, frame_dim)\"]\n        HeightFreqs[\"freqs[1] = rope_params(1024, height_dim)\"]\n        WidthFreqs[\"freqs[2] = rope_params(1024, width_dim)\"]\n    end\n    \n    subgraph \"3D Grid Expansion\"\n        GridSizes[\"grid_sizes = (f, h, w)\"]\n        ExpandFrame[\"freqs[0][:f].expand(f, h, w, -)\"]\n        ExpandHeight[\"freqs[1][:h].expand(f, h, w, -)\"]\n        ExpandWidth[\"freqs[2][:w].expand(f, h, w, -)\"]\n    end\n    \n    subgraph \"Concatenation\"\n        ConcatFreqs[\"torch.cat([f_exp, h_exp, w_exp], dim=-1)\"]\n        ReshapeSeq[\"reshape(f*h*w, 1, head_dim)\"]\n    end\n    \n    TotalDim --\u003e FrameDim\n    TotalDim --\u003e HeightDim\n    TotalDim --\u003e WidthDim\n    \n    FrameDim --\u003e FrameFreqs\n    HeightDim --\u003e HeightFreqs\n    WidthDim --\u003e WidthFreqs\n    \n    GridSizes --\u003e ExpandFrame\n    GridSizes --\u003e ExpandHeight\n    GridSizes --\u003e ExpandWidth\n    \n    FrameFreqs --\u003e ExpandFrame\n    HeightFreqs --\u003e ExpandHeight\n    WidthFreqs --\u003e ExpandWidth\n    \n    ExpandFrame --\u003e ConcatFreqs\n    ExpandHeight --\u003e ConcatFreqs\n    ExpandWidth --\u003e ConcatFreqs\n    ConcatFreqs --\u003e ReshapeSeq\n```\n\n**Diagram: 3D Position Encoding** - Shows how the embedding dimension is split and frequencies are computed for each spatial-temporal dimension.\n\nSources: [lightx2v/models/networks/wan/infer/utils.py:127-140](), [lightx2v/models/schedulers/wan/scheduler.py:33-40]()\n\n---\n\n### Frequency Cache Initialization\n\nThe scheduler pre-computes and caches frequency parameters during initialization:\n\n```python\n# From WanScheduler.__init__\nself.freqs = torch.cat([\n    self.rope_params(1024, self.head_size - 4 * (self.head_size // 6)),  # Frame\n    self.rope_params(1024, 2 * (self.head_size // 6)),                   # Height\n    self.rope_params(1024, 2 * (self.head_size // 6)),                   # Width\n], dim=1).to(torch.device(AI_DEVICE))\n```\n\n**Key parameters:**\n- `max_seq_len=1024`: Maximum supported sequence length\n- `theta=10000`: Base frequency (standard RoPE parameter)\n- Frequencies stored as complex numbers: `torch.polar(ones, freqs)`\n\nSources: [lightx2v/models/schedulers/wan/scheduler.py:33-49]()\n\n---\n\n### 3D Grid Computation Functions\n\n| Function | Purpose | Key Feature |\n|----------|---------|-------------|\n| `compute_freqs()` | Standard 3D grid | Expands frequencies across full grid |\n| `compute_freqs_dist()` | Distributed 3D grid | Splits grid by rank for sequence parallelism |\n| `compute_freqs_causvid()` | Causal video | Supports `start_frame` offset for incremental generation |\n| `pad_freqs()` | Padding utility | Extends frequency tensor to target length |\n\nSources: [lightx2v/models/networks/wan/infer/utils.py:127-185]()\n\n---\n\n## Integration with Transformer Inference\n\n### Self-Attention Pipeline\n\n```mermaid\nsequenceDiagram\n    participant Block as Transformer Block\n    participant Phase as Self-Attn Phase\n    participant Norm as LayerNorm\n    participant Proj as Q/K/V Projection\n    participant RoPE as apply_rope_func\n    participant Attn as Attention Module\n    \n    Block-\u003e\u003ePhase: infer_self_attn(x)\n    Phase-\u003e\u003eNorm: norm1.apply(x)\n    Note over Phase,Norm: Modulate with scale/shift\n    Norm--\u003e\u003ePhase: norm1_out\n    \n    Phase-\u003e\u003eProj: q = self_attn_q.apply(norm1_out)\n    Phase-\u003e\u003eProj: k = self_attn_k.apply(norm1_out)\n    Phase-\u003e\u003eProj: v = self_attn_v.apply(norm1_out)\n    Proj--\u003e\u003ePhase: q, k, v tensors\n    \n    Note over Phase: Reshape to [seq_len, num_heads, head_dim]\n    Phase-\u003e\u003ePhase: q = q.view(s, n, d)\n    Phase-\u003e\u003ePhase: k = k.view(s, n, d)\n    \n    Phase-\u003e\u003eRoPE: apply_rope_func(q, k, cos_sin)\n    Note over RoPE: Apply rotary embeddings\u003cbr/\u003eusing cached frequencies\n    RoPE--\u003e\u003ePhase: rotated q, k\n    \n    Phase-\u003e\u003eAttn: self_attn_1.apply(q, k, v)\n    Attn--\u003e\u003ePhase: attn_out\n    \n    Phase-\u003e\u003eProj: self_attn_o.apply(attn_out)\n    Proj--\u003e\u003eBlock: output\n```\n\n**Diagram: RoPE in Self-Attention Flow** - Detailed sequence showing where RoPE is applied in the attention computation.\n\nSources: [lightx2v/models/networks/wan/infer/transformer_infer.py:172-247]()\n\n---\n\n### Pre-Infer Stage Integration\n\nThe pre-infer stage provides the `cos_sin` cache to all transformer blocks:\n\n```python\n# From WanPreInfer\ngrid_sizes_t, grid_sizes_h, grid_sizes_w = x.shape[2:]  # After patch embedding\ncos_sin = compute_freqs(\n    self.head_size, \n    (grid_sizes_t, grid_sizes_h, grid_sizes_w), \n    self.scheduler.freqs\n)\n```\n\nThe `cos_sin` tensor is stored in the scheduler and accessed by transformer blocks during inference:\n\n```python\n# From WanTransformerInfer.get_scheduler_values()\nself.cos_sin = self.scheduler.cos_sin\n```\n\nSources: [lightx2v/models/networks/wan/infer/pre_infer.py:96-104](), [lightx2v/models/networks/wan/infer/transformer_infer.py:77-78]()\n\n---\n\n## Distributed Processing\n\n### Sequence Parallelism\n\nWhen sequence parallelism is enabled, RoPE computation is distributed across GPUs:\n\n```mermaid\ngraph TB\n    subgraph \"Rank 0\"\n        Seq0[\"Sequence chunk 0\"]\n        Freq0[\"compute_freqs_dist()\u003cbr/\u003erank=0\"]\n        Rope0[\"apply_rope_func()\u003cbr/\u003elocal chunk\"]\n    end\n    \n    subgraph \"Rank 1\"\n        Seq1[\"Sequence chunk 1\"]\n        Freq1[\"compute_freqs_dist()\u003cbr/\u003erank=1\"]\n        Rope1[\"apply_rope_func()\u003cbr/\u003elocal chunk\"]\n    end\n    \n    subgraph \"Rank N-1\"\n        SeqN[\"Sequence chunk N-1\"]\n        FreqN[\"compute_freqs_dist()\u003cbr/\u003erank=N-1\"]\n        RopeN[\"apply_rope_func()\u003cbr/\u003elocal chunk\"]\n    end\n    \n    SharedFreqs[\"Shared frequency cache\u003cbr/\u003escheduler.freqs\"]\n    WorldSize[\"world_size from seq_p_group\"]\n    \n    SharedFreqs --\u003e Freq0\n    SharedFreqs --\u003e Freq1\n    SharedFreqs --\u003e FreqN\n    \n    WorldSize --\u003e Freq0\n    WorldSize --\u003e Freq1\n    WorldSize --\u003e FreqN\n    \n    Freq0 --\u003e Rope0\n    Freq1 --\u003e Rope1\n    FreqN --\u003e RopeN\n    \n    Seq0 --\u003e Rope0\n    Seq1 --\u003e Rope1\n    SeqN --\u003e RopeN\n```\n\n**Diagram: Distributed RoPE Computation** - Each rank computes RoPE for its local sequence chunk using the shared frequency cache.\n\nSources: [lightx2v/models/networks/wan/infer/utils.py:143-161]()\n\n---\n\n### Frequency Distribution Algorithm\n\n```python\n# From compute_freqs_dist()\nworld_size = dist.get_world_size(seq_p_group)\ncur_rank = dist.get_rank(seq_p_group)\n\n# Compute full frequency grid\nfreqs_i = compute_full_grid(freqs, grid_sizes)\n\n# Pad to match distributed sequence length\nfreqs_i = pad_freqs(freqs_i, s * world_size)\n\n# Slice for current rank\ns_per_rank = s\nfreqs_i_rank = freqs_i[(cur_rank * s_per_rank):((cur_rank + 1) * s_per_rank), :, :]\n```\n\n**Key aspects:**\n- Each rank processes `s_per_rank` tokens\n- Frequencies are padded to ensure even distribution\n- No communication needed during RoPE application\n- Follows pre-infer stage sequence splitting\n\nSources: [lightx2v/models/networks/wan/infer/utils.py:143-161](), [lightx2v/models/networks/wan/model.py:503-526]()\n\n---\n\n## Causal Video Generation\n\n### Incremental RoPE for Autoregressive Models\n\nFor causal video models like `WanCausVidModel`, RoPE supports incremental frame generation:\n\n```python\n# compute_freqs_causvid with frame offset\nfreqs_i = torch.cat([\n    freqs[0][start_frame : start_frame + f].view(f, 1, 1, -1).expand(f, h, w, -1),\n    freqs[1][:h].view(1, h, 1, -1).expand(f, h, w, -1),\n    freqs[2][:w].view(1, 1, w, -1).expand(f, h, w, -1),\n], dim=-1).reshape(seq_len, 1, -1)\n```\n\n**Parameters:**\n- `start_frame`: Offset for current generation window\n- `f`: Number of frames in current chunk\n- Enables consistent positional encoding across generation steps\n\nSources: [lightx2v/models/networks/wan/infer/utils.py:164-177]()\n\n---\n\n## Configuration and Optimization\n\n### RoPE Configuration Options\n\n| Config Key | Type | Default | Description |\n|------------|------|---------|-------------|\n| `rope_type` | str | `\"flashinfer\"` | Implementation backend |\n| `rope_chunk` | bool | `False` | Enable chunked processing |\n| `rope_chunk_size` | int | `100` | Tokens per chunk |\n| `head_size` | int | Computed | `dim // num_heads` |\n| `freq_dim` | int | Model-specific | Frequency embedding dimension |\n\nSources: [lightx2v/models/networks/wan/infer/transformer_infer.py:36-58](), [lightx2v/utils/set_config.py:33-34]()\n\n---\n\n### Performance Considerations\n\n**Memory optimization:**\n- Frequency cache is computed once during scheduler initialization\n- `cos_sin` cache is reused across all transformer blocks\n- Chunked processing reduces peak memory for long sequences\n\n**Computational efficiency:**\n- FlashInfer provides fastest in-place operations\n- Complex number operations (torch) are more efficient than naive rotation\n- Distributed computation scales linearly with number of GPUs\n\n**Precision:**\n- Frequencies stored as `torch.complex` for exact rotation\n- Query/key tensors converted to target dtype after rotation\n- Supports mixed precision inference (BF16/FP16)\n\nSources: [lightx2v/models/networks/wan/infer/utils.py:12-124](), [lightx2v/models/schedulers/wan/scheduler.py:33-49]()\n\n---\n\n## Model-Specific Implementations\n\n### Wan Models (Video Generation)\n\n- Uses 3D position encoding (frame, height, width)\n- Supports both standard and causal generation\n- Integrates with sequence parallelism for large resolutions\n- Head size typically 64-128 dimensions\n\nSources: [lightx2v/models/networks/wan/infer/transformer_infer.py:36-247]()\n\n---\n\n### Qwen Image Models\n\nQwen models use similar RoPE with 2D position encoding:\n\n```python\n# From QwenImageTransformerInfer\nrope_funcs = {\n    \"flashinfer\": apply_qwen_rope_with_flashinfer,\n    \"torch\": apply_qwen_rope_with_torch,\n    \"torch_naive\": apply_qwen_rope_with_torch_naive,\n}\n```\n\n**Key differences:**\n- 2D position encoding (height, width only)\n- Different frequency computation for image patches\n- Supports layered generation for progressive image synthesis\n\nSources: [lightx2v/models/networks/qwen_image/infer/transformer_infer.py:10-20]()\n\n---\n\n## Extension Points\n\n### Adding Custom RoPE Implementations\n\nTo add a platform-specific RoPE implementation:\n\n1. **Create RoPE class** with `apply()` method:\n```python\nclass CustomRoPE:\n    def apply(self, xq, xk, cos_sin_cache):\n        # Custom implementation\n        return xq_rotated, xk_rotated\n```\n\n2. **Register in platform-specific registry**:\n```python\nROPE_REGISTER[\"custom\"] = CustomRoPE\n```\n\n3. **Configure in model config**:\n```json\n{\n    \"rope_type\": \"custom\"\n}\n```\n\nThe registry-based design automatically uses platform-specific implementations when available, falling back to default implementations otherwise.\n\nSources: [lightx2v/models/networks/wan/infer/transformer_infer.py:42-54](), [lightx2v/utils/registry_factory.py]()\n\n---\n\n## Summary\n\n**RoPE in LightX2V provides:**\n\n1. **3D position encoding** for video generation (frame, height, width)\n2. **Multiple implementations** (flashinfer, torch, naive, chunked)\n3. **Platform-specific optimizations** via registry pattern\n4. **Distributed processing** with sequence parallelism\n5. **Causal generation support** for autoregressive models\n6. **Memory efficiency** through frequency caching and chunked processing\n\nThe system balances performance, flexibility, and compatibility across different hardware platforms and model architectures."])</script><script>self.__next_f.push([1,"43:T5abe,"])</script><script>self.__next_f.push([1,"# Audio Adapter and Cross-Attention\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides a deep dive into the `AudioAdapter` architecture used in LightX2V for speech-to-video (S2V) generation. The AudioAdapter integrates audio conditioning signals into the diffusion transformer through a specialized perceiver cross-attention mechanism that runs as a post-adapter phase after standard transformer block operations.\n\nThe AudioAdapter consists of three main components:\n1. **AudioProjection**: MLP-based projection that aligns audio features to video frame tokens\n2. **TimeEmbedding**: Time-dependent modulation for adaptive audio integration\n3. **Perceiver Cross-Attention**: Variable-length flash attention with person masking support\n\nFor the overall transformer block structure, see [Transformer Block Architecture](#7.1). For audio encoding details, see [Input Encoder Architecture](#4.5). For the complete audio-to-video workflow, see [WanAudioRunner](#5.2).\n\n---\n\n## AudioAdapter Architecture Overview\n\nThe `AudioAdapter` class coordinates audio feature processing and integration into the transformer. It is instantiated by `WanAudioRunner` and manages the complete audio conditioning pipeline.\n\n### Component Structure\n\n```mermaid\ngraph TB\n    subgraph \"AudioAdapter Components\"\n        AudioProj[\"AudioProjection\u003cbr/\u003eMLP-based audio-to-video alignment\"]\n        AudioPE[\"audio_pe\u003cbr/\u003eLearnable positional embeddings\u003cbr/\u003e[128, 1024]\"]\n        TimeEmb[\"TimeEmbedding\u003cbr/\u003eTimestep modulation\"]\n    end\n    \n    subgraph \"Perceiver Attention Layers (commented out)\"\n        CA[\"ca: nn.ModuleList\u003cbr/\u003ePerceiverAttentionCA layers\"]\n        Note[\"Note: CA layers defined but weights\u003cbr/\u003eloaded directly in transformer blocks\"]\n    end\n    \n    subgraph \"Forward Flow\"\n        AudioIn[\"Audio Features\u003cbr/\u003e[B, audio_len, 768]\"]\n        AProjOut[\"forward_audio_proj()\u003cbr/\u003e[B, latent_frames, 128, 1024]\"]\n        TimeOut[\"TimeEmbedding.forward()\u003cbr/\u003e[B, 3, dim]\"]\n    end\n    \n    AudioIn --\u003e AudioProj\n    AudioProj --\u003e AudioPE\n    AudioPE --\u003e AProjOut\n    \n    TimeOut --\u003e TimeEmb\n```\n\n**AudioAdapter Class Structure and Data Flow**\n\n**Sources:** [lightx2v/models/input_encoders/hf/seko_audio/audio_adapter.py:253-323]()\n\n---\n\n## AudioProjection: Audio-to-Video Feature Alignment\n\nThe `AudioProjection` class converts raw audio features from `SekoAudioEncoder` into video-aligned token representations suitable for cross-attention.\n\n### Architecture and Data Flow\n\n| Component | Function | Input Shape | Output Shape |\n|-----------|----------|-------------|--------------|\n| `linear_interpolation()` | Align audio frames to video frames | `[B, audio_len, 768]` | `[B, video_frames, 768]` |\n| `transformer_decoder` | Refine audio features (optional) | `[B, video_frames, 768]` | `[B, video_frames, 768]` |\n| `F.pad()` | Add neighbor context | `[B, video_frames, 768]` | `[B, video_frames+4, 768]` |\n| `unfold()` | Create sliding windows | `[B, video_frames+4, 768]` | `[B, video_frames, 768, 5]` |\n| `mlp` | Project to token space | `[B, video_frames, 3840]` | `[B, video_frames, 32768]` |\n| `rearrange()` | Split into tokens | `[B, video_frames, 32768]` | `[B, video_frames, 32, 1024]` |\n\n### Implementation Details\n\n```mermaid\ngraph TB\n    AudioFeat[\"Audio Features\u003cbr/\u003e[B, audio_len, 768]\u003cbr/\u003efrom SekoAudioEncoder\"]\n    \n    Interp[\"linear_interpolation()\u003cbr/\u003eResize to video_frames\u003cbr/\u003e[B, video_frames, 768]\"]\n    \n    TDecoder[\"transformer_decoder\u003cbr/\u003e(optional, 4 layers)\u003cbr/\u003eRefine features\"]\n    \n    Pad[\"F.pad(..., (0,0,2,2))\u003cbr/\u003eAdd 2 left + 2 right neighbors\u003cbr/\u003e[B, video_frames+4, 768]\"]\n    \n    Unfold[\"unfold(dimension=1, size=5, step=1)\u003cbr/\u003eSliding window\u003cbr/\u003e[B, video_frames, 768, 5]\"]\n    \n    Concat[\"rearrange('B T C W -\u003e B T (W C)')\u003cbr/\u003eConcatenate neighbors\u003cbr/\u003e[B, video_frames, 3840]\"]\n    \n    MLP[\"mlp: Sequential(\u003cbr/\u003e  Linear(3840, 1024),\u003cbr/\u003e  ReLU,\u003cbr/\u003e  Linear(1024, 1024),\u003cbr/\u003e  ReLU,\u003cbr/\u003e  Linear(1024, 32768)\u003cbr/\u003e)\"]\n    \n    Split[\"rearrange('B T (N C) -\u003e B T N C')\u003cbr/\u003eSplit into 32 tokens\u003cbr/\u003e[B, video_frames, 32, 1024]\"]\n    \n    Norm[\"LayerNorm(1024)\u003cbr/\u003eNormalize per token\"]\n    \n    Output[\"Projected Audio Tokens\u003cbr/\u003e[B, video_frames, 32, 1024]\"]\n    \n    AudioFeat --\u003e Interp\n    Interp --\u003e TDecoder\n    TDecoder --\u003e Pad\n    Pad --\u003e Unfold\n    Unfold --\u003e Concat\n    Concat --\u003e MLP\n    MLP --\u003e Split\n    Split --\u003e Norm\n    Norm --\u003e Output\n```\n\n**AudioProjection Forward Pass: Audio-to-Token Transformation**\n\nThe key insight is that each video frame gets 32 audio tokens derived from a 5-frame temporal window (2 left neighbors + current + 2 right neighbors), providing temporal context for audio-visual alignment.\n\n**Sources:** [lightx2v/models/input_encoders/hf/seko_audio/audio_adapter.py:174-224]()\n\n### Rearrangement to 4x Temporal Resolution\n\nAfter projection, audio tokens are rearranged to match the 4x temporal oversampling used in video generation:\n\n```python\n# In AudioAdapter.rearange_audio_features()\ndef rearange_audio_features(self, audio_feature: torch.Tensor):\n    # audio_feature: [B, video_frames, 32, 1024]\n    \n    # Repeat first frame 4 times to match initial video structure\n    audio_feature_0 = audio_feature[:, :1]  # [B, 1, 32, 1024]\n    audio_feature_0 = torch.repeat_interleave(audio_feature_0, repeats=4, dim=1)\n    \n    # Concatenate with rest\n    audio_feature = torch.cat([audio_feature_0, audio_feature[:, 1:]], dim=1)\n    # [B, 4*latent_frames, 32, 1024]\n    \n    # Group into latent frames with 4 temporal positions each\n    audio_feature = rearrange(audio_feature, \"B (T S) N C -\u003e B T (S N) C\", S=4)\n    # Final shape: [B, latent_frames, 128, 1024]\n    \n    return audio_feature\n```\n\nThis transforms from video frame resolution to latent frame resolution, where each latent frame represents 4 video frames (128 tokens = 4  32).\n\n**Sources:** [lightx2v/models/input_encoders/hf/seko_audio/audio_adapter.py:305-311]()\n\n---\n\n## TimeEmbedding: Time-Dependent Audio Modulation\n\nThe `TimeEmbedding` module generates time-dependent modulation parameters that control how audio features are integrated at different diffusion timesteps.\n\n### Architecture\n\n```mermaid\ngraph TB\n    Timestep[\"timestep\u003cbr/\u003e[B] or [B, N]\u003cbr/\u003eDiffusion timestep values\"]\n    \n    TProj[\"Timesteps (timesteps_proj)\u003cbr/\u003eSinusoidal encoding\u003cbr/\u003eflip_sin_to_cos=True\"]\n    \n    TEmbed[\"TimestepEmbedding (time_embedder)\u003cbr/\u003eLinear + SiLU + Linear\u003cbr/\u003etime_freq_dim -\u003e dim\"]\n    \n    ActFn[\"SiLU activation\"]\n    \n    TimeProj[\"time_proj\u003cbr/\u003eLinear(dim, time_proj_dim)\"]\n    \n    Output[\"timestep_proj\u003cbr/\u003e[B, time_proj_dim]\u003cbr/\u003eUsed for shift/scale/gate\"]\n    \n    Timestep --\u003e TProj\n    TProj --\u003e TEmbed\n    TEmbed --\u003e ActFn\n    ActFn --\u003e TimeProj\n    TimeProj --\u003e Output\n```\n\n**TimeEmbedding Module: Timestep to Modulation Parameters**\n\n### Integration into Scheduler\n\nThe `EulerScheduler` for audio models calls `AudioAdapter.time_embedding()` during `step_pre()`:\n\n```python\n# In EulerScheduler.step_pre() for audio models\ndef step_pre(self, step_index):\n    super().step_pre(step_index)\n    \n    # Load time embedding to GPU if offloaded\n    if self.audio_adapter.cpu_offload:\n        self.audio_adapter.time_embedding.to(AI_DEVICE)\n    \n    # Generate time-dependent embeddings\n    # timestep_input shape: [B, N] where N is sequence length\n    self.audio_adapter_t_emb = self.audio_adapter.time_embedding(self.timestep_input).unflatten(1, (3, -1))\n    # Output shape: [B, 3, dim] where 3 = [shift, scale, gate]\n    \n    # Offload back to CPU if needed\n    if self.audio_adapter.cpu_offload:\n        self.audio_adapter.time_embedding.to(\"cpu\")\n```\n\nThe resulting `audio_adapter_t_emb` with shape `[B, 3, dim]` provides shift, scale, and gate parameters that modulate the perceiver cross-attention in each transformer block.\n\n**Sources:** [lightx2v/models/input_encoders/hf/seko_audio/audio_adapter.py:226-251](), [lightx2v/models/schedulers/wan/audio/scheduler.py:29-35]()\n\n---\n\n## Pre-Inference: Audio Conditioning Setup\n\nThe `WanAudioPreInfer` class prepares audio-specific data structures for integration into transformer blocks.\n\n### Audio Features in adapter_args\n\nDuring pre-inference, audio features are packaged into `adapter_args` dictionary:\n\n```python\n# In WanAudioPreInfer.infer()\nreturn WanPreInferModuleOutput(\n    embed=embed,\n    grid_sizes=grid_sizes,\n    x=x.squeeze(0),\n    embed0=embed0.squeeze(0),\n    context=context,\n    cos_sin=self.cos_sin,\n    valid_token_len=valid_token_len,\n    valid_latent_num=valid_latent_num,\n    adapter_args={\n        \"audio_encoder_output\": inputs[\"audio_encoder_output\"],\n        \"person_mask_latens\": person_mask_latens\n    },\n)\n```\n\n### Person Mask Processing\n\nFor multi-person support, person masks are aligned to latent grid:\n\n```python\nperson_mask_latens = inputs[\"person_mask_latens\"]\nif person_mask_latens is not None:\n    # Expand mask to match temporal dimension\n    person_mask_latens = person_mask_latens.expand(-1, grid_sizes_t, -1, -1)\n    # Reshape to token sequence: [num_persons, total_tokens]\n    person_mask_latens = person_mask_latens.reshape(person_mask_latens.shape[0], -1)\n```\n\nThe `person_mask_latens` tensor has shape `[num_persons, total_tokens]` where:\n- `num_persons` is the number of separate audio tracks\n- `total_tokens = grid_sizes_t * grid_sizes_h * grid_sizes_w`\n\nEach mask indicates which spatial regions should be influenced by the corresponding audio track.\n\n**Sources:** [lightx2v/models/networks/wan/infer/audio/pre_infer.py:140-204]()\n\n---\n\n## Perceiver Cross-Attention Architecture\n\nThe perceiver cross-attention mechanism in Phase 3 integrates audio features through a specialized attention layer with time-dependent modulation and person masking.\n\n### Transformer Block Integration\n\n```mermaid\ngraph TB\n    subgraph \"Standard Phases (0-2)\"\n        Phase0[\"Phase 0: Self-Attention\"]\n        Phase1[\"Phase 1: Cross-Attention (Text/Image)\"]\n        Phase2[\"Phase 2: Feed-Forward\"]\n    end\n    \n    subgraph \"Phase 3: Audio Post-Adapter\"\n        Input[\"x: [total_tokens, dim]\u003cbr/\u003efrom Phase 2 output\"]\n        \n        Align[\"align_hidden_states_and_mask()\u003cbr/\u003eSplit by n_query_tokens\"]\n        \n        HiddenAlign[\"hidden_states_aligned\u003cbr/\u003e[n_query_tokens, dim]\"]\n        HiddenTail[\"hidden_states_tail\u003cbr/\u003e[remaining_tokens, dim]\"]\n        \n        NormKV[\"norm_kv: LayerNorm\u003cbr/\u003eNormalize audio features\"]\n        NormQ[\"norm_q: LayerNorm\u003cbr/\u003eNormalize query states\"]\n        \n        Modulate[\"AdaLN modulation\u003cbr/\u003eshift, scale, gate from t_emb\"]\n        \n        ToQ[\"to_q: Linear(dim, dim)\u003cbr/\u003eProject queries\"]\n        ToKV[\"to_kv: Linear(1024, 2*dim)\u003cbr/\u003eProject audio K, V\"]\n        \n        FlashAttn[\"flash_attn_varlen_func\u003cbr/\u003eVariable-length attention\"]\n        \n        ToOut[\"to_out: Linear(dim, dim)\u003cbr/\u003eOutput projection\"]\n        \n        Gate[\"Multiply by gate\u003cbr/\u003eTime-dependent gating\"]\n        \n        Residual[\"residual * person_mask\u003cbr/\u003eApply person masking\"]\n        \n        Concat[\"Concatenate aligned + tail\"]\n        \n        Output[\"x: [total_tokens, dim]\u003cbr/\u003eto next block\"]\n    end\n    \n    Phase0 --\u003e Phase1\n    Phase1 --\u003e Phase2\n    Phase2 --\u003e Input\n    \n    Input --\u003e Align\n    Align --\u003e HiddenAlign\n    Align --\u003e HiddenTail\n    \n    HiddenAlign --\u003e NormQ\n    NormQ --\u003e Modulate\n    Modulate --\u003e ToQ\n    ToQ --\u003e FlashAttn\n    \n    NormKV --\u003e ToKV\n    ToKV --\u003e FlashAttn\n    \n    FlashAttn --\u003e ToOut\n    ToOut --\u003e Gate\n    Gate --\u003e Residual\n    \n    Residual --\u003e Concat\n    HiddenTail --\u003e Concat\n    Concat --\u003e Output\n```\n\n**Audio Post-Adapter Integration as Phase 3 in Transformer Block**\n\n**Sources:** [lightx2v/models/networks/wan/infer/audio/transformer_infer.py:29-110]()\n\n### Weight Structure in Transformer Phase\n\nThe audio adapter weights are integrated directly into transformer block phases. For a block that includes audio attention, Phase 3 contains:\n\n| Weight Name | Shape | Purpose |\n|-------------|-------|---------|\n| `norm_kv` | `LayerNorm(1024)` | Normalize audio KV features |\n| `norm_q` | `LayerNorm(dim)` | Normalize query from hidden states |\n| `shift_scale_gate` | `[1, 3, dim]` | Learnable modulation parameters |\n| `to_q` | `Linear(dim, dim)` | Project queries |\n| `to_kv` | `Linear(1024, 2*dim)` | Project audio features to K, V |\n| `to_out` | `Linear(dim, dim)` | Output projection |\n\nWhere `dim = num_attention_heads * attention_head_dim` (e.g., 40  64 = 2560).\n\n**Sources:** [lightx2v/models/input_encoders/hf/seko_audio/audio_adapter.py:108-136]()\n\n---\n\n## Perceiver Attention Implementation\n\nThe `perceiver_attention_ca()` method implements the core cross-attention computation with time-dependent modulation.\n\n### Attention Computation Flow\n\n```mermaid\ngraph TB\n    AudioIn[\"audio_encoder_output\u003cbr/\u003e[T, 128, 1024]\u003cbr/\u003eAudio features for T frames\"]\n    \n    LatentsIn[\"latents (hidden_states_aligned)\u003cbr/\u003e[n_query_tokens, dim]\u003cbr/\u003eSpatial tokens to update\"]\n    \n    TimeEmb[\"t_emb (audio_adapter_t_emb)\u003cbr/\u003e[1, 3, dim]\u003cbr/\u003eshift, scale, gate from scheduler\"]\n    \n    NormKV[\"norm_kv.apply(audio)\u003cbr/\u003eLayerNorm\"]\n    \n    NormQ[\"norm_q.apply(latents)\u003cbr/\u003eLayerNorm\"]\n    \n    GetParams[\"(t_emb + shift_scale_gate).chunk(3)\u003cbr/\u003eExtract shift, scale, gate\"]\n    \n    Modulate[\"norm_q * (1 + scale) + shift\u003cbr/\u003eAdaptive modulation\"]\n    \n    ProjectQ[\"to_q.apply(modulated_q)\u003cbr/\u003e[n_query_tokens, dim]\"]\n    \n    ProjectKV[\"to_kv.apply(audio).chunk(2)\u003cbr/\u003eSplit into K, V\"]\n    \n    Reshape[\"Reshape to attention heads\u003cbr/\u003eQ, K, V: [N, num_heads, head_dim]\"]\n    \n    FlashAttn[\"flash_attn_varlen_func()\u003cbr/\u003eEfficient variable-length attention\"]\n    \n    ReshapeOut[\"Reshape back\u003cbr/\u003e[n_query_tokens, num_heads * head_dim]\"]\n    \n    ProjectOut[\"to_out.apply(out)\u003cbr/\u003eOutput projection\"]\n    \n    GateOut[\"out * gate\u003cbr/\u003eTime-dependent gating\"]\n    \n    Output[\"residual\u003cbr/\u003e[n_query_tokens, dim]\"]\n    \n    AudioIn --\u003e NormKV\n    LatentsIn --\u003e NormQ\n    TimeEmb --\u003e GetParams\n    \n    NormKV --\u003e ProjectKV\n    NormQ --\u003e Modulate\n    GetParams --\u003e Modulate\n    \n    Modulate --\u003e ProjectQ\n    \n    ProjectQ --\u003e Reshape\n    ProjectKV --\u003e Reshape\n    \n    Reshape --\u003e FlashAttn\n    FlashAttn --\u003e ReshapeOut\n    ReshapeOut --\u003e ProjectOut\n    \n    ProjectOut --\u003e GateOut\n    GetParams --\u003e GateOut\n    \n    GateOut --\u003e Output\n```\n\n**Perceiver Cross-Attention Computation Pipeline**\n\n### Code Implementation\n\n```python\ndef perceiver_attention_ca(self, phase, audio_encoder_output, latents, t_emb):\n    # Normalize inputs\n    audio_encoder_output = phase.norm_kv.apply(audio_encoder_output)\n    \n    # Extract time-dependent modulation parameters\n    shift, scale, gate = (t_emb + phase.shift_scale_gate.tensor)[0].chunk(3, dim=0)\n    \n    # Apply AdaLN to query\n    norm_q = phase.norm_q.apply(latents)\n    latents = norm_q * (1 + scale) + shift\n    \n    # Project to Q, K, V\n    q = phase.to_q.apply(latents)\n    k, v = phase.to_kv.apply(audio_encoder_output).chunk(2, dim=-1)\n    \n    # Reshape to attention heads\n    q = q.view(q.size(0), self.num_heads, self.head_dim)  # [N_q, num_heads, head_dim]\n    k = k.view(k.size(0), self.num_heads, self.head_dim)  # [N_k, num_heads, head_dim]\n    v = v.view(v.size(0), self.num_heads, self.head_dim)  # [N_v, num_heads, head_dim]\n    \n    # Variable-length flash attention\n    out = flash_attn_varlen_func(\n        q=q, k=k, v=v,\n        cu_seqlens_q=self.perceiver_attn_cu_seqlens_q,\n        cu_seqlens_k=self.perceiver_attn_cu_seqlens_k,\n        max_seqlen_q=self.max_seqlen_q,\n        max_seqlen_k=self.max_seqlen_k,\n        dropout_p=0.0,\n        softmax_scale=None,\n        causal=False,\n    )\n    \n    # Reshape and project output\n    out = out.view(-1, self.num_heads * self.head_dim)\n    return phase.to_out.apply(out) * gate\n```\n\n**Sources:** [lightx2v/models/networks/wan/infer/audio/transformer_infer.py:78-110]()\n\n### Variable-Length Attention Setup\n\nThe attention uses cumulative sequence lengths to handle variable numbers of tokens per frame:\n\n```python\n# During post_adapter_states initialization\nn_tokens_per_rank = torch.tensor(x.size(0), dtype=torch.int32)\nself.n_query_tokens = calculate_n_query_tokens(sp_rank, sp_size, n_tokens_per_rank, n_tokens)\n\n# Get sequence lengths for Q (latent tokens) and K (audio tokens)\nself.q_lens, self.k_lens, self.max_seqlen_q, self.max_seqlen_k, self.t0, self.t1 = \\\n    get_qk_lens_audio_range(\n        n_tokens_per_rank=n_tokens_per_rank,\n        n_query_tokens=self.n_query_tokens,\n        n_tokens_per_frame=pre_frame_tokens,\n        sp_rank=sp_rank,\n        num_tokens_x4=128  # Audio tokens per latent frame\n    )\n\n# Create cumulative sequence length tensors\nself.perceiver_attn_cu_seqlens_q = torch.cat([\n    self.q_lens.new_zeros([1]), \n    self.q_lens\n]).cumsum(0, dtype=torch.int32).to(device, non_blocking=True)\n\nself.perceiver_attn_cu_seqlens_k = torch.cat([\n    self.k_lens.new_zeros([1]), \n    self.k_lens\n]).cumsum(0, dtype=torch.int32).to(device, non_blocking=True)\n```\n\nThe `cu_seqlens_q` and `cu_seqlens_k` tensors enable flash attention to process multiple sequences with different lengths efficiently.\n\n**Sources:** [lightx2v/models/networks/wan/infer/audio/transformer_infer.py:46-53](), [lightx2v/models/input_encoders/hf/seko_audio/audio_adapter.py:28-71]()\n\n---\n\n## Hidden States Alignment and Masking\n\nThe `align_hidden_states_and_mask()` function splits hidden states based on `n_query_tokens` to handle sequence parallelism and multi-person scenarios.\n\n### Alignment Logic\n\n```python\ndef align_hidden_states_and_mask(n_query_tokens, hidden_states, person_mask_latens):\n    if n_query_tokens \u003e 0:\n        # Normal case: split hidden states\n        hidden_states_aligned = hidden_states[:n_query_tokens]\n        hidden_states_tail = hidden_states[n_query_tokens:]\n        \n        if person_mask_latens is not None:\n            person_mask_aligned = person_mask_latens[:, :n_query_tokens]\n        else:\n            person_mask_aligned = None\n    else:\n        # Edge case: rank with no tokens (sequence parallelism padding)\n        # Still perform attention with 1 token for FSDP compatibility\n        hidden_states_aligned = hidden_states[:1]\n        hidden_states_tail = hidden_states[1:]\n        \n        if person_mask_latens is not None:\n            person_mask_aligned = person_mask_latens[:, :1]\n        else:\n            person_mask_aligned = None\n    \n    return hidden_states_aligned, hidden_states_tail, person_mask_aligned\n```\n\n**Purpose:**\n- `n_query_tokens`: Number of tokens on current rank that should receive audio attention\n- `hidden_states_aligned`: Tokens that participate in audio cross-attention\n- `hidden_states_tail`: Tokens that bypass audio attention (e.g., reference frames)\n- `person_mask_aligned`: Masks for multi-person audio track assignment\n\n**Sources:** [lightx2v/models/input_encoders/hf/seko_audio/audio_adapter.py:88-105]()\n\n### Multi-Person Integration\n\nWhen multiple audio tracks are provided (`num_audios \u003e 1`), the system processes each audio separately and combines residuals:\n\n```python\n# In infer_post_adapter()\nhidden_states_aligned, hidden_states_tail, person_mask_aligned = \\\n    align_hidden_states_and_mask(self.n_query_tokens, x, person_mask_latens)\n\ntotal_residual = None\nfor i in range(audio_encoder_output.shape[0]):\n    # Process each audio track\n    audio_encoder = audio_encoder_output[i]\n    audio_encoder = audio_encoder[self.t0:self.t1].reshape(-1, audio_encoder.size(-1))\n    \n    # Perceiver cross-attention\n    residual = self.perceiver_attention_ca(\n        phase, audio_encoder, hidden_states_aligned, \n        self.scheduler.audio_adapter_t_emb\n    )\n    \n    residual = residual.to(ori_dtype)\n    \n    # Apply person mask\n    if person_mask_aligned is not None:\n        residual = residual * person_mask_aligned[i].unsqueeze(-1)\n    \n    # Accumulate residuals\n    if total_residual is None:\n        total_residual = residual\n    else:\n        total_residual += residual\n\n# Combine aligned tokens (with audio) and tail tokens (without audio)\nx = torch.cat([hidden_states_aligned + total_residual, hidden_states_tail], dim=0)\n```\n\n**Multi-Person Workflow:**\n1. Each audio track generates a residual via perceiver attention\n2. Residual is masked by corresponding person mask (spatial regions)\n3. Residuals are summed to combine multiple audio influences\n4. Final result is added to aligned hidden states\n\n**Sources:** [lightx2v/models/networks/wan/infer/audio/transformer_infer.py:56-74]()\n\n---\n\n## Summary\n\nMulti-modal conditioning in LightX2V follows a hierarchical integration strategy:\n\n1. **Encoding Stage**: Each modality is independently encoded by specialized models\n2. **Pre-Inference Stage**: Conditions are projected to model dimension and prepared\n3. **Transformer Integration**: \n   - **AdaLN**: Time conditions modulate all normalizations\n   - **Cross-Attention**: Text and image conditions guide via attention\n   - **Post-Adapter**: Audio conditions refine via specialized attention (S2V only)\n\nThis design allows flexible combination of modalities while maintaining clean separation of concerns.\n\n**Key Classes and Functions:**\n- Encoding: `T5EncoderModel`, `CLIPModel`, `SekoAudioEncoderModel`\n- Preparation: `WanPreInfer.infer()`, `WanAudioPreInfer.infer()`\n- Integration: `WanTransformerInfer.infer_block()`, `infer_self_attn()`, `infer_cross_attn()`, `infer_post_adapter()`\n- Scheduling: `WanScheduler`, `EulerScheduler`\n\n**Sources:** [lightx2v/models/networks/wan/infer/pre_infer.py](), [lightx2v/models/networks/wan/infer/transformer_infer.py](), [lightx2v/models/networks/wan/infer/audio/transformer_infer.py]()"])</script><script>self.__next_f.push([1,"44:T3c54,"])</script><script>self.__next_f.push([1,"# Triton Kernel Implementation\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json](configs/seko_talk/seko_talk_24_fp8_dist_compile_nbhd_attn.json)\n- [docs/EN/source/getting_started/quickstart.md](docs/EN/source/getting_started/quickstart.md)\n- [docs/ZH_CN/source/getting_started/quickstart.md](docs/ZH_CN/source/getting_started/quickstart.md)\n- [lightx2v/__init__.py](lightx2v/__init__.py)\n- [lightx2v/common/ops/attn/utils/ring_comm.py](lightx2v/common/ops/attn/utils/ring_comm.py)\n- [lightx2v/common/ops/mm/triton_kernels.py](lightx2v/common/ops/mm/triton_kernels.py)\n- [pyproject.toml](pyproject.toml)\n- [scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh](scripts/seko_talk/run_seko_talk_24_fp8_dist_compile_nbhd_attn.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis page documents the custom Triton kernel implementations in LightX2V that provide GPU-accelerated quantization and matrix multiplication operations. These kernels enable efficient inference with INT8 and FP8 quantized models by implementing low-level CUDA operations using the Triton programming framework.\n\nFor information about the broader quantization system architecture and backend selection, see [6.1](#6.1). For attention mechanism implementations, see [6.2](#6.2).\n\n## Triton Kernel Architecture\n\nThe Triton kernel system consists of two main categories of operations: quantization kernels that convert high-precision tensors to low-precision formats, and GEMM (General Matrix Multiplication) kernels that perform efficient matrix operations on quantized data.\n\n```mermaid\ngraph TB\n    subgraph \"Triton Kernel Components\"\n        QuantKernels[\"Quantization Kernels\"]\n        GEMMKernels[\"GEMM Kernels\"]\n        Utils[\"Utility Functions\"]\n    end\n    \n    subgraph \"Quantization Operations\"\n        INT8Quant[\"int8_quantize_kernel\"]\n        FP8Quant[\"fp8_quantize_kernel\"]\n        INT8QuantWrap[\"int8_quantize_triton\"]\n        FP8QuantWrap[\"fp8_quantize_triton\"]\n    end\n    \n    subgraph \"GEMM Operations\"\n        INT8GEMM[\"int8_gemm_bias_kernel\"]\n        FP8GEMM[\"fp8_gemm_kernel\"]\n        INT8GEMMWrap[\"int8_gemm_bias_triton\"]\n        FP8GEMMWrap[\"fp8_gemm_bias_triton\"]\n    end\n    \n    subgraph \"Utility Operations\"\n        GELU[\"gelu\"]\n        TypeCast[\"upcast_if_fp8\"]\n        TypeComp[\"get_higher_dtype\"]\n    end\n    \n    subgraph \"Quantized Linear Layers\"\n        TritonInt8Linear[\"TritonQuantLinearInt8\"]\n        TritonFp8Linear[\"TritonQuantLinearFp8\"]\n    end\n    \n    QuantKernels --\u003e INT8Quant\n    QuantKernels --\u003e FP8Quant\n    INT8Quant --\u003e INT8QuantWrap\n    FP8Quant --\u003e FP8QuantWrap\n    \n    GEMMKernels --\u003e INT8GEMM\n    GEMMKernels --\u003e FP8GEMM\n    INT8GEMM --\u003e INT8GEMMWrap\n    FP8GEMM --\u003e FP8GEMMWrap\n    \n    Utils --\u003e GELU\n    Utils --\u003e TypeCast\n    Utils --\u003e TypeComp\n    \n    INT8QuantWrap --\u003e TritonInt8Linear\n    INT8GEMMWrap --\u003e TritonInt8Linear\n    FP8QuantWrap --\u003e TritonFp8Linear\n    FP8GEMMWrap --\u003e TritonFp8Linear\n    \n    GELU -.-\u003e|\"Optional Fusion\"| INT8GEMMWrap\n    TypeCast -.-\u003e|\"Type Management\"| GEMMKernels\n```\n\n**Diagram: Triton Kernel System Architecture**\n\nSources: [lightx2v/common/ops/mm/triton_kernels.py:1-600](), [lightx2v/models/input_encoders/hf/q_linear.py:34-154]()\n\n## Quantization Kernels\n\nQuantization kernels convert high-precision floating-point tensors to low-precision integer or floating-point formats. LightX2V implements both INT8 and FP8 quantization kernels.\n\n### INT8 Quantization\n\nThe `int8_quantize_kernel` performs per-token symmetric quantization to 8-bit signed integers. It computes a scaling factor per row by finding the maximum absolute value and mapping the range to [-127, 127].\n\n**Kernel Implementation:**\n\n[lightx2v/common/ops/mm/triton_kernels.py:13-25]()\n\n**Key Operations:**\n1. Load row data from input tensor `X`\n2. Compute scale factor: `scale = 127.0 / max(abs(x))`\n3. Apply scaling and rounding: `x_scaled = x * scale + 0.5 * sign(x)`\n4. Store quantized values to `OUT` as INT8\n5. Store reciprocal scale `1/scale` for dequantization\n\n**Wrapper Function:**\n\n[lightx2v/common/ops/mm/triton_kernels.py:27-36]()\n\nThe wrapper `int8_quantize_triton` reshapes input, allocates output buffers, determines optimal block size using `next_power_of_2`, and launches the kernel with appropriate grid dimensions.\n\n### FP8 Quantization\n\nThe `fp8_quantize_kernel` quantizes to FP8 E4M3 format (4 exponent bits, 3 mantissa bits) with a maximum representable value of 448.0.\n\n**Kernel Implementation:**\n\n[lightx2v/common/ops/mm/triton_kernels.py:38-53]()\n\n**Key Operations:**\n1. Load row data from input tensor `X`\n2. Find absolute maximum value with epsilon clamping\n3. Compute scale: `scale = absmax / 448.0`\n4. Scale and clamp values to [-448, 448]\n5. Store scaled values (converted to FP8 format after kernel)\n6. Store scale factors for dequantization\n\n**Wrapper Function:**\n\n[lightx2v/common/ops/mm/triton_kernels.py:55-66]()\n\nThe `fp8_quantize_triton` wrapper performs similar reshaping and allocation operations, then converts the scaled float32 output to `torch.float8_e4m3fn` format.\n\n```mermaid\ngraph LR\n    Input[\"Input Tensor\u003cbr/\u003e(BF16/FP32)\"]\n    Reshape[\"Reshape to 2D\"]\n    Kernel[\"Quantization Kernel\"]\n    Scale[\"Compute Scales\u003cbr/\u003ePer-Row\"]\n    Quant[\"Quantize Values\"]\n    Store[\"Store Results\"]\n    Output[\"Quantized Tensor\u003cbr/\u003e(INT8/FP8)\"]\n    Scales[\"Scale Factors\u003cbr/\u003e(FP32)\"]\n    \n    Input --\u003e Reshape\n    Reshape --\u003e Kernel\n    Kernel --\u003e Scale\n    Scale --\u003e Quant\n    Quant --\u003e Store\n    Store --\u003e Output\n    Store --\u003e Scales\n    \n    classDef kernel fill:#f9f9f9\n    class Kernel,Scale,Quant kernel\n```\n\n**Diagram: Quantization Kernel Data Flow**\n\nSources: [lightx2v/common/ops/mm/triton_kernels.py:13-66]()\n\n## GEMM Kernels\n\nGEMM (General Matrix Multiplication) kernels perform efficient matrix multiplication on quantized data. These kernels implement optimized algorithms with support for bias addition and optional GELU activation fusion.\n\n### INT8 GEMM with Bias\n\nThe `int8_gemm_bias_kernel` performs matrix multiplication `C = A @ B + bias` where A and B are INT8 tensors with per-row/per-column scaling factors.\n\n**Kernel Configuration:**\n\n[lightx2v/common/ops/mm/triton_kernels.py:90-98]()\n\nThe kernel uses Triton's autotuning framework to select optimal block sizes from multiple configurations based on matrix dimensions.\n\n**Kernel Implementation:**\n\n[lightx2v/common/ops/mm/triton_kernels.py:99-190]()\n\n**Algorithm Steps:**\n1. **Program ID mapping**: Maps 1D program ID to 2D tile coordinates (M, N) using group ordering for better L2 cache locality\n2. **Tile computation**: Computes tiles using blocked matrix multiplication\n3. **Accumulation**: Accumulates results in higher precision (typically FP32)\n4. **Dequantization**: Applies per-row and per-column scales: `result = acc * a_scales[:, None] * b_scales[None, :]`\n5. **Bias addition**: Adds bias vector\n6. **Optional GELU**: Applies GELU activation if `fuse_gelu=True`\n7. **Store**: Writes results to output tensor\n\n**Wrapper Functions:**\n\n[lightx2v/common/ops/mm/triton_kernels.py:193-231]()\n\n- `int8_gemm_bias_triton`: Full GEMM with bias and optional GELU fusion\n- `int8_gemm_triton`: GEMM without bias (sets bias to zero)\n\n### FP8 GEMM\n\nThe `fp8_gemm_kernel` performs matrix multiplication on FP8 E4M3 tensors.\n\n**Kernel Implementation:**\n\n[lightx2v/common/ops/mm/triton_kernels.py:234-365]()\n\nThe FP8 kernel follows a similar structure to INT8 but operates on `torch.float8_e4m3fn` data types. It includes:\n\n- Tile-based computation with autotuned block sizes\n- Per-row and per-column scaling\n- Optional bias and GELU fusion\n- Optimized memory access patterns\n\n**Wrapper Functions:**\n\n[lightx2v/common/ops/mm/triton_kernels.py:368-401]()\n\n| Function | Purpose | Bias Support | GELU Support |\n|----------|---------|--------------|--------------|\n| `fp8_gemm_triton` | Basic FP8 GEMM | No | No |\n| `fp8_gemm_bias_triton` | FP8 GEMM with bias | Yes | Yes |\n\n```mermaid\ngraph TB\n    subgraph \"Input Preparation\"\n        A[\"Matrix A (INT8/FP8)\"]\n        B[\"Matrix B (INT8/FP8)\"]\n        AScale[\"A Scales (FP32)\"]\n        BScale[\"B Scales (FP32)\"]\n        Bias[\"Bias (optional)\"]\n    end\n    \n    subgraph \"GEMM Kernel Execution\"\n        TileLoad[\"Load Tiles\"]\n        DotProduct[\"Dot Product\u003cbr/\u003e(Accumulate in FP32)\"]\n        Dequant[\"Apply Scales\u003cbr/\u003eacc * a_scale * b_scale\"]\n        BiasAdd[\"Add Bias\"]\n        Activation[\"GELU (optional)\"]\n        StoreResult[\"Store to Output\"]\n    end\n    \n    subgraph \"Output\"\n        C[\"Matrix C (BF16/FP32)\"]\n    end\n    \n    A --\u003e TileLoad\n    B --\u003e TileLoad\n    TileLoad --\u003e DotProduct\n    DotProduct --\u003e Dequant\n    AScale --\u003e Dequant\n    BScale --\u003e Dequant\n    Dequant --\u003e BiasAdd\n    Bias --\u003e BiasAdd\n    BiasAdd --\u003e Activation\n    Activation --\u003e StoreResult\n    StoreResult --\u003e C\n    \n    classDef compute fill:#f0f0f0\n    class DotProduct,Dequant,BiasAdd,Activation compute\n```\n\n**Diagram: GEMM Kernel Execution Flow**\n\nSources: [lightx2v/common/ops/mm/triton_kernels.py:90-401]()\n\n## Integration with Quantized Linear Layers\n\nThe Triton kernels integrate into quantized linear layer implementations that replace standard PyTorch linear layers in quantized models.\n\n### TritonQuantLinearInt8\n\nThe `TritonQuantLinearInt8` class implements a quantized linear layer using INT8 Triton kernels.\n\n**Class Structure:**\n\n[lightx2v/models/input_encoders/hf/q_linear.py:37-95]()\n\n**Buffer Registration:**\n- `weight`: INT8 weight tensor\n- `weight_scale`: FP32 per-output-channel scale factors\n- `bias`: Optional FP32 bias vector\n\n**Forward Pass:**\n1. Quantize input activations using `int8_quantize_triton`\n2. Perform matrix multiplication using `int8_gemm_bias_triton` or `int8_gemm_triton`\n3. Return dequantized output in BF16/FP32\n\n### TritonQuantLinearFp8\n\nThe `TritonQuantLinearFp8` class implements FP8 quantized linear layers.\n\n**Class Structure:**\n\n[lightx2v/models/input_encoders/hf/q_linear.py:97-154]()\n\n**Buffer Registration:**\n- `weight`: FP8 E4M3 weight tensor\n- `weight_scale`: FP32 per-output-channel scale factors\n- `bias`: Optional FP32 bias vector\n\n**Forward Pass:**\n1. Quantize input activations using `fp8_quantize_triton`\n2. Perform matrix multiplication using `fp8_gemm_bias_triton` or `fp8_gemm_triton`\n3. Return dequantized output in BF16/FP32\n\n```mermaid\ngraph TB\n    subgraph \"Quantized Linear Layer Forward Pass\"\n        InputBF16[\"Input Activation\u003cbr/\u003e(BF16/FP32)\u003cbr/\u003e[batch, in_features]\"]\n        ActQuant[\"act_quant_func()\u003cbr/\u003eTriton Quantization Kernel\"]\n        QuantAct[\"Quantized Activation\u003cbr/\u003e(INT8/FP8)\"]\n        ActScale[\"Activation Scales\"]\n        \n        WeightBuffer[\"Weight Buffer\u003cbr/\u003e(INT8/FP8)\u003cbr/\u003e[out_features, in_features]\"]\n        WeightScaleBuffer[\"Weight Scale Buffer\u003cbr/\u003e(FP32)\"]\n        BiasBuffer[\"Bias Buffer\u003cbr/\u003e(FP32)\"]\n        \n        GEMM[\"Triton GEMM Kernel\u003cbr/\u003eint8_gemm_bias_triton\u003cbr/\u003eor fp8_gemm_bias_triton\"]\n        \n        OutputBF16[\"Output\u003cbr/\u003e(BF16/FP32)\u003cbr/\u003e[batch, out_features]\"]\n    end\n    \n    InputBF16 --\u003e ActQuant\n    ActQuant --\u003e QuantAct\n    ActQuant --\u003e ActScale\n    \n    QuantAct --\u003e GEMM\n    ActScale --\u003e GEMM\n    WeightBuffer --\u003e GEMM\n    WeightScaleBuffer --\u003e GEMM\n    BiasBuffer --\u003e GEMM\n    \n    GEMM --\u003e OutputBF16\n    \n    classDef buffer fill:#e8e8e8\n    classDef compute fill:#f0f0f0\n    class WeightBuffer,WeightScaleBuffer,BiasBuffer buffer\n    class ActQuant,GEMM compute\n```\n\n**Diagram: Quantized Linear Layer Architecture**\n\nSources: [lightx2v/models/input_encoders/hf/q_linear.py:37-154](), [lightx2v/common/ops/mm/triton_kernels.py:27-66](), [lightx2v/common/ops/mm/triton_kernels.py:193-231]()\n\n## Kernel Backend Selection\n\nLightX2V provides multiple quantization kernel backends. The Triton kernels serve as a default fallback implementation when other specialized backends are unavailable.\n\n**Backend Priority (from `q_linear.py`):**\n\n| Backend | INT8 Support | FP8 Support | Requirements |\n|---------|--------------|-------------|--------------|\n| VLLM | Yes (`VllmQuantLinearInt8`) | Yes (`VllmQuantLinearFp8`) | `vllm` package |\n| SGL | No | Yes (`SglQuantLinearFp8`) | `sgl_kernel` package |\n| TorchAO | Yes (`TorchaoQuantLinearInt8`) | Yes (`TorchaoQuantLinearFp8`) | `torchao` package |\n| Q8 Kernels | Yes (via imports) | Yes (via imports) | `q8_kernels` package |\n| **Triton** | **Yes** (`TritonQuantLinearInt8`) | **Yes** (`TritonQuantLinearFp8`) | **Built-in (triton)** |\n\nSources: [lightx2v/models/input_encoders/hf/q_linear.py:1-154]()\n\n## Utility Functions\n\nThe Triton kernel module provides utility functions for activation and type management.\n\n### GELU Activation\n\nThe `gelu` function implements the Gaussian Error Linear Unit activation using the approximation `gelu(x) = x * sigmoid(1.702 * x)`.\n\n[lightx2v/common/ops/mm/triton_kernels.py:8-11]()\n\nThis function is JIT-compiled with Triton and can be fused into GEMM kernels when `fuse_gelu=True`.\n\n### Type Utilities\n\n**upcast_if_fp8:**\n[lightx2v/common/ops/mm/triton_kernels.py:68-72]()\n\nUpcasts FP8 types to FP16 for compatibility with operations that don't support FP8.\n\n**get_higher_dtype:**\n[lightx2v/common/ops/mm/triton_kernels.py:74-88]()\n\nDetermines the higher precision type between two dtypes according to the ordering: `INT8 \u003c FP16 \u003c BF16 \u003c FP32`.\n\nSources: [lightx2v/common/ops/mm/triton_kernels.py:8-88]()\n\n## Performance Optimizations\n\nThe Triton kernels incorporate several performance optimizations:\n\n### Block Size Autotuning\n\nGEMM kernels use Triton's `@autotune` decorator to automatically select optimal block sizes based on input matrix dimensions:\n\n[lightx2v/common/ops/mm/triton_kernels.py:90-98]()\n\n**Tuned Parameters:**\n- `BLOCK_M`: Block size for M dimension (128-256)\n- `BLOCK_N`: Block size for N dimension (128)\n- `BLOCK_K`: Block size for K dimension (64-128)\n- `num_stages`: Pipeline stages (3-4)\n- `num_warps`: Number of warps (8)\n\n### Memory Access Patterns\n\nThe kernels optimize memory access through:\n\n1. **Group ordering**: Reorders program IDs to improve L2 cache hit rates\n   [lightx2v/common/ops/mm/triton_kernels.py:129-136]()\n\n2. **Contiguous access**: Uses `tl.max_contiguous` and `tl.multiple_of` hints for vectorized loads\n   [lightx2v/common/ops/mm/triton_kernels.py:140-141]()\n\n3. **Pointer arithmetic**: Advances pointers in blocks to enable coalesced memory transactions\n   [lightx2v/common/ops/mm/triton_kernels.py:161-162]()\n\n### Operator Fusion\n\nThe GEMM kernels support fusing multiple operations into a single kernel launch:\n\n- **Scale application**: Dequantization scales fused with GEMM\n- **Bias addition**: Bias vector addition fused into output computation\n- **GELU activation**: Optional GELU activation fused when `fuse_gelu=True`\n\nThis fusion reduces memory bandwidth requirements by eliminating intermediate tensor materialization.\n\nSources: [lightx2v/common/ops/mm/triton_kernels.py:90-401]()\n\n## Platform Compatibility\n\nThe Triton kernels provide cross-platform quantization support:\n\n**Linux Support:**\n- Default backend in Conda environment setup\n- Automatic installation with `triton` package\n\n**Windows Support:**\n- Requires `triton-windows` package\n- Default quantization backend when VLLM/SGL unavailable\n- Documented in Windows setup guides\n\nSources: [docs/EN/source/getting_started/quickstart.md:256-257](), [docs/ZH_CN/source/getting_started/quickstart.md:241-242]()"])</script><script>self.__next_f.push([1,"45:T5f7b,"])</script><script>self.__next_f.push([1,"# VAE Tiling and Distributed Processing\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document covers the VAE (Variational AutoEncoder) tiling and distributed processing mechanisms in LightX2V. These features enable efficient processing of high-resolution images and videos by splitting workloads across multiple GPUs and processing large frames in smaller tiles. For general VAE architecture and latent space concepts, see [VAE System and Latent Space](#4.6). For distributed inference strategies across the entire model, see [Distributed and Parallel Inference](#6.5).\n\n**Sources:** [lightx2v/models/video_encoders/hf/wan/vae.py:1-429](), [lightx2v/models/runners/wan/wan_runner.py:159-456]()\n\n---\n\n## Configuration Parameters\n\nThe VAE tiling and distributed processing are controlled through configuration parameters that are set during runner initialization and passed to the VAE constructor.\n\n### Core Configuration Keys\n\n| Configuration Key | Type | Default | Description |\n|------------------|------|---------|-------------|\n| `use_tiling_vae` | bool | False | Enable tiling for processing large images/videos |\n| `parallel` | bool/dict | False | Enable distributed VAE processing |\n| `vae_parallel` | bool | True | Enable VAE-specific parallelism (when parallel is dict) |\n| `vae_cpu_offload` | bool | False | Offload VAE to CPU when not in use |\n| `use_stream_vae` | bool | False | Enable streaming decode for memory efficiency |\n\n### Configuration Flow\n\n```mermaid\ngraph TB\n    Config[\"Configuration Dict\"]\n    Runner[\"Runner (WanRunner/WanAudioRunner)\"]\n    LoadVAE[\"load_vae_encoder()/load_vae_decoder()\"]\n    VAEConfig[\"VAE Config Dict\"]\n    VAEInit[\"WanVAE.__init__()\"]\n    \n    Config --\u003e|\"use_tiling_vae\u003cbr/\u003eparallel\u003cbr/\u003evae_cpu_offload\"| Runner\n    Runner --\u003e|\"build config\"| LoadVAE\n    LoadVAE --\u003e|\"vae_config dict\"| VAEConfig\n    VAEConfig --\u003e|\"use_tiling\u003cbr/\u003eparallel\u003cbr/\u003ecpu_offload\"| VAEInit\n    \n    VAEConfig -.-\u003e|\"vae_path\"| VAEInit\n    VAEConfig -.-\u003e|\"device\"| VAEInit\n    VAEConfig -.-\u003e|\"dtype\"| VAEInit\n```\n\n**Diagram: Configuration parameter flow from user config to VAE initialization**\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:166-221](), [lightx2v/models/video_encoders/hf/wan/vae.py:271-350]()\n\n---\n\n## Distributed VAE Processing\n\nThe distributed VAE processing feature splits the spatial dimensions of the latent space across multiple GPUs, enabling parallel encoding and decoding of large images.\n\n### Grid Splitting Strategy\n\nThe system uses a 2D grid splitting approach that divides latent dimensions across GPUs. The `_adjust_latent_for_grid_splitting` method optimizes the split to prefer balanced grids.\n\n```mermaid\ngraph LR\n    subgraph \"Priority Grid Patterns (8 GPUs)\"\n        G1[\"24 Grid\u003cbr/\u003e(Preferred)\"]\n        G2[\"42 Grid\u003cbr/\u003e(Preferred)\"]\n        G3[\"18 Grid\u003cbr/\u003e(Fallback)\"]\n        G4[\"81 Grid\u003cbr/\u003e(Fallback)\"]\n    end\n    \n    Latent[\"Latent Tensor\u003cbr/\u003e[C, T, H, W]\"]\n    Split[\"Grid Split Decision\"]\n    Adjust[\"Dimension Adjustment\u003cbr/\u003ewith Padding\"]\n    \n    Latent --\u003e Split\n    Split --\u003e|\"check divisibility\"| G1\n    Split --\u003e|\"check divisibility\"| G2\n    Split --\u003e|\"minimal padding\"| Adjust\n    Adjust --\u003e G1\n    Adjust --\u003e G2\n    \n    G1 -.-\u003e|\"not divisible\"| G3\n    G2 -.-\u003e|\"not divisible\"| G4\n```\n\n**Diagram: Grid splitting priority and decision logic for 8-GPU configuration**\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:302-352]()\n\n### Grid Splitting Implementation\n\nThe grid splitting algorithm follows this priority order:\n\n| World Size | Priority Grids | Rationale |\n|-----------|----------------|-----------|\n| 8 GPUs | (2,4), (4,2), (1,8), (8,1) | Balanced splits reduce communication overhead |\n| 4 GPUs | (2,2), (1,4), (4,1) | Square grids are optimal for 4 devices |\n| 2 GPUs | (1,2), (2,1) | Simple horizontal or vertical split |\n\nThe method pads latent dimensions when necessary to ensure divisibility:\n\n```python\n# From wan_runner.py:302-352\ndef _adjust_latent_for_grid_splitting(self, latent_h, latent_w, world_size):\n    \"\"\"\n    Adjust latent dimensions for optimal 2D grid splitting.\n    Prefers balanced grids like 2x4 or 4x2 over 1x8 or 8x1.\n    \"\"\"\n    # Priority grid selection\n    # Calculate minimal padding if needed\n    # Return adjusted dimensions and grid shape\n```\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:302-352]()\n\n### Distributed Encoding Flow\n\n```mermaid\nsequenceDiagram\n    participant Runner as WanRunner\n    participant Adjust as _adjust_latent_for_grid_splitting\n    participant VAE as VAE Encoder\n    participant Dist as torch.distributed\n    \n    Runner-\u003e\u003eRunner: Calculate ori_latent_h, ori_latent_w\n    Runner-\u003e\u003eDist: Check dist.is_initialized()\n    Dist--\u003e\u003eRunner: world_size \u003e 1\n    \n    Runner-\u003e\u003eAdjust: latent_h, latent_w, world_size\n    Adjust-\u003e\u003eAdjust: Find optimal grid (world_size_h, world_size_w)\n    Adjust-\u003e\u003eAdjust: Add padding if needed\n    Adjust--\u003e\u003eRunner: adjusted latent_h, latent_w, grid shape\n    \n    Runner-\u003e\u003eVAE: encode(input, world_size_h, world_size_w)\n    VAE-\u003e\u003eVAE: Split spatial dims across GPUs\n    VAE--\u003e\u003eRunner: distributed latent tensor\n```\n\n**Diagram: Distributed encoding sequence with grid optimization**\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:360-456]()\n\n### Parallel Configuration Detection\n\nThe `get_vae_parallel` method determines whether VAE parallelism is enabled:\n\n```python\n# From wan_runner.py:159-164\ndef get_vae_parallel(self):\n    if isinstance(self.config.get(\"parallel\", False), bool):\n        return self.config.get(\"parallel\", False)\n    if isinstance(self.config.get(\"parallel\", False), dict):\n        return self.config.get(\"parallel\", {}).get(\"vae_parallel\", True)\n    return False\n```\n\nThis allows fine-grained control: if `parallel` is a dictionary, only `vae_parallel` controls VAE distribution, enabling scenarios where the transformer uses different parallelism than the VAE.\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:159-164]()\n\n---\n\n## VAE Tiling Architecture\n\nTiling enables processing of large images that would otherwise exceed GPU memory by breaking them into smaller spatial tiles. Each tile is processed independently and reconstructed.\n\n### Tiling Configuration Flow\n\n```mermaid\ngraph TB\n    UserConfig[\"use_tiling_vae: true\"]\n    LoadVAEEnc[\"load_vae_encoder()\"]\n    LoadVAEDec[\"load_vae_decoder()\"]\n    VAEConfigEnc[\"vae_config\u003cbr/\u003e{use_tiling: True}\"]\n    VAEConfigDec[\"vae_config\u003cbr/\u003e{use_tiling: True}\"]\n    \n    WanVAEEnc[\"WanVAE (Encoder)\"]\n    WanVAEDec[\"WanVAE (Decoder)\"]\n    \n    EncodeMethod[\"encode() method\"]\n    DecodeMethod[\"decode() method\"]\n    TiledEncode[\"Tile-based encoding\"]\n    TiledDecode[\"Tile-based decoding\"]\n    \n    UserConfig --\u003e LoadVAEEnc\n    UserConfig --\u003e LoadVAEDec\n    \n    LoadVAEEnc --\u003e VAEConfigEnc\n    LoadVAEDec --\u003e VAEConfigDec\n    \n    VAEConfigEnc --\u003e WanVAEEnc\n    VAEConfigDec --\u003e WanVAEDec\n    \n    WanVAEEnc --\u003e EncodeMethod\n    WanVAEDec --\u003e DecodeMethod\n    \n    EncodeMethod -.-\u003e|\"if use_tiling\"| TiledEncode\n    DecodeMethod -.-\u003e|\"if use_tiling\"| TiledDecode\n```\n\n**Diagram: VAE tiling configuration and execution path**\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:166-221](), [lightx2v/models/video_encoders/hf/wan/vae.py:271-429]()\n\n### VAE Class Hierarchy\n\n```mermaid\ngraph TB\n    WanVAE[\"WanVAE\u003cbr/\u003evae.py\"]\n    Wan2_2_VAE[\"Wan2_2_VAE\u003cbr/\u003evae_2_2.py\"]\n    WanVAE_tiny[\"WanVAE_tiny\u003cbr/\u003evae_tiny.py\"]\n    Wan2_2_VAE_tiny[\"Wan2_2_VAE_tiny\u003cbr/\u003evae_tiny.py\"]\n    \n    Encoder[\"Encoder Module\"]\n    Decoder[\"Decoder Module\"]\n    \n    WanVAE --\u003e Encoder\n    WanVAE --\u003e Decoder\n    Wan2_2_VAE --\u003e Encoder\n    Wan2_2_VAE --\u003e Decoder\n    \n    Encoder -.-\u003e|\"contains\"| ResidualBlock\n    Encoder -.-\u003e|\"contains\"| AttentionBlock\n    Encoder -.-\u003e|\"contains\"| Resample\n    \n    Decoder -.-\u003e|\"contains\"| ResidualBlock\n    Decoder -.-\u003e|\"contains\"| AttentionBlock\n    Decoder -.-\u003e|\"contains\"| Resample\n    \n    ResidualBlock[\"ResidualBlock\u003cbr/\u003eCausalConv3d layers\"]\n    AttentionBlock[\"AttentionBlock\u003cbr/\u003eSpatial attention\"]\n    Resample[\"Resample\u003cbr/\u003eUp/Downsample\"]\n```\n\n**Diagram: VAE class hierarchy and component structure**\n\n**Sources:** [lightx2v/models/video_encoders/hf/wan/vae.py:1-429](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py:1-550]()\n\n### Tiling Use Cases\n\nTiling is beneficial in these scenarios:\n\n| Scenario | Resolution | Memory Constraint | Tiling Benefit |\n|----------|-----------|-------------------|----------------|\n| High-res image generation | 1280720 | Limited VRAM | Reduces peak memory usage |\n| Long video sequences | Many frames | Memory fragmentation | Processes chunks independently |\n| Multi-resolution training | Variable sizes | Fixed GPU memory | Handles diverse resolutions |\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:178-211]()\n\n---\n\n## Streaming VAE Decode\n\nStreaming decode processes VAE output in chunks to reduce memory footprint, particularly useful for long video sequences.\n\n### Streaming Architecture\n\n```mermaid\ngraph LR\n    Latents[\"Latent Tensor\u003cbr/\u003e[C, T, H, W]\"]\n    DecodeStream[\"decode_stream()\"]\n    \n    subgraph \"Chunk Processing\"\n        Chunk1[\"Chunk 1\u003cbr/\u003e[C, 1, H, W]\"]\n        Chunk2[\"Chunk 2\u003cbr/\u003e[C, 4, H, W]\"]\n        ChunkN[\"Chunk N\u003cbr/\u003e[C, 4, H, W]\"]\n    end\n    \n    FeatCache[\"feat_cache\u003cbr/\u003eTemporal context\"]\n    Output1[\"Frame Segment 1\"]\n    Output2[\"Frame Segment 2\"]\n    OutputN[\"Frame Segment N\"]\n    \n    Latents --\u003e DecodeStream\n    DecodeStream --\u003e Chunk1\n    DecodeStream --\u003e Chunk2\n    DecodeStream --\u003e ChunkN\n    \n    Chunk1 --\u003e FeatCache\n    FeatCache --\u003e Chunk2\n    Chunk2 --\u003e FeatCache\n    FeatCache --\u003e ChunkN\n    \n    Chunk1 --\u003e Output1\n    Chunk2 --\u003e Output2\n    ChunkN --\u003e OutputN\n```\n\n**Diagram: Streaming decode processes latents in temporal chunks with feature caching**\n\n**Sources:** [lightx2v/models/runners/default_runner.py:404-415](), [lightx2v/models/video_encoders/hf/wan/vae.py:1-429]()\n\n### Streaming vs. Standard Decode\n\n```mermaid\ngraph TB\n    Runner[\"DefaultRunner.run_main()\"]\n    UseStream{use_stream_vae?}\n    \n    StandardPath[\"run_vae_decoder(latents)\"]\n    StreamPath[\"run_vae_decoder_stream(latents)\"]\n    \n    StandardDecode[\"decode(latents)\u003cbr/\u003eFull tensor in memory\"]\n    StreamDecode[\"decode_stream(latents)\u003cbr/\u003eYields frame segments\"]\n    \n    StandardResult[\"gen_video = output\"]\n    StreamResult[\"gen_video = torch.cat(frames)\"]\n    \n    Runner --\u003e UseStream\n    UseStream --\u003e|\"False\"| StandardPath\n    UseStream --\u003e|\"True\"| StreamPath\n    \n    StandardPath --\u003e StandardDecode\n    StreamPath --\u003e StreamDecode\n    \n    StandardDecode --\u003e StandardResult\n    \n    StreamDecode --\u003e|\"yield\"| Frame1[\"Frame Segment 1\"]\n    StreamDecode --\u003e|\"yield\"| Frame2[\"Frame Segment 2\"]\n    StreamDecode --\u003e|\"yield\"| FrameN[\"Frame Segment N\"]\n    \n    Frame1 --\u003e StreamResult\n    Frame2 --\u003e StreamResult\n    FrameN --\u003e StreamResult\n```\n\n**Diagram: Decision flow between standard and streaming VAE decode**\n\n**Sources:** [lightx2v/models/runners/default_runner.py:379-415]()\n\n### Feature Caching in Streaming\n\nThe VAE uses `feat_cache` to maintain temporal consistency across chunks:\n\n```python\n# From default_runner.py:404-415\n@ProfilingContext4DebugL1(...)\ndef run_vae_decoder_stream(self, latents):\n    if self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n        self.vae_decoder = self.load_vae_decoder()\n\n    for frame_segment in self.vae_decoder.decode_stream(latents.to(GET_DTYPE())):\n        yield frame_segment\n```\n\nThe `feat_cache` parameter in VAE layers (e.g., `Resample`, `ResidualBlock`) stores intermediate features from previous chunks to maintain causal consistency.\n\n**Sources:** [lightx2v/models/runners/default_runner.py:404-415](), [lightx2v/models/video_encoders/hf/wan/vae.py:86-241]()\n\n---\n\n## Audio-to-Video VAE Integration\n\nIn audio-driven video generation, the VAE handles special requirements for streaming and temporal consistency.\n\n### WanAudioRunner VAE Usage\n\n```mermaid\ngraph TB\n    AudioRunner[\"WanAudioRunner\"]\n    \n    subgraph \"Encoder Phase\"\n        ReadImage[\"read_image_input()\"]\n        ResizeImage[\"resize_image()\"]\n        VAEEncode[\"run_vae_encoder()\"]\n    end\n    \n    subgraph \"Segment Processing\"\n        InitSegment[\"init_run_segment()\"]\n        PrepPrev[\"prepare_prev_latents()\"]\n        RunSegment[\"run_segment()\"]\n    end\n    \n    subgraph \"Decoder Phase\"\n        UseStreamVAE{use_stream_vae?}\n        StreamDecode[\"end_run_segment_stream()\"]\n        StandardDecode[\"run_vae_decoder()\u003cbr/\u003eend_run_segment()\"]\n    end\n    \n    AudioRunner --\u003e ReadImage\n    ReadImage --\u003e ResizeImage\n    ResizeImage --\u003e VAEEncode\n    \n    AudioRunner --\u003e InitSegment\n    InitSegment --\u003e PrepPrev\n    PrepPrev --\u003e VAEEncode\n    PrepPrev --\u003e RunSegment\n    \n    RunSegment --\u003e UseStreamVAE\n    UseStreamVAE --\u003e|\"True\"| StreamDecode\n    UseStreamVAE --\u003e|\"False\"| StandardDecode\n```\n\n**Diagram: VAE integration in audio-driven video pipeline**\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:371-435](), [lightx2v/models/runners/wan/wan_audio_runner.py:628-660]()\n\n### Previous Latents Encoding\n\nFor temporal consistency in audio-driven video, previous frames are encoded:\n\n```python\n# From wan_audio_runner.py:464-516\ndef prepare_prev_latents(self, prev_video, prev_frame_length):\n    \"\"\"Prepare previous latents for conditioning\"\"\"\n    # Process last N frames from previous video\n    # Apply noise and masking for wan2.1\n    # Encode with VAE\n    # Generate mask for temporal blending\n```\n\nThis mechanism:\n1. Extracts the last `prev_frame_length` frames from previous segment\n2. Applies noise and masking (for non-wan2.2 models) via `FramePreprocessorTorchVersion`\n3. Encodes through VAE to get `prev_latents`\n4. Creates `prev_mask` for temporal blending in the diffusion process\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:464-516]()\n\n### Streaming Decode for Audio-Video\n\n```python\n# From wan_audio_runner.py:633-659\ndef end_run_segment_stream(self, latents, valid_duration):\n    # Stream decode frame segments\n    for origin_seg in self.run_vae_decoder_stream(latents):\n        origin_seg = torch.clamp(origin_seg, -1, 1).to(torch.float)\n        # Convert to ComfyUI format\n        video_seg = wan_vae_to_comfy(origin_seg[:, :, :valid_T].cpu())\n        # Extract corresponding audio segment\n        audio_seg = self.segment.audio_array[:, audio_start:audio_end].sum(dim=0)\n        # Publish to livestream controller\n        self.va_controller.pub_livestream(video_seg, audio_seg, ...)\n```\n\nStreaming decode enables real-time or near-real-time audio-driven video generation by processing and publishing frame segments as they're decoded.\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:628-660]()\n\n---\n\n## VAE Component Architecture\n\n### Core Components\n\n```mermaid\ngraph TB\n    subgraph \"Encoder Path\"\n        Input[\"Input Tensor\u003cbr/\u003e[B, C, T, H, W]\"]\n        EncBlocks[\"Encoder Blocks\u003cbr/\u003eResidualBlock + AttentionBlock\"]\n        Downsample[\"Downsample Layers\u003cbr/\u003eResample (downsample3d)\"]\n        LatentOut[\"Latent Output\u003cbr/\u003e[B, latent_dim, T', H', W']\"]\n    end\n    \n    subgraph \"Decoder Path\"\n        LatentIn[\"Latent Input\u003cbr/\u003e[B, latent_dim, T', H', W']\"]\n        DecBlocks[\"Decoder Blocks\u003cbr/\u003eResidualBlock + AttentionBlock\"]\n        Upsample[\"Upsample Layers\u003cbr/\u003eResample (upsample3d)\"]\n        Output[\"Output Tensor\u003cbr/\u003e[B, C, T, H, W]\"]\n    end\n    \n    Input --\u003e EncBlocks\n    EncBlocks --\u003e Downsample\n    Downsample --\u003e LatentOut\n    \n    LatentIn --\u003e DecBlocks\n    DecBlocks --\u003e Upsample\n    Upsample --\u003e Output\n    \n    CausalConv3d[\"CausalConv3d\u003cbr/\u003eTemporal causality\"]\n    RMS_norm[\"RMS_norm\u003cbr/\u003eNormalization\"]\n    \n    EncBlocks -.-\u003e|\"uses\"| CausalConv3d\n    DecBlocks -.-\u003e|\"uses\"| CausalConv3d\n    EncBlocks -.-\u003e|\"uses\"| RMS_norm\n    DecBlocks -.-\u003e|\"uses\"| RMS_norm\n```\n\n**Diagram: VAE encoder-decoder architecture with key components**\n\n**Sources:** [lightx2v/models/video_encoders/hf/wan/vae.py:24-268](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py:22-267]()\n\n### CausalConv3d Implementation\n\nThe `CausalConv3d` layer maintains temporal causality by padding only in the temporal dimension's past:\n\n```python\n# From vae.py:24-50\nclass CausalConv3d(nn.Conv3d):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._padding = (\n            self.padding[2], self.padding[2],  # W dimension (both sides)\n            self.padding[1], self.padding[1],  # H dimension (both sides)\n            2 * self.padding[0], 0,            # T dimension (only past)\n        )\n        self.padding = (0, 0, 0)\n\n    def forward(self, x, cache_x=None):\n        # Use cached frames if provided\n        # Apply asymmetric padding\n        return super().forward(x)\n```\n\nThis ensures that frame $t$ only depends on frames $\\leq t$, crucial for autoregressive video generation.\n\n**Sources:** [lightx2v/models/video_encoders/hf/wan/vae.py:24-50](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py:22-48]()\n\n### Resample Module\n\nThe `Resample` module handles spatial upsampling/downsampling with temporal processing:\n\n| Mode | Spatial Operation | Temporal Operation | Use Case |\n|------|------------------|-------------------|----------|\n| `upsample2d` | 2 nearest + Conv2d | None | Simple 2D upsampling |\n| `upsample3d` | 2 nearest + Conv2d | CausalConv3d (3,1,1) | Video upsampling with temporal consistency |\n| `downsample2d` | Stride-2 Conv2d | None | Simple 2D downsampling |\n| `downsample3d` | Stride-2 Conv2d | CausalConv3d stride=(2,1,1) | Video downsampling with temporal reduction |\n\n**Sources:** [lightx2v/models/video_encoders/hf/wan/vae.py:87-200](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py:85-201]()\n\n---\n\n## Memory Optimization Strategies\n\n### Channels Last 3D Format\n\nThe VAE supports `channels_last_3d` memory format for Conv3d operations to reduce memory layout conversion overhead:\n\n```python\n# From vae.py:52-62\ndef convert_to_channels_last_3d(module):\n    \"\"\"\n    Recursively convert all Conv3d weights in module to channels_last_3d format.\n    This eliminates NCHW\u003c-\u003eNHWC format conversion overhead in cuDNN.\n    \"\"\"\n    for child in module.children():\n        if isinstance(child, nn.Conv3d):\n            child.weight.data = child.weight.data.to(memory_format=torch.channels_last_3d)\n        else:\n            convert_to_channels_last_3d(child)\n```\n\nThis optimization is enabled via the `GET_USE_CHANNELS_LAST_3D()` environment function.\n\n**Sources:** [lightx2v/models/video_encoders/hf/wan/vae.py:52-62](), [lightx2v/models/video_encoders/hf/wan/vae_2_2.py:50-60]()\n\n### CPU Offloading\n\nVAE modules can be offloaded to CPU when not in use:\n\n```python\n# From wan_runner.py:166-195\ndef load_vae_encoder(self):\n    vae_offload = self.config.get(\"vae_cpu_offload\", self.config.get(\"cpu_offload\"))\n    if vae_offload:\n        vae_device = torch.device(\"cpu\")\n    else:\n        vae_device = torch.device(AI_DEVICE)\n    \n    vae_config = {\n        \"vae_path\": ...,\n        \"device\": vae_device,\n        \"cpu_offload\": vae_offload,\n        ...\n    }\n```\n\nCombined with lazy loading, this enables inference with limited GPU memory.\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:166-221]()\n\n### Lazy Loading Pattern\n\n```mermaid\nsequenceDiagram\n    participant Runner\n    participant VAE as VAE Encoder/Decoder\n    participant GPU\n    participant CPU\n    \n    Note over Runner,CPU: lazy_load or unload_modules = True\n    \n    Runner-\u003e\u003eRunner: Check if VAE loaded\n    Runner-\u003e\u003eRunner: self.vae_encoder = self.load_vae_encoder()\n    Runner-\u003e\u003eVAE: Load from disk\n    VAE-\u003e\u003eCPU: Weights on CPU\n    \n    Runner-\u003e\u003eVAE: encode(input)\n    VAE-\u003e\u003eGPU: Copy weights to GPU\n    GPU-\u003e\u003eGPU: Compute encoding\n    GPU--\u003e\u003eVAE: Latent output\n    VAE--\u003e\u003eRunner: Return latents\n    \n    Runner-\u003e\u003eVAE: Delete reference\n    VAE-\u003e\u003eGPU: Free GPU memory\n    GPU-\u003e\u003eGPU: torch.cuda.empty_cache()\n```\n\n**Diagram: Lazy loading lifecycle for VAE encoder**\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:428-454](), [lightx2v/models/runners/default_runner.py:395-415]()\n\n---\n\n## Integration Points\n\n### Runner Integration\n\n| Runner Class | VAE Usage | Tiling Support | Distributed Support |\n|--------------|-----------|----------------|---------------------|\n| `WanRunner` | Encode/Decode for I2V, T2V |  |  |\n| `WanAudioRunner` | Encode reference image, decode video segments |  |  |\n| `DefaultRunner` | Base encode/decode with streaming |  |  |\n| `QwenImageRunner` | Image encoding/decoding |  |  |\n| `HunyuanVideo15Runner` | Video encoding/decoding with LightTAE |  |  |\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:166-221](), [lightx2v/models/runners/wan/wan_audio_runner.py:424-435](), [lightx2v/models/runners/default_runner.py:393-415]()\n\n### VAE Loading Methods\n\n```python\n# From wan_runner.py:214-221\ndef load_vae(self):\n    vae_encoder = self.load_vae_encoder()\n    if vae_encoder is None or self.config.get(\"use_tae\", False):\n        vae_decoder = self.load_vae_decoder()\n    else:\n        vae_decoder = vae_encoder  # Share same instance\n    return vae_encoder, vae_decoder\n```\n\nThe VAE encoder and decoder can share the same instance when both are needed, reducing memory usage.\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:214-221]()\n\n---\n\n## Performance Considerations\n\n### Tiling vs. Distributed Trade-offs\n\n| Feature | Tiling | Distributed |\n|---------|--------|-------------|\n| Memory reduction | High (processes tiles sequentially) | Moderate (splits across GPUs) |\n| Throughput | Lower (sequential processing) | Higher (parallel processing) |\n| Communication overhead | None | GPU-to-GPU communication |\n| Best for | Single-GPU, very large images | Multi-GPU, moderate sizes |\n| Boundary artifacts | Possible at tile edges | None (if properly padded) |\n\n### Streaming Decode Benefits\n\n1. **Memory footprint**: Processes and discards frames progressively\n2. **Latency**: Enables real-time playback (frames available as decoded)\n3. **Compatibility**: Works with livestreaming via `VAController`\n\n**Sources:** [lightx2v/models/runners/default_runner.py:379-415](), [lightx2v/models/runners/wan/wan_audio_runner.py:628-660]()\n\n### Distributed Processing Performance\n\nThe grid splitting strategy significantly impacts performance:\n\n- **Balanced grids** (e.g., 24 for 8 GPUs): Minimize communication-to-computation ratio\n- **Linear grids** (e.g., 18): Higher communication overhead but simpler\n- **Square grids** (e.g., 22 for 4 GPUs): Optimal for isotropic images\n\nThe `_adjust_latent_for_grid_splitting` method automatically selects the best configuration based on latent dimensions and available GPUs.\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:302-352]()"])</script><script>self.__next_f.push([1,"46:T8d26,"])</script><script>self.__next_f.push([1,"# Lazy Loading and Async Weight Streaming\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [app/utils/image_page.py](app/utils/image_page.py)\n- [configs/z_image/z_image_turbo_t2i_offload.json](configs/z_image/z_image_turbo_t2i_offload.json)\n- [examples/z-image-turbo/z_image_turbo.py](examples/z-image-turbo/z_image_turbo.py)\n- [lightx2v/common/modules/weight_module.py](lightx2v/common/modules/weight_module.py)\n- [lightx2v/models/input_encoders/hf/z_image/qwen3_model.py](lightx2v/models/input_encoders/hf/z_image/qwen3_model.py)\n- [lightx2v/models/networks/base_model.py](lightx2v/models/networks/base_model.py)\n- [lightx2v/models/networks/z_image/model.py](lightx2v/models/networks/z_image/model.py)\n- [lightx2v/models/networks/z_image/weights/post_weights.py](lightx2v/models/networks/z_image/weights/post_weights.py)\n- [lightx2v/models/networks/z_image/weights/transformer_weights.py](lightx2v/models/networks/z_image/weights/transformer_weights.py)\n- [lightx2v/models/runners/z_image/z_image_runner.py](lightx2v/models/runners/z_image/z_image_runner.py)\n- [lightx2v/models/video_encoders/hf/z_image/vae.py](lightx2v/models/video_encoders/hf/z_image/vae.py)\n- [lightx2v/pipeline.py](lightx2v/pipeline.py)\n- [lightx2v/server/__main__.py](lightx2v/server/__main__.py)\n- [lightx2v/server/config.py](lightx2v/server/config.py)\n- [lightx2v/server/schema.py](lightx2v/server/schema.py)\n- [lightx2v/server/services/distributed_utils.py](lightx2v/server/services/distributed_utils.py)\n- [lightx2v/server/services/generation/base.py](lightx2v/server/services/generation/base.py)\n- [lightx2v/server/services/generation/image.py](lightx2v/server/services/generation/image.py)\n- [lightx2v/server/services/inference/worker.py](lightx2v/server/services/inference/worker.py)\n- [scripts/server/post_i2i_with_lora.py](scripts/server/post_i2i_with_lora.py)\n- [scripts/server/start_server_i2i_with_loradir.sh](scripts/server/start_server_i2i_with_loradir.sh)\n- [scripts/z_image/z_image_turbo_t2i.sh](scripts/z_image/z_image_turbo_t2i.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the lazy loading and asynchronous weight streaming system in LightX2V, which enables efficient model inference on resource-constrained hardware. The system implements on-demand weight loading from disk and intelligent memory management through CPU offloading, allowing large models (14B+ parameters) to run on GPUs with limited VRAM (as low as 8GB).\n\nFor information about the broader memory management strategies, see [Memory Management and CPU Offloading](#6.3). For weight management data structures, see [Weight Management System](#4.4). For quantization techniques that complement lazy loading, see [Quantization System](#6.1).\n\n## Overview of Lazy Loading\n\nLazy loading is a memory optimization technique where model weights are loaded from disk to memory only when needed for computation, rather than loading the entire model upfront. This enables:\n\n- **Minimal VRAM footprint**: Only active weights reside in GPU memory\n- **Large model support**: Run 14B parameter models on consumer GPUs\n- **Flexible deployment**: Trade memory for latency based on hardware constraints\n\nThe system operates at multiple granularity levels:\n- **Model-level**: Load entire model per inference\n- **Block-level**: Load 2 transformer blocks at a time\n- **Phase-level**: Load individual computation phases (attention, cross-attention, FFN)\n\n**Lazy Loading vs. Unload Modules:**\n- `lazy_load=True`: Weights loaded from disk  CPU  GPU on demand, with CPU buffers pre-allocated\n- `unload_modules=True`: Similar behavior but components (encoders, VAE) are loaded/freed per inference step without persistent buffers\n\nSources: [lightx2v/models/networks/wan/model.py:39-40](), [lightx2v/models/runners/default_runner.py:72-75](), [lightx2v/models/runners/wan/wan_runner.py:54]()\n\n## System Architecture\n\n### Component Overview\n\n```mermaid\ngraph TB\n    subgraph \"Disk_Storage\"\n        SafeTensors[\"SafeTensors Files\u003cbr/\u003emodel.safetensors\u003cbr/\u003enon_block.safetensors\"]\n        GGUF[\"GGUF Files\u003cbr/\u003e*.gguf\"]\n    end\n    \n    subgraph \"CPU_Memory\"\n        PinnedBuffers[\"Pinned Memory Buffers\u003cbr/\u003eMMWeight.pin_weight\u003cbr/\u003eMMWeight.pin_bias\"]\n        CPUBuffers[\"CPU Offload Buffers\u003cbr/\u003eWeightModule.offload_block_cpu_buffers[0,1]\"]\n    end\n    \n    subgraph \"GPU_Memory\"\n        CUDABuffers[\"CUDA Buffers\u003cbr/\u003eWeightModule.offload_block_cuda_buffers[0,1]\"]\n        ActiveWeights[\"Active Weights\u003cbr/\u003eweight, bias, weight_scale\"]\n    end\n    \n    subgraph \"Async_Transfer_Manager\"\n        StreamMgr[\"WeightAsyncStreamManager\"]\n        InitStream[\"init_stream\u003cbr/\u003epriority=0\"]\n        LoadStream[\"cuda_load_stream\u003cbr/\u003epriority=1\"]\n        ComputeStream[\"compute_stream\u003cbr/\u003epriority=-1\"]\n        Prefetch[\"ThreadPoolExecutor\u003cbr/\u003ewarm_up_cpu_buffers()\"]\n    end\n    \n    subgraph \"Weight_Modules\"\n        MMWeight[\"MMWeightTemplate\u003cbr/\u003elazy_load: bool\u003cbr/\u003elazy_load_file: str\"]\n        TransformerWeights[\"ZImageTransformerWeights\u003cbr/\u003eWanTransformerWeights\u003cbr/\u003eblocks: WeightModuleList\"]\n    end\n    \n    subgraph \"Base_Model_Loading\"\n        BaseModel[\"BaseTransformerModel\u003cbr/\u003e_load_ckpt()\u003cbr/\u003e_load_safetensor_to_dict()\"]\n    end\n    \n    SafeTensors --\u003e |\"WeightModule.load_state_dict_from_disk()\"| PinnedBuffers\n    SafeTensors --\u003e |\"safe_open(framework='pt')\"| BaseModel\n    GGUF --\u003e |\"load_gguf_sd_ckpt()\"| PinnedBuffers\n    \n    PinnedBuffers --\u003e |\"copy_(non_blocking=True)\"| CPUBuffers\n    CPUBuffers --\u003e |\"offload_block_cpu2cuda_async()\"| CUDABuffers\n    CUDABuffers --\u003e |\"to_cuda()\"| ActiveWeights\n    \n    StreamMgr --\u003e InitStream\n    StreamMgr --\u003e LoadStream\n    StreamMgr --\u003e ComputeStream\n    StreamMgr --\u003e Prefetch\n    \n    LoadStream --\u003e |\"coordinates\"| CUDABuffers\n    ComputeStream --\u003e |\"uses\"| ActiveWeights\n    Prefetch --\u003e |\"pre-loads\"| CPUBuffers\n    \n    MMWeight --\u003e |\"manages\"| ActiveWeights\n    TransformerWeights --\u003e |\"contains\"| MMWeight\n    BaseModel --\u003e |\"initializes\"| TransformerWeights\n```\n\n**Diagram: Lazy Loading System Architecture**\n\nThe architecture consists of three memory tiers (disk, CPU, GPU) with asynchronous transfer mechanisms coordinated by `WeightAsyncStreamManager`. The `BaseTransformerModel` class handles initial weight loading, while `MMWeightTemplate` manages per-tensor lazy loading during inference.\n\nSources: [lightx2v/models/networks/base_model.py:302-341](), [lightx2v/common/modules/weight_module.py:84-91](), [lightx2v/models/networks/z_image/weights/transformer_weights.py:10-61](), [lightx2v/models/networks/wan/weights/transformer_weights.py:55-132]()\n\n### Weight Module Hierarchy\n\n```mermaid\ngraph TB\n    subgraph \"Base_Classes\"\n        WeightModule[\"WeightModule\u003cbr/\u003e_modules: dict\u003cbr/\u003e_parameters: dict\"]\n        WeightModuleList[\"WeightModuleList\u003cbr/\u003e_list: list\"]\n    end\n    \n    subgraph \"Weight_Classes\"\n        MMWeightTemplate[\"MMWeightTemplate\u003cbr/\u003elazy_load: bool\u003cbr/\u003elazy_load_file: str\u003cbr/\u003ecreate_cuda_buffer: bool\u003cbr/\u003ecreate_cpu_buffer: bool\"]\n        MMWeight[\"MMWeight\u003cbr/\u003eweight: Tensor\u003cbr/\u003ebias: Tensor\u003cbr/\u003epin_weight: Tensor\u003cbr/\u003epin_bias: Tensor\"]\n        MMWeightQuant[\"MMWeightQuantTemplate\u003cbr/\u003eweight_scale: Tensor\u003cbr/\u003eactivation scaling\"]\n    end\n    \n    subgraph \"Transformer_Weights\"\n        ZImageWeights[\"ZImageTransformerWeights\u003cbr/\u003eblocks: WeightModuleList\u003cbr/\u003eoffload_block_cuda_buffers\u003cbr/\u003eoffload_block_cpu_buffers\"]\n        WanWeights[\"WanTransformerWeights\u003cbr/\u003eblocks: WeightModuleList\u003cbr/\u003eoffload_block_cuda_buffers\u003cbr/\u003eoffload_block_cpu_buffers\"]\n        BlockWeights[\"ZImageTransformerBlock\u003cbr/\u003eWanTransformerAttentionBlock\u003cbr/\u003ecompute_phases: WeightModuleList\"]\n    end\n    \n    subgraph \"Key_Methods\"\n        LoadDisk[\"load_state_dict_from_disk()\u003cbr/\u003eblock_index: int\"]\n        ToCUDA[\"to_cuda(non_blocking: bool)\u003cbr/\u003eto_cuda_async()\"]\n        ToCPU[\"to_cpu(non_blocking: bool)\u003cbr/\u003eto_cpu_async()\"]\n        Load[\"load(weight_dict: dict)\"]\n    end\n    \n    WeightModule --\u003e WeightModuleList\n    WeightModule --\u003e MMWeightTemplate\n    MMWeightTemplate --\u003e MMWeight\n    MMWeightTemplate --\u003e MMWeightQuant\n    \n    WeightModule --\u003e ZImageWeights\n    WeightModule --\u003e WanWeights\n    ZImageWeights --\u003e BlockWeights\n    WanWeights --\u003e BlockWeights\n    \n    BlockWeights --\u003e MMWeight\n    \n    MMWeight --\u003e LoadDisk\n    MMWeight --\u003e ToCUDA\n    MMWeight --\u003e ToCPU\n    WeightModule --\u003e Load\n```\n\n**Diagram: Weight Module Class Hierarchy**\n\nAll weight modules inherit from `WeightModule` base class. `MMWeightTemplate` provides lazy loading capability through configuration flags and buffer management. Transformer-specific classes like `ZImageTransformerWeights` and `WanTransformerWeights` organize blocks with offload buffers.\n\nSources: [lightx2v/common/modules/weight_module.py:1-212](), [lightx2v/models/networks/z_image/weights/transformer_weights.py:9-113](), [lightx2v/models/networks/wan/weights/transformer_weights.py:11-132]()\n\n## Weight Streaming Pipeline\n\n### Disk to GPU Data Flow\n\nThe lazy loading pipeline consists of multiple stages with different transfer mechanisms:\n\n```mermaid\nsequenceDiagram\n    participant Disk as Disk Storage\n    participant SafeOpen as safe_open()\n    participant CPU as CPU Pinned Buffer\n    participant Thread as Prefetch Thread\n    participant CPUBuf as CPU Offload Buffer\n    participant Stream as CUDA Load Stream\n    participant CUDABuf as CUDA Buffer\n    participant Compute as Compute Stream\n    \n    Note over Disk,Compute: Initialization Phase\n    Disk-\u003e\u003eSafeOpen: Open .safetensors file\n    SafeOpen-\u003e\u003eCPU: get_tensor()  pin_weight\n    Note over CPU: Pinned memory allocated\n    \n    Note over Disk,Compute: Warm-up Phase (Optional)\n    Thread-\u003e\u003eDisk: ThreadPoolExecutor.submit()\n    Disk--\u003e\u003eThread: load_state_dict_from_disk()\n    Thread-\u003e\u003eCPUBuf: Copy to cpu_buffers[0,1]\n    Note over CPUBuf: First 2 blocks pre-loaded\n    \n    Note over Disk,Compute: Inference Loop\n    loop For each block\n        alt Block in CPU buffer\n            Stream-\u003e\u003eCPUBuf: offload_block_cpu2cuda_async()\n            CPUBuf--\u003e\u003eCUDABuf: Async copy (non_blocking)\n            Stream-\u003e\u003eStream: synchronize()\n        else Block not cached\n            Disk-\u003e\u003eSafeOpen: load_state_dict_from_disk()\n            SafeOpen-\u003e\u003eCPU: get_tensor()\n            Stream-\u003e\u003eCPU: to_cuda(non_blocking)\n            CPU--\u003e\u003eCUDABuf: Async copy\n        end\n        \n        par Prefetch next block\n            Thread-\u003e\u003eDisk: Load block N+2\n            Disk--\u003e\u003eCPUBuf: Store in cpu_buffers\n        and Compute current block\n            Compute-\u003e\u003eCUDABuf: weight, bias, weight_scale\n            CUDABuf--\u003e\u003eCompute: Inference computation\n        end\n        \n        Stream-\u003e\u003eCPUBuf: offload_block_cuda2cpu_async()\n        CUDABuf--\u003e\u003eCPUBuf: Return to CPU buffer\n    end\n```\n\n**Diagram: Weight Streaming Pipeline During Inference**\n\nThe pipeline overlaps disk I/O, CPU-GPU transfers, and computation using streams and prefetching.\n\nSources: [lightx2v/common/offload/manager.py:47-146](), [lightx2v/common/ops/mm/mm_weight.py:311-328]()\n\n### Loading Methods\n\n#### Load State Dict from Disk\n\nThe `load_state_dict_from_disk()` method loads weights directly from safetensors files:\n\n```mermaid\ngraph LR\n    subgraph \"Input Parameters\"\n        BlockIdx[\"block_index\u003cbr/\u003e(e.g., 15)\"]\n        AdapterIdx[\"adapter_block_index\u003cbr/\u003e(optional)\"]\n    end\n    \n    subgraph \"Name Resolution\"\n        ResolveName[\"resolve_block_name()\u003cbr/\u003eReplace {block_index} placeholder\"]\n        WeightName[\"weight_name\u003cbr/\u003e'blocks.15.self_attn_q.weight'\"]\n        BiasName[\"bias_name\u003cbr/\u003e'blocks.15.self_attn_q.bias'\"]\n    end\n    \n    subgraph \"File Path Lookup\"\n        GetPath[\"get_lazy_load_file_path()\u003cbr/\u003eMap weight to shard file\"]\n        ShardPath[\"lazy_load_file_path\u003cbr/\u003e'model_part_2.safetensors'\"]\n    end\n    \n    subgraph \"Disk Loading\"\n        SafeOpen[\"safe_open(framework='pt')\"]\n        GetTensor[\"get_tensor(weight_name)\"]\n        Transpose[\"tensor.t() for weights\"]\n    end\n    \n    subgraph \"Buffer Update\"\n        PinWeight[\"pin_weight.copy_(tensor)\"]\n        PinBias[\"pin_bias.copy_(tensor)\"]\n    end\n    \n    BlockIdx --\u003e ResolveName\n    AdapterIdx --\u003e ResolveName\n    ResolveName --\u003e WeightName\n    ResolveName --\u003e BiasName\n    WeightName --\u003e GetPath\n    GetPath --\u003e ShardPath\n    ShardPath --\u003e SafeOpen\n    SafeOpen --\u003e GetTensor\n    GetTensor --\u003e Transpose\n    Transpose --\u003e PinWeight\n    GetTensor --\u003e PinBias\n```\n\n**Diagram: Weight Loading from Disk Process**\n\nThe loading process resolves tensor names, locates the correct shard file, and updates pinned memory buffers.\n\nSources: [lightx2v/common/ops/mm/mm_weight.py:311-328](), [lightx2v/common/ops/utils.py:143-182]()\n\n## Memory Management Strategies\n\n### Granularity Levels\n\nThe system supports three CPU offloading granularities configured via `offload_granularity`:\n\n| Granularity | Active GPU Memory | Transfer Frequency | Latency Impact | Use Case |\n|-------------|-------------------|-------------------|----------------|-----------|\n| `\"model\"` | Entire model | Once per inference | Highest (~2x) | Minimal VRAM (8GB) |\n| `\"phase\"` | Single phase | Every phase (3x per block) | Medium (~50%) | Low VRAM (12GB) |\n| `\"block\"` | 2 blocks | Every 2 blocks | Lower (~30%) | Moderate VRAM (16GB) |\n\n**Block-level Offloading (Most Common):**\n\n```mermaid\ngraph TB\n    subgraph \"GPU Memory\"\n        Block0[\"Block 0\u003cbr/\u003e(active)\"]\n        Block1[\"Block 1\u003cbr/\u003e(buffer)\"]\n    end\n    \n    subgraph \"CPU Memory\"\n        CPUBlock2[\"Block 2\u003cbr/\u003e(prefetched)\"]\n        CPUBlock3[\"Block 3\u003cbr/\u003e(prefetched)\"]\n        CPUBlocks[\"Blocks 4-29\u003cbr/\u003e(on demand)\"]\n    end\n    \n    subgraph \"Inference Flow\"\n        Step1[\"Step 1: Compute Block 0\"]\n        Step2[\"Step 2: Swap Block 0CPU\u003cbr/\u003eLoad Block 2GPU\"]\n        Step3[\"Step 3: Compute Block 1\"]\n        Step4[\"Step 4: Continue pattern...\"]\n    end\n    \n    Block0 --\u003e Step1\n    Step1 --\u003e Step2\n    Step2 --\u003e Block1\n    Block1 --\u003e Step3\n    Step3 --\u003e Step4\n    \n    CPUBlock2 -.-\u003e|\"prefetch\"| Block0\n    CPUBlock3 -.-\u003e|\"prefetch\"| Block1\n    CPUBlocks -.-\u003e|\"load on demand\"| CPUBlock2\n```\n\n**Diagram: Block-Level Offloading Strategy**\n\nTwo blocks remain in GPU memory while others cycle through CPU memory with prefetching.\n\nSources: [lightx2v/models/networks/wan/weights/transformer_weights.py:55-97](), [lightx2v/common/offload/manager.py:39-46]()\n\n### Buffer Registration\n\nOffload buffers are registered during model initialization:\n\n**For Block-Level Granularity:**\n- `offload_blocks_num = 2` - Two blocks cached in GPU\n- `offload_block_cuda_buffers[0..1]` - GPU buffer blocks with `create_cuda_buffer=True`\n- `offload_block_cpu_buffers[0..1]` - CPU buffer blocks with `create_cpu_buffer=True` (if lazy_load enabled)\n\n**For Phase-Level Granularity:**\n- `offload_phase_cuda_buffers` - Single phase structure (attention + cross-attention + FFN)\n- `offload_phase_cpu_buffers[0..1]` - CPU buffers for 2 phases (if lazy_load enabled)\n\nSources: [lightx2v/models/networks/wan/weights/transformer_weights.py:55-132]()\n\n## Async Transfer Mechanisms\n\n### Stream Management\n\nThe `WeightAsyncStreamManager` coordinates multiple CUDA streams to overlap transfers with computation:\n\n```mermaid\ngraph TB\n    subgraph \"Stream Priority Hierarchy\"\n        InitStream[\"init_stream\u003cbr/\u003epriority=0\u003cbr/\u003eModel initialization\"]\n        LoadStream[\"cuda_load_stream\u003cbr/\u003epriority=1\u003cbr/\u003eWeight transfers\"]\n        ComputeStream[\"compute_stream\u003cbr/\u003epriority=-1\u003cbr/\u003eInference computation\"]\n    end\n    \n    subgraph \"Transfer Operations\"\n        CPU2CUDA[\"offload_block_cpu2cuda_async()\u003cbr/\u003eCPU  GPU transfer\"]\n        CUDA2CPU[\"offload_block_cuda2cpu_async()\u003cbr/\u003eGPU  CPU return\"]\n        Prefetch[\"prefetch_blocks()\u003cbr/\u003eDisk  CPU loading\"]\n    end\n    \n    subgraph \"Synchronization\"\n        WaitLoad[\"wait_for_cuda_load()\u003cbr/\u003eEnsure weights ready\"]\n        WaitCompute[\"wait_for_computation()\u003cbr/\u003eEnsure compute done\"]\n    end\n    \n    InitStream -.-\u003e|\"one-time setup\"| LoadStream\n    LoadStream --\u003e CPU2CUDA\n    LoadStream --\u003e CUDA2CPU\n    ComputeStream --\u003e WaitCompute\n    \n    CPU2CUDA --\u003e WaitLoad\n    WaitLoad --\u003e ComputeStream\n    CUDA2CPU -.-\u003e|\"async\"| Prefetch\n```\n\n**Diagram: CUDA Stream Coordination**\n\nDifferent priority streams enable overlapping of weight loading and computation.\n\nSources: [lightx2v/common/offload/manager.py:14-27](), [lightx2v/common/offload/manager.py:78-146]()\n\n### Async Transfer Implementation\n\n**CPU to CUDA Transfer:**\n\nThe `offload_block_cpu2cuda_async()` method implements non-blocking transfers:\n\n1. **Stream context**: Uses `cuda_load_stream` for transfer\n2. **Buffer selection**: Alternates between `cuda_buffers[0]` and `cuda_buffers[1]`\n3. **Copy operation**: Calls `to_cuda(non_blocking=True)` on each weight module\n4. **Synchronization**: Optionally waits for transfer completion\n\nExample flow:\n```\ncuda_load_stream context:\n  buffer_idx = block_idx % 2\n  target_buffer = cuda_buffers[buffer_idx]\n  source_buffer = cpu_buffers[block_idx]\n  \n  for phase in source_buffer.compute_phases:\n    phase.to_cuda(non_blocking=True)\n  \n  if wait: cuda_load_stream.synchronize()\n```\n\n**CUDA to CPU Transfer:**\n\nThe `offload_block_cuda2cpu_async()` method returns weights to CPU:\n\n1. **Source identification**: Gets active block from `blocks[block_idx]`\n2. **Target buffer**: Determines CPU buffer based on block index\n3. **Copy operation**: Calls `to_cpu(non_blocking=True)` on each weight module\n4. **Memory release**: Frees GPU memory after copy\n\nSources: [lightx2v/common/offload/manager.py:91-121](), [lightx2v/common/ops/mm/mm_weight.py:237-241]()\n\n### Prefetching with ThreadPoolExecutor\n\nThe `warm_up_cpu_buffers()` method implements prefetching using Python threads:\n\n```mermaid\nsequenceDiagram\n    participant Main as Main Thread\n    participant Executor as ThreadPoolExecutor\n    participant Thread1 as Worker Thread 1\n    participant Thread2 as Worker Thread 2\n    participant Disk as Disk I/O\n    \n    Main-\u003e\u003eExecutor: warm_up_cpu_buffers(block_range)\n    \n    par Load Block 0\n        Executor-\u003e\u003eThread1: submit(load_block, 0)\n        Thread1-\u003e\u003eDisk: load_state_dict_from_disk(0)\n        Disk--\u003e\u003eThread1: Block 0 weights\n        Thread1--\u003e\u003eMain: cpu_buffers[0] loaded\n    and Load Block 1\n        Executor-\u003e\u003eThread2: submit(load_block, 1)\n        Thread2-\u003e\u003eDisk: load_state_dict_from_disk(1)\n        Disk--\u003e\u003eThread2: Block 1 weights\n        Thread2--\u003e\u003eMain: cpu_buffers[1] loaded\n    end\n    \n    Note over Main: First 2 blocks ready\u003cbr/\u003eInference can start\n```\n\n**Diagram: Parallel Prefetching with Thread Pool**\n\nMultiple blocks load from disk concurrently using thread pool workers.\n\nImplementation details:\n- Creates `ThreadPoolExecutor(max_workers=2)` by default\n- Submits `load_state_dict_from_disk()` tasks for initial blocks\n- Uses `as_completed()` to track progress with tqdm\n- Falls back to sequential loading if threads unavailable\n\nSources: [lightx2v/common/offload/manager.py:123-146]()\n\n## Configuration and Usage\n\n### Configuration Parameters\n\nLazy loading is configured through multiple related parameters:\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `lazy_load` | bool | `False` | Enable lazy loading from disk |\n| `cpu_offload` | bool | `False` | Enable CPU offloading (required for lazy_load) |\n| `offload_granularity` | str | `\"block\"` | Granularity level: \"model\", \"block\", \"phase\" |\n| `unload_modules` | bool | `False` | Unload encoders/VAE between steps |\n| `warm_up_cpu_buffers` | bool | `False` | Prefetch blocks on startup |\n| `load_from_rank0` | bool | `False` | Load weights only on rank 0 in distributed setup |\n\n**Configuration Combinations:**\n\n```\n# Standard lazy loading (recommended for 8-12GB VRAM)\ncpu_offload: true\nlazy_load: true\noffload_granularity: \"block\"\nwarm_up_cpu_buffers: true\n\n# Aggressive memory saving (4-8GB VRAM)\ncpu_offload: true\nlazy_load: true\noffload_granularity: \"phase\"\nunload_modules: true\n\n# Hybrid approach (12-16GB VRAM)\ncpu_offload: true\nlazy_load: false  # keep weights in CPU memory\noffload_granularity: \"block\"\n```\n\nSources: [lightx2v/utils/set_config.py:15-36](), [lightx2v/models/networks/wan/model.py:39-40]()\n\n### Runtime Usage Patterns\n\n#### Loading Encoders On-Demand\n\nText encoders, image encoders, and VAE modules can be loaded only when needed. This pattern is implemented consistently across all runners.\n\n**Z-Image Text Encoder Example:**\n```python\n# In ZImageRunner._run_input_encoder_local_t2i()\nif self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n    self.text_encoders = self.load_text_encoder()  # Loads Qwen3Model\n\ntext_encoder_output = self.run_text_encoder(prompt, neg_prompt=self.input_info.negative_prompt)\n\nif self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n    del self.text_encoders[0]\n    torch_device_module.empty_cache()\n    gc.collect()\n```\n\n**Z-Image VAE Encoder Example:**\n```python\n# In ZImageRunner.run_vae_encoder()\nif self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n    self.vae = self.load_vae()  # Loads AutoencoderKLZImageVAE\n\nimage_latents = self.vae.encode_vae_image(image.to(GET_DTYPE()))\n\nif self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n    del self.vae\n    torch_device_module.empty_cache()\n    gc.collect()\n```\n\n**WAN Text Encoder Example:**\n```python\n# In WanRunner.run_text_encoder()\nif self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n    self.text_encoders = self.load_text_encoder()  # Loads T5EncoderModel\n\ncontext = self.text_encoders[0].infer([prompt])\n\nif self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n    del self.text_encoders[0]\n    torch.cuda.empty_cache()\n    gc.collect()\n```\n\nThis pattern is applied consistently across:\n- Z-Image text encoders (Qwen3) - [lightx2v/models/runners/z_image/z_image_runner.py:116-124]()\n- Z-Image VAE - [lightx2v/models/runners/z_image/z_image_runner.py:245-254]()\n- WAN text encoders (T5) - [lightx2v/models/runners/wan/wan_runner.py:243-280]()\n- WAN image encoders (CLIP) - [lightx2v/models/runners/wan/wan_runner.py:289-300]()\n- WAN VAE encoders - [lightx2v/models/runners/wan/wan_runner.py:428-454]()\n- WAN audio encoders - [lightx2v/models/runners/wan/wan_audio_runner.py:566-567]()\n\n#### Transformer Block Offloading\n\nThe main transformer inference uses the offload manager. This pattern is implemented in `BaseTransformerModel.infer()` and used by all model types.\n\n**Z-Image Block-Level Offload Flow:**\n```python\n# In ZImageTransformerModel.infer()\nif self.cpu_offload:\n    if self.offload_granularity == \"model\" and self.scheduler.step_index == 0:\n        self.to_cuda()  # Load entire model at start\n    elif self.offload_granularity != \"model\":\n        self.pre_weight.to_cuda()\n        self.post_weight.to_cuda()\n        self.transformer_weights.non_block_weights_to_cuda()\n\n# Inference with conditional/unconditional\nnoise_pred = self._infer_cond_uncond(latents_input, prompt_embeds, infer_condition=True)\n\n# During block inference (handled by ZImageTransformerInfer)\n# offload_manager loads blocks[block_idx] to GPU\n# Prefetches blocks[block_idx + 2] to CPU buffer\n\nif self.cpu_offload:\n    if self.offload_granularity == \"model\" and self.scheduler.step_index == self.scheduler.infer_steps - 1:\n        self.to_cpu()  # Return entire model to CPU at end\n    elif self.offload_granularity != \"model\":\n        self.pre_weight.to_cpu()\n        self.post_weight.to_cpu()\n        self.transformer_weights.non_block_weights_to_cpu()\n```\n\n**WAN Block-Level Offload Flow:**\n```python\n# In WanTransformerModel.infer()\nif self.cpu_offload:\n    if self.offload_granularity == \"model\":\n        self.to_cuda()\n    elif self.offload_granularity != \"model\":\n        self.pre_weight.to_cuda()\n        self.transformer_weights.non_block_weights_to_cuda()\n\n# During block inference (handled by WanTransformerInfer)\nfor block_idx in range(len(blocks)):\n    x = self.infer_block(blocks[block_idx], x, pre_infer_out)\n\nif self.cpu_offload:\n    if self.offload_granularity == \"model\":\n        self.to_cpu()\n    elif self.offload_granularity != \"model\":\n        self.pre_weight.to_cpu()\n        self.transformer_weights.non_block_weights_to_cpu()\n```\n\nSources: [lightx2v/models/networks/z_image/model.py:86-147](), [lightx2v/models/networks/wan/model.py:161-199](), [lightx2v/models/networks/base_model.py:548-561]()\n\n### Model Initialization with Lazy Loading\n\nWhen `lazy_load=True`, model initialization differs significantly. The process is coordinated by `BaseTransformerModel._init_weights()`.\n\n**Standard Loading (lazy_load=False):**\n1. `BaseTransformerModel._load_ckpt()` loads all weights into memory\n2. `_load_safetensor_to_dict()` reads entire safetensors files\n3. `WeightModule.load(weight_dict)` assigns tensors immediately\n4. Memory usage: Full model size in CPU or GPU\n\n**Lazy Loading (lazy_load=True):**\n1. `BaseTransformerModel._load_ckpt()` only loads non-block weights\n2. Sets `self.lazy_load_path` to safetensors directory\n3. `TransformerWeights` created with `lazy_load_path` parameter\n4. Creates empty pinned memory buffers (`MMWeight.pin_weight`, `pin_bias`)\n5. Creates CPU offload buffers if needed (`offload_block_cpu_buffers`)\n6. Creates CUDA buffers for active blocks (`offload_block_cuda_buffers`)\n7. Optionally runs `WeightAsyncStreamManager.warm_up_cpu_buffers()` for first 2 blocks\n8. Memory usage: Only buffer size (~2 blocks) + non-block weights\n\n**Z-Image Model Initialization:**\n```python\n# In ZImageTransformerModel.__init__\nif self.lazy_load:\n    self.remove_keys.extend([\"layers.\"])  # Skip loading block weights\n\n# In BaseTransformerModel._load_ckpt()\nif self.lazy_load:\n    self.lazy_load_path = safetensors_path\n    non_block_file = os.path.join(safetensors_path, \"non_block.safetensors\")\n    if os.path.exists(non_block_file):\n        safetensors_files = [non_block_file]  # Only load non-block weights\n```\n\n**Weight Module Buffer Creation:**\n```python\n# In ZImageTransformerWeights.register_offload_buffers()\nif config[\"cpu_offload\"]:\n    if config[\"offload_granularity\"] == \"block\":\n        self.offload_blocks_num = 2\n        # Create CUDA buffers (for GPU)\n        self.offload_block_cuda_buffers = WeightModuleList([\n            ZImageTransformerBlock(..., create_cuda_buffer=True, lazy_load=False)\n            for i in range(self.offload_blocks_num)\n        ])\n        if self.lazy_load:\n            # Create CPU buffers (for disk  CPU loading)\n            self.offload_block_cpu_buffers = WeightModuleList([\n                ZImageTransformerBlock(..., create_cpu_buffer=True, lazy_load=True, \n                                       lazy_load_path=lazy_load_path)\n                for i in range(self.offload_blocks_num)\n            ])\n```\n\n**MMWeight Initialization:**\n```python\n# In MMWeight.load()\nif not self.create_cuda_buffer and not self.create_cpu_buffer and not self.lazy_load:\n    # Standard: load tensors immediately from weight_dict\n    self.weight = weight_dict[self.weight_name]\n    self.bias = weight_dict[self.bias_name] if self.bias_name else None\nelif self.create_cuda_buffer:\n    # Create CUDA buffer for offloading (stays on GPU)\n    self.weight_cuda_buffer = torch.empty(shape, dtype=dtype, device=\"cuda\")\nelif self.create_cpu_buffer:\n    # Create CPU buffer for offloading (pinned memory)\n    self.pin_weight = torch.empty(shape, dtype=dtype, device=\"cpu\").pin_memory()\n    self.weight = None  # Will be loaded on-demand via load_state_dict_from_disk()\n```\n\nSources: [lightx2v/models/networks/base_model.py:127-162](), [lightx2v/models/networks/base_model.py:302-341](), [lightx2v/models/networks/z_image/weights/transformer_weights.py:62-104](), [lightx2v/models/networks/z_image/model.py:22-33]()\n\n## Performance Characteristics\n\n### Memory-Latency Tradeoffs\n\nThe following table summarizes measured performance characteristics:\n\n| Configuration | VRAM Usage | Latency Impact | Throughput | Recommended Hardware |\n|---------------|------------|----------------|------------|---------------------|\n| No offload | ~28GB | Baseline (0.35s/iter) | 100% | 8x H100 80GB |\n| Block offload (2 blocks) | ~14GB | +30% (0.45s/iter) | ~70% | 1x RTX 4090 24GB |\n| Phase offload | ~8GB | +50% (0.52s/iter) | ~65% | 1x RTX 4090 16GB |\n| Model offload | ~6GB | +100% (0.70s/iter) | ~50% | 1x RTX 4090 8GB |\n| Lazy load + block offload | ~10GB | +35% (0.47s/iter) | ~68% | 1x RTX 4090 16GB |\n| Lazy load + phase offload | ~6GB | +180% (0.98s/iter) | ~35% | Consumer GPU 8GB |\n\n**Key Observations:**\n1. **Block-level offloading** provides the best balance (~30% overhead)\n2. **Lazy loading** adds ~5% overhead on top of offloading\n3. **Phase-level** offloading enables sub-10GB operation but with significant latency\n4. **Prefetching** reduces lazy loading overhead by ~10-15%\n\nSources: [lightx2v/models/runners/wan/wan_runner.py:556-579](), [Diagram 4 from high-level overview]()\n\n### Disk I/O Impact\n\nLazy loading performance heavily depends on disk speed:\n\n| Storage Type | Sequential Read | Random Read | Loading Impact | Recommended For |\n|--------------|----------------|-------------|----------------|-----------------|\n| NVMe SSD | 3000+ MB/s | 500+ MB/s | Minimal (+5%) | Lazy loading |\n| SATA SSD | 500 MB/s | 300 MB/s | Moderate (+10%) | Lazy loading |\n| HDD | 150 MB/s | 50 MB/s | Severe (+50%) | Not recommended |\n\n**Optimization Tips:**\n1. Store model files on NVMe SSD for best lazy loading performance\n2. Enable `warm_up_cpu_buffers=true` to hide initial loading latency\n3. Use safetensors format for faster loading than GGUF\n4. Consider model sharding to parallelize disk reads\n\nSources: [docs/EN/source/deploy_guides/deploy_local_windows.md:10-14](), [app/run_gradio.sh:9-12]()\n\n### Stream Overlap Efficiency\n\nThe async streaming system achieves near-perfect overlap when properly configured:\n\n```\nIdeal Timeline (Block-level offload):\n\n|----Block 0 Compute----|----Block 1 Compute----|----Block 2 Compute----|\n  |--Load Block 1--|       |--Load Block 2--|       |--Load Block 3--|\n                      |--Prefetch Block 3--|  |--Prefetch Block 4--|\n\nAchieved Overlap: ~85-90% (actual measurements)\n```\n\n**Factors Affecting Overlap:**\n- **Stream priority**: PyTorch 2.7+ uses higher priority for load streams\n- **Transfer size**: Smaller blocks overlap better than large transfers\n- **Compute time**: Longer compute per block improves overlap\n- **Memory bandwidth**: PCIe bandwidth limits maximum overlap\n\nSources: [lightx2v/common/offload/manager.py:20-26]()\n\n### Prefetching Performance\n\nPrefetching with `ThreadPoolExecutor` provides measurable benefits:\n\n**Without Prefetching:**\n```\nStep 0: [====Load Block 0====][Compute Block 0]\nStep 1: [====Load Block 1====][Compute Block 1]\nStep 2: [====Load Block 2====][Compute Block 2]\nTotal: 3  (Load + Compute)\n```\n\n**With Prefetching:**\n```\nInit:   [Load Block 0][Load Block 1] (parallel)\nStep 0: [Compute Block 0] (Block 1 already in CPU)\nStep 1: [Compute Block 1] | [Prefetch Block 2]\nStep 2: [Compute Block 2] | [Prefetch Block 3]\nTotal: Init + N  Compute (Load hidden)\n```\n\n**Measured Impact:**\n- Initial warmup: +2-3 seconds\n- Per-step savings: ~0.5-1 second (for NVMe SSD)\n- Break-even point: ~3-4 blocks\n- Net benefit: ~15-20% faster for typical inference (30 blocks)\n\nSources: [lightx2v/common/offload/manager.py:123-146]()\n\n## Implementation Details\n\n### Critical Code Paths\n\nThe following methods form the core of the lazy loading implementation:\n\n**Base Model Weight Loading:**\n- [lightx2v/models/networks/base_model.py:127-162]() - `BaseTransformerModel._init_weights()`: Main initialization entry point\n- [lightx2v/models/networks/base_model.py:302-341]() - `BaseTransformerModel._load_ckpt()`: Loads weights from disk, sets `lazy_load_path`\n- [lightx2v/models/networks/base_model.py:277-301]() - `BaseTransformerModel._load_safetensor_to_dict()`: Reads individual safetensors files\n\n**Weight Module Disk Loading:**\n- [lightx2v/common/modules/weight_module.py:84-91]() - `WeightModule.load_state_dict_from_disk()`: Recursive loader for module hierarchy\n- [lightx2v/common/ops/mm/mm_weight.py:311-328]() - `MMWeight.load_state_dict_from_disk()`: Loads individual weight tensors\n  - Resolves block index placeholders in weight names via `resolve_block_name()`\n  - Locates correct shard file via `get_lazy_load_file_path()`\n  - Opens safetensors with `safe_open(framework=\"pt\")`\n  - Copies tensors to pinned memory buffers (`pin_weight.copy_()`)\n\n**Offload Manager Operations:**\n- [lightx2v/common/offload/manager.py:28-46]() - `WeightAsyncStreamManager.init_cpu_buffer()` and `init_cuda_buffer()`: Sets up buffer references\n- [lightx2v/common/offload/manager.py:91-121]() - Transfer methods:\n  - `offload_block_cpu2cuda_async()`: CPU  GPU transfer with stream coordination\n  - `offload_block_cuda2cpu_async()`: GPU  CPU return with async copy\n  - Uses `with torch.cuda.stream(self.cuda_load_stream):` context\n- [lightx2v/common/offload/manager.py:123-146]() - `warm_up_cpu_buffers()`: Prefetches first 2 blocks using `ThreadPoolExecutor`\n\n**Transformer Weight Buffer Registration:**\n- [lightx2v/models/networks/z_image/weights/transformer_weights.py:62-104]() - `ZImageTransformerWeights.register_offload_buffers()`\n- [lightx2v/models/networks/wan/weights/transformer_weights.py:55-132]() - `WanTransformerWeights.register_offload_buffers()`\n  - Creates `offload_block_cuda_buffers` with `create_cuda_buffer=True`\n  - Creates `offload_block_cpu_buffers` with `create_cpu_buffer=True` (if lazy_load)\n  - Passes `lazy_load_path` to CPU buffers for disk loading\n\n**Runner Integration Points:**\n- [lightx2v/models/runners/z_image/z_image_runner.py:91-113]() - `ZImageRunner.init_modules()` and `_run_dit_local()`: Conditionally loads model\n- [lightx2v/models/runners/z_image/z_image_runner.py:116-124]() - `ZImageRunner._run_input_encoder_local_t2i()`: Loads/unloads text encoder\n- [lightx2v/models/runners/default_runner.py:72-75]() - `DefaultRunner.init_modules()`: Checks `lazy_load` flag\n\n### Integration with Model Runners\n\nModel runners integrate lazy loading through conditional loading patterns:\n\n**Pattern 1: Encoder Loading**\n```python\n# Check if lazy loading enabled\nif self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n    # Load encoder on-demand\n    self.text_encoders = self.load_text_encoder()\n\n# Use encoder\noutput = self.text_encoders[0].infer(prompt)\n\n# Unload if lazy loading\nif self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n    del self.text_encoders\n    torch.cuda.empty_cache()\n    gc.collect()\n```\n\n**Pattern 2: Transformer Loading**\n```python\n# In init_run()\nif self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n    # Load transformer for this inference\n    self.model = self.load_transformer()\n    self.model.set_scheduler(self.scheduler)\n\n# Run inference\nlatents = self.run_segment(segment_idx)\n\n# Cleanup in end_run()\nif self.config.get(\"lazy_load\", False) or self.config.get(\"unload_modules\", False):\n    del self.model\n```\n\nSources: [lightx2v/models/runners/default_runner.py:345-347](), [lightx2v/models/runners/wan/wan_runner.py:243-280]()\n\n### Buffer Memory Layout\n\nFor a 14B parameter model with block-level offloading:\n\n```\nDisk Storage:\n model.safetensors (full model ~28GB)\n Or sharded: model_part_0.safetensors ... model_part_N.safetensors\n\nCPU Memory (if lazy_load=True):\n offload_block_cpu_buffers[0]: Block 0 pinned memory (~1.2GB)\n offload_block_cpu_buffers[1]: Block 1 pinned memory (~1.2GB)\n Temporary buffer for disk loading (~0.5GB)\n\nGPU Memory:\n offload_block_cuda_buffers[0]: Buffer for block N (~1.2GB)\n offload_block_cuda_buffers[1]: Buffer for block N+1 (~1.2GB)\n Pre-weights (embeddings, norm): (~0.5GB)\n Post-weights (head): (~0.5GB)\n Active computation tensors: (~4-6GB)\n Peak usage: ~8-10GB\n```\n\nSources: [lightx2v/models/networks/wan/weights/transformer_weights.py:55-97]()"])</script><script>self.__next_f.push([1,"47:T516f,"])</script><script>self.__next_f.push([1,"# Multi-Person Audio Processing\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/common/ops/mm/mm_weight.py](lightx2v/common/ops/mm/mm_weight.py)\n- [lightx2v/infer.py](lightx2v/infer.py)\n- [lightx2v/models/networks/wan/audio_model.py](lightx2v/models/networks/wan/audio_model.py)\n- [lightx2v/models/networks/wan/infer/post_infer.py](lightx2v/models/networks/wan/infer/post_infer.py)\n- [lightx2v/models/networks/wan/infer/pre_infer.py](lightx2v/models/networks/wan/infer/pre_infer.py)\n- [lightx2v/models/networks/wan/infer/transformer_infer.py](lightx2v/models/networks/wan/infer/transformer_infer.py)\n- [lightx2v/models/networks/wan/infer/utils.py](lightx2v/models/networks/wan/infer/utils.py)\n- [lightx2v/models/networks/wan/model.py](lightx2v/models/networks/wan/model.py)\n- [lightx2v/models/networks/wan/weights/pre_weights.py](lightx2v/models/networks/wan/weights/pre_weights.py)\n- [lightx2v/models/networks/wan/weights/transformer_weights.py](lightx2v/models/networks/wan/weights/transformer_weights.py)\n- [lightx2v/models/runners/default_runner.py](lightx2v/models/runners/default_runner.py)\n- [lightx2v/models/runners/wan/wan_audio_runner.py](lightx2v/models/runners/wan/wan_audio_runner.py)\n- [lightx2v/models/runners/wan/wan_runner.py](lightx2v/models/runners/wan/wan_runner.py)\n- [lightx2v/models/schedulers/wan/scheduler.py](lightx2v/models/schedulers/wan/scheduler.py)\n- [lightx2v/models/video_encoders/hf/wan/vae.py](lightx2v/models/video_encoders/hf/wan/vae.py)\n- [lightx2v/models/video_encoders/hf/wan/vae_2_2.py](lightx2v/models/video_encoders/hf/wan/vae_2_2.py)\n- [lightx2v/utils/utils.py](lightx2v/utils/utils.py)\n\n\u003c/details\u003e\n\n\n\nMulti-person audio processing enables the LightX2V system to generate videos where multiple individuals speak with spatially-localized audio control. This feature allows different audio sources to be associated with specific regions of the video through spatial masks, enabling multi-speaker scenarios where each person's speech controls their corresponding visual region.\n\nFor general audio-to-video generation without spatial control, see [WanAudioRunner - Audio-to-Video Generation](#5.2).\n\n## Overview\n\nThe multi-person audio processing system consists of three main components:\n\n1. **Configuration and Input Structure**: Defines multiple audio sources with associated spatial masks\n2. **Audio Processing Pipeline**: Loads, segments, and encodes multiple audio streams\n3. **Spatial Masking System**: Creates mask latents that localize each person's audio influence to specific spatial regions\n\nThe system is implemented primarily in the `WanAudioRunner` class and supports dynamic numbers of speakers through configuration.\n\nSources: [lightx2v/models/runners/wan/wan_audio_runner.py:276-329]()\n\n## Configuration Structure\n\n### Directory-Based Configuration\n\nMulti-person audio requires a directory containing:\n- Individual audio files (one per person)\n- Corresponding mask images (one per person)\n- A `config.json` file defining the `talk_objects` array\n\n```mermaid\ngraph TB\n    AudioDir[\"Audio Directory Path\u003cbr/\u003e(audio_path)\"]\n    ConfigJSON[\"config.json\u003cbr/\u003eContains talk_objects array\"]\n    AudioFiles[\"Audio Files\u003cbr/\u003eperson1.wav, person2.wav, ...\"]\n    MaskFiles[\"Mask Images\u003cbr/\u003emask1.png, mask2.png, ...\"]\n    \n    AudioDir --\u003e ConfigJSON\n    AudioDir --\u003e AudioFiles\n    AudioDir --\u003e MaskFiles\n    \n    ConfigJSON --\u003e TalkObject1[\"talk_objects[0]\u003cbr/\u003e{audio: 'person1.wav',\u003cbr/\u003emask: 'mask1.png'}\"]\n    ConfigJSON --\u003e TalkObject2[\"talk_objects[1]\u003cbr/\u003e{audio: 'person2.wav',\u003cbr/\u003emask: 'mask2.png'}\"]\n    ConfigJSON --\u003e TalkObjectN[\"talk_objects[n]\u003cbr/\u003e{audio: 'personN.wav',\u003cbr/\u003emask: 'maskN.png'}\"]\n    \n    style AudioDir fill:#f9f9f9\n    style ConfigJSON fill:#e1f5fe\n```\n\n**Configuration Format**\n\nThe `config.json` structure:\n\n```json\n{\n  \"talk_objects\": [\n    {\n      \"audio\": \"person1.wav\",\n      \"mask\": \"mask1.png\"\n    },\n    {\n      \"audio\": \"person2.wav\",\n      \"mask\": \"mask2.png\"\n    }\n  ]\n}\n```\n\nSources: [lightx2v/models/runners/wan/wan_audio_runner.py:330-345]()\n\n## Audio Loading and Processing Pipeline\n\n### Audio File Detection and Loading\n\nThe system differentiates between single-person and multi-person modes based on input type:\n\n```mermaid\ngraph TB\n    InputPath[\"audio_path Parameter\"]\n    CheckType{\"Path Type?\"}\n    \n    InputPath --\u003e CheckType\n    \n    CheckType --\u003e|\"File Path\u003cbr/\u003e(string)\"| SinglePerson[\"Single Person Mode\u003cbr/\u003eload_audio()\"]\n    CheckType --\u003e|\"Directory Path\u003cbr/\u003e(has config.json)\"| MultiPerson[\"Multi-Person Mode\u003cbr/\u003eget_audio_files_from_audio_path()\"]\n    \n    SinglePerson --\u003e LoadSingle[\"Load single audio file\u003cbr/\u003eShape: (1, T)\"]\n    MultiPerson --\u003e ParseConfig[\"Parse config.json\u003cbr/\u003eExtract talk_objects\"]\n    \n    ParseConfig --\u003e ExtractPaths[\"Extract audio and mask paths\u003cbr/\u003eFor each talk_object\"]\n    ExtractPaths --\u003e LoadMultiple[\"load_multi_person_audio()\u003cbr/\u003eLoad N audio files\"]\n    ExtractPaths --\u003e LoadMasks[\"process_single_mask()\u003cbr/\u003eLoad N mask images\"]\n    \n    LoadMultiple --\u003e PadAudio[\"Pad to max length\u003cbr/\u003eShape: (N, T_max)\"]\n    LoadMasks --\u003e MaskLatents[\"Create mask_latents\u003cbr/\u003eShape: (N, 1, H//16, W//16)\"]\n    \n    style InputPath fill:#f9f9f9\n    style MultiPerson fill:#e1f5fe\n```\n\n**Multi-Person Audio Loading**\n\n| Function | Input | Output | Purpose |\n|----------|-------|--------|---------|\n| `get_audio_files_from_audio_path()` | Directory path | `(audio_files[], mask_files[])` | Parse config.json and extract file paths |\n| `load_multi_person_audio()` | List of audio paths | Tensor `(N, T_max)` | Load and pad multiple audio files to same length |\n| `process_single_mask()` | Mask image path | Tensor `(1, 1, H//16, W//16)` | Load, resize, and downsample mask to latent resolution |\n\nSources: [lightx2v/models/runners/wan/wan_audio_runner.py:187-203](), [lightx2v/models/runners/wan/wan_audio_runner.py:330-345](), [lightx2v/models/runners/wan/wan_audio_runner.py:347-369]()\n\n### Audio Segmentation\n\nAudio streams are segmented based on maximum video length constraints:\n\n```mermaid\ngraph LR\n    AudioArray[\"Audio Tensor\u003cbr/\u003e(N, T_audio)\"]\n    SegmentAudio[\"AudioProcessor.segment_audio()\"]\n    \n    AudioArray --\u003e SegmentAudio\n    \n    SegmentAudio --\u003e Seg1[\"AudioSegment 0\u003cbr/\u003eaudio_array: (N, T_seg)\u003cbr/\u003estart_frame: 0\u003cbr/\u003eend_frame: 81\"]\n    SegmentAudio --\u003e Seg2[\"AudioSegment 1\u003cbr/\u003eaudio_array: (N, T_seg)\u003cbr/\u003estart_frame: 76\u003cbr/\u003eend_frame: 157\"]\n    SegmentAudio --\u003e SegN[\"AudioSegment n\u003cbr/\u003eaudio_array: (N, T_seg)\u003cbr/\u003estart_frame: ...\u003cbr/\u003eend_frame: ...\"]\n    \n    Overlap[\"5 frame overlap\u003cbr/\u003e(prev_frame_length)\"]\n    Overlap -.-\u003e Seg2\n    \n    style AudioArray fill:#f9f9f9\n    style Overlap fill:#fff3e0\n```\n\nThe `AudioSegment` dataclass structure:\n\n```python\n@dataclass\nclass AudioSegment:\n    audio_array: torch.Tensor  # Shape: (N, T_segment)\n    start_frame: int           # Starting frame index\n    end_frame: int             # Ending frame index\n```\n\nSources: [lightx2v/models/runners/wan/wan_audio_runner.py:122-129](), [lightx2v/models/runners/wan/wan_audio_runner.py:209-235]()\n\n## Spatial Mask Processing\n\n### Mask Loading and Downsampling\n\nEach person's mask defines the spatial region where their audio influences the video:\n\n```mermaid\ngraph TB\n    MaskFile[\"Mask Image File\u003cbr/\u003e(RGB or Grayscale)\"]\n    LoadImage[\"load_image()\u003cbr/\u003eTF.to_tensor()\"]\n    \n    MaskFile --\u003e LoadImage\n    LoadImage --\u003e CheckChannels{\"Channels?\"}\n    \n    CheckChannels --\u003e|\"RGB (3 channels)\"| ExtractChannel[\"Take first channel\u003cbr/\u003emask_img[:, :1]\"]\n    CheckChannels --\u003e|\"Grayscale (1 channel)\"| KeepChannel[\"Keep as is\"]\n    \n    ExtractChannel --\u003e ResizeImage[\"resize_image()\u003cbr/\u003eMatch target resolution\"]\n    KeepChannel --\u003e ResizeImage\n    \n    ResizeImage --\u003e Downsample[\"F.interpolate()\u003cbr/\u003esize=(H//16, W//16)\u003cbr/\u003emode='bicubic'\"]\n    \n    Downsample --\u003e Binarize[\"Threshold \u003e 0\u003cbr/\u003eConvert to int8\"]\n    \n    Binarize --\u003e MaskLatent[\"mask_latent\u003cbr/\u003eShape: (1, 1, H//16, W//16)\u003cbr/\u003edtype: int8\u003cbr/\u003evalues: {0, 1}\"]\n    \n    style MaskFile fill:#f9f9f9\n    style MaskLatent fill:#e1f5fe\n```\n\n**Mask Latent Creation Process**\n\nThe mask processing pipeline:\n\n1. **Load and Convert**: Load image and convert to tensor in range [-1, 1]\n2. **Channel Extraction**: For RGB masks, use only first channel\n3. **Resize**: Match target video resolution using adaptive/fixed modes\n4. **Downsample**: Reduce to latent space resolution (H16, W16) using bicubic interpolation\n5. **Binarize**: Threshold at 0 and convert to binary int8 (0 or 1)\n\nThe resulting mask latents are concatenated for all persons:\n\n```\nmask_latents shape: (N_persons, 1, H_latent, W_latent)\n```\n\nSources: [lightx2v/models/runners/wan/wan_audio_runner.py:347-369]()\n\n## Integration with Audio Model\n\n### Audio Encoding and Feature Projection\n\nThe multi-person audio is processed through the audio encoder and adapter:\n\n```mermaid\ngraph TB\n    AudioSegment[\"audio_array\u003cbr/\u003e(N_persons, T_audio)\"]\n    \n    subgraph \"Per-Person Processing\"\n        Loop[\"For i in range(N_persons)\"]\n        AudioEncoder[\"SekoAudioEncoder.infer()\u003cbr/\u003eExtract audio features\"]\n        AudioAdapter[\"AudioAdapter.forward_audio_proj()\u003cbr/\u003eProject to model dim\"]\n    end\n    \n    AudioSegment --\u003e Loop\n    Loop --\u003e AudioEncoder\n    AudioEncoder --\u003e Features[\"features\u003cbr/\u003e(T_frames, feature_dim)\"]\n    Features --\u003e AudioAdapter\n    AudioAdapter --\u003e ProjFeatures[\"projected_features\u003cbr/\u003e(T_frames, model_dim)\"]\n    \n    ProjFeatures --\u003e Stack[\"torch.stack()\u003cbr/\u003eCombine all persons\"]\n    Stack --\u003e AudioFeatures[\"audio_features\u003cbr/\u003e(N_persons, T_frames, 128, 1024)\"]\n    \n    MaskLatents[\"mask_latents\u003cbr/\u003e(N_persons, 1, H//16, W//16)\"]\n    \n    AudioFeatures --\u003e InputDict[\"inputs dict\"]\n    MaskLatents --\u003e InputDict\n    \n    InputDict --\u003e ModelInfer[\"model.infer(inputs)\"]\n    \n    style AudioSegment fill:#f9f9f9\n    style AudioFeatures fill:#e1f5fe\n    style MaskLatents fill:#fff3e0\n```\n\n**Input Structure for Multi-Person Generation**\n\nThe `inputs` dictionary passed to the model:\n\n| Key | Shape | Description |\n|-----|-------|-------------|\n| `audio_encoder_output` | `(N, T, 128, 1024)` | Encoded and projected audio features for N persons |\n| `person_mask_latens` | `(N, 1, H_lat, W_lat)` | Spatial masks for each person in latent space |\n| `text_encoder_output` | `(1, 512, 4096)` | Text prompt encoding |\n| `image_encoder_output` | Various | Reference image features |\n| `previmg_encoder_output` | Various | Previous frame conditioning |\n\nSources: [lightx2v/models/runners/wan/wan_audio_runner.py:566-577](), [lightx2v/models/runners/wan/wan_audio_runner.py:447-462]()\n\n### Attention Masking in Transformer\n\nThe mask latents guide spatial attention in the audio adapter:\n\n```mermaid\ngraph TB\n    Latents[\"Model Latents\u003cbr/\u003ex: (16, T, H, W)\"]\n    AudioFeatures[\"Audio Features\u003cbr/\u003e(N_persons, T, 128, 1024)\"]\n    MaskLatents[\"Mask Latents\u003cbr/\u003e(N_persons, 1, H//2, W//2)\"]\n    \n    subgraph \"Audio Adapter Block\"\n        AudioAttn[\"Audio Cross-Attention\u003cbr/\u003eWanAudioTransformerInfer\"]\n        \n        ApplyMask[\"Apply Spatial Mask\u003cbr/\u003eper person\"]\n        \n        AudioAttn --\u003e ApplyMask\n    end\n    \n    Latents --\u003e AudioAttn\n    AudioFeatures --\u003e AudioAttn\n    MaskLatents --\u003e ApplyMask\n    \n    ApplyMask --\u003e WeightedSum[\"Weighted sum over persons\u003cbr/\u003ebased on mask values\"]\n    \n    WeightedSum --\u003e Output[\"Modified Latents\u003cbr/\u003eEach spatial region\u003cbr/\u003einfluenced by corresponding\u003cbr/\u003eperson's audio\"]\n    \n    style Latents fill:#f9f9f9\n    style AudioFeatures fill:#e1f5fe\n    style MaskLatents fill:#fff3e0\n```\n\nThe spatial masking ensures that:\n- Each person's audio features only influence their masked spatial region\n- Multiple persons can be processed in parallel\n- Masks are downsampled to match the latent resolution (H/2, W/2 relative to full latent space)\n\nSources: [lightx2v/models/networks/wan/audio_model.py:138-163](), [lightx2v/models/networks/wan/infer/audio/transformer_infer.py]()\n\n## Code Architecture\n\n### Key Classes and Their Roles\n\n```mermaid\ngraph TB\n    WanAudioRunner[\"WanAudioRunner\u003cbr/\u003eMain runner class\"]\n    AudioProcessor[\"AudioProcessor\u003cbr/\u003eAudio loading/segmentation\"]\n    AudioSegment[\"AudioSegment\u003cbr/\u003eData class\"]\n    FramePreprocessor[\"FramePreprocessorTorchVersion\u003cbr/\u003eFrame noise/masking\"]\n    \n    WanAudioRunner --\u003e AudioProcessor\n    WanAudioRunner --\u003e AudioSegment\n    WanAudioRunner --\u003e FramePreprocessor\n    \n    subgraph \"Key Methods\"\n        ReadAudioInput[\"read_audio_input()\u003cbr/\u003eLoad multi-person audio\"]\n        GetAudioFiles[\"get_audio_files_from_audio_path()\u003cbr/\u003eParse config.json\"]\n        ProcessMask[\"process_single_mask()\u003cbr/\u003eLoad and process masks\"]\n        InitRunSegment[\"init_run_segment()\u003cbr/\u003ePrepare per-segment data\"]\n    end\n    \n    WanAudioRunner --\u003e ReadAudioInput\n    WanAudioRunner --\u003e GetAudioFiles\n    WanAudioRunner --\u003e ProcessMask\n    WanAudioRunner --\u003e InitRunSegment\n    \n    subgraph \"Model Integration\"\n        AudioAdapter[\"AudioAdapter\u003cbr/\u003eProject audio features\"]\n        WanAudioModel[\"WanAudioModel\u003cbr/\u003eDiffusion model\"]\n        WanAudioTransformer[\"WanAudioTransformerInfer\u003cbr/\u003eAudio-conditioned inference\"]\n    end\n    \n    InitRunSegment --\u003e AudioAdapter\n    AudioAdapter --\u003e WanAudioModel\n    WanAudioModel --\u003e WanAudioTransformer\n    \n    style WanAudioRunner fill:#e1f5fe\n    style AudioAdapter fill:#fff3e0\n```\n\n**Class Responsibilities**\n\n| Class | File | Primary Responsibilities |\n|-------|------|--------------------------|\n| `WanAudioRunner` | wan_audio_runner.py:276-820 | Orchestrates multi-person audio-to-video generation |\n| `AudioProcessor` | wan_audio_runner.py:174-252 | Loads and segments audio streams |\n| `AudioSegment` | wan_audio_runner.py:122-129 | Data container for audio segment metadata |\n| `FramePreprocessorTorchVersion` | wan_audio_runner.py:131-172 | Adds noise/masking to previous frames |\n| `AudioAdapter` | audio_adapter.py | Projects audio features to model dimension with spatial masking |\n| `WanAudioModel` | audio_model.py:19-164 | Audio-conditioned diffusion model |\n| `WanAudioTransformerInfer` | infer/audio/transformer_infer.py | Handles audio cross-attention with masking |\n\nSources: [lightx2v/models/runners/wan/wan_audio_runner.py:276-820]()\n\n### Multi-Person Processing Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Runner as WanAudioRunner\n    participant AP as AudioProcessor\n    participant Encoder as SekoAudioEncoder\n    participant Adapter as AudioAdapter\n    participant Model as WanAudioModel\n    \n    User-\u003e\u003eRunner: run_pipeline(audio_path='dir/')\n    Runner-\u003e\u003eRunner: read_audio_input()\n    Runner-\u003e\u003eRunner: get_audio_files_from_audio_path()\n    Note over Runner: Parse config.json\u003cbr/\u003eExtract talk_objects\n    \n    Runner-\u003e\u003eAP: load_multi_person_audio(audio_files)\n    AP--\u003e\u003eRunner: audio_array (N, T_max)\n    \n    Runner-\u003e\u003eRunner: process_single_mask() for each\n    Note over Runner: Load N mask images\u003cbr/\u003eDownsample to latent res\n    Runner-\u003e\u003eRunner: torch.cat(mask_latents)\n    \n    Runner-\u003e\u003eAP: segment_audio()\n    AP--\u003e\u003eRunner: AudioSegment list\n    \n    loop For each segment\n        Runner-\u003e\u003eRunner: init_run_segment()\n        \n        loop For each person (i in N)\n            Runner-\u003e\u003eEncoder: infer(audio_array[i])\n            Encoder--\u003e\u003eRunner: features[i]\n            Runner-\u003e\u003eAdapter: forward_audio_proj(features[i])\n            Adapter--\u003e\u003eRunner: proj_features[i]\n        end\n        \n        Runner-\u003e\u003eRunner: torch.stack(proj_features)\n        Note over Runner: audio_features (N, T, 128, 1024)\u003cbr/\u003emask_latents (N, 1, H, W)\n        \n        Runner-\u003e\u003eModel: infer(inputs)\n        Note over Model: Apply spatial masks\u003cbr/\u003ein audio cross-attention\n        Model--\u003e\u003eRunner: latents\n        \n        Runner-\u003e\u003eRunner: run_vae_decoder()\n        Runner-\u003e\u003eRunner: end_run_segment()\n    end\n```\n\n**Key Processing Stages**\n\n1. **Configuration Parsing** [330-345](): Load `config.json` and extract audio/mask paths\n2. **Audio Loading** [187-203](): Load N audio files and pad to same length\n3. **Mask Processing** [347-369](): Load N masks and downsample to latent resolution\n4. **Segmentation** [209-235](): Divide audio into overlapping segments\n5. **Per-Segment Processing** [553-582]():\n   - Encode each person's audio segment\n   - Project features through adapter\n   - Stack into single tensor with person dimension\n6. **Model Inference**: Apply audio features with spatial masking\n7. **Decode and Combine**: Generate video frames for segment\n\nSources: [lightx2v/models/runners/wan/wan_audio_runner.py:291-328](), [lightx2v/models/runners/wan/wan_audio_runner.py:553-582]()\n\n## Configuration Parameters\n\n### Multi-Person Specific Settings\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `audio_num` | int | Computed | Number of persons (computed from talk_objects) |\n| `with_mask` | bool | Computed | Whether masks are provided (multi-person requires True) |\n| `prev_frame_length` | int | 5 | Number of previous frames for temporal consistency |\n| `target_video_length` | int | 81 | Maximum frames per segment |\n| `audio_sr` | int | 16000 | Audio sample rate |\n| `target_fps` | int | 16 | Target video frame rate |\n\n### Audio Processing Parameters\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `compile_max_audios` | int | Maximum number of persons for compilation |\n| `audio_encoder_path` | str | Path to audio encoder (TencentGameMate-chinese-hubert-large) |\n| `adapter_model_path` | str | Path to audio adapter weights |\n| `audio_encoder_cpu_offload` | bool | Offload audio encoder to CPU |\n| `adapter_cpu_offload` | bool | Offload audio adapter to CPU |\n| `adapter_quantized` | bool | Use quantized adapter |\n| `adapter_quant_scheme` | str | Quantization scheme (fp8, int8, mxfp4, etc.) |\n\nSources: [lightx2v/models/runners/wan/wan_audio_runner.py:278-285](), [lightx2v/models/networks/wan/audio_model.py:61-98]()\n\n## Compilation Support\n\nThe system supports JIT compilation with multi-person configurations:\n\n```python\n# Example compilation for different person counts\ncompile_shapes = [[480, 832], [720, 1280]]\nmax_audio_num = 2  # Support up to 2 persons\n\n# Compiles graphs for:\n# - 1 person with mask (shape variants)\n# - 1 person without mask (shape variants)  \n# - 2 persons with masks (shape variants)\n```\n\nThe compilation process pre-generates optimized compute graphs for different combinations of:\n- Number of persons (1 to `compile_max_audios`)\n- Spatial resolutions from `compile_shapes`\n- With/without mask configurations\n\nGraph naming convention:\n```\ngraph_{height}x{width}_audio_num_{N}_mask_{True/False}\n```\n\nSources: [lightx2v/models/networks/wan/audio_model.py:61-135]()\n\n## Usage Example\n\n### Directory Structure\n\n```\naudio_input/\n config.json\n person1.wav\n person2.wav\n mask1.png\n mask2.png\n```\n\n### Command Line\n\n```bash\npython -m lightx2v.infer \\\n    --model_cls wan2.2_audio \\\n    --task s2v \\\n    --model_path /path/to/model \\\n    --config_json config.json \\\n    --image_path reference.jpg \\\n    --audio_path audio_input/ \\\n    --save_result_path output.mp4\n```\n\n### Python API\n\n```python\nfrom lightx2v import LightX2VPipeline\n\npipeline = LightX2VPipeline(\n    model_cls=\"wan2.2_audio\",\n    model_path=\"/path/to/model\",\n    config_json=\"config.json\"\n)\n\nresult = pipeline.generate(\n    task=\"s2v\",\n    image_path=\"reference.jpg\",\n    audio_path=\"audio_input/\",  # Directory with config.json\n    save_result_path=\"output.mp4\"\n)\n```\n\nSources: [lightx2v/infer.py:78-94](), [lightx2v/models/runners/wan/wan_audio_runner.py:291-328]()\n\n## Error Handling and Validation\n\n### Configuration Validation\n\n```python\n# Check for config.json in audio directory\naudio_config_path = os.path.join(audio_path, \"config.json\")\nassert os.path.exists(audio_config_path), \"config.json not found in audio_path\"\n\n# Validate talk_objects structure\nfor talk_object in audio_config[\"talk_objects\"]:\n    audio_file = os.path.join(audio_path, talk_object[\"audio\"])\n    mask_file = os.path.join(audio_path, talk_object[\"mask\"])\n    assert os.path.exists(audio_file), f\"Audio file not found: {audio_file}\"\n    assert os.path.exists(mask_file), f\"Mask file not found: {mask_file}\"\n```\n\n### Mask Format Validation\n\n- Masks must be image files (PNG, JPG, etc.)\n- Can be RGB or grayscale\n- For RGB masks, only the first channel is used\n- Values are thresholded at 0: regions \u003e 0 are considered active\n\n### Dimension Consistency\n\nThe system ensures:\n- All audio files are padded to the same length\n- All masks are resized to match the target video resolution\n- Mask latents match the model's latent space dimensions (H16, W16)\n\nSources: [lightx2v/models/runners/wan/wan_audio_runner.py:330-345](), [lightx2v/models/runners/wan/wan_audio_runner.py:347-369]()"])</script><script>self.__next_f.push([1,"48:Tdfcb,"])</script><script>self.__next_f.push([1,"# Developer Tools and Workflows\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/utils/global_paras.py](lightx2v/utils/global_paras.py)\n- [tools/convert/converter.py](tools/convert/converter.py)\n- [tools/convert/quant/__init__.py](tools/convert/quant/__init__.py)\n- [tools/convert/quant/quant.py](tools/convert/quant/quant.py)\n- [tools/convert/quant_adapter.py](tools/convert/quant_adapter.py)\n- [tools/convert/readme.md](tools/convert/readme.md)\n- [tools/convert/readme_zh.md](tools/convert/readme_zh.md)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides comprehensive guidance for developers extending, customizing, and deploying LightX2V. It covers model conversion utilities, quantization workflows, configuration management, production deployment patterns, and weight management systems.\n\nFor information about:\n- Core pipeline and runner architecture, see [Core Architecture](#4)\n- Model-specific runner implementations, see [Model Runners and Tasks](#5)\n- Performance optimization techniques, see [Performance Optimization](#6)\n\n---\n\n## Model Conversion and Quantization Tools\n\nThe `converter.py` utility provides comprehensive model weight manipulation capabilities including format conversion, architecture transformation, quantization, and LoRA merging. This tool is critical for preparing models for deployment and adapting weights between different frameworks.\n\n### Converter Architecture and Workflow\n\n```mermaid\ngraph TB\n    subgraph \"Input Sources\"\n        PyTorch[\".pth Files\"]\n        SafeTensors[\".safetensors Files\"]\n        Directory[\"Model Directory\"]\n        LoRAFiles[\"LoRA .safetensors\"]\n    end\n    \n    subgraph \"Converter Core (converter.py)\"\n        LoadWeights[\"load_weights()\u003cbr/\u003eLazy loading with safe_open\"]\n        MergeWeights[\"merge_weights()\u003cbr/\u003eCombine multiple files\"]\n        ConvertKeys[\"convert_key()\u003cbr/\u003eRegex-based key mapping\"]\n        ApplyLoRA[\"load_loras()\u003cbr/\u003eLoRALoader.apply_lora()\"]\n        QuantizeModel[\"quantize_model()\u003cbr/\u003eCONVERT_WEIGHT_REGISTER\"]\n        SaveWeights[\"save_weights()\u003cbr/\u003eSingle/Block/Chunked\"]\n    end\n    \n    subgraph \"Quantization Backends\"\n        INT8[\"QuantWeightINT8\u003cbr/\u003etorch.int8\"]\n        FP8[\"QuantWeightFP8\u003cbr/\u003etorch.float8_e4m3fn\"]\n        NVFP4[\"QuantWeightNVFP4\u003cbr/\u003ew4a4 + global_scale\"]\n        MxFP[\"QuantWeightMxFP4/6/8\u003cbr/\u003eBlock scaling\"]\n    end\n    \n    subgraph \"Key Mapping Rules\"\n        WanDitRules[\"get_key_mapping_rules()\u003cbr/\u003ewan_dit: 40+ patterns\"]\n        ForwardRules[\"forward: LightX2V  Diffusers\"]\n        BackwardRules[\"backward: Diffusers  LightX2V\"]\n    end\n    \n    subgraph \"Output Formats\"\n        SingleFile[\"Single .safetensors\u003cbr/\u003e--single_file\"]\n        ByBlock[\"Block-wise files\u003cbr/\u003e--save_by_block\u003cbr/\u003eblock_0.safetensors...\"]\n        Chunked[\"Chunked files\u003cbr/\u003e--chunk-size 100\u003cbr/\u003epart0.safetensors...\"]\n        IndexJSON[\"model.safetensors.index.json\"]\n    end\n    \n    PyTorch --\u003e LoadWeights\n    SafeTensors --\u003e LoadWeights\n    Directory --\u003e LoadWeights\n    LoadWeights --\u003e MergeWeights\n    MergeWeights --\u003e ConvertKeys\n    LoRAFiles --\u003e ApplyLoRA\n    ConvertKeys --\u003e ApplyLoRA\n    ApplyLoRA --\u003e QuantizeModel\n    \n    ConvertKeys --\u003e WanDitRules\n    WanDitRules --\u003e ForwardRules\n    WanDitRules --\u003e BackwardRules\n    \n    QuantizeModel --\u003e INT8\n    QuantizeModel --\u003e FP8\n    QuantizeModel --\u003e NVFP4\n    QuantizeModel --\u003e MxFP\n    \n    INT8 --\u003e SaveWeights\n    FP8 --\u003e SaveWeights\n    NVFP4 --\u003e SaveWeights\n    MxFP --\u003e SaveWeights\n    \n    SaveWeights --\u003e SingleFile\n    SaveWeights --\u003e ByBlock\n    SaveWeights --\u003e Chunked\n    ByBlock --\u003e IndexJSON\n    Chunked --\u003e IndexJSON\n```\n\n**Sources:** [tools/convert/converter.py:1-885](), [tools/convert/readme.md:1-446]()\n\n### Format Conversion\n\nThe converter supports bidirectional conversion between PyTorch (`.pth`) and SafeTensors (`.safetensors`) formats. SafeTensors provides lazy loading capabilities critical for handling large models.\n\n**Key Operations:**\n\n| Operation | Implementation | Memory Efficiency |\n|-----------|---------------|-------------------|\n| Load `.pth` | `torch.load()` with `weights_only=True` | Loads entire file into memory |\n| Load `.safetensors` | `safe_open()` with lazy loading | Only loads tensors on access |\n| Save `.pth` | `torch.save()` | Single operation |\n| Save `.safetensors` | `st.save_file()` | Memory-mapped write |\n\n**Example: .pth to .safetensors**\n```bash\npython tools/convert/converter.py \\\n    --source /path/to/model.pth \\\n    --output /path/to/output \\\n    --output_ext .safetensors \\\n    --output_name model \\\n    --single_file\n```\n\n**Sources:** [tools/convert/converter.py:462-504](), [tools/convert/readme.md:424-445]()\n\n### Architecture Conversion: LightX2V  Diffusers\n\nThe converter implements bidirectional architecture conversion through regex-based key mapping rules. This enables interoperability with Hugging Face Diffusers library.\n\n```mermaid\ngraph LR\n    subgraph \"LightX2V Format\"\n        L1[\"blocks.0.self_attn.q.weight\"]\n        L2[\"blocks.0.cross_attn.k_img.weight\"]\n        L3[\"head.head.weight\"]\n        L4[\"text_embedding.0.weight\"]\n    end\n    \n    subgraph \"Key Mapping Engine\"\n        Rules[\"get_key_mapping_rules()\u003cbr/\u003edirection='forward'/'backward'\"]\n        Regex[\"Pre-compiled regex patterns\u003cbr/\u003e40+ transformation rules\"]\n        Parallel[\"ThreadPoolExecutor\u003cbr/\u003eParallel key conversion\"]\n    end\n    \n    subgraph \"Diffusers Format\"\n        D1[\"blocks.0.attn1.to_q.weight\"]\n        D2[\"blocks.0.attn2.add_k_proj.weight\"]\n        D3[\"proj_out.weight\"]\n        D4[\"condition_embedder.text_embedder.linear_1.weight\"]\n    end\n    \n    L1 --\u003e Rules\n    L2 --\u003e Rules\n    L3 --\u003e Rules\n    L4 --\u003e Rules\n    \n    Rules --\u003e Regex\n    Regex --\u003e Parallel\n    \n    Parallel --\u003e D1\n    Parallel --\u003e D2\n    Parallel --\u003e D3\n    Parallel --\u003e D4\n```\n\n**Supported Model Types:**\n\n| Model Type | `--model_type` | Conversion Patterns |\n|------------|----------------|---------------------|\n| Wan DiT | `wan_dit` | 40+ regex rules for blocks, attention, embedders |\n| Wan Animate DiT | `wan_animate_dit` | Includes face_adapter patterns |\n| Qwen Image DiT | `qwen_image_dit` | Dual-stream architecture mappings |\n| Z-Image DiT | `z_image_dit` | Attention and feed-forward patterns |\n| HunyuanVideo DiT | `hunyuan_dit` | Img/txt dual-tower mappings |\n\n**Example Key Mapping Rules:**\n\nThe converter applies patterns like [tools/convert/converter.py:92-120]():\n- `blocks.(\\d+).self_attn.q.`  `blocks.\\1.attn1.to_q.`\n- `blocks.(\\d+).cross_attn.k_img.`  `blocks.\\1.attn2.add_k_proj.`\n- `head.head`  `proj_out`\n\n**Forward Conversion (LightX2V  Diffusers):**\n```bash\npython tools/convert/converter.py \\\n    --source /path/to/Wan2.1-I2V-14B-480P \\\n    --output /path/to/Wan2.1-I2V-14B-480P-Diffusers \\\n    --output_ext .safetensors \\\n    --model_type wan_dit \\\n    --direction forward \\\n    --chunk-size 100\n```\n\n**Backward Conversion (Diffusers  LightX2V):**\n```bash\npython tools/convert/converter.py \\\n    --source /path/to/Wan2.1-I2V-14B-480P-Diffusers \\\n    --output /path/to/Wan2.1-I2V-14B-480P \\\n    --model_type wan_dit \\\n    --direction backward \\\n    --save_by_block\n```\n\n**Sources:** [tools/convert/converter.py:36-312](), [tools/convert/converter.py:506-545](), [tools/convert/readme.md:397-421]()\n\n### Quantization System\n\nThe converter implements a plugin-based quantization system using `CONVERT_WEIGHT_REGISTER` to support multiple quantization formats. Each format provides 2-8x memory reduction with varying quality/performance tradeoffs.\n\n```mermaid\ngraph TB\n    subgraph \"Quantization Entry Point\"\n        QuantModel[\"quantize_model()\u003cbr/\u003econverter.py:314-431\"]\n        Registry[\"CONVERT_WEIGHT_REGISTER\u003cbr/\u003ePlugin registry\"]\n    end\n    \n    subgraph \"Quantization Plugins\"\n        INT8Plugin[\"QuantWeightINT8\u003cbr/\u003ePer-channel symmetric\u003cbr/\u003eqmin=-128, qmax=127\"]\n        FP8Plugin[\"QuantWeightFP8\u003cbr/\u003eE4M3 format\u003cbr/\u003efloat_quantize()\"]\n        NVFP4Plugin[\"QuantWeightNVFP4\u003cbr/\u003ew4a4 QAT\u003cbr/\u003escaled_nvfp4_quant()\"]\n        MxFPPlugin[\"QuantWeightMxFP4/6/8\u003cbr/\u003eBlock-wise scaling\u003cbr/\u003escaled_mxfp_quant()\"]\n    end\n    \n    subgraph \"Target Selection\"\n        ModelTypeMap[\"model_type_keys_map\u003cbr/\u003econverter.py:817-873\"]\n        TargetKeys[\"target_keys: ['attn', 'ffn']\u003cbr/\u003ekey_idx: 2\u003cbr/\u003eignore_key: ['ca', 'audio']\"]\n    end\n    \n    subgraph \"Quantized Output\"\n        WeightQuant[\"tensor.weight  int8/fp8/nvfp4\u003cbr/\u003eQuantized weights\"]\n        WeightScale[\"tensor.weight_scale  float\u003cbr/\u003ePer-channel scales\"]\n        GlobalScale[\"tensor.weight_global_scale\u003cbr/\u003eNVFP4 only\"]\n    end\n    \n    QuantModel --\u003e Registry\n    Registry --\u003e INT8Plugin\n    Registry --\u003e FP8Plugin\n    Registry --\u003e NVFP4Plugin\n    Registry --\u003e MxFPPlugin\n    \n    QuantModel --\u003e ModelTypeMap\n    ModelTypeMap --\u003e TargetKeys\n    \n    INT8Plugin --\u003e WeightQuant\n    FP8Plugin --\u003e WeightQuant\n    NVFP4Plugin --\u003e WeightQuant\n    MxFPPlugin --\u003e WeightQuant\n    \n    INT8Plugin --\u003e WeightScale\n    FP8Plugin --\u003e WeightScale\n    NVFP4Plugin --\u003e WeightScale\n    NVFP4Plugin --\u003e GlobalScale\n    MxFPPlugin --\u003e WeightScale\n```\n\n**Quantization Format Comparison:**\n\n| Format | Dtype | Memory Reduction | Quality | Backend Requirement |\n|--------|-------|------------------|---------|---------------------|\n| INT8 | `torch.int8` | 4x (FP32 baseline) | Good | Triton/VLLM |\n| FP8 | `torch.float8_e4m3fn` | 2x (BF16 baseline) | Excellent | SGL/TorchAO |\n| NVFP4 | Custom 4-bit | 8x (FP32 baseline) | Good (QAT trained) | Q8F kernels |\n| MxFP4/6/8 | Microsoft formats | 8x/5.3x/4x | Variable | lightx2v_kernel |\n\n**Target Module Selection:**\n\nThe converter uses model-specific configurations to determine which modules to quantize [tools/convert/converter.py:817-873]():\n\n```python\nmodel_type_keys_map = {\n    \"wan_dit\": {\n        \"key_idx\": 2,\n        \"target_keys\": [\"self_attn\", \"cross_attn\", \"ffn\"],\n        \"ignore_key\": [\"ca\", \"audio\"]\n    },\n    \"qwen_image_dit\": {\n        \"key_idx\": 2,\n        \"target_keys\": [\"attn\", \"img_mlp\", \"txt_mlp\", \"txt_mod\", \"img_mod\"],\n        \"ignore_key\": None\n    }\n}\n```\n\nThe `key_idx` parameter determines the position in the split key path to check (e.g., `blocks.0.self_attn.q.weight` splits into parts, index 2 is `self_attn`).\n\n**Example: Wan DiT FP8 Quantization**\n```bash\npython tools/convert/converter.py \\\n    --source /path/to/Wan2.1-I2V-14B-480P/ \\\n    --output /path/to/output \\\n    --output_name wan2.1_i2v_480p_fp8 \\\n    --model_type wan_dit \\\n    --linear_type fp8 \\\n    --non_linear_dtype torch.bfloat16 \\\n    --quantized \\\n    --save_by_block\n```\n\n**ComfyUI Compatibility Mode:**\n\nThe converter supports ComfyUI's scaled FP8 format with `--comfyui_mode` flag [tools/convert/converter.py:365-366](), [tools/convert/converter.py:396-398](), [tools/convert/converter.py:428-429]():\n- Adds `scaled_fp8` marker tensor\n- Uses `tensor.scale_weight` instead of `tensor.weight_scale` suffix\n- Supports `--full_quantized` for quantizing all tensors\n\n**Sources:** [tools/convert/converter.py:314-431](), [tools/convert/quant/quant.py:1-142](), [tools/convert/readme.md:41-59]()\n\n### LoRA Merging\n\nThe converter supports merging multiple LoRA formats into base models using the `LoRALoader` class. It automatically detects LoRA formats and handles key mapping for architecture conversion.\n\n**Supported LoRA Formats:**\n\nThe system detects and applies 7 different LoRA naming conventions [tools/convert/readme.md:84-99]():\n\n1. **Standard**: `{key}.lora_up.weight` / `{key}.lora_down.weight`\n2. **Diffusers**: `{key}_lora.up.weight` / `{key}_lora.down.weight`\n3. **Diffusers V2**: `{key}.lora_B.weight` / `{key}.lora_A.weight`\n4. **Diffusers V3**: `{key}.lora.up.weight` / `{key}.lora.down.weight`\n5. **Mochi**: `{key}.lora_B` / `{key}.lora_A` (no `.weight` suffix)\n6. **Transformers**: `{key}.lora_linear_layer.up.weight` / `{key}.lora_linear_layer.down.weight`\n7. **Qwen**: `{key}.lora_B.default.weight` / `{key}.lora_A.default.weight`\n\nAdditionally supports diff formats: `.diff`, `.diff_b`, `.diff_m`\n\n**LoRA Application Formula:**\n\nThe LoRA loader applies deltas using [lightx2v/utils/lora_loader.py]():\n```\nweight_new = weight_base + strength * alpha * (lora_up @ lora_down)\n```\n\nWhere:\n- `strength`: Overall LoRA intensity (default 1.0)\n- `alpha`: Per-LoRA scaling factor (optional)\n\n**Key Conversion Modes:**\n\nThe `--lora_key_convert` parameter controls how LoRA keys are handled [tools/convert/converter.py:564-582]():\n\n| Mode | Behavior | Use Case |\n|------|----------|----------|\n| `auto` | Try with conversion first, fallback to original keys | Recommended default |\n| `same` | Use LoRA keys as-is, no conversion | LoRA already in target format |\n| `convert` | Apply same key conversion as model | LoRA in source format |\n\n**Example: Merge Multiple LoRAs with Quantization**\n```bash\npython tools/convert/converter.py \\\n    --source /path/to/base_model/ \\\n    --output /path/to/output \\\n    --model_type wan_dit \\\n    --lora_path /path/to/lora1.safetensors /path/to/lora2.safetensors \\\n    --lora_strength 1.0 0.8 \\\n    --lora_alpha 32.0 64.0 \\\n    --quantized \\\n    --linear_type fp8 \\\n    --single_file\n```\n\n**Sources:** [tools/convert/converter.py:433-460](), [tools/convert/converter.py:548-582](), [tools/convert/readme.md:283-361]()\n\n### Saving Strategies\n\nThe converter provides three saving strategies optimized for different use cases, balancing memory usage, disk I/O, and file organization.\n\n```mermaid\ngraph TB\n    subgraph \"Converted Weights\"\n        WeightDict[\"state_dict\u003cbr/\u003eAll model parameters\"]\n    end\n    \n    subgraph \"Single File Mode\"\n        SingleCheck[\"Total size \u003e 10GB?\u003cbr/\u003eMemory warning\"]\n        SingleSave[\"st.save_file()\u003cbr/\u003eentire dict\"]\n        SingleOutput[\"model.safetensors\"]\n    end\n    \n    subgraph \"Block-wise Mode\"\n        BlockPattern[\"re.compile(r'blocks\\\\.(\\d+)\\\\.')\"]\n        BlockGroup[\"defaultdict(dict)\u003cbr/\u003eGroup by block index\"]\n        BlockSave[\"Save block_0.safetensors\u003cbr/\u003eblock_1.safetensors...\"]\n        NonBlock[\"non_block.safetensors\"]\n    end\n    \n    subgraph \"Chunked Mode\"\n        ChunkSize[\"--chunk-size N\u003cbr/\u003eDefault: 100 keys per file\"]\n        ChunkIterate[\"Iterate state_dict\"]\n        ChunkSave[\"model_part0.safetensors\u003cbr/\u003emodel_part1.safetensors...\"]\n    end\n    \n    subgraph \"Index Generation\"\n        IndexFile[\"model.safetensors.index.json\"]\n        WeightMap[\"weight_map: {key: filename}\"]\n        Metadata[\"metadata: {total_size}\"]\n    end\n    \n    WeightDict --\u003e SingleCheck\n    SingleCheck --\u003e SingleSave\n    SingleSave --\u003e SingleOutput\n    \n    WeightDict --\u003e BlockPattern\n    BlockPattern --\u003e BlockGroup\n    BlockGroup --\u003e BlockSave\n    BlockGroup --\u003e NonBlock\n    \n    WeightDict --\u003e ChunkSize\n    ChunkSize --\u003e ChunkIterate\n    ChunkIterate --\u003e ChunkSave\n    \n    BlockSave --\u003e IndexFile\n    NonBlock --\u003e IndexFile\n    ChunkSave --\u003e IndexFile\n    \n    IndexFile --\u003e WeightMap\n    IndexFile --\u003e Metadata\n```\n\n**Strategy Comparison:**\n\n| Strategy | Command | Best For | Memory Usage | File Count |\n|----------|---------|----------|--------------|------------|\n| Single File | `--single_file` | Small models (\u003c10GB), final deployment | High (loads entire model) | 1 |\n| Block-wise | `--save_by_block` | Backward conversion, block-level offloading | Medium | ~48 (for 48 blocks) |\n| Chunked | `--chunk-size 100` | Forward conversion, distributed loading | Low | Variable |\n\n**Block-wise Saving Details:**\n\nThe block-wise strategy uses regex to group weights by transformer block index [tools/convert/converter.py:637-668]():\n- Pattern: `r\"blocks\\.(\\d+)\\.\"`\n- Special handling for `wan_animate_dit`: Face adapter blocks indexed as `block_idx * 5`\n- Non-block weights (embedders, head) saved to `non_block.safetensors`\n\n**Index File Format:**\n\nWhen using block-wise or chunked saving, the converter generates `diffusion_pytorch_model.safetensors.index.json` [tools/convert/converter.py:694-699]():\n```json\n{\n  \"metadata\": {\n    \"total_size\": 27982408704\n  },\n  \"weight_map\": {\n    \"blocks.0.self_attn.q.weight\": \"block_0.safetensors\",\n    \"blocks.0.self_attn.k.weight\": \"block_0.safetensors\",\n    ...\n  }\n}\n```\n\n**Sources:** [tools/convert/converter.py:604-703](), [tools/convert/readme.md:71-77]()\n\n### Parallel Processing and Memory Optimization\n\nThe converter implements parallel processing for large-scale conversions and provides memory-efficient loading strategies.\n\n**Parallel Key Conversion:**\n\nFor models with \u003e1000 keys, parallel processing accelerates conversion [tools/convert/converter.py:525-543]():\n```python\n# Enabled by default with --parallel (True)\n# Uses ThreadPoolExecutor for CPU-bound regex operations\nnum_workers = min(8, multiprocessing.cpu_count())\nwith ThreadPoolExecutor(max_workers=num_workers) as executor:\n    future_to_key = {executor.submit(convert_key, key): key \n                     for key in keys_list}\n```\n\n**Lazy Loading for SafeTensors:**\n\nLarge multi-file models use lazy loading to reduce memory footprint [tools/convert/converter.py:481-492]():\n```python\nwith safe_open(file_path, framework=\"pt\", device=args.device) as f:\n    if len(keys) \u003e 100:  # Show progress for large files\n        for k in tqdm(keys, desc=f\"Loading {basename}\", leave=False):\n            weights[k] = f.get_tensor(k)\n```\n\n**Memory Management:**\n\nThe converter employs several memory optimization techniques:\n- Garbage collection between file loads: `gc.collect()` [tools/convert/converter.py:504]()\n- Streaming writes for large models\n- Device placement control via `--device cpu/cuda`\n- Memory warnings for single-file saves \u003e10GB [tools/convert/converter.py:620-625]()\n\n**Sources:** [tools/convert/converter.py:462-505](), [tools/convert/converter.py:525-543]()\n\n---\n\n## Configuration System\n\nLightX2V uses a hierarchical configuration system that combines JSON files, Python API parameters, and runtime updates. Configuration flows through `set_config()` to build a unified config dictionary consumed by runners.\n\n### Configuration Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Configuration Sources\"\n        JSONFile[\"JSON Config Files\u003cbr/\u003econfigs/*/model.json\"]\n        APIParams[\"Python API Parameters\u003cbr/\u003ecreate_generator() args\"]\n        CLIArgs[\"CLI Arguments\u003cbr/\u003e--arg value\"]\n        RuntimeUpdates[\"Runtime Updates\u003cbr/\u003erunner.set_config()\"]\n    end\n    \n    subgraph \"Pipeline Configuration\"\n        Pipeline[\"LightX2VPipeline\u003cbr/\u003epipeline.py:62-449\"]\n        SetInferConfig[\"set_infer_config()\u003cbr/\u003ePipeline attributes\"]\n        SetInferConfigJSON[\"set_infer_config_json()\u003cbr/\u003eJSON loading\"]\n    end\n    \n    subgraph \"Global Config Builder\"\n        SetConfig[\"set_config(pipeline)\u003cbr/\u003eutils/set_config.py\"]\n        ValidateConfigPaths[\"validate_config_paths()\u003cbr/\u003eCheck model file existence\"]\n        SetParallelConfig[\"set_parallel_config()\u003cbr/\u003eDistributed setup\"]\n    end\n    \n    subgraph \"Runner Configuration\"\n        RunnerConfig[\"runner.config\u003cbr/\u003eImmutable at init\"]\n        RunnerSetConfig[\"runner.set_config(dict)\u003cbr/\u003eRuntime parameter updates\"]\n    end\n    \n    subgraph \"Configuration Categories\"\n        ModelPaths[\"Model Paths\u003cbr/\u003emodel_path, vae_path, etc.\"]\n        InferParams[\"Inference Parameters\u003cbr/\u003einfer_steps, guidance_scale\"]\n        OptimFlags[\"Optimization Flags\u003cbr/\u003equantized, cpu_offload\"]\n        ParallelConfig[\"Parallel Configuration\u003cbr/\u003ecfg_p_size, seq_p_size\"]\n    end\n    \n    JSONFile --\u003e SetInferConfigJSON\n    APIParams --\u003e SetInferConfig\n    CLIArgs --\u003e SetInferConfig\n    \n    SetInferConfig --\u003e Pipeline\n    SetInferConfigJSON --\u003e Pipeline\n    \n    Pipeline --\u003e SetConfig\n    SetConfig --\u003e ValidateConfigPaths\n    SetConfig --\u003e SetParallelConfig\n    \n    SetConfig --\u003e RunnerConfig\n    RuntimeUpdates --\u003e RunnerSetConfig\n    \n    RunnerConfig --\u003e ModelPaths\n    RunnerConfig --\u003e InferParams\n    RunnerConfig --\u003e OptimFlags\n    RunnerConfig --\u003e ParallelConfig\n```\n\n**Sources:** [lightx2v/pipeline.py:1-449](), [lightx2v/utils/set_config.py]()\n\n### JSON Configuration Files\n\nJSON configuration files provide template configurations for different model architectures and tasks. The pipeline loads these via `set_infer_config_json()`.\n\n**Configuration Loading:**\n\n[lightx2v/pipeline.py:239-243]():\n```python\ndef set_infer_config_json(self, config_json):\n    logger.info(f\"Loading infer config from {config_json}\")\n    with open(config_json, \"r\") as f:\n        config_json = json.load(f)\n    self.update(config_json)  # Updates pipeline attributes\n```\n\n**Common Configuration Parameters:**\n\n| Category | Parameters | Purpose |\n|----------|-----------|---------|\n| Model Identity | `model_cls`, `task`, `model_path` | Identify runner and weights |\n| Inference | `infer_steps`, `guidance_scale`, `sample_shift` | Diffusion process control |\n| Resolution | `target_height`, `target_width`, `target_video_length` | Output dimensions |\n| Optimization | `dit_quantized`, `cpu_offload`, `attn_mode` | Performance tuning |\n| LoRA | `lora_configs`, `lora_dynamic_apply` | LoRA management |\n\n**Example JSON Configuration:**\n\n```json\n{\n  \"model_cls\": \"wan2.1\",\n  \"task\": \"i2v\",\n  \"infer_steps\": 50,\n  \"target_height\": 480,\n  \"target_width\": 832,\n  \"target_video_length\": 81,\n  \"guidance_scale\": 5.0,\n  \"sample_shift\": 5.0,\n  \"dit_quantized\": true,\n  \"dit_quant_scheme\": \"fp8-sgl\",\n  \"cpu_offload\": true,\n  \"offload_granularity\": \"block\"\n}\n```\n\n**Sources:** [lightx2v/pipeline.py:239-243]()\n\n### Pipeline Configuration Methods\n\nThe `LightX2VPipeline` class provides a fluent API for configuring inference through `enable_*` methods. These methods set attributes that are later consumed by `set_config()`.\n\n**Configuration Method Categories:**\n\n```mermaid\ngraph LR\n    subgraph \"LightX2VPipeline Methods\"\n        CreateGen[\"create_generator()\u003cbr/\u003ePrimary initialization\"]\n        EnableLightVAE[\"enable_lightvae()\u003cbr/\u003euse_lightvae, use_tae\"]\n        EnableQuant[\"enable_quantize()\u003cbr/\u003edit_quantized, quant_scheme\"]\n        EnableOffload[\"enable_offload()\u003cbr/\u003ecpu_offload, granularity\"]\n        EnableCompile[\"enable_compile()\u003cbr/\u003ecompile=True\"]\n        EnableLoRA[\"enable_lora()\u003cbr/\u003elora_configs, dynamic_apply\"]\n        SwitchLoRA[\"switch_lora()\u003cbr/\u003eRuntime LoRA change\"]\n        EnableCache[\"enable_cache()\u003cbr/\u003eTeaCache, MagCache\"]\n        EnableParallel[\"enable_parallel()\u003cbr/\u003ecfg_p_size, seq_p_size\"]\n    end\n    \n    subgraph \"Configuration State\"\n        PipelineAttrs[\"Pipeline Attributes\u003cbr/\u003eself.dit_quantized\u003cbr/\u003eself.cpu_offload\u003cbr/\u003eself.lora_configs\"]\n    end\n    \n    CreateGen --\u003e PipelineAttrs\n    EnableLightVAE --\u003e PipelineAttrs\n    EnableQuant --\u003e PipelineAttrs\n    EnableOffload --\u003e PipelineAttrs\n    EnableCompile --\u003e PipelineAttrs\n    EnableLoRA --\u003e PipelineAttrs\n    EnableCache --\u003e PipelineAttrs\n    EnableParallel --\u003e PipelineAttrs\n    \n    SwitchLoRA -.Runtime.-\u003e PipelineAttrs\n```\n\n**Method Usage Example:**\n\n```python\nfrom lightx2v.pipeline import LightX2VPipeline\n\npipeline = LightX2VPipeline(\n    task=\"i2v\",\n    model_path=\"/path/to/Wan2.1-I2V-14B-480P\",\n    model_cls=\"wan2.1\"\n)\n\n# Configure inference parameters\npipeline.create_generator(\n    attn_mode=\"sage_attn\",\n    infer_steps=50,\n    num_frames=81,\n    height=480,\n    width=832,\n    guidance_scale=5.0\n)\n\n# Enable optimizations\npipeline.enable_quantize(\n    dit_quantized=True,\n    quant_scheme=\"fp8-sgl\",\n    text_encoder_quantized=True\n)\n\npipeline.enable_offload(\n    cpu_offload=True,\n    offload_granularity=\"block\",\n    text_encoder_offload=True\n)\n\npipeline.enable_lora(\n    lora_configs=[{\n        \"path\": \"/path/to/lora.safetensors\",\n        \"strength\": 1.0\n    }],\n    lora_dynamic_apply=True\n)\n\n# Generate\npipeline.generate(\n    seed=42,\n    prompt=\"A cat walking in the garden\",\n    negative_prompt=\"\",\n    save_result_path=\"output.mp4\"\n)\n```\n\n**Enable Method Details:**\n\n**`enable_quantize()`** [lightx2v/pipeline.py:260-296]():\n- Sets quantization flags for DiT, text encoder, image encoder\n- Configures quantization scheme: `int8-triton`, `fp8-sgl`, `fp8-torchao`, `nvfp4-q8`\n- Specifies quantized checkpoint paths\n\n**`enable_offload()`** [lightx2v/pipeline.py:298-335]():\n- Enables CPU offloading for DiT, encoders, VAE\n- Sets granularity: `model`, `block`, `phase`\n- Model-specific encoder offload flags\n\n**`enable_lora()`** [lightx2v/pipeline.py:354-356]():\n- Configures LoRA file paths and strengths\n- Sets `lora_dynamic_apply` for runtime switching\n\n**`switch_lora()`** [lightx2v/pipeline.py:358-366]():\n- Runtime LoRA switching without model reload\n- Requires `lora_dynamic_apply=True`\n- Calls `runner.switch_lora()` internally\n\n**Sources:** [lightx2v/pipeline.py:129-398]()\n\n### Runtime Configuration Updates\n\nRunners support runtime configuration updates through the `set_config()` method, enabling parameter changes without reinitializing the model. This is critical for server deployments handling multiple requests with different parameters.\n\n**Runtime Update Flow:**\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant HTTPServer\n    participant Worker as TorchrunInferenceWorker\n    participant Runner\n    \n    Client-\u003e\u003eHTTPServer: POST /v1/tasks/video/\u003cbr/\u003e{\"infer_steps\": 10, \"lora_name\": \"style.safetensors\"}\n    HTTPServer-\u003e\u003eWorker: process_request(task_data)\n    Worker-\u003e\u003eWorker: switch_lora(\"style.safetensors\", 1.0)\n    Worker-\u003e\u003eRunner: set_config({\"infer_steps\": 10})\n    Runner-\u003e\u003eRunner: Update self.config[\"infer_steps\"]\n    Runner-\u003e\u003eRunner: Update scheduler.num_inference_steps\n    Worker-\u003e\u003eRunner: run_pipeline(input_info)\n    Runner--\u003e\u003eWorker: Generated video\n    Worker--\u003e\u003eHTTPServer: Task result\n    HTTPServer--\u003e\u003eClient: Response\n```\n\n**Updatable Parameters:**\n\nRuntime updates typically affect inference-time parameters without requiring model reloading:\n\n| Parameter | Description | Impact |\n|-----------|-------------|--------|\n| `infer_steps` | Number of diffusion steps | Scheduler reconfiguration |\n| `guidance_scale` | CFG strength | Conditional guidance weight |\n| `target_height/width` | Output resolution | Grid size recalculation |\n| `target_video_length` | Video length | Latent dimensions |\n| `resize_mode` | Image resizing strategy | Input preprocessing |\n\n**Implementation in Runner:**\n\nRunners implement `set_config()` to handle parameter updates [lightx2v/models/runners/default_runner.py]():\n```python\ndef set_config(self, config_dict):\n    \"\"\"Update runtime configuration parameters\"\"\"\n    for key, value in config_dict.items():\n        if key in self.config:\n            self.config[key] = value\n    \n    # Update dependent components\n    if \"infer_steps\" in config_dict:\n        self.scheduler.set_timesteps(\n            config_dict[\"infer_steps\"],\n            device=self.device\n        )\n```\n\n**Server Integration:**\n\nThe HTTP server uses runtime updates for per-request configuration [lightx2v/server/services/inference/worker.py:93-96]():\n```python\ntask_data[\"task\"] = self.runner.config[\"task\"]\nupdate_input_info_from_dict(self.input_info, task_data)\nself.runner.set_config(task_data)\nself.runner.run_pipeline(self.input_info)\n```\n\n**Sources:** [lightx2v/server/services/inference/worker.py:66-96](), [lightx2v/pipeline.py:189-237]()\n\n---\n\n## Production Deployment Tools\n\nLightX2V provides production-grade deployment infrastructure through the HTTP server system with distributed inference support, dynamic LoRA management, and robust task handling.\n\n### HTTP Server Architecture\n\nThe server implements a master-worker architecture using `torchrun` for distributed GPU coordination. The rank-0 process handles HTTP requests while all ranks participate in inference.\n\n```mermaid\ngraph TB\n    subgraph \"HTTP Layer (Rank 0 Only)\"\n        FastAPI[\"FastAPI Server\u003cbr/\u003eapi_server.py\"]\n        VideoEndpoint[\"/v1/tasks/video/\"]\n        ImageEndpoint[\"/v1/tasks/image/\"]\n        StatusEndpoint[\"/v1/tasks/status/\"]\n        TaskManager[\"TaskManager\u003cbr/\u003eQueue management\"]\n    end\n    \n    subgraph \"Service Layer (Rank 0)\"\n        DistInferSvc[\"DistributedInferenceService\u003cbr/\u003esubmit_task_async()\"]\n        VideoGenSvc[\"VideoGenerationService\u003cbr/\u003eProcess T2V/I2V/A2V\"]\n        ImageGenSvc[\"ImageGenerationService\u003cbr/\u003eProcess T2I/I2I\"]\n        FileService[\"FileService\u003cbr/\u003eFile I/O, base64 handling\"]\n    end\n    \n    subgraph \"Distributed Inference (All Ranks)\"\n        Rank0[\"TorchrunInferenceWorker\u003cbr/\u003eRank 0 (Master)\"]\n        Rank1[\"TorchrunInferenceWorker\u003cbr/\u003eRank 1\"]\n        RankN[\"TorchrunInferenceWorker\u003cbr/\u003eRank N\"]\n        DistMgr[\"DistributedManager\u003cbr/\u003eGloo process group\"]\n    end\n    \n    subgraph \"Inference Execution (All Ranks)\"\n        Runner0[\"Runner (Rank 0)\u003cbr/\u003eRUNNER_REGISTER\"]\n        Runner1[\"Runner (Rank 1)\u003cbr/\u003eRUNNER_REGISTER\"]\n        RunnerN[\"Runner (Rank N)\u003cbr/\u003eRUNNER_REGISTER\"]\n    end\n    \n    FastAPI --\u003e VideoEndpoint\n    FastAPI --\u003e ImageEndpoint\n    FastAPI --\u003e StatusEndpoint\n    \n    VideoEndpoint --\u003e TaskManager\n    ImageEndpoint --\u003e TaskManager\n    StatusEndpoint --\u003e TaskManager\n    \n    TaskManager --\u003e DistInferSvc\n    DistInferSvc --\u003e VideoGenSvc\n    DistInferSvc --\u003e ImageGenSvc\n    VideoGenSvc --\u003e FileService\n    ImageGenSvc --\u003e FileService\n    \n    DistInferSvc --\u003e Rank0\n    Rank0 --\u003e DistMgr\n    Rank1 --\u003e DistMgr\n    RankN --\u003e DistMgr\n    \n    DistMgr -.Broadcast task_data.-\u003e Rank1\n    DistMgr -.Broadcast task_data.-\u003e RankN\n    \n    Rank0 --\u003e Runner0\n    Rank1 --\u003e Runner1\n    RankN --\u003e RunnerN\n    \n    Runner0 -.CFG/Seq Parallel.-\u003e Runner1\n    Runner1 -.CFG/Seq Parallel.-\u003e RunnerN\n```\n\n**Server Initialization:**\n\nLaunch via `torchrun` for multi-GPU deployment [scripts/server/start_server_i2i_with_loradir.sh:14-20]():\n```bash\npython -m lightx2v.server \\\n    --model_cls qwen_image \\\n    --task i2i \\\n    --model_path /path/to/model \\\n    --lora_dir /path/to/loras \\\n    --config_json configs/qwen_image/qwen_image_i2i.json \\\n    --port 8000\n```\n\nFor distributed inference:\n```bash\ntorchrun --nproc_per_node=4 \\\n    -m lightx2v.server \\\n    --model_cls wan2.1 \\\n    --task i2v \\\n    --model_path /path/to/Wan2.1-I2V-14B-480P \\\n    --config_json configs/wan/wan_i2v.json \\\n    --parallel cfg_p_size=2 seq_p_size=2\n```\n\n**Worker Initialization:**\n\nEach rank initializes a `TorchrunInferenceWorker` [lightx2v/server/services/inference/worker.py:27-64]():\n```python\ndef init(self, args) -\u003e bool:\n    # Initialize distributed environment\n    if self.world_size \u003e 1:\n        self.dist_manager.init_process_group()\n    \n    # Load LoRA directory\n    self.lora_dir = getattr(args, \"lora_dir\", None)\n    \n    # Build config and initialize runner\n    config = set_config(args)\n    if config[\"parallel\"]:\n        set_parallel_config(config)\n    self.runner = init_runner(config)\n    \n    return True\n```\n\n**Sources:** [lightx2v/server/services/inference/worker.py:16-64](), [lightx2v/server/__main__.py:1-30]()\n\n### Dynamic LoRA Management\n\nThe server supports per-request LoRA switching without model reinitialization, enabling multi-tenant deployments where different requests use different LoRAs.\n\n**LoRA Directory Structure:**\n\n```\n/path/to/loras/\n style_anime.safetensors\n style_realistic.safetensors\n character_catgirl.safetensors\n effect_watercolor.safetensors\n```\n\n**Dynamic LoRA Workflow:**\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant HTTPServer\n    participant Worker\n    participant Model\n    \n    Note over Client,Model: Request 1: Anime Style\n    Client-\u003e\u003eHTTPServer: POST {\"lora_name\": \"style_anime.safetensors\", \"lora_strength\": 1.0}\n    HTTPServer-\u003e\u003eWorker: process_request(task_data)\n    Worker-\u003e\u003eWorker: switch_lora(\"style_anime.safetensors\", 1.0)\n    Worker-\u003e\u003eModel: _update_lora(lora_path, 1.0)\n    Model-\u003e\u003eModel: Load LoRA weights\u003cbr/\u003eApply to linear layers\n    Worker-\u003e\u003eModel: run_pipeline()\n    Model--\u003e\u003eWorker: Generated output\n    Worker--\u003e\u003eHTTPServer: Result\n    HTTPServer--\u003e\u003eClient: Response\n    \n    Note over Client,Model: Request 2: Realistic Style (Different LoRA)\n    Client-\u003e\u003eHTTPServer: POST {\"lora_name\": \"style_realistic.safetensors\", \"lora_strength\": 0.8}\n    HTTPServer-\u003e\u003eWorker: process_request(task_data)\n    Worker-\u003e\u003eWorker: switch_lora(\"style_realistic.safetensors\", 0.8)\n    Worker-\u003e\u003eModel: _remove_lora()\u003cbr/\u003eRemove previous LoRA\n    Worker-\u003e\u003eModel: _update_lora(new_lora_path, 0.8)\n    Model-\u003e\u003eModel: Load new LoRA weights\n    Worker-\u003e\u003eModel: run_pipeline()\n    Model--\u003e\u003eWorker: Generated output\n    Worker--\u003e\u003eHTTPServer: Result\n    HTTPServer--\u003e\u003eClient: Response\n    \n    Note over Client,Model: Request 3: No LoRA\n    Client-\u003e\u003eHTTPServer: POST {\"lora_name\": null}\n    HTTPServer-\u003e\u003eWorker: process_request(task_data)\n    Worker-\u003e\u003eWorker: switch_lora(None, 1.0)\n    Worker-\u003e\u003eModel: _remove_lora()\n    Model-\u003e\u003eModel: Remove all LoRA modifications\n    Worker-\u003e\u003eModel: run_pipeline()\n    Model--\u003e\u003eWorker: Generated output (base model)\n```\n\n**LoRA Switching Implementation:**\n\nThe worker manages LoRA state and delegates to model [lightx2v/server/services/inference/worker.py:126-157]():\n```python\ndef switch_lora(self, lora_name: str, lora_strength: float):\n    # Remove current LoRA if lora_name is None\n    if lora_name is None:\n        if self.current_lora_name is not None:\n            logger.info(f\"Removing LoRA: {self.current_lora_name}\")\n            if hasattr(self.runner.model, \"_remove_lora\"):\n                self.runner.model._remove_lora()\n            self.current_lora_name = None\n        return\n    \n    # Check if LoRA or strength changed\n    current_strength = getattr(self, \"current_lora_strength\", None)\n    if lora_name != self.current_lora_name or lora_strength != current_strength:\n        lora_path = self._lora_path(lora_name)  # Resolve from lora_dir\n        logger.info(f\"Applying LoRA: {lora_name} with strength={lora_strength}\")\n        \n        # Update LoRA on model\n        if hasattr(self.runner.model, \"_update_lora\"):\n            self.runner.model._update_lora(lora_path, lora_strength)\n            self.current_lora_name = lora_name\n            self.current_lora_strength = lora_strength\n```\n\n**Request Schema with LoRA:**\n\nThe `BaseTaskRequest` schema includes LoRA parameters [lightx2v/server/schema.py:18-38]():\n```python\nclass BaseTaskRequest(BaseModel):\n    task_id: str = Field(default_factory=generate_task_id)\n    prompt: str = Field(\"\", description=\"Generation prompt\")\n    # ... other fields ...\n    lora_name: Optional[str] = Field(\n        None, \n        description=\"LoRA filename to load from lora_dir, None to disable LoRA\"\n    )\n    lora_strength: float = Field(1.0, description=\"LoRA strength\")\n```\n\n**Example Request with LoRA:**\n\n[scripts/server/post_i2i_with_lora.py:14-28]():\n```python\nimport requests\n\nresponse = requests.post(\n    \"http://localhost:8000/v1/tasks/image/\",\n    json={\n        \"prompt\": \"turn the style of the photo to anime style\",\n        \"image_path\": \"\u003cbase64_encoded_image\u003e\",\n        \"lora_name\": \"style_anime.safetensors\",\n        \"lora_strength\": 1.0\n    }\n)\n```\n\n**LoRA Caching and Performance:**\n\n- LoRA weights remain loaded until explicitly removed or replaced\n- No model reloading required between requests with same LoRA\n- LoRA switching adds ~0.5-2s latency depending on LoRA size\n- Memory overhead: ~100-500MB per LoRA depending on rank and target layers\n\n**Sources:** [lightx2v/server/services/inference/worker.py:66-165](), [lightx2v/server/schema.py:18-77](), [scripts/server/post_i2i_with_lora.py:1-29]()\n\n### Distributed Task Management\n\nThe server uses a dual-backend distributed system: `gloo` for task broadcast and `nccl` for GPU computation. This separation ensures reliable task distribution with long timeouts while maintaining fast GPU communication.\n\n**Distributed Communication Architecture:**\n\n```mermaid\ngraph TB\n    subgraph \"Rank 0 (Master)\"\n        HTTPHandler[\"HTTP Request Handler\"]\n        TaskBroadcast[\"broadcast_task_data()\u003cbr/\u003eGloo backend\"]\n        InferRank0[\"Inference (NCCL)\"]\n    end\n    \n    subgraph \"Rank 1-N (Workers)\"\n        WorkerLoop[\"worker_loop()\u003cbr/\u003eBlocking receive\"]\n        TaskReceive[\"broadcast_task_data()\u003cbr/\u003eGloo receive\"]\n        InferRank1N[\"Inference (NCCL)\"]\n    end\n    \n    subgraph \"Communication Channels\"\n        GlooGroup[\"Gloo Process Group\u003cbr/\u003e30-day timeout\u003cbr/\u003eTask distribution\"]\n        NCCLGroup[\"NCCL Process Group\u003cbr/\u003eDefault timeout\u003cbr/\u003eGPU computation\"]\n    end\n    \n    subgraph \"Synchronization\"\n        Barrier[\"dist.barrier()\u003cbr/\u003eAfter each task\"]\n    end\n    \n    HTTPHandler --\u003e TaskBroadcast\n    TaskBroadcast --\u003e GlooGroup\n    GlooGroup -.Pickle + Chunked bytes.-\u003e TaskReceive\n    WorkerLoop --\u003e TaskReceive\n    \n    TaskBroadcast --\u003e InferRank0\n    TaskReceive --\u003e InferRank1N\n    \n    InferRank0 --\u003e NCCLGroup\n    InferRank1N --\u003e NCCLGroup\n    \n    InferRank0 --\u003e Barrier\n    InferRank1N --\u003e Barrier\n```\n\n**Process Group Initialization:**\n\nThe `DistributedManager` creates separate process groups for task distribution and computation [lightx2v/server/services/distributed_utils.py:22-50]():\n```python\ndef init_process_group(self) -\u003e bool:\n    self.rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n    self.world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    \n    if self.world_size \u003e 1:\n        # Initialize main backend (NCCL for GPU, gloo for CPU)\n        backend = \"nccl\" if torch.cuda.is_available() else \"gloo\"\n        dist.init_process_group(backend=backend, init_method=\"env://\")\n        \n        # Create gloo group for task distribution with long timeout\n        task_timeout = timedelta(days=30)\n        self.task_pg = dist.new_group(backend=\"gloo\", timeout=task_timeout)\n        logger.info(\"Created gloo process group with 30-day timeout\")\n```\n\n**Task Broadcast Protocol:**\n\nThe system broadcasts task data using chunked byte transfer to handle large payloads [lightx2v/server/services/distributed_utils.py:120-156]():\n\n**Rank 0 (Master):**\n```python\ndef broadcast_task_data(self, task_data: Optional[Any] = None):\n    if self.is_rank_zero():\n        # Send stop signal if shutting down\n        stop_signal = torch.tensor([1 if task_data is None else 0], dtype=torch.int32)\n        dist.broadcast(stop_signal, src=0, group=self.task_pg)\n        \n        if task_data is not None:\n            # Serialize and broadcast\n            task_bytes = pickle.dumps(task_data)\n            task_length = torch.tensor([len(task_bytes)], dtype=torch.int32)\n            dist.broadcast(task_length, src=0, group=self.task_pg)\n            self._broadcast_byte_chunks(task_bytes)  # 1MB chunks\n            return task_data\n```\n\n**Rank 1-N (Workers):**\n```python\ndef broadcast_task_data(self, task_data: Optional[Any] = None):\n    else:  # Worker ranks\n        # Receive stop signal\n        stop_signal = torch.tensor([0], dtype=torch.int32)\n        dist.broadcast(stop_signal, src=0, group=self.task_pg)\n        \n        if stop_signal.item() == 1:\n            return None  # Shutdown signal\n        \n        # Receive task data\n        task_length = torch.tensor([0], dtype=torch.int32)\n        dist.broadcast(task_length, src=0, group=self.task_pg)\n        task_bytes = self._receive_byte_chunks(int(task_length.item()))\n        return pickle.loads(task_bytes)\n```\n\n**Chunked Byte Transfer:**\n\nLarge payloads are transferred in 1MB chunks to avoid memory spikes [lightx2v/server/services/distributed_utils.py:87-118]():\n```python\nCHUNK_SIZE = 1024 * 1024  # 1MB\n\ndef _broadcast_byte_chunks(self, data_bytes: bytes) -\u003e None:\n    total_length = len(data_bytes)\n    num_full_chunks = total_length // self.CHUNK_SIZE\n    \n    # Broadcast full chunks\n    for i in range(num_full_chunks):\n        start_idx = i * self.CHUNK_SIZE\n        chunk = data_bytes[start_idx:start_idx + self.CHUNK_SIZE]\n        task_tensor = torch.tensor(list(chunk), dtype=torch.uint8)\n        dist.broadcast(task_tensor, src=0, group=self.task_pg)\n    \n    # Broadcast remainder\n    if total_length % self.CHUNK_SIZE:\n        chunk = data_bytes[-remaining:]\n        task_tensor = torch.tensor(list(chunk), dtype=torch.uint8)\n        dist.broadcast(task_tensor, src=0, group=self.task_pg)\n```\n\n**Worker Loop:**\n\nNon-master ranks run a blocking loop waiting for tasks [lightx2v/server/services/inference/worker.py:167-191]():\n```python\nasync def worker_loop(self):\n    while True:\n        try:\n            # Block until task received from rank 0\n            task_data = self.dist_manager.broadcast_task_data()\n            \n            if task_data is None:  # Shutdown signal\n                logger.info(f\"Rank {self.rank} received stop signal\")\n                break\n            \n            # Execute inference\n            await self.process_request(task_data)\n            \n        except Exception as e:\n            logger.error(f\"Rank {self.rank} worker loop error: {e}\")\n            if self.world_size \u003e 1:\n                try:\n                    self.dist_manager.barrier()  # Sync on error\n                except Exception:\n                    break  # Exit if barrier fails\n```\n\n**Graceful Shutdown:**\n\nThe server implements timeout-based cleanup to handle hung processes [lightx2v/server/services/distributed_utils.py:52-75]():\n```python\ndef cleanup(self, timeout: int = 2):\n    self._shutting_down = True\n    \n    def _destroy():\n        try:\n            dist.destroy_process_group()\n        except Exception:\n            pass\n    \n    # Run cleanup in thread with timeout\n    t = threading.Thread(target=_destroy, daemon=True)\n    t.start()\n    t.join(timeout=timeout)\n    \n    if t.is_alive():\n        logger.warning(f\"Rank {self.rank} cleanup timed out, forcing exit\")\n```\n\n**Sources:** [lightx2v/server/services/distributed_utils.py:1-157](), [lightx2v/server/services/inference/worker.py:167-194]()\n\n---\n\n## Weight Management System\n\nThe `WeightModule` hierarchy provides a unified interface for model weight loading, LoRA management, and device placement. This system abstracts safetensors, lazy loading, and distributed weight management.\n\n### WeightModule Hierarchy\n\n```mermaid\ngraph TB\n    subgraph \"Base Classes\"\n        WeightModule[\"WeightModule\u003cbr/\u003eBase weight container\u003cbr/\u003eweight_module.py:1-185\"]\n        WeightModuleList[\"WeightModuleList\u003cbr/\u003eList of modules\u003cbr/\u003eweight_module.py:187-212\"]\n    end\n    \n    subgraph \"Core Operations\"\n        Load[\"load(weight_dict)\u003cbr/\u003eLoad from dict\"]\n        StateDict[\"state_dict()\u003cbr/\u003eExport weights\"]\n        LoadStateDict[\"load_state_dict()\u003cbr/\u003eLoad by block index\"]\n        LoadFromDisk[\"load_state_dict_from_disk()\u003cbr/\u003eLazy loading\"]\n    end\n    \n    subgraph \"LoRA Operations\"\n        RegisterLoRA[\"register_lora(dict, strength)\u003cbr/\u003eInitial registration\"]\n        UpdateLoRA[\"update_lora(dict, strength)\u003cbr/\u003eDynamic update\"]\n        RemoveLoRA[\"remove_lora()\u003cbr/\u003eRemove modifications\"]\n    end\n    \n    subgraph \"Device Management\"\n        ToCPU[\"to_cpu(non_blocking)\u003cbr/\u003eMove to CPU\"]\n        ToCUDA[\"to_cuda(non_blocking)\u003cbr/\u003eMove to GPU\"]\n        ToCPUAsync[\"to_cpu_async()\u003cbr/\u003eAsync transfer\"]\n        ToCUDAAsync[\"to_cuda_async()\u003cbr/\u003eAsync transfer\"]\n    end\n    \n    subgraph \"Concrete Implementations\"\n        MMWeight[\"MMWeight\u003cbr/\u003eMemory-mapped weight\"]\n        MMWeightInt8[\"MMWeightInt8\u003cbr/\u003eQuantized weights\"]\n        MMWeightFP8[\"MMWeightFP8\u003cbr/\u003eFP8 weights\"]\n    end\n    \n    WeightModule --\u003e WeightModuleList\n    WeightModule --\u003e Load\n    WeightModule --\u003e StateDict\n    WeightModule --\u003e LoadStateDict\n    WeightModule --\u003e LoadFromDisk\n    \n    WeightModule --\u003e RegisterLoRA\n    WeightModule --\u003e UpdateLoRA\n    WeightModule --\u003e RemoveLoRA\n    \n    WeightModule --\u003e ToCPU\n    WeightModule --\u003e ToCUDA\n    WeightModule --\u003e ToCPUAsync\n    WeightModule --\u003e ToCUDAAsync\n    \n    WeightModule -.Implements.-\u003e MMWeight\n    WeightModule -.Implements.-\u003e MMWeightInt8\n    WeightModule -.Implements.-\u003e MMWeightFP8\n```\n\n**Module Registration:**\n\nWeights are organized hierarchically using `add_module()` and `register_parameter()` [lightx2v/common/modules/weight_module.py:9-15]():\n```python\nclass WeightModule:\n    def __init__(self):\n        self._modules = {}    # Child modules\n        self._parameters = {}  # Weight parameters\n    \n    def add_module(self, name, module):\n        self._modules[name] = module\n        setattr(self, name, module)  # Enable attribute access\n    \n    def register_parameter(self, name, param):\n        self._parameters[name] = param\n        setattr(self, name, param)\n```\n\n**Recursive Operations:**\n\nAll operations recursively traverse the module tree [lightx2v/common/modules/weight_module.py:17-24]():\n```python\ndef load(self, weight_dict):\n    \"\"\"Load weights from dictionary\"\"\"\n    for _, module in self._modules.items():\n        if hasattr(module, \"load\"):\n            module.load(weight_dict)\n    \n    for _, parameter in self._parameters.items():\n        if hasattr(parameter, \"load\"):\n            parameter.load(weight_dict)\n```\n\n**Sources:** [lightx2v/common/modules/weight_module.py:1-212]()\n\n### LoRA Dynamic Application\n\nThe weight system supports three LoRA operations: `register_lora()` for initial setup, `update_lora()` for dynamic switching, and `remove_lora()` for cleanup.\n\n**LoRA Lifecycle:**\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Runner\n    participant Model as WeightModule\n    participant Linear as MMWeight\n    \n    Note over User,Linear: Initial LoRA Registration\n    User-\u003e\u003eRunner: enable_lora(lora_configs)\n    Runner-\u003e\u003eModel: register_lora(lora_dict, strength=1.0)\n    Model-\u003e\u003eLinear: register_lora(lora_dict, 1.0)\n    Linear-\u003e\u003eLinear: Store lora_up, lora_down\u003cbr/\u003eCompute delta = up @ down\n    Linear-\u003e\u003eLinear: Store original weight\n    \n    Note over User,Linear: Dynamic LoRA Update\n    User-\u003e\u003eRunner: switch_lora(new_path, strength=0.8)\n    Runner-\u003e\u003eModel: update_lora(new_lora_dict, 0.8)\n    Model-\u003e\u003eLinear: update_lora(new_lora_dict, 0.8)\n    Linear-\u003e\u003eLinear: Restore original weight\n    Linear-\u003e\u003eLinear: Load new lora_up, lora_down\n    Linear-\u003e\u003eLinear: Apply with new strength\n    \n    Note over User,Linear: LoRA Removal\n    User-\u003e\u003eRunner: switch_lora(\"\", 0.0)\n    Runner-\u003e\u003eModel: remove_lora()\n    Model-\u003e\u003eLinear: remove_lora()\n    Linear-\u003e\u003eLinear: Restore original weight\n    Linear-\u003e\u003eLinear: Clear lora buffers\n```\n\n**LoRA Registration:**\n\nInitial LoRA setup during model initialization [lightx2v/common/modules/weight_module.py:35-42]():\n```python\ndef register_lora(self, weight_dict, strength):\n    \"\"\"Register LoRA weights for initial application\"\"\"\n    for _, module in self._modules.items():\n        if hasattr(module, \"register_lora\"):\n            module.register_lora(weight_dict, strength)\n    \n    for _, parameter in self._parameters.items():\n        if hasattr(parameter, \"register_lora\"):\n            parameter.register_lora(weight_dict, strength)\n```\n\n**LoRA Update:**\n\nDynamic switching without model reload [lightx2v/common/modules/weight_module.py:44-51]():\n```python\ndef update_lora(self, weight_dict, strength):\n    \"\"\"Update LoRA weights at runtime\"\"\"\n    for _, module in self._modules.items():\n        if hasattr(module, \"update_lora\"):\n            module.update_lora(weight_dict, strength)\n    \n    for _, parameter in self._parameters.items():\n        if hasattr(parameter, \"update_lora\"):\n            parameter.update_lora(weight_dict, strength)\n```\n\n**LoRA Removal:**\n\nClean removal of LoRA modifications [lightx2v/common/modules/weight_module.py:53-60]():\n```python\ndef remove_lora(self):\n    \"\"\"Remove all LoRA modifications\"\"\"\n    for _, module in self._modules.items():\n        if hasattr(module, \"remove_lora\"):\n            module.remove_lora()\n    \n    for _, parameter in self._parameters.items():\n        if hasattr(parameter, \"remove_lora\"):\n            parameter.remove_lora()\n```\n\n**MMWeight LoRA Implementation:**\n\nThe concrete `MMWeight` class implements the actual LoRA delta computation and application:\n\n```python\n# Simplified implementation (actual in lightx2v/common/modules/weight/mmweight.py)\nclass MMWeight:\n    def register_lora(self, weight_dict, strength):\n        # Find matching lora_up and lora_down tensors\n        lora_up = weight_dict.get(f\"{self.name}.lora_up.weight\")\n        lora_down = weight_dict.get(f\"{self.name}.lora_down.weight\")\n        \n        if lora_up is not None and lora_down is not None:\n            # Store original weight\n            self.original_weight = self.weight.clone()\n            \n            # Compute and apply delta\n            delta = strength * (lora_up @ lora_down)\n            self.weight = self.original_weight + delta\n            \n            # Store for future updates\n            self.lora_up = lora_up\n            self.lora_down = lora_down\n            self.lora_strength = strength\n    \n    def update_lora(self, weight_dict, strength):\n        # Restore original weight\n        if hasattr(self, 'original_weight'):\n            self.weight = self.original_weight.clone()\n        \n        # Apply new LoRA\n        self.register_lora(weight_dict, strength)\n    \n    def remove_lora(self):\n        # Restore original weight\n        if hasattr(self, 'original_weight'):\n            self.weight = self.original_weight\n            del self.original_weight\n            del self.lora_up\n            del self.lora_down\n```\n\n**Sources:** [lightx2v/common/modules/weight_module.py:35-60]()\n\n### Lazy Loading and State Dict Operations\n\nThe weight system supports lazy loading from disk for memory-efficient model initialization, particularly useful with CPU offloading strategies.\n\n**State Dict Export:**\n\nExport weights to dictionary format [lightx2v/common/modules/weight_module.py:62-71]():\n```python\ndef state_dict(self, destination=None):\n    \"\"\"Export all weights to dictionary\"\"\"\n    if destination is None:\n        destination = {}\n    \n    # Export parameters first\n    for _, param in self._parameters.items():\n        if param is not None:\n            param.state_dict(destination)\n    \n    # Then export modules\n    for _, module in self._modules.items():\n        if module is not None:\n            module.state_dict(destination)\n    \n    return destination\n```\n\n**Block-wise Loading:**\n\nLoad specific transformer blocks by index [lightx2v/common/modules/weight_module.py:73-82]():\n```python\ndef load_state_dict(self, destination, block_index, adapter_block_index=None):\n    \"\"\"Load state dict for specific block\"\"\"\n    if destination is None:\n        destination = {}\n    \n    for _, param in self._parameters.items():\n        if param is not None:\n            param.load_state_dict(destination, block_index, adapter_block_index)\n    \n    for _, module in self._modules.items():\n        if module is not None:\n            module.load_state_dict(destination, block_index, adapter_block_index)\n    \n    return destination\n```\n\n**Lazy Loading from Disk:**\n\nLoad weights on-demand for memory efficiency [lightx2v/common/modules/weight_module.py:84-90]():\n```python\ndef load_state_dict_from_disk(self, block_index, adapter_block_index=None):\n    \"\"\"Lazy load weights from safetensors files\"\"\"\n    for _, param in self._parameters.items():\n        if param is not None:\n            param.load_state_dict_from_disk(block_index, adapter_block_index)\n    \n    for _, module in self._modules.items():\n        if module is not None:\n            module.load_state_dict_from_disk(block_index, adapter_block_index)\n```\n\nThis enables the lazy loading offload strategy where weights are streamed from disk  CPU  GPU only when needed for computation, minimizing GPU memory usage.\n\n**Device Placement:**\n\nThe weight system provides synchronous and asynchronous device transfers [lightx2v/common/modules/weight_module.py:100-184]():\n\n| Method | Blocking | Use Case |\n|--------|----------|----------|\n| `to_cpu()` | Yes | Simple offloading |\n| `to_cuda()` | Yes | Simple loading |\n| `to_cpu_async()` | No | Pipeline overlap with computation |\n| `to_cuda_async()` | No | Prefetch for next block |\n\n**Sources:** [lightx2v/common/modules/weight_module.py:62-212]()\n\n---\n\n## Debugging and Profiling\n\nLightX2V includes built-in profiling infrastructure for performance monitoring and debugging. The system tracks execution time at multiple granularities and provides metrics for optimization analysis.\n\n### Profiling System Architecture\n\nThe profiling system uses context managers to measure execution time for different components:\n\n```mermaid\ngraph TB\n    subgraph \"Profiling Levels\"\n        Level0[\"PROFILING_DEBUG_LEVEL=0\u003cbr/\u003eDisabled\"]\n        Level1[\"PROFILING_DEBUG_LEVEL=1\u003cbr/\u003ePhase timing\"]\n        Level2[\"PROFILING_DEBUG_LEVEL=2\u003cbr/\u003eBlock-level timing\"]\n    end\n    \n    subgraph \"Context Managers\"\n        ProfilingL1[\"ProfilingContext4DebugL1\u003cbr/\u003ePre-infer, Transformer, Post-infer\"]\n        ProfilingL2[\"ProfilingContext4DebugL2\u003cbr/\u003ePer-block timing\"]\n    end\n    \n    subgraph \"Metrics Collection\"\n        StartTime[\"record_start_time()\"]\n        EndTime[\"record_end_time()\"]\n        ExcludedTime[\"record_excluded_time()\u003cbr/\u003eOffload overhead\"]\n        MetricsIntegration[\"Metrics integration\u003cbr/\u003ePrometheus/Custom\"]\n    end\n    \n    subgraph \"Output\"\n        LogOutput[\"Logger output\u003cbr/\u003eTime breakdown\"]\n        MetricsExport[\"Metrics export\u003cbr/\u003ePerformance monitoring\"]\n    end\n    \n    Level1 --\u003e ProfilingL1\n    Level2 --\u003e ProfilingL2\n    \n    ProfilingL1 --\u003e StartTime\n    ProfilingL1 --\u003e EndTime\n    ProfilingL2 --\u003e StartTime\n    ProfilingL2 --\u003e EndTime\n    \n    StartTime --\u003e ExcludedTime\n    EndTime --\u003e ExcludedTime\n    \n    ExcludedTime --\u003e LogOutput\n    ExcludedTime --\u003e MetricsExport\n    \n    ProfilingL1 --\u003e MetricsIntegration\n    ProfilingL2 --\u003e MetricsIntegration\n```\n\n**Environment Configuration:**\n\nSet profiling level via environment variable [lightx2v/pipeline.py:4]():\n```bash\nexport PROFILING_DEBUG_LEVEL=2\n```\n\n**Profiling Context Usage:**\n\n```python\nfrom lightx2v.utils.profiling import ProfilingContext4DebugL1, ProfilingContext4DebugL2\n\n# Phase-level profiling\nwith ProfilingContext4DebugL1(\"pre_infer\"):\n    # Pre-inference operations (encoding, latent preparation)\n    text_embeddings = self.encode_text(prompt)\n    latents = self.prepare_latents(shape, seed)\n\nwith ProfilingContext4DebugL1(\"transformer_infer\"):\n    # Block-level profiling within transformer\n    for i, block in enumerate(self.transformer_blocks):\n        with ProfilingContext4DebugL2(f\"block_{i}\"):\n            latents = block(latents, text_embeddings, timestep)\n\nwith ProfilingContext4DebugL1(\"post_infer\"):\n    # Post-inference operations (VAE decode)\n    frames = self.vae.decode(latents)\n```\n\n**Excluded Time Tracking:**\n\nThe profiling system can exclude overhead from measurements (e.g., CPU offloading time):\n\n```python\nwith ProfilingContext4DebugL2(\"block_5\"):\n    # Measured time\n    \n    # Exclude offload overhead from measurement\n    with excluded_time_context():\n        block.to_cuda_async()  # Not counted in block time\n    \n    # Measured time continues\n    output = block(input)\n```\n\n**Performance Metrics:**\n\nThe profiling system can integrate with custom metrics collection:\n- Iteration time (s/iter)\n- Block execution time\n- Phase breakdown (pre/transformer/post percentages)\n- Memory usage correlation\n\n**Sources:** [lightx2v/pipeline.py:4]()\n\n---\n\n## Additional Developer Utilities\n\n### Model Registry Pattern\n\nThe `RUNNER_REGISTER` factory enables extensible runner registration:\n\n```python\nfrom lightx2v.utils.registry_factory import RUNNER_REGISTER\n\n@RUNNER_REGISTER(\"custom_model\")\nclass CustomModelRunner(DefaultRunner):\n    def __init__(self, config):\n        super().__init__(config)\n    \n    def load_transformer(self):\n        # Custom transformer loading\n        pass\n    \n    def load_text_encoder(self):\n        # Custom text encoder\n        pass\n    \n    def run_pipeline(self, input_info):\n        # Custom pipeline logic\n        pass\n```\n\nThen use via:\n```python\npipeline = LightX2VPipeline(\n    task=\"t2v\",\n    model_path=\"/path/to/model\",\n    model_cls=\"custom_model\"\n)\n```\n\n**Sources:** [lightx2v/utils/registry_factory.py](), [lightx2v/pipeline.py:29-32]()\n\n### Quantization Plugin Development\n\nDevelopers can add custom quantization schemes:\n\n```python\nfrom lightx2v.utils.registry_factory import CONVERT_WEIGHT_REGISTER\nfrom tools.convert.quant import QuantTemplate\n\n@CONVERT_WEIGHT_REGISTER(\"custom_quant\")\nclass QuantWeightCustom(QuantTemplate):\n    def __init__(self, weight):\n        super().__init__(weight)\n        self.weight_quant_func = self.load_custom_weight\n    \n    @torch.no_grad()\n    def load_custom_weight(self, w, comfyui_mode=False):\n        # Custom quantization logic\n        w_q, scales = custom_quantize(w)\n        return w_q, scales, self.extra\n```\n\nThen use with:\n```bash\npython tools/convert/converter.py \\\n    --linear_type custom_quant \\\n    --quantized\n```\n\n**Sources:** [tools/convert/quant/quant.py:1-142](), [lightx2v/utils/registry_factory.py]()\n\n### Configuration File Templates\n\nDevelopers should structure config JSON files following this pattern:\n\n```json\n{\n  \"model_cls\": \"wan2.1\",\n  \"task\": \"i2v\",\n  \"model_path\": \"/path/to/model\",\n  \n  \"infer_steps\": 50,\n  \"target_height\": 480,\n  \"target_width\": 832,\n  \"guidance_scale\": 5.0,\n  \n  \"dit_quantized\": true,\n  \"dit_quant_scheme\": \"fp8-sgl\",\n  \"cpu_offload\": true,\n  \"offload_granularity\": \"block\",\n  \n  \"t5_quantized\": true,\n  \"clip_quantized\": false,\n  \n  \"self_attn_1_type\": \"sage_attn\",\n  \"cross_attn_1_type\": \"flash_attn2\",\n  \n  \"lora_configs\": [],\n  \"lora_dynamic_apply\": false\n}\n```\n\n**Sources:** [lightx2v/pipeline.py:239-243]()"])</script><script>self.__next_f.push([1,"49:T6ed1,"])</script><script>self.__next_f.push([1,"# Model Conversion and Quantization Tools\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/utils/global_paras.py](lightx2v/utils/global_paras.py)\n- [tools/convert/converter.py](tools/convert/converter.py)\n- [tools/convert/quant/__init__.py](tools/convert/quant/__init__.py)\n- [tools/convert/quant/quant.py](tools/convert/quant/quant.py)\n- [tools/convert/quant_adapter.py](tools/convert/quant_adapter.py)\n- [tools/convert/readme.md](tools/convert/readme.md)\n- [tools/convert/readme_zh.md](tools/convert/readme_zh.md)\n\n\u003c/details\u003e\n\n\n\nThis document covers the model weight conversion and quantization toolkit located in [tools/convert/](), which provides offline weight manipulation capabilities for LightX2V models. This includes format conversion, architecture conversion between LightX2V and Diffusers, weight quantization to reduce model size, and LoRA merging. For runtime quantization and inference-time optimization, see [Quantization System](#6.1). For LoRA loading and switching during inference, see [LoRA Dynamic Application](#6.6).\n\n## Overview and Purpose\n\nThe conversion toolkit consists of three primary components:\n\n1. **Main Converter** ([tools/convert/converter.py]()): Multi-purpose command-line tool for weight manipulation\n2. **Quantization Implementations** ([tools/convert/quant/quant.py]()): Per-precision quantization classes registered with `CONVERT_WEIGHT_REGISTER`\n3. **Adapter Quantizer** ([tools/convert/quant_adapter.py]()): Specialized tool for quantizing audio adapters\n\nThe converter operates offline on saved model weights, producing modified weight files that can be loaded by LightX2V runners. This differs from runtime optimizations which are applied dynamically during inference.\n\n## Converter Tool Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Entry Point\"\n        CLI[\"main()\u003cbr/\u003etools/convert/converter.py:727\"]\n        ArgParser[\"argparse.ArgumentParser\u003cbr/\u003eCommand-line arguments\"]\n    end\n    \n    subgraph \"Core Conversion Pipeline\"\n        ConvertWeights[\"convert_weights()\u003cbr/\u003etools/convert/converter.py:462\"]\n        LoadWeights[\"Load weights\u003cbr/\u003etorch.load or safetensors.safe_open\"]\n        KeyConvert[\"Key conversion\u003cbr/\u003eget_key_mapping_rules()\"]\n        LoRAApply[\"LoRA application\u003cbr/\u003eload_loras()\"]\n        Quantize[\"Quantization\u003cbr/\u003equantize_model()\"]\n        SaveWeights[\"Save weights\u003cbr/\u003esafetensors.save_file or torch.save\"]\n    end\n    \n    subgraph \"Key Mapping System\"\n        GetRules[\"get_key_mapping_rules()\u003cbr/\u003etools/convert/converter.py:36\"]\n        WanDitRules[\"wan_dit rules\u003cbr/\u003eunified_rules list\"]\n        Direction[\"direction: forward/backward\u003cbr/\u003eLightX2V  Diffusers\"]\n    end\n    \n    subgraph \"Quantization Registry\"\n        QuantRegister[\"CONVERT_WEIGHT_REGISTER\u003cbr/\u003elightx2v.utils.registry_factory\"]\n        INT8Class[\"QuantWeightINT8\u003cbr/\u003etools/convert/quant/quant.py:26\"]\n        FP8Class[\"QuantWeightFP8\u003cbr/\u003etools/convert/quant/quant.py:52\"]\n        NVFP4Class[\"QuantWeightNVFP4\u003cbr/\u003etools/convert/quant/quant.py:127\"]\n        MXFP4Class[\"QuantWeightMxFP4\u003cbr/\u003etools/convert/quant/quant.py:82\"]\n        MXFP6Class[\"QuantWeightMxFP6\u003cbr/\u003etools/convert/quant/quant.py:97\"]\n        MXFP8Class[\"QuantWeightMxFP8\u003cbr/\u003etools/convert/quant/quant.py:112\"]\n    end\n    \n    subgraph \"LoRA System\"\n        LoRALoader[\"LoRALoader\u003cbr/\u003elightx2v.utils.lora_loader\"]\n        DetectFormat[\"Auto-detect format\u003cbr/\u003eStandard/Diffusers/Mochi/etc\"]\n        ApplyDelta[\"Apply LoRA deltas\u003cbr/\u003eW + alpha * strength * BA\"]\n    end\n    \n    CLI --\u003e ArgParser\n    ArgParser --\u003e ConvertWeights\n    ConvertWeights --\u003e LoadWeights\n    LoadWeights --\u003e KeyConvert\n    KeyConvert --\u003e LoRAApply\n    LoRAApply --\u003e Quantize\n    Quantize --\u003e SaveWeights\n    \n    KeyConvert --\u003e GetRules\n    GetRules --\u003e WanDitRules\n    GetRules --\u003e Direction\n    \n    Quantize --\u003e QuantRegister\n    QuantRegister --\u003e INT8Class\n    QuantRegister --\u003e FP8Class\n    QuantRegister --\u003e NVFP4Class\n    QuantRegister --\u003e MXFP4Class\n    QuantRegister --\u003e MXFP6Class\n    QuantRegister --\u003e MXFP8Class\n    \n    LoRAApply --\u003e LoRALoader\n    LoRALoader --\u003e DetectFormat\n    DetectFormat --\u003e ApplyDelta\n```\n\n**Converter Tool Architecture**: The converter follows a linear pipeline where weights are loaded, keys optionally converted between architectures, LoRAs merged, quantization applied, and results saved. Each stage is optional and controlled by command-line arguments.\n\nSources: [tools/convert/converter.py:1-884](), [tools/convert/quant/quant.py:1-142]()\n\n## Supported Model Types\n\nThe converter supports multiple model architectures through the `--model_type` parameter, each with specific key conversion rules and quantization targets:\n\n| Model Type | Description | Key Index | Target Keys for Quantization | Ignore Keys |\n|------------|-------------|-----------|------------------------------|-------------|\n| `wan_dit` | WAN DiT models (default) | 2 | `self_attn`, `cross_attn`, `ffn` | `ca`, `audio` |\n| `wan_animate_dit` | WAN Animate models | 2 | `self_attn`, `cross_attn`, `ffn` | None |\n| `qwen_image_dit` | Qwen Image DiT | 2 | `attn`, `img_mlp`, `txt_mlp`, `txt_mod`, `img_mod` | None |\n| `z_image_dit` | Z-Image models | 2 | `attention`, `feed_forward`, `adaLN_modulation`, `linear` | None |\n| `hunyuan_dit` | Hunyuan Video DiT 1.5 | 2 | `img_mod`, `img_attn_*`, `img_mlp`, `txt_mod`, `txt_attn_*`, `txt_mlp` | None |\n| `wan_t5` | T5 text encoder | 2 | `attn`, `ffn` | None |\n| `wan_clip` | CLIP vision encoder | 3 | `attn`, `mlp` | `textual` |\n| `qwen25vl_llm` | Qwen2.5-VL LLM | 3 | `self_attn`, `mlp` | `visual` |\n\nThe `key_idx` column indicates which position in the split key path is checked to determine if a layer should be quantized. The `target_keys` list defines module names that will be quantized when `--quantized` is enabled.\n\nSources: [tools/convert/converter.py:817-873]()\n\n## Format Conversion\n\n### PyTorch and SafeTensors Conversion\n\nThe converter handles loading from and saving to both `.pth` (PyTorch native) and `.safetensors` formats. SafeTensors provides faster loading and better memory safety.\n\n**Loading Weights**:\n```python\n# PyTorch format (.pth, .pt)\nweights = torch.load(file_path, map_location=args.device, weights_only=True)\n\n# SafeTensors format (.safetensors) with lazy loading\nwith safe_open(file_path, framework=\"pt\", device=args.device) as f:\n    weights = {k: f.get_tensor(k) for k in f.keys()}\n```\n\nThe loader automatically detects file format by extension and uses lazy loading for large SafeTensors files to reduce memory consumption [tools/convert/converter.py:474-492]().\n\n**Saving Strategies**:\n\nThree saving modes are available, controlled by command-line flags:\n\n| Mode | Flag | Use Case | Output Structure |\n|------|------|----------|------------------|\n| Single file | `--single_file` | Small models, easy distribution | `{output_name}.safetensors` |\n| By block | `--save_by_block` | Backward conversion, organized structure | `block_{N}.safetensors`, `non_block.safetensors` |\n| Chunked | `--chunk-size N` | Forward conversion, memory-efficient | `{output_name}_part{N}.safetensors` + index.json |\n\n**Single File Saving** [tools/convert/converter.py:611-636]():\n```bash\npython converter.py \\\n    --source /path/to/model/ \\\n    --output /path/to/output \\\n    --output_name model \\\n    --single_file\n```\nLoads entire model into memory and saves as one file. Warning issued if model exceeds 10GB.\n\n**Block-Based Saving** [tools/convert/converter.py:637-667]():\n```bash\npython converter.py \\\n    --source /path/to/model/ \\\n    --output /path/to/output \\\n    --save_by_block\n```\nGroups weights by transformer block index using regex pattern `blocks\\.(\\d+)\\.`. Recommended for backward conversion (Diffusers  LightX2V) as it matches the original file structure.\n\n**Chunked Saving** [tools/convert/converter.py:669-692]():\n```bash\npython converter.py \\\n    --source /path/to/model/ \\\n    --output /path/to/output \\\n    --chunk-size 100\n```\nSaves every N weights to a new file and generates `diffusion_pytorch_model.safetensors.index.json` mapping keys to files. This is the default for forward conversion.\n\nSources: [tools/convert/converter.py:462-703]()\n\n## Architecture Conversion: LightX2V  Diffusers\n\n### Key Mapping System\n\nThe converter implements bidirectional key mapping between LightX2V's internal architecture and the Diffusers library format. This enables interoperability with the broader Diffusers ecosystem.\n\n```mermaid\ngraph LR\n    subgraph \"LightX2V Format\"\n        L_Head[\"head.head\u003cbr/\u003ehead.modulation\"]\n        L_Blocks[\"blocks.N.self_attn.q/k/v/o\u003cbr/\u003eblocks.N.cross_attn.q/k/v/o\u003cbr/\u003eblocks.N.ffn.0/2\u003cbr/\u003eblocks.N.modulation\"]\n        L_Embed[\"text_embedding.0/2\u003cbr/\u003etime_embedding.0/2\u003cbr/\u003etime_projection.1\"]\n        L_Img[\"img_emb.proj.0/1/3/4\"]\n    end\n    \n    subgraph \"Diffusers Format\"\n        D_Head[\"proj_out\u003cbr/\u003escale_shift_table\"]\n        D_Blocks[\"blocks.N.attn1.to_q/to_k/to_v/to_out.0\u003cbr/\u003eblocks.N.attn2.to_q/to_k/to_v/to_out.0\u003cbr/\u003eblocks.N.ffn.net.0.proj/net.2\u003cbr/\u003eblocks.N.scale_shift_table\"]\n        D_Embed[\"condition_embedder.text_embedder.linear_1/2\u003cbr/\u003econdition_embedder.time_embedder.linear_1/2\u003cbr/\u003econdition_embedder.time_proj\"]\n        D_Img[\"condition_embedder.image_embedder.norm1/ff/norm2\"]\n    end\n    \n    L_Head --\u003e|forward| D_Head\n    D_Head --\u003e|backward| L_Head\n    \n    L_Blocks --\u003e|forward| D_Blocks\n    D_Blocks --\u003e|backward| L_Blocks\n    \n    L_Embed --\u003e|forward| D_Embed\n    D_Embed --\u003e|backward| L_Embed\n    \n    L_Img --\u003e|forward| D_Img\n    D_Img --\u003e|backward| L_Img\n```\n\n**Key Mapping Between LightX2V and Diffusers Architectures**: The converter applies regex-based transformations to translate weight keys between the two formats. The `direction` parameter controls the mapping direction.\n\n### Key Mapping Rules\n\nThe key mapping is defined by a list of unified rules with forward and backward patterns [tools/convert/converter.py:36-311]():\n\n**Example rules for wan_dit**:\n```python\n{\n    \"forward\": (r\"^head\\.head$\", \"proj_out\"),\n    \"backward\": (r\"^proj_out$\", \"head.head\"),\n},\n{\n    \"forward\": (r\"blocks\\.(\\d+)\\.self_attn\\.q\\.\", r\"blocks.\\1.attn1.to_q.\"),\n    \"backward\": (r\"blocks\\.(\\d+)\\.attn1\\.to_q\\.\", r\"blocks.\\1.self_attn.q.\"),\n}\n```\n\nEach rule is a regex pattern-replacement pair. The `\\1` syntax captures block indices to maintain structural mapping.\n\n**Key Conversion Process**:\n\n1. Pre-compile all regex patterns for performance [tools/convert/converter.py:512]()\n2. For each weight key, apply all rules in sequence [tools/convert/converter.py:514-519]()\n3. For large models (\u003e1000 keys), use parallel processing with `ThreadPoolExecutor` [tools/convert/converter.py:525-538]()\n\n```bash\n# LightX2V  Diffusers\npython converter.py \\\n    --source /path/to/Wan2.1-I2V-14B-480P \\\n    --output /path/to/Wan2.1-I2V-14B-480P-Diffusers \\\n    --model_type wan_dit \\\n    --direction forward \\\n    --chunk-size 100\n\n# Diffusers  LightX2V\npython converter.py \\\n    --source /path/to/Wan2.1-I2V-14B-480P-Diffusers \\\n    --output /path/to/Wan2.1-I2V-14B-480P \\\n    --model_type wan_dit \\\n    --direction backward \\\n    --save_by_block\n```\n\nSources: [tools/convert/converter.py:36-311](), [tools/convert/converter.py:506-545]()\n\n## Quantization System\n\n### Quantization Overview\n\n```mermaid\ngraph TB\n    subgraph \"Quantization Entry Point\"\n        QuantModel[\"quantize_model()\u003cbr/\u003etools/convert/converter.py:314\"]\n        Args[\"target_keys, adapter_keys\u003cbr/\u003ekey_idx, linear_type\u003cbr/\u003enon_linear_dtype\"]\n    end\n    \n    subgraph \"Weight Processing Loop\"\n        CheckKey[\"Check key matches\u003cbr/\u003etarget_keys or adapter_keys\"]\n        Skip[\"Skip non-2D tensors\u003cbr/\u003eConvert to non_linear_dtype\"]\n        Quantizer[\"Get quantizer from registry\u003cbr/\u003eCONVERT_WEIGHT_REGISTER[linear_type]\"]\n    end\n    \n    subgraph \"Quantization Classes\"\n        INT8[\"QuantWeightINT8\u003cbr/\u003ePer-channel absmax\u003cbr/\u003etorch.int8\"]\n        FP8[\"QuantWeightFP8\u003cbr/\u003ePer-channel E4M3\u003cbr/\u003etorch.float8_e4m3fn\"]\n        NVFP4[\"QuantWeightNVFP4\u003cbr/\u003eBlock FP4\u003cbr/\u003eRequires lightx2v_kernel\"]\n        MXFP[\"QuantWeightMxFP4/6/8\u003cbr/\u003eMX format\u003cbr/\u003eRequires lightx2v_kernel\"]\n    end\n    \n    subgraph \"Output\"\n        QuantWeight[\"w_q: Quantized weight\"]\n        Scales[\"scales: Per-channel/block\u003cbr/\u003eor global scale\"]\n        GlobalScale[\"weight_global_scale\u003cbr/\u003e(NVFP4 only)\"]\n        Stats[\"Quantization statistics\u003cbr/\u003eSize reduction, count\"]\n    end\n    \n    QuantModel --\u003e Args\n    QuantModel --\u003e CheckKey\n    CheckKey --\u003e|Match| Quantizer\n    CheckKey --\u003e|No match| Skip\n    \n    Quantizer --\u003e INT8\n    Quantizer --\u003e FP8\n    Quantizer --\u003e NVFP4\n    Quantizer --\u003e MXFP\n    \n    INT8 --\u003e QuantWeight\n    FP8 --\u003e QuantWeight\n    NVFP4 --\u003e QuantWeight\n    MXFP --\u003e QuantWeight\n    \n    QuantWeight --\u003e Scales\n    NVFP4 --\u003e GlobalScale\n    QuantModel --\u003e Stats\n```\n\n**Quantization Pipeline**: Each 2D weight tensor matching the target criteria is passed through a quantizer class that returns quantized weights, per-channel/block scales, and optional global scales.\n\n### Quantization Formats\n\nThe converter supports six quantization formats, each with different precision-memory tradeoffs:\n\n#### INT8 Quantization\n\n**Implementation**: `QuantWeightINT8` [tools/convert/quant/quant.py:26-49]()\n\n**Algorithm**:\n- Per-channel absmax quantization\n- Scale: `max_val / 127` where `max_val = abs(w).amax(dim=1)`\n- Quantized value: `clamp(round(w / scale), -128, 127)`\n- Output dtype: `torch.int8`\n\n**Memory**: ~4x reduction (float32  int8, plus per-channel scales)\n\n**Usage**:\n```bash\npython converter.py \\\n    --source /path/to/model \\\n    --output /path/to/output \\\n    --quantized \\\n    --linear_type int8 \\\n    --model_type wan_dit\n```\n\n#### FP8 Quantization\n\n**Implementation**: `QuantWeightFP8` [tools/convert/quant/quant.py:52-79]()\n\n**Algorithm**:\n- Per-channel E4M3 quantization\n- Scale: `max_val / finfo.max` where `finfo = torch.finfo(torch.float8_e4m3fn)`\n- Uses `float_quantize` from qtorch for proper FP8 rounding\n- Output dtype: `torch.float8_e4m3fn`\n\n**Memory**: ~4x reduction with potentially better quality than INT8\n\n**Usage**:\n```bash\npython converter.py \\\n    --source /path/to/model \\\n    --output /path/to/output \\\n    --quantized \\\n    --linear_type fp8 \\\n    --non_linear_dtype torch.bfloat16 \\\n    --model_type wan_dit\n```\n\n#### NVFP4 Quantization\n\n**Implementation**: `QuantWeightNVFP4` [tools/convert/quant/quant.py:127-141]()\n\n**Algorithm**:\n- Block-wise FP4 quantization using custom kernels\n- Global scale: `2688.0 / max(abs(w))` for entire tensor\n- Calls `scaled_nvfp4_quant()` from `lightx2v_kernel.gemm`\n- Returns quantized weights + per-block scales + global scale\n\n**Requirements**: Install `lightx2v_kernel` following [lightx2v_kernel/README.md]()\n\n**Memory**: ~8x reduction (most aggressive)\n\n**Usage**:\n```bash\npython converter.py \\\n    --source /path/to/model \\\n    --output /path/to/output \\\n    --quantized \\\n    --linear_type nvfp4 \\\n    --model_type wan_dit\n```\n\n#### MX Format Quantization (MXFP4/6/8)\n\n**Implementations**: \n- `QuantWeightMxFP4` [tools/convert/quant/quant.py:82-94]()\n- `QuantWeightMxFP6` [tools/convert/quant/quant.py:97-109]()\n- `QuantWeightMxFP8` [tools/convert/quant/quant.py:112-124]()\n\n**Algorithm**:\n- Microscaling (MX) format from Microsoft\n- Block-wise quantization with shared exponent per block\n- Calls `scaled_mxfp{4,6,8}_quant()` from `lightx2v_kernel.gemm`\n- Input must be `torch.bfloat16`\n\n**Requirements**: Install `lightx2v_kernel` following [lightx2v_kernel/README.md]()\n\n**Memory**: \n- MXFP4: ~8x reduction\n- MXFP6: ~5.3x reduction\n- MXFP8: ~4x reduction\n\n**Usage**:\n```bash\npython converter.py \\\n    --source /path/to/model \\\n    --output /path/to/output \\\n    --quantized \\\n    --linear_type mxfp6 \\\n    --model_type wan_dit\n```\n\nSources: [tools/convert/quant/quant.py:1-142]()\n\n### Quantization Output Format\n\n**Standard Mode** (for LightX2V):\n```\noriginal_key: quantized_weight\noriginal_key_scale: per_channel_scales\noriginal_key_global_scale: global_scale  # NVFP4 only\n```\n\n**ComfyUI Mode** (`--comfyui_mode`):\n```\noriginal_key: quantized_weight\noriginal_key.replace('.weight', '.scale_weight'): per_channel_scales\nscaled_fp8: torch.zeros(2, dtype=torch.float8_e4m3fn)  # marker tensor\n```\n\nThe ComfyUI mode alters the scale key naming to match ComfyUI's expected format and adds a marker tensor [tools/convert/converter.py:427-429]().\n\n**Full Quantization Mode** (`--comfyui_mode --full_quantized`):\nQuantizes all tensors to the specified dtype without per-channel scaling, useful for full-model FP8 in ComfyUI [tools/convert/converter.py:585-589]().\n\n### Quantization Statistics\n\nAfter quantization, the converter reports [tools/convert/converter.py:420-425]():\n- Number of tensors quantized\n- Original size of quantized tensors (MB)\n- Size after quantization including scales (MB)\n- Size of non-quantized tensors (MB)\n- Total final model size (MB)\n- Size reduction percentage\n\nSources: [tools/convert/converter.py:314-430]()\n\n## LoRA Merging\n\n### LoRA Loading and Application\n\nThe converter uses `LoRALoader` from [lightx2v.utils.lora_loader]() to handle multiple LoRA formats and apply them to base model weights [tools/convert/converter.py:433-459]().\n\n**Supported LoRA Formats**:\n1. **Standard**: `{key}.lora_up.weight`, `{key}.lora_down.weight`\n2. **Diffusers**: `{key}_lora.up.weight`, `{key}_lora.down.weight`\n3. **Diffusers V2**: `{key}.lora_B.weight`, `{key}.lora_A.weight`\n4. **Diffusers V3**: `{key}.lora.up.weight`, `{key}.lora.down.weight`\n5. **Mochi**: `{key}.lora_B`, `{key}.lora_A` (no `.weight` suffix)\n6. **Transformers**: `{key}.lora_linear_layer.up.weight`, `{key}.lora_linear_layer.down.weight`\n7. **Qwen**: `{key}.lora_B.default.weight`, `{key}.lora_A.default.weight`\n8. **Diff formats**: `.diff`, `.diff_b`, `.diff_m` (direct weight differences)\n\n**Application Formula**:\n```\nW_new = W_base + strength * (alpha * lora_up @ lora_down)\n```\nWhere:\n- `alpha`: Scaling factor (default: uses LoRA's internal alpha if present)\n- `strength`: Additional multiplicative factor (default: 1.0)\n\n### LoRA Key Conversion\n\nWhen converting architectures, LoRA keys must also be converted to match the target format. The `--lora_key_convert` parameter controls this:\n\n| Mode | Behavior | Use Case |\n|------|----------|----------|\n| `auto` | Try with conversion first, fallback to original | Default, handles most cases |\n| `same` | Use original LoRA keys without conversion | LoRA already in target format |\n| `convert` | Apply same key conversion as model | LoRA in source format |\n\nThe auto mode attempts to match keys after applying conversion rules, and if no matches are found, tries without conversion [tools/convert/converter.py:564-582]().\n\n### Multiple LoRA Merging\n\nMultiple LoRAs can be merged sequentially with different strengths:\n\n```bash\npython converter.py \\\n    --source /path/to/base_model/ \\\n    --output /path/to/output \\\n    --lora_path /path/to/lora1.safetensors /path/to/lora2.safetensors \\\n    --lora_alpha 32.0 32.0 \\\n    --lora_strength 1.0 0.8 \\\n    --single_file\n```\n\n**Parameter Handling** [tools/convert/converter.py:549-561]():\n- If single alpha/strength provided with multiple LoRAs, it's replicated for all\n- Otherwise, number of alpha/strength values must match number of LoRA paths\n- LoRAs are applied in order, each modifying the accumulated weights\n\n### LoRA + Quantization Pipeline\n\nLoRA merging occurs **before** quantization in the pipeline [tools/convert/converter.py:548-583](), allowing merged weights to be quantized:\n\n```bash\npython converter.py \\\n    --source /path/to/base_model/ \\\n    --output /path/to/output \\\n    --lora_path /path/to/lora.safetensors \\\n    --lora_strength 1.0 \\\n    --quantized \\\n    --linear_type fp8 \\\n    --single_file\n```\n\nThis produces a single merged and quantized model file.\n\nSources: [tools/convert/converter.py:433-582](), [tools/convert/readme.md:83-99]()\n\n## Audio Adapter Quantization\n\nFor audio adapter models used in speech-to-video tasks, a specialized quantization tool is provided at [tools/convert/quant_adapter.py]().\n\n**Purpose**: Quantize audio adapter cross-attention weights while preserving other components in BF16.\n\n**Algorithm**:\n1. Load audio adapter safetensors file\n2. Identify weights matching pattern: `ca*.to*.weight`\n3. Quantize matched weights to FP8 using `FloatQuantizer` with per-channel scaling\n4. Convert non-matched weights to BF16\n5. Save with `_scale` suffixes for quantized weight scales\n\n**Usage**:\n```bash\npython tools/convert/quant_adapter.py \\\n    --model_path models/SekoTalk-Distill/audio_adapter_model.safetensors \\\n    --output_path models/SekoTalk-Distill-fp8/audio_adapter_model_fp8.safetensors\n```\n\n**Alternative Quantization Formats**:\nThe script includes commented code for MXFP4/6/8 quantization using `QuantWeightMxFP4/6/8` classes [tools/convert/quant_adapter.py:63-66]().\n\nSources: [tools/convert/quant_adapter.py:1-81]()\n\n## Command-Line Reference\n\n### Basic Arguments\n\n| Argument | Short | Type | Default | Description |\n|----------|-------|------|---------|-------------|\n| `--source` | `-s` | str | Required | Input path (file or directory) |\n| `--output` | `-o` | str | Required | Output directory path |\n| `--output_ext` | `-o_e` | str | `.safetensors` | Output format: `.pth` or `.safetensors` |\n| `--output_name` | `-o_n` | str | `converted` | Output file name prefix |\n| `--model_type` | `-t` | str | `wan_dit` | Model architecture type |\n| `--device` | | str | `cuda` | Device for processing: `cpu` or `cuda` |\n\n### Architecture Conversion Arguments\n\n| Argument | Short | Type | Default | Description |\n|----------|-------|------|---------|-------------|\n| `--direction` | `-d` | str | `None` | Conversion direction: `forward`, `backward`, or `None` |\n| `--chunk-size` | `-c` | int | 100 | Chunk size for chunked saving (0 = no chunking) |\n| `--save_by_block` | `-b` | flag | False | Save weights grouped by transformer block |\n| `--single_file` | | flag | False | Save as single file (high memory usage) |\n| `--copy_no_weight_files` | | flag | False | Copy non-weight files from source directory |\n| `--parallel` | | flag | True | Use parallel processing for key conversion |\n| `--no-parallel` | | flag | False | Disable parallel processing |\n\n### Quantization Arguments\n\n| Argument | Type | Default | Description |\n|----------|------|---------|-------------|\n| `--quantized` | flag | False | Enable quantization |\n| `--bits` | int | 8 | Quantization bit width (currently only 8) |\n| `--linear_type` | str | Required | Quantization format: `int8`, `fp8`, `nvfp4`, `mxfp4`, `mxfp6`, `mxfp8` |\n| `--non_linear_dtype` | str | `torch.float32` | Dtype for non-quantized weights: `torch.bfloat16`, `torch.float16`, `torch.float32` |\n| `--comfyui_mode` | flag | False | ComfyUI-compatible output format (int8/fp8 only) |\n| `--full_quantized` | flag | False | Quantize all tensors (ComfyUI mode only) |\n\n### LoRA Arguments\n\n| Argument | Type | Default | Description |\n|----------|------|---------|-------------|\n| `--lora_path` | str[] | None | Path(s) to LoRA file(s), space-separated for multiple |\n| `--lora_alpha` | float[] | None | Alpha value(s) for LoRA scaling, one per LoRA or single value |\n| `--lora_strength` | float[] | 1.0 | Strength multiplier(s) for LoRA deltas, one per LoRA or single value |\n| `--lora_key_convert` | str | `auto` | Key conversion mode: `auto`, `same`, or `convert` |\n\nSources: [tools/convert/converter.py:727-803]()\n\n## Usage Patterns\n\n### Pattern 1: Basic Quantization\n\nConvert a full-precision model to INT8:\n```bash\npython converter.py \\\n    --source /path/to/Wan2.1-I2V-14B-480P/ \\\n    --output /path/to/output \\\n    --model_type wan_dit \\\n    --quantized \\\n    --linear_type int8 \\\n    --save_by_block\n```\n\n**Result**: Multiple safetensors files (`block_0.safetensors`, `block_1.safetensors`, ..., `non_block.safetensors`) with INT8-quantized attention and FFN weights.\n\n### Pattern 2: Architecture Conversion with Quantization\n\nConvert to Diffusers format and quantize to FP8:\n```bash\npython converter.py \\\n    --source /path/to/Wan2.1-I2V-14B-480P \\\n    --output /path/to/output \\\n    --model_type wan_dit \\\n    --direction forward \\\n    --quantized \\\n    --linear_type fp8 \\\n    --non_linear_dtype torch.bfloat16 \\\n    --chunk-size 100\n```\n\n**Result**: Chunked safetensors files with FP8-quantized weights and Diffusers-compatible key names.\n\n### Pattern 3: Multi-LoRA Merge with Quantization\n\nMerge multiple LoRAs into base model and quantize:\n```bash\npython converter.py \\\n    --source /path/to/base_model/ \\\n    --output /path/to/output \\\n    --model_type wan_dit \\\n    --lora_path /path/to/style_lora.safetensors /path/to/motion_lora.safetensors \\\n    --lora_strength 1.0 0.8 \\\n    --quantized \\\n    --linear_type fp8 \\\n    --single_file\n```\n\n**Result**: Single safetensors file with both LoRAs merged and quantized to FP8.\n\n### Pattern 4: ComfyUI-Compatible Output\n\nCreate FP8 model for ComfyUI:\n```bash\npython converter.py \\\n    --source /path/to/Wan2.1-I2V-14B-480P/ \\\n    --output /path/to/output \\\n    --model_type wan_dit \\\n    --quantized \\\n    --linear_type fp8 \\\n    --non_linear_dtype torch.bfloat16 \\\n    --single_file \\\n    --comfyui_mode\n```\n\n**Result**: Single safetensors file with ComfyUI-compatible scale key naming and marker tensor.\n\n### Pattern 5: Architecture Round-Trip\n\nConvert to Diffusers and back:\n```bash\n# Step 1: LightX2V  Diffusers\npython converter.py \\\n    --source /path/to/original \\\n    --output /path/to/diffusers \\\n    --model_type wan_dit \\\n    --direction forward \\\n    --chunk-size 100\n\n# Step 2: Diffusers  LightX2V\npython converter.py \\\n    --source /path/to/diffusers \\\n    --output /path/to/restored \\\n    --model_type wan_dit \\\n    --direction backward \\\n    --save_by_block\n```\n\n**Result**: Original architecture restored with block-based file organization.\n\nSources: [tools/convert/readme.md:100-445](), [tools/convert/readme_zh.md:78-387]()\n\n## Implementation Details\n\n### Memory Optimization Strategies\n\n**Lazy Loading** [tools/convert/converter.py:481-492]():\n- SafeTensors files use `safe_open()` context manager\n- Tensors loaded on-demand with progress bar for large files\n- Avoids loading entire file into memory at once\n\n**Garbage Collection** [tools/convert/converter.py:412](), [tools/convert/converter.py:502-504]():\n- Explicit `gc.collect()` calls after processing each weight or file\n- Helps free memory between large operations\n- Critical for processing multiple large files sequentially\n\n**Parallel Processing** [tools/convert/converter.py:525-538]():\n- Enabled by default with `--parallel` flag\n- Uses `ThreadPoolExecutor` with up to 8 workers\n- Applied to key conversion for models with \u003e1000 keys\n- Provides significant speedup for large models\n\n**Chunked Saving** [tools/convert/converter.py:670-692]():\n- Saves weights incrementally to avoid memory spike\n- Index file (`diffusion_pytorch_model.safetensors.index.json`) maps keys to chunk files\n- Configurable chunk size (default: 100 keys per chunk)\n\n### Error Handling\n\n**Duplicate Key Detection** [tools/convert/converter.py:494-496]():\n```python\nduplicate_keys = set(weights.keys()) \u0026 set(merged_weights.keys())\nif duplicate_keys:\n    raise ValueError(f\"Duplicate keys found: {duplicate_keys} in file {file_path}\")\n```\n\n**Quantization Validation** [tools/convert/quant/quant.py:16-22]():\n```python\nif weight.dim() != 2:\n    raise ValueError(f\"Only 2D tensors supported. Got {weight.dim()}D tensor\")\nif torch.isnan(weight).any():\n    raise ValueError(\"Tensor contains NaN values\")\n```\n\n**LoRA Parameter Validation** [tools/convert/converter.py:551-561]():\nEnsures alpha and strength lists match LoRA path count, with auto-replication for single values.\n\n### Registry System Integration\n\nThe quantization system uses `CONVERT_WEIGHT_REGISTER` from [lightx2v.utils.registry_factory]() to dynamically select quantizer classes [tools/convert/converter.py:389]():\n\n```python\nquantizer = CONVERT_WEIGHT_REGISTER[linear_type](tensor)\nw_q, scales, extra = quantizer.weight_quant_func(tensor, comfyui_mode)\n```\n\nThis allows adding new quantization formats by registering new classes with the decorator:\n```python\n@CONVERT_WEIGHT_REGISTER(\"custom_format\")\nclass QuantWeightCustom(QuantTemplate):\n    ...\n```\n\nSources: [tools/convert/converter.py:462-703](), [tools/convert/quant/quant.py:1-142]()"])</script><script>self.__next_f.push([1,"4a:T6821,"])</script><script>self.__next_f.push([1,"# Configuration System\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [configs/seko_talk/shot/rs2v/rs2v.json](configs/seko_talk/shot/rs2v/rs2v.json)\n- [configs/seko_talk/shot/stream/f2v.json](configs/seko_talk/shot/stream/f2v.json)\n- [configs/seko_talk/shot/stream/s2v.json](configs/seko_talk/shot/stream/s2v.json)\n- [lightx2v/models/schedulers/scheduler.py](lightx2v/models/schedulers/scheduler.py)\n- [lightx2v/models/schedulers/wan/audio/scheduler.py](lightx2v/models/schedulers/wan/audio/scheduler.py)\n- [lightx2v/shot_runner/rs2v_infer.py](lightx2v/shot_runner/rs2v_infer.py)\n- [lightx2v/shot_runner/shot_base.py](lightx2v/shot_runner/shot_base.py)\n- [lightx2v/shot_runner/stream_infer.py](lightx2v/shot_runner/stream_infer.py)\n- [lightx2v/utils/input_info.py](lightx2v/utils/input_info.py)\n- [lightx2v/utils/set_config.py](lightx2v/utils/set_config.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe Configuration System is the central parameter management framework in LightX2V that controls all aspects of model initialization, inference behavior, optimization settings, and hardware configuration. It provides a flexible three-tier configuration approach (defaults  JSON files  runtime arguments) and ensures consistent parameter propagation across all system components.\n\nFor information about model loading and initialization, see [Model Variants and Tasks](#5). For optimization-specific configuration, see [Performance Optimization](#6).\n\n---\n\n## Configuration Architecture Overview\n\nThe configuration system uses a hierarchical approach where settings are merged from multiple sources with increasing priority. The final configuration is stored in a `LockableDict` that can be locked after initialization to prevent accidental modification during inference.\n\n### Configuration Loading Flow\n\n```mermaid\nflowchart TD\n    CLI[\"Command Line Arguments\u003cbr/\u003e(argparse)\"]\n    Defaults[\"Default Config\u003cbr/\u003eget_default_config()\"]\n    ConfigJSON[\"User Config JSON\u003cbr/\u003e(--config_json)\"]\n    ModelJSON[\"Model Config JSON\u003cbr/\u003e(model_path/config.json)\"]\n    QuantJSON[\"Quantization Config JSON\u003cbr/\u003e(dit_quantized_ckpt/config.json)\"]\n    \n    SetConfig[\"set_config(args)\u003cbr/\u003e[set_config.py:37-128]\"]\n    LockableDict[\"LockableDict\u003cbr/\u003econfig object\"]\n    Validation[\"validate_config_paths()\u003cbr/\u003ePath validation\"]\n    ParallelConfig[\"set_parallel_config()\u003cbr/\u003eDevice mesh setup\"]\n    \n    Defaults --\u003e SetConfig\n    CLI --\u003e SetConfig\n    ConfigJSON --\u003e SetConfig\n    ModelJSON --\u003e SetConfig\n    QuantJSON --\u003e SetConfig\n    \n    SetConfig --\u003e LockableDict\n    LockableDict --\u003e Validation\n    Validation --\u003e ParallelConfig\n    ParallelConfig --\u003e RunnerInit[\"Runner Initialization\"]\n    \n    LockableDict -.-\u003e|\"config.lock()\"| LockedConfig[\"Locked Config\u003cbr/\u003e(read-only during inference)\"]\n```\n\n**Sources:** [lightx2v/utils/set_config.py:37-128](), [lightx2v/infer.py:39-169](), [lightx2v/pipeline.py:172-178]()\n\n---\n\n## Configuration Sources and Priority\n\n### Priority Order (Lowest to Highest)\n\n| Priority | Source | Purpose | Example |\n|----------|--------|---------|---------|\n| 1 | Default Config | Base defaults for all parameters | `use_bfloat16=True`, `cpu_offload=False` |\n| 2 | Model Config JSON | Model architecture parameters from HuggingFace/ModelScope | `dim=5120`, `num_layers=48` |\n| 3 | Quantization Config JSON | Quantization scheme settings | `dit_quant_scheme=\"fp8-triton\"` |\n| 4 | User Config JSON | User-provided inference settings | `infer_steps=50`, `sample_guide_scale=5.0` |\n| 5 | Command Line Args | Runtime overrides | `--seed 42`, `--infer_steps 100` |\n\n### Default Configuration Structure\n\n```mermaid\nclassDiagram\n    class DefaultConfig {\n        +bool do_mm_calib\n        +bool cpu_offload\n        +tuple vae_stride\n        +tuple patch_size\n        +str feature_caching\n        +float teacache_thresh\n        +bool use_bfloat16\n        +list lora_configs\n        +bool parallel\n        +bool seq_parallel\n        +bool cfg_parallel\n        +bool enable_cfg\n        +bool use_image_encoder\n    }\n    \n    class LockableDict {\n        -dict _data\n        -bool _locked\n        +update(dict)\n        +lock()\n        +temporarily_unlocked()\n        +__getitem__(key)\n        +__setitem__(key, value)\n    }\n    \n    DefaultConfig --|\u003e LockableDict : \"wrapped in\"\n```\n\n**Sources:** [lightx2v/utils/set_config.py:14-34](), [lightx2v/utils/lockable_dict.py]()\n\n---\n\n## Configuration Parameters\n\n### Core Inference Parameters\n\n```mermaid\ngraph LR\n    subgraph \"Sampling Parameters\"\n        A[\"infer_steps\u003cbr/\u003e(int)\"]\n        B[\"sample_guide_scale\u003cbr/\u003e(float)\"]\n        C[\"sample_shift\u003cbr/\u003e(float)\"]\n        D[\"seed\u003cbr/\u003e(int)\"]\n    end\n    \n    subgraph \"Output Parameters\"\n        E[\"target_video_length\u003cbr/\u003e(int)\"]\n        F[\"target_height\u003cbr/\u003e(int)\"]\n        G[\"target_width\u003cbr/\u003e(int)\"]\n        H[\"fps\u003cbr/\u003e(int)\"]\n    end\n    \n    subgraph \"Model Architecture\"\n        I[\"dim\u003cbr/\u003e(int)\"]\n        J[\"num_layers\u003cbr/\u003e(int)\"]\n        K[\"num_heads\u003cbr/\u003e(int)\"]\n        L[\"patch_size\u003cbr/\u003e(tuple)\"]\n    end\n    \n    subgraph \"Optimization\"\n        M[\"cpu_offload\u003cbr/\u003e(bool)\"]\n        N[\"dit_quantized\u003cbr/\u003e(bool)\"]\n        O[\"feature_caching\u003cbr/\u003e(str)\"]\n        P[\"attn_mode\u003cbr/\u003e(str)\"]\n    end\n```\n\n**Sources:** [lightx2v/utils/set_config.py:14-34](), [configs/ltx2/ltx2_distill_fp8.json:1-22]()\n\n### Configuration Parameter Categories\n\n#### Task-Specific Parameters\n\n| Parameter | Type | Tasks | Description |\n|-----------|------|-------|-------------|\n| `task` | str | All | One of: `t2v`, `i2v`, `s2v`, `t2i`, `i2i`, `t2av`, `i2av` |\n| `target_video_length` | int | Video tasks | Number of frames to generate |\n| `target_height` | int | All | Output height in pixels |\n| `target_width` | int | All | Output width in pixels |\n| `use_image_encoder` | bool | i2v, s2v | Whether to use CLIP image encoder |\n| `audio_sr` | int | s2v, t2av, i2av | Audio sample rate (default: 16000) |\n| `prev_frame_length` | int | s2v | Frames to use for temporal consistency |\n\n**Sources:** [lightx2v/models/runners/wan/wan_audio_runner.py:278-284](), [lightx2v/infer.py:72-89]()\n\n#### Optimization Parameters\n\n| Parameter | Type | Description | Default |\n|-----------|------|-------------|---------|\n| `cpu_offload` | bool | Enable weight offloading to CPU | `False` |\n| `offload_granularity` | str | Offload at `\"model\"`, `\"block\"`, or `\"phase\"` level | `\"block\"` |\n| `lazy_load` | bool | Load weights from disk on-demand | `False` |\n| `dit_quantized` | bool | Use quantized model weights | `False` |\n| `dit_quant_scheme` | str | Quantization scheme (e.g., `\"fp8-triton\"`) | `None` |\n| `feature_caching` | str | Feature caching strategy | `\"NoCaching\"` |\n| `teacache_thresh` | float | Threshold for TeaCache | `0.26` |\n| `clean_cuda_cache` | bool | Clear CUDA cache frequently | `False` |\n\n**Sources:** [lightx2v/models/networks/wan/model.py:43-62](), [lightx2v/utils/set_config.py:14-34]()\n\n#### Attention and Rope Parameters\n\n| Parameter | Type | Options | Description |\n|-----------|------|---------|-------------|\n| `attn_mode` / `self_attn_1_type` | str | `\"flash_attn2\"`, `\"flash_attn3\"`, `\"sage_attn2\"` | Attention implementation |\n| `rope_type` | str | `\"flashinfer\"`, `\"torch\"`, `\"torch_naive\"` | Rotary position embedding method |\n| `rope_chunk` | bool | - | Use chunked RoPE for memory efficiency |\n| `rope_chunk_size` | int | - | Size of RoPE chunks |\n| `double_precision_rope` | bool | - | Use FP64 for RoPE computation |\n| `modulate_type` | str | `\"triton\"`, `\"torch\"` | Modulation implementation backend |\n\n**Sources:** [lightx2v/models/networks/wan/infer/transformer_infer.py:36-58](), [lightx2v/pipeline.py:186-234]()\n\n---\n\n## Configuration Loading Process\n\n### Main Configuration Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant ArgParser as \"argparse.ArgumentParser\"\n    participant SetConfig as \"set_config()\"\n    participant DefaultConfig as \"get_default_config()\"\n    participant LockableDict\n    participant FileSystem\n    participant Runner\n    \n    User-\u003e\u003eArgParser: Command line arguments\n    ArgParser-\u003e\u003eSetConfig: args object\n    \n    SetConfig-\u003e\u003eDefaultConfig: Get base defaults\n    DefaultConfig--\u003e\u003eSetConfig: LockableDict with defaults\n    \n    SetConfig-\u003e\u003eSetConfig: Update from args.__dict__\n    \n    opt User provides --config_json\n        SetConfig-\u003e\u003eFileSystem: Read config_json\n        FileSystem--\u003e\u003eSetConfig: JSON config dict\n        SetConfig-\u003e\u003eLockableDict: config.update(json_config)\n    end\n    \n    opt Model config exists\n        SetConfig-\u003e\u003eFileSystem: Read model_path/config.json\n        FileSystem--\u003e\u003eSetConfig: Model config dict\n        SetConfig-\u003e\u003eLockableDict: config.update(model_config)\n    end\n    \n    opt Quantized model config exists\n        SetConfig-\u003e\u003eFileSystem: Read dit_quantized_ckpt/config.json\n        FileSystem--\u003e\u003eSetConfig: Quant config dict\n        SetConfig-\u003e\u003eLockableDict: config.update(quant_config)\n    end\n    \n    SetConfig-\u003e\u003eSetConfig: Adjust target_video_length for VAE stride\n    \n    SetConfig--\u003e\u003eUser: Final config (LockableDict)\n    \n    User-\u003e\u003eRunner: Initialize with config\n    Runner-\u003e\u003eLockableDict: config.lock()\n    Note over LockableDict: Config is now read-only\n```\n\n**Sources:** [lightx2v/utils/set_config.py:37-128](), [lightx2v/infer.py:144-163]()\n\n### Model-Specific Config Loading Paths\n\nDifferent model families have special configuration loading logic:\n\n```python\n# From set_config.py:47-106\n\n# HunyuanVideo: Load from transformer subdirectory\nif config[\"model_cls\"] in [\"hunyuan_video_1.5\", \"hunyuan_video_1.5_distill\"]:\n    config[\"transformer_model_path\"] = os.path.join(\n        config[\"model_path\"], \n        \"transformer\", \n        config[\"transformer_model_name\"]\n    )\n    # Load transformer config\n    \n# LongCat Image: Load both root and transformer configs\nelif config[\"model_cls\"] == \"longcat_image\":\n    # Load root config.json\n    # Load transformer/config.json\n    \n# Standard models (Wan, etc): Try multiple paths\nelse:\n    # Try: model_path/config.json\n    # Try: model_path/low_noise_model/config.json\n    # Try: model_path/distill_models/low_noise_model/config.json\n    # Try: model_path/original/config.json\n    # Try: model_path/transformer/config.json\n```\n\n**Sources:** [lightx2v/utils/set_config.py:47-106]()\n\n---\n\n## JSON Configuration Files\n\n### Structure of Configuration JSON\n\nExample from LTX2 distilled FP8 configuration:\n\n```json\n{\n    \"infer_steps\": 8,\n    \"target_video_length\": 121,\n    \"target_height\": 512,\n    \"target_width\": 768,\n    \"attn_type\": \"sage_attn2\",\n    \"sample_guide_scale\": 1,\n    \"sample_shift\": [2.05, 0.95],\n    \"enable_cfg\": false,\n    \"cpu_offload\": false,\n    \"num_channels_latents\": 128,\n    \"fps\": 24,\n    \"audio_fps\": 24000,\n    \"audio_mel_bins\": 16,\n    \"double_precision_rope\": true,\n    \"dit_quantized\": true,\n    \"dit_quantized_ckpt\": \"Lightricks/LTX-2/ltx-2-19b-distilled-fp8.safetensors\",\n    \"dit_quant_scheme\": \"fp8-pertensor\",\n    \"skip_fp8_block_index\": [0, 43, 44, 45, 46, 47],\n    \"distilled_sigma_values\": [1.0, 0.99375, 0.9875, 0.98125, 0.975, 0.909375, 0.725, 0.421875, 0.0]\n}\n```\n\n**Sources:** [configs/ltx2/ltx2_distill_fp8.json:1-22]()\n\n### Model Architecture Config (Auto-loaded from HuggingFace/ModelScope)\n\nExample model architecture config (stored in `model_path/config.json`):\n\n| Parameter | Type | Description | Example Value |\n|-----------|------|-------------|---------------|\n| `dim` | int | Model hidden dimension | `5120` |\n| `num_layers` | int | Number of transformer blocks | `48` |\n| `num_heads` | int | Number of attention heads | `40` |\n| `in_dim` | int | Input dimension | `16` |\n| `out_dim` | int | Output dimension | `16` |\n| `freq_dim` | int | Frequency embedding dimension | `256` |\n| `vae_stride` | list[int] | VAE downsampling factors | `[4, 8, 8]` |\n| `patch_size` | list[int] | Spatial patch size | `[1, 2, 2]` |\n\n**Sources:** [lightx2v/models/networks/wan/model.py:39-62]()\n\n---\n\n## Configuration Propagation\n\n### How Config Flows Through System Components\n\n```mermaid\ngraph TD\n    Config[\"Config\u003cbr/\u003e(LockableDict)\"]\n    \n    Config --\u003e Runner[\"Runner\u003cbr/\u003eself.config = config\"]\n    \n    Runner --\u003e Model[\"Model.__init__\u003cbr/\u003e(config)\"]\n    Runner --\u003e Scheduler[\"Scheduler.__init__\u003cbr/\u003e(config)\"]\n    Runner --\u003e TextEnc[\"Text Encoders\u003cbr/\u003eload_text_encoder()\"]\n    Runner --\u003e VAE[\"VAE\u003cbr/\u003eload_vae()\"]\n    Runner --\u003e ImgEnc[\"Image Encoder\u003cbr/\u003eload_image_encoder()\"]\n    \n    Model --\u003e PreInfer[\"PreInfer\u003cbr/\u003econfig passed to __init__\"]\n    Model --\u003e TransformerInfer[\"TransformerInfer\u003cbr/\u003econfig stored\"]\n    Model --\u003e PostInfer[\"PostInfer\u003cbr/\u003econfig passed\"]\n    \n    Model --\u003e PreWeights[\"PreWeights\u003cbr/\u003econfig determines quant\"]\n    Model --\u003e TransformerWeights[\"TransformerWeights\u003cbr/\u003econfig for offload/lazy\"]\n    \n    TransformerWeights --\u003e Blocks[\"Block Weights\u003cbr/\u003equant scheme from config\"]\n    \n    Scheduler --\u003e GridSizes[\"Grid Size Calc\u003cbr/\u003efrom patch_size\"]\n    Scheduler --\u003e Timesteps[\"Timestep Schedule\u003cbr/\u003efrom infer_steps\"]\n    \n    style Config fill:#f9f9f9\n    style Runner fill:#e1f5ff\n    style Model fill:#fff4e1\n```\n\n**Sources:** [lightx2v/models/runners/default_runner.py:56-99](), [lightx2v/models/networks/wan/model.py:39-100]()\n\n### Config Access Patterns in Code\n\nThe configuration is accessed throughout the codebase using standard dict patterns with fallback defaults:\n\n```python\n# Common access patterns from codebase:\n\n# 1. Direct access (assumes key exists)\nself.task = self.config[\"task\"]\nself.dim = self.config[\"dim\"]\n\n# 2. Safe access with default\nself.cpu_offload = self.config.get(\"cpu_offload\", False)\nself.lazy_load = self.config.get(\"lazy_load\", False)\n\n# 3. Nested dict access for complex configs\nclip_offload = self.config.get(\"clip_cpu_offload\", \n                                self.config.get(\"cpu_offload\", False))\n\n# 4. Boolean flag checking\nif self.config.get(\"changing_resolution\", False):\n    # Special logic for multi-resolution\n\n# 5. Parallel config access\nif self.config[\"seq_parallel\"]:\n    self.seq_p_group = self.config.get(\"device_mesh\").get_group(mesh_dim=\"seq_p\")\n```\n\n**Sources:** [lightx2v/models/networks/wan/model.py:49-64](), [lightx2v/models/runners/wan/wan_runner.py:66-236]()\n\n---\n\n## Parallel Processing Configuration\n\n### Device Mesh Configuration\n\nFor distributed inference, the configuration system sets up a device mesh that defines how work is split across GPUs:\n\n```mermaid\ngraph TD\n    ParallelConfig[\"config['parallel']\u003cbr/\u003e(dict or bool)\"]\n    \n    ParallelConfig --\u003e|\"tensor_p_size \u003e 1\"| TensorP[\"Tensor Parallel\u003cbr/\u003e1D mesh\"]\n    ParallelConfig --\u003e|\"tensor_p_size == 1\"| SeqCfgP[\"Seq/CFG Parallel\u003cbr/\u003e2D mesh\"]\n    \n    TensorP --\u003e DeviceMesh1D[\"init_device_mesh\u003cbr/\u003e(tensor_p,)\u003cbr/\u003e1D mesh\"]\n    \n    SeqCfgP --\u003e CheckSeq{{\"seq_p_size \u003e 1?\"}}\n    SeqCfgP --\u003e CheckCfg{{\"cfg_p_size \u003e 1?\"}}\n    \n    CheckSeq --\u003e|Yes| EnableSeq[\"config['seq_parallel'] = True\"]\n    CheckCfg --\u003e|Yes| EnableCfg[\"config['cfg_parallel'] = True\"]\n    \n    EnableSeq --\u003e DeviceMesh2D[\"init_device_mesh\u003cbr/\u003e(cfg_p, seq_p)\u003cbr/\u003e2D mesh\"]\n    EnableCfg --\u003e DeviceMesh2D\n    \n    DeviceMesh2D --\u003e GroupSeq[\"seq_p_group = \u003cbr/\u003edevice_mesh.get_group('seq_p')\"]\n    DeviceMesh2D --\u003e GroupCfg[\"cfg_p_group = \u003cbr/\u003edevice_mesh.get_group('cfg_p')\"]\n```\n\n**Parallel Config Parameters:**\n\n| Parameter | Type | Description | Example |\n|-----------|------|-------------|---------|\n| `parallel` | dict | Enables distributed processing | `{\"seq_p_size\": 2, \"cfg_p_size\": 2}` |\n| `tensor_p_size` | int | Number of GPUs for tensor parallelism | `8` |\n| `seq_p_size` | int | Number of GPUs for sequence parallelism | `2` |\n| `cfg_p_size` | int | Number of GPUs for CFG parallelism | `2` |\n| `seq_p_fp8_comm` | bool | Use FP8 for sequence parallel communication | `false` |\n| `seq_p_head_parallel` | bool | Enable head-level parallelism | `false` |\n| `seq_p_tensor_fusion` | bool | Fuse tensors in communication | `false` |\n| `vae_parallel` | bool | Enable VAE parallel decoding | `true` |\n\n**Sources:** [lightx2v/utils/set_config.py:131-159](), [lightx2v/models/networks/wan/infer/transformer_infer.py:63-73]()\n\n---\n\n## Quantization Configuration\n\n### Quantization Parameter Structure\n\n```mermaid\ngraph TD\n    QuantConfig[\"Quantization Config\"]\n    \n    QuantConfig --\u003e DitQuant[\"dit_quantized\u003cbr/\u003e(bool)\"]\n    QuantConfig --\u003e DitCkpt[\"dit_quantized_ckpt\u003cbr/\u003e(path)\"]\n    QuantConfig --\u003e Scheme[\"dit_quant_scheme\u003cbr/\u003e(str)\"]\n    QuantConfig --\u003e Skip[\"skip_fp8_block_index\u003cbr/\u003e(list)\"]\n    \n    Scheme --\u003e INT8[\"int8-*\u003cbr/\u003etriton/vllm/sgl/q8f/torchao\"]\n    Scheme --\u003e FP8[\"fp8-*\u003cbr/\u003etriton/vllm/sgl/q8f/torchao/pertensor\"]\n    Scheme --\u003e NVFP4[\"nvfp4\u003cbr/\u003e4-bit NVIDIA format\"]\n    Scheme --\u003e MXFP[\"mxfp4/6/8\u003cbr/\u003eMicroscaling formats\"]\n    Scheme --\u003e GGUF[\"gguf-*\u003cbr/\u003eQ8_0/Q6_K/Q5_K/Q4_K/Q3_K\"]\n    \n    QuantConfig --\u003e TextEncQuant[\"t5_quantized\u003cbr/\u003et5_quant_scheme\"]\n    QuantConfig --\u003e ClipQuant[\"clip_quantized\u003cbr/\u003eclip_quant_scheme\"]\n    QuantConfig --\u003e AdapterQuant[\"adapter_quantized\u003cbr/\u003eadapter_quant_scheme\"]\n```\n\n**Component-Specific Quantization:**\n\nEach major component can be quantized independently:\n\n| Component | Quantized Flag | Scheme Config | Checkpoint Config |\n|-----------|----------------|---------------|-------------------|\n| Diffusion Transformer | `dit_quantized` | `dit_quant_scheme` | `dit_quantized_ckpt` |\n| T5 Text Encoder | `t5_quantized` | `t5_quant_scheme` | `t5_quantized_ckpt` |\n| CLIP Image Encoder | `clip_quantized` | `clip_quant_scheme` | `clip_quantized_ckpt` |\n| Audio Adapter | `adapter_quantized` | `adapter_quant_scheme` | - |\n\n**Sources:** [lightx2v/models/networks/wan/model.py:63-96](), [lightx2v/models/runners/wan/wan_runner.py:128-157]()\n\n---\n\n## LockableDict: Thread-Safe Configuration\n\nThe configuration system uses `LockableDict`, a special dictionary that can be locked to prevent modification after initialization:\n\n```python\n# From usage patterns in the codebase:\n\n# 1. Initial setup - config is mutable\nconfig = get_default_config()  # Returns LockableDict\nconfig.update({\"infer_steps\": 50})\nconfig[\"task\"] = \"t2v\"\n\n# 2. Lock before passing to runner\nconfig.lock()  # Now read-only\n\n# 3. Temporary unlock for specific modifications\nwith config.temporarily_unlocked():\n    config[\"some_parameter\"] = new_value\n\n# 4. Attempting to modify locked config raises error\nconfig[\"task\"] = \"i2v\"  # Raises RuntimeError\n```\n\n**Key Methods:**\n\n| Method | Description | Usage |\n|--------|-------------|-------|\n| `lock()` | Make config read-only | Called after runner initialization |\n| `temporarily_unlocked()` | Context manager for temporary modification | Use when runner needs to modify config |\n| `update(dict)` | Merge dict into config | Respects lock status |\n\n**Sources:** [lightx2v/models/runners/default_runner.py:95](), [lightx2v/models/runners/default_runner.py:168-169]()\n\n---\n\n## Advanced Configuration Patterns\n\n### Feature Caching Configuration\n\nThe system supports multiple feature caching strategies to skip redundant transformer block computations:\n\n| Strategy | Config Value | Parameters | Description |\n|----------|--------------|------------|-------------|\n| No Caching | `\"NoCaching\"` | - | Compute all blocks every step |\n| TeaCache | `\"Tea\"` | `teacache_thresh` | Threshold-based caching |\n| TaylorSeer | `\"TaylorSeer\"` | - | Taylor expansion prediction |\n| AdaCache | `\"Ada\"` | - | Adaptive metrics |\n| MagCache | `\"Mag\"` | - | Magnitude-based caching |\n| Custom | `\"Custom\"` | - | User-defined strategy |\n\n**Configuration Example:**\n\n```json\n{\n    \"feature_caching\": \"Tea\",\n    \"teacache_thresh\": 0.26\n}\n```\n\n**Sources:** [lightx2v/models/networks/wan/model.py:106-126](), [lightx2v/models/schedulers/wan/scheduler.py:30]()\n\n### Changing Resolution Configuration\n\nFor multi-resolution inference (used in some tasks):\n\n```python\n{\n    \"changing_resolution\": true,\n    \"resolution_rate\": [0.5, 0.75, 1.0],  # Progressive resolution scaling\n}\n```\n\nThis causes the VAE to encode the input at multiple resolutions, and the scheduler switches between them during inference.\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:384-394]()\n\n### LoRA Configuration\n\nLoRA adapters can be configured in two modes:\n\n```python\n{\n    \"lora_configs\": [\n        {\n            \"name\": \"style_lora\",\n            \"path\": \"/path/to/lora.safetensors\",\n            \"strength\": 1.0\n        }\n    ],\n    \"lora_dynamic_apply\": true  # or false for static merge\n}\n```\n\n- **Dynamic mode** (`lora_dynamic_apply=True`): LoRA computed at runtime, allows switching\n- **Static mode** (`lora_dynamic_apply=False`): LoRA merged into base weights before inference\n\n**Sources:** [lightx2v/models/runners/wan/wan_runner.py:35-60](), [lightx2v/models/networks/wan/model.py:143-154]()\n\n---\n\n## Configuration Validation\n\n### Path Validation\n\nThe system validates that all configured paths exist before starting inference:\n\n```python\ndef validate_config_paths(config):\n    \"\"\"Validates model_path and related checkpoint paths\"\"\"\n    # Check model_path exists\n    # Check dit_original_ckpt if specified\n    # Check dit_quantized_ckpt if specified\n    # etc.\n```\n\n**Sources:** [lightx2v/utils/utils.py]() (referenced but not shown in provided files)\n\n### Task-Specific Validation\n\nCertain parameter combinations are validated for specific tasks:\n\n```python\n# From set_config.py:107-117\nif config[\"task\"] in [\"i2v\", \"s2v\"]:\n    if config[\"target_video_length\"] % config[\"vae_stride\"][0] != 1:\n        # Adjust target_video_length to be compatible with VAE stride\n        config[\"target_video_length\"] = (\n            config[\"target_video_length\"] // config[\"vae_stride\"][0] \n            * config[\"vae_stride\"][0] + 1\n        )\n```\n\n**Sources:** [lightx2v/utils/set_config.py:107-117]()\n\n---\n\n## Configuration in LightX2VPipeline\n\nThe `LightX2VPipeline` class provides a high-level API for configuration:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Pipeline as \"LightX2VPipeline\"\n    participant SetConfig as \"set_config()\"\n    participant Runner\n    \n    User-\u003e\u003ePipeline: __init__(model_path, model_cls, task)\n    Note over Pipeline: Store basic parameters\n    \n    User-\u003e\u003ePipeline: enable_quantize(...)\n    Note over Pipeline: Set quantization parameters\n    \n    User-\u003e\u003ePipeline: enable_offload(...)\n    Note over Pipeline: Set offload parameters\n    \n    User-\u003e\u003ePipeline: enable_lora(...)\n    Note over Pipeline: Set LoRA parameters\n    \n    User-\u003e\u003ePipeline: create_generator(infer_steps, ...)\n    Pipeline-\u003e\u003ePipeline: set_infer_config()\n    Pipeline-\u003e\u003eSetConfig: set_config(self)\n    SetConfig--\u003e\u003ePipeline: Complete config\n    Pipeline-\u003e\u003eRunner: init_runner(config)\n    Runner--\u003e\u003ePipeline: Initialized runner\n```\n\n**Pipeline Configuration Methods:**\n\n| Method | Purpose | Parameters |\n|--------|---------|------------|\n| `__init__()` | Set model and task | `model_path`, `model_cls`, `task` |\n| `enable_quantize()` | Configure quantization | `dit_quantized`, `quant_scheme`, etc. |\n| `enable_offload()` | Configure CPU offloading | `cpu_offload`, `offload_granularity` |\n| `enable_lora()` | Configure LoRA adapters | `lora_configs` |\n| `create_generator()` | Set inference parameters | `infer_steps`, `guidance_scale`, etc. |\n| `set_infer_config()` | Set parameters programmatically | Individual config keys |\n| `set_infer_config_json()` | Load from JSON file | Path to JSON file |\n\n**Sources:** [lightx2v/pipeline.py:59-239]()\n\n---\n\n## Configuration Best Practices\n\n### Recommended Configuration Workflow\n\n1. **Start with a JSON template** for your model and task\n2. **Override with command-line args** for experimentation\n3. **Lock config** after runner initialization to prevent bugs\n4. **Use config files** for reproducible inference\n\n### Example: Configuring LTX2 with FP8 and Offloading\n\n```python\n# Option 1: Using JSON file\npipe = LightX2VPipeline(\n    model_path=\"Lightricks/LTX-2\",\n    model_cls=\"ltx2\",\n    task=\"t2av\"\n)\n\npipe.create_generator(\n    config_json=\"configs/ltx2/ltx2_distill_fp8.json\"\n)\n\n# Option 2: Programmatic configuration\npipe.enable_quantize(\n    dit_quantized=True,\n    dit_quantized_ckpt=\"Lightricks/LTX-2/ltx-2-19b-distilled-fp8.safetensors\",\n    quant_scheme=\"fp8-pertensor\",\n    skip_fp8_block_index=[0, 43, 44, 45, 46, 47]\n)\n\npipe.enable_offload(\n    cpu_offload=True,\n    offload_granularity=\"block\"\n)\n\npipe.create_generator(\n    attn_mode=\"sage_attn2\",\n    infer_steps=8,\n    height=512,\n    width=768,\n    num_frames=121,\n    guidance_scale=1.0\n)\n```\n\n**Sources:** [examples/ltx2/ltxt_t2av_distilled_fp8.py:1-32](), [lightx2v/pipeline.py:126-239]()\n\n---\n\n## Configuration Debugging\n\n### Printing Configuration\n\nThe system provides a pretty-printer for configuration:\n\n```python\ndef print_config(config):\n    config_to_print = config.copy()\n    config_to_print.pop(\"device_mesh\", None)  # Remove unprintable objects\n    \n    if config[\"parallel\"]:\n        if dist.get_rank() == 0:  # Only rank 0 prints in distributed mode\n            logger.info(f\"config:\\n{json.dumps(config_to_print, \n                                               ensure_ascii=False, \n                                               indent=4)}\")\n```\n\n**Sources:** [lightx2v/utils/set_config.py:161-169]()\n\n### Common Configuration Issues\n\n| Issue | Symptom | Solution |\n|-------|---------|----------|\n| VAE stride mismatch | Runtime error about frame count | Adjust `target_video_length` to `(N-1) % vae_stride[0] == 0` |\n| Quantization mismatch | Slow inference or errors | Ensure quantized checkpoint matches `quant_scheme` |\n| Memory OOM | CUDA out of memory | Enable `cpu_offload` or reduce batch size |\n| Missing model config | Error loading model | Ensure `model_path/config.json` exists |\n| Locked config modification | RuntimeError | Use `config.temporarily_unlocked()` context |\n\n---\n\n## Summary\n\nThe Configuration System in LightX2V provides:\n\n1. **Hierarchical merging** from defaults  JSON  CLI args\n2. **Centralized parameter management** via `LockableDict`\n3. **Model-specific auto-configuration** from HuggingFace/ModelScope\n4. **Component-level granularity** for optimization settings\n5. **Validation and safety** through locking and path validation\n6. **High-level API** through `LightX2VPipeline` methods\n\nThe configuration flows through the entire system from initialization through inference, controlling model architecture, optimization strategies, hardware configuration, and inference behavior.\n\n**Key Files:**\n- [lightx2v/utils/set_config.py]() - Main configuration logic\n- [lightx2v/pipeline.py]() - High-level pipeline API\n- [lightx2v/infer.py]() - CLI configuration example\n- [configs/]() directory - JSON configuration templates"])</script><script>self.__next_f.push([1,"4b:T6ab8,"])</script><script>self.__next_f.push([1,"# InputInfo and Data Structures\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [configs/seko_talk/shot/rs2v/rs2v.json](configs/seko_talk/shot/rs2v/rs2v.json)\n- [configs/seko_talk/shot/stream/f2v.json](configs/seko_talk/shot/stream/f2v.json)\n- [configs/seko_talk/shot/stream/s2v.json](configs/seko_talk/shot/stream/s2v.json)\n- [lightx2v/models/schedulers/scheduler.py](lightx2v/models/schedulers/scheduler.py)\n- [lightx2v/models/schedulers/wan/audio/scheduler.py](lightx2v/models/schedulers/wan/audio/scheduler.py)\n- [lightx2v/shot_runner/rs2v_infer.py](lightx2v/shot_runner/rs2v_infer.py)\n- [lightx2v/shot_runner/shot_base.py](lightx2v/shot_runner/shot_base.py)\n- [lightx2v/shot_runner/stream_infer.py](lightx2v/shot_runner/stream_infer.py)\n- [lightx2v/utils/input_info.py](lightx2v/utils/input_info.py)\n- [lightx2v/utils/set_config.py](lightx2v/utils/set_config.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis page documents the `InputInfo` dataclass system used throughout LightX2V to structure and validate input parameters for inference tasks. These dataclasses provide type-safe containers for task-specific inputs (prompts, seeds, file paths, shapes) and runtime state (overlap frames, audio clips, latent tensors).\n\nFor information about configuration management that works alongside InputInfo, see [Configuration System](#8.2). For details on how runners consume InputInfo structures, see [Runner System and Registry Pattern](#4.1).\n\n**Sources:** [lightx2v/utils/input_info.py:1-410]()\n\n---\n\n## InputInfo Class Hierarchy\n\nThe LightX2V system defines task-specific `InputInfo` dataclasses that inherit common patterns but vary in their fields based on task requirements. Each dataclass corresponds to a specific generation task.\n\n```mermaid\ngraph TB\n    subgraph \"Video Generation Tasks\"\n        T2VInputInfo[\"T2VInputInfo\u003cbr/\u003eText-to-Video\"]\n        I2VInputInfo[\"I2VInputInfo\u003cbr/\u003eImage-to-Video\"]\n        S2VInputInfo[\"S2VInputInfo\u003cbr/\u003eSpeech-to-Video\u003cbr/\u003e(Audio-driven)\"]\n        RS2VInputInfo[\"RS2VInputInfo\u003cbr/\u003eReferenced S2V\u003cbr/\u003e(Long-form audio)\"]\n        Flf2vInputInfo[\"Flf2vInputInfo\u003cbr/\u003eFirst+Last Frame to Video\"]\n        VaceInputInfo[\"VaceInputInfo\u003cbr/\u003eVideo Editing\"]\n        AnimateInputInfo[\"AnimateInputInfo\u003cbr/\u003eCharacter Animation\"]\n        WorldPlayI2VInputInfo[\"WorldPlayI2VInputInfo\u003cbr/\u003eI2V with Pose Control\"]\n        WorldPlayT2VInputInfo[\"WorldPlayT2VInputInfo\u003cbr/\u003eT2V with Pose Control\"]\n    end\n    \n    subgraph \"Image Generation Tasks\"\n        T2IInputInfo[\"T2IInputInfo\u003cbr/\u003eText-to-Image\"]\n        I2IInputInfo[\"I2IInputInfo\u003cbr/\u003eImage-to-Image\"]\n    end\n    \n    subgraph \"Audio+Video Tasks\"\n        T2AVInputInfo[\"T2AVInputInfo\u003cbr/\u003eText-to-Audio+Video\"]\n        I2AVInputInfo[\"I2AVInputInfo\u003cbr/\u003eImage-to-Audio+Video\"]\n    end\n    \n    subgraph \"Special Patterns\"\n        SekoTalkInputs[\"SekoTalkInputs\u003cbr/\u003eUses UNSET sentinel\u003cbr/\u003eFor S2V/RS2V tasks\"]\n    end\n    \n    subgraph \"Helper Functions\"\n        init_empty[\"init_empty_input_info(task)\u003cbr/\u003eFactory function\"]\n        init_from_args[\"init_input_info_from_args(task, args)\u003cbr/\u003eFrom argparse\"]\n        fill_defaults[\"fill_input_info_from_defaults()\u003cbr/\u003eMerge defaults\"]\n        update_dict[\"update_input_info_from_dict()\u003cbr/\u003eUpdate from dict\"]\n        update_obj[\"update_input_info_from_object()\u003cbr/\u003eUpdate from object\"]\n    end\n    \n    init_empty --\u003e T2VInputInfo\n    init_empty --\u003e I2VInputInfo\n    init_empty --\u003e S2VInputInfo\n    init_empty --\u003e RS2VInputInfo\n    init_empty --\u003e T2IInputInfo\n    init_empty --\u003e I2IInputInfo\n    init_empty --\u003e T2AVInputInfo\n    init_empty --\u003e I2AVInputInfo\n    init_empty --\u003e WorldPlayI2VInputInfo\n    init_empty --\u003e WorldPlayT2VInputInfo\n    \n    init_from_args --\u003e SekoTalkInputs\n    \n    fill_defaults -.uses.-\u003e T2VInputInfo\n    fill_defaults -.uses.-\u003e I2VInputInfo\n    update_dict -.modifies.-\u003e S2VInputInfo\n    update_obj -.modifies.-\u003e RS2VInputInfo\n```\n\n**Sources:** [lightx2v/utils/input_info.py:16-315]()\n\n---\n\n## InputInfo Classes Overview\n\n| Class Name | Task | Key Fields | Purpose |\n|------------|------|------------|---------|\n| `T2VInputInfo` | `t2v` | `prompt`, `latent_shape`, `target_shape` | Text-to-video generation |\n| `I2VInputInfo` | `i2v` | `prompt`, `image_path`, `original_shape`, `resized_shape`, `pose` | Image-to-video with optional pose conditioning |\n| `S2VInputInfo` | `s2v` | `prompt`, `image_path`, `audio_path`, `audio_num`, `with_mask`, `audio_clip`, `overlap_frame`, `overlap_latent` | Speech-driven video (audio-to-video) with temporal consistency |\n| `RS2VInputInfo` | `rs2v` | All `S2VInputInfo` fields + `ref_state`, `is_first`, `is_last` | Referenced speech-to-video for long-form audio with state management |\n| `T2IInputInfo` | `t2i` | `prompt`, `target_shape`, `image_shapes`, `txt_seq_lens`, `aspect_ratio` | Text-to-image generation |\n| `I2IInputInfo` | `i2i` | `prompt`, `image_path`, `processed_image_size`, `original_size` | Image-to-image editing |\n| `T2AVInputInfo` | `t2av` | `prompt`, `audio_latent_shape`, `latent_shape` | Text-to-audio+video (LTX2) |\n| `I2AVInputInfo` | `i2av` | `prompt`, `image_path`, `image_strength` | Image-to-audio+video with conditioning strength |\n| `Flf2vInputInfo` | `flf2v` | `image_path`, `last_frame_path` | First and last frame to video interpolation |\n| `VaceInputInfo` | `vace` | `src_ref_images`, `src_video`, `src_mask` | Video editing with masks |\n| `AnimateInputInfo` | `animate` | `src_pose_path`, `src_face_path`, `src_bg_path`, `src_mask_path` | Character animation with pose/face controls |\n| `WorldPlayI2VInputInfo` | `worldplay_i2v` | `pose`, `viewmats`, `Ks`, `action`, `model_type`, `chunk_latent_frames` | WorldPlay I2V with camera pose control |\n| `WorldPlayT2VInputInfo` | `worldplay_t2v` | `pose`, `viewmats`, `Ks`, `action`, `model_type` | WorldPlay T2V with camera pose control |\n| `SekoTalkInputs` | `s2v`, `rs2v` | All fields use `UNSET` default | Special pattern for runtime field validation |\n\n**Sources:** [lightx2v/utils/input_info.py:16-284]()\n\n---\n\n## Common Fields Across InputInfo Types\n\nAll `InputInfo` dataclasses share a core set of fields that handle generation parameters, output control, and shape specifications.\n\n### Universal Fields\n\n```python\n# Core generation parameters\nseed: int                          # Random seed for reproducibility\nprompt: str                        # Primary text prompt\nprompt_enhanced: str               # Enhanced/expanded prompt (optional)\nnegative_prompt: str               # Negative prompt for guidance\n\n# Output control\nsave_result_path: str              # File path to save result\nreturn_result_tensor: bool         # Whether to return tensor (for ComfyUI)\n\n# Shape specifications\nresize_mode: str                   # \"adaptive\", \"crop\", \"pad\", etc.\ntarget_shape: list                 # Desired output shape [F, H, W] or [H, W]\n```\n\n### Video-Specific Fields\n\nVideo generation tasks include additional shape tracking fields:\n\n```python\n# Shape tracking through pipeline\noriginal_shape: list               # Input image/video shape\nresized_shape: list                # After preprocessing resize\nlatent_shape: list                 # Latent space dimensions [B, F, H, W]\n```\n\n### Runtime State Fields\n\nSome tasks maintain state across multiple clips or frames:\n\n```python\n# Temporal consistency (S2V, RS2V)\noverlap_frame: torch.Tensor        # Previous frame(s) for consistency\noverlap_latent: torch.Tensor       # Previous latent(s) for initialization\naudio_clip: torch.Tensor           # Preprocessed audio segment\n\n# Reference state management (RS2V)\nref_state: int                     # Reference frame index\nis_first: bool                     # First clip in sequence\nis_last: bool                      # Last clip in sequence\n```\n\n**Sources:** [lightx2v/utils/input_info.py:17-144]()\n\n---\n\n## Task-Specific InputInfo Details\n\n### T2VInputInfo - Text-to-Video\n\nSimplest video generation input with only text prompt and shape requirements.\n\n```python\n@dataclass\nclass T2VInputInfo:\n    seed: int\n    prompt: str\n    prompt_enhanced: str\n    negative_prompt: str\n    save_result_path: str\n    return_result_tensor: bool\n    resize_mode: str\n    latent_shape: list              # [B, F, H, W] in latent space\n    target_shape: list              # [F, H, W] in pixel space\n```\n\n**Sources:** [lightx2v/utils/input_info.py:16-28]()\n\n### I2VInputInfo - Image-to-Video\n\nExtends T2V with image conditioning and supports optional pose control for WorldPlay models.\n\n```python\n@dataclass\nclass I2VInputInfo:\n    # ... inherits T2V fields\n    image_path: str                 # Reference image path\n    original_shape: list            # Original image dimensions\n    resized_shape: list             # After preprocessing\n    pose: str = None                # Optional: WorldPlay pose string\n```\n\n**Sources:** [lightx2v/utils/input_info.py:30-47]()\n\n### S2VInputInfo - Speech-to-Video (Audio-Driven)\n\nMost complex video input, supporting multi-person audio and temporal consistency.\n\n```python\n@dataclass\nclass S2VInputInfo:\n    # ... inherits I2V fields\n    audio_path: str                 # Audio file or directory\n    audio_num: int                  # Number of audio sources\n    with_mask: bool                 # Use spatial masks for multi-person\n    stream_config: dict             # Streaming generation config\n    \n    # Runtime state for temporal consistency\n    overlap_frame: torch.Tensor     # Previous frames\n    overlap_latent: torch.Tensor    # Previous latents\n    audio_clip: torch.Tensor        # Preprocessed audio segment\n```\n\nThe `audio_path` can be:\n- Single audio file: `\"path/to/audio.mp3\"`\n- Directory for multi-person: `\"path/to/talk_objects/\"` (see [Multi-Person Audio Processing](#7.7))\n\n**Sources:** [lightx2v/utils/input_info.py:87-112]()\n\n### RS2VInputInfo - Referenced Speech-to-Video\n\nExtends S2V with additional state management for long-form audio generation.\n\n```python\n@dataclass\nclass RS2VInputInfo:\n    # ... inherits S2VInputInfo fields\n    ref_state: int                  # Reference frame selection state\n    is_first: bool                  # First clip in sequence\n    is_last: bool                   # Last clip in sequence\n```\n\nUsed by `ShotRS2VPipeline` to generate long videos by chaining clips with varying reference frame strategies.\n\n**Sources:** [lightx2v/utils/input_info.py:114-144](), [lightx2v/shot_runner/rs2v_infer.py:25-106]()\n\n### T2IInputInfo and I2IInputInfo - Image Generation\n\nImage generation tasks use different shape fields optimized for 2D generation.\n\n```python\n@dataclass\nclass T2IInputInfo:\n    # ... core fields\n    image_shapes: list              # Multiple image dimensions\n    txt_seq_lens: list              # [pos_len, neg_len] for encoders\n    aspect_ratio: str               # \"1:1\", \"16:9\", etc.\n\n@dataclass\nclass I2IInputInfo:\n    # ... T2I fields plus\n    image_path: str\n    processed_image_size: int       # Size after preprocessing\n    original_size: list             # Original dimensions\n```\n\n**Sources:** [lightx2v/utils/input_info.py:169-200]()\n\n### T2AVInputInfo and I2AVInputInfo - Audio+Video\n\nLTX2 model tasks that generate synchronized audio and video.\n\n```python\n@dataclass\nclass T2AVInputInfo:\n    # ... core fields\n    audio_latent_shape: list        # Audio latent dimensions\n    latent_shape: list              # Video latent dimensions\n    target_shape: list              # Output video shape\n\n@dataclass\nclass I2AVInputInfo:\n    # ... T2AV fields plus\n    image_path: str\n    image_strength: float           # Conditioning strength [0, 1]\n    original_shape: list\n    resized_shape: list\n```\n\n**Sources:** [lightx2v/utils/input_info.py:202-233]()\n\n### WorldPlay InputInfo - Camera-Controlled Generation\n\nWorldPlay models support camera pose and action conditioning via specialized InputInfo.\n\n```python\n@dataclass\nclass WorldPlayI2VInputInfo:\n    # ... standard I2V fields\n    pose: str                       # \"w-3, right-0.5\" or JSON path\n    model_type: str = \"ar\"          # \"ar\" or \"bi\"\n    chunk_latent_frames: int = 4    # Chunking for long videos\n    \n    # Computed from pose string during processing\n    viewmats: torch.Tensor = None   # Camera view matrices\n    Ks: torch.Tensor = None         # Camera intrinsics\n    action: torch.Tensor = None     # Action embeddings\n```\n\nThe `pose` field supports:\n- **Pose string**: `\"w-3, right-0.5\"` (parsed by WorldPlay pose utilities)\n- **JSON path**: Path to JSON file with camera trajectory\n\n**Sources:** [lightx2v/utils/input_info.py:235-284]()\n\n---\n\n## UNSET Sentinel and Optional Fields\n\n### The UNSET Pattern\n\nLightX2V uses a special sentinel value `UNSET` to distinguish between \"field not provided\" and \"field explicitly set to None\".\n\n```python\nclass _UnsetType:\n    def __repr__(self):\n        return \"UNSET\"\n\nUNSET = _UnsetType()\n```\n\nThis allows runtime validation to determine which fields were actually provided by the user versus which should use defaults.\n\n**Sources:** [lightx2v/utils/input_info.py:8-13]()\n\n### SekoTalkInputs - UNSET-Based InputInfo\n\n`SekoTalkInputs` uses `UNSET` as the default for all fields, enabling flexible runtime validation:\n\n```python\n@dataclass\nclass SekoTalkInputs:\n    infer_steps: int | Any = UNSET\n    seed: int | Any = UNSET\n    prompt: str | Any = UNSET\n    # ... all fields default to UNSET\n    \n    @classmethod\n    def from_args(cls, args, **overrides):\n        \"\"\"Build from argparse, filtering by field names.\"\"\"\n        field_names = {f.name for f in fields(cls)}\n        data = {k: v for k, v in vars(args).items() if k in field_names}\n        data.update(overrides)\n        return cls(**data)\n    \n    def normalize_unset_to_none(self):\n        \"\"\"Replace UNSET with None before inference.\"\"\"\n        for f in fields(self):\n            if getattr(self, f.name) is UNSET:\n                setattr(self, f.name, None)\n        return self\n```\n\n**Workflow:**\n1. Create with `from_args()` - only provided fields are set, rest remain `UNSET`\n2. Fill from defaults with `fill_input_info_from_defaults()`\n3. Call `normalize_unset_to_none()` before passing to runner\n\n**Sources:** [lightx2v/utils/input_info.py:317-369]()\n\n---\n\n## Initialization and Helper Functions\n\n### Factory Functions\n\n```mermaid\ngraph LR\n    subgraph \"Initialization Entry Points\"\n        init_empty[\"init_empty_input_info(task)\"]\n        init_from_args[\"init_input_info_from_args(task, args, **overrides)\"]\n    end\n    \n    subgraph \"Task-Specific Creation\"\n        init_empty --\u003e T2VInputInfo\n        init_empty --\u003e I2VInputInfo\n        init_empty --\u003e S2VInputInfo\n        init_empty --\u003e T2IInputInfo\n        init_from_args --\u003e SekoTalkInputs\n    end\n    \n    subgraph \"Post-Initialization Helpers\"\n        fill[\"fill_input_info_from_defaults(input_info, defaults)\"]\n        update_dict[\"update_input_info_from_dict(input_info, data)\"]\n        update_obj[\"update_input_info_from_object(input_info, obj)\"]\n        normalize[\"input_info.normalize_unset_to_none()\"]\n    end\n    \n    T2VInputInfo --\u003e fill\n    SekoTalkInputs --\u003e fill\n    fill --\u003e update_dict\n    update_dict --\u003e normalize\n```\n\n### init_empty_input_info(task)\n\nCreates an empty InputInfo instance for the specified task:\n\n```python\ndef init_empty_input_info(task):\n    if task == \"t2v\":\n        return T2VInputInfo()\n    elif task == \"i2v\":\n        return I2VInputInfo()\n    elif task == \"s2v\":\n        return S2VInputInfo()\n    # ... all task types\n    else:\n        raise ValueError(f\"Unsupported task: {task}\")\n```\n\n**Sources:** [lightx2v/utils/input_info.py:286-315]()\n\n### init_input_info_from_args(task, args, **overrides)\n\nCreates InputInfo from argparse.Namespace with override support:\n\n```python\ndef init_input_info_from_args(task, args, **overrides):\n    if task in [\"s2v\", \"rs2v\"]:\n        return SekoTalkInputs.from_args(args, **overrides)\n    else:\n        raise ValueError(f\"Unsupported task: {task}\")\n```\n\nPriority: `args` \u003c `overrides`. The overrides allow callers to inject specific values that take precedence over argparse results.\n\n**Sources:** [lightx2v/utils/input_info.py:371-376]()\n\n### fill_input_info_from_defaults(input_info, defaults)\n\nFills `UNSET` fields from a defaults dictionary:\n\n```python\ndef fill_input_info_from_defaults(input_info, defaults):\n    for key in input_info.__dataclass_fields__:\n        if key in defaults and getattr(input_info, key) is UNSET:\n            setattr(input_info, key, defaults[key])\n```\n\nUsed by `ShotPipeline.check_input_info()` to merge config-level defaults with user inputs.\n\n**Sources:** [lightx2v/utils/input_info.py:378-382]()\n\n### Update Functions\n\n```python\ndef update_input_info_from_dict(input_info, data):\n    \"\"\"Update InputInfo fields from dictionary.\"\"\"\n    for key in input_info.__dataclass_fields__:\n        if key in data:\n            setattr(input_info, key, data[key])\n\ndef update_input_info_from_object(input_info, obj):\n    \"\"\"Update InputInfo fields from object attributes.\"\"\"\n    for key in input_info.__dataclass_fields__:\n        if hasattr(obj, key):\n            setattr(input_info, key, getattr(obj, key))\n```\n\nThese provide flexible ways to update InputInfo from various sources (dicts, config objects, etc.).\n\n**Sources:** [lightx2v/utils/input_info.py:384-394]()\n\n---\n\n## Integration with Configuration System\n\n### ALL_INPUT_INFO_KEYS\n\nThe configuration system needs to distinguish between InputInfo fields (which should be passed to runners) and config-only fields (which control system behavior). This is achieved through `ALL_INPUT_INFO_KEYS`:\n\n```python\ndef get_all_input_info_keys():\n    \"\"\"Extract all field names from all InputInfo dataclasses.\"\"\"\n    all_keys = set()\n    current_module = inspect.currentframe().f_globals\n    \n    for name, obj in current_module.items():\n        if inspect.isclass(obj) and name.endswith(\"InputInfo\") and hasattr(obj, \"__dataclass_fields__\"):\n            all_keys.update(obj.__dataclass_fields__.keys())\n    \n    return all_keys\n\nALL_INPUT_INFO_KEYS = get_all_input_info_keys()\n```\n\n**Purpose:** Provides a global set of all possible InputInfo field names for filtering.\n\n**Sources:** [lightx2v/utils/input_info.py:396-410]()\n\n### Configuration Filtering\n\nThe `set_args2config()` function uses `ALL_INPUT_INFO_KEYS` to separate InputInfo fields from config fields:\n\n```python\ndef set_args2config(args):\n    config = get_default_config()\n    # Only add non-InputInfo keys to config\n    config.update({k: v for k, v in vars(args).items() if k not in ALL_INPUT_INFO_KEYS})\n    return config\n```\n\nThis ensures:\n- **Config dict** receives: `model_path`, `infer_steps`, `cpu_offload`, etc.\n- **InputInfo** receives: `seed`, `prompt`, `image_path`, `save_result_path`, etc.\n\n**Sources:** [lightx2v/utils/set_config.py:38-41]()\n\n### default_input_info in Config JSON\n\nConfig JSON files can specify default values for InputInfo fields:\n\n```json\n{\n    \"model_cls\": \"seko_talk\",\n    \"task\": \"s2v\",\n    \"target_video_length\": 33,\n    \"infer_steps\": 4,\n    \n    \"default_input_info\": {\n        \"infer_steps\": 4,\n        \"resize_mode\": \"adaptive\",\n        \"prompt\": \"A male speaking with focused gaze\",\n        \"negative_prompt\": \"...\",\n        \"image_path\": \"assets/inputs/audio/seko_input.png\",\n        \"audio_path\": \"assets/inputs/audio/seko_input.mp3\",\n        \"save_result_path\": \"save_results/output.mp4\"\n    }\n}\n```\n\nThese defaults are applied via `fill_input_info_from_defaults()` in pipelines.\n\n**Sources:** [configs/seko_talk/shot/stream/s2v.json:27-36](), [configs/seko_talk/shot/rs2v/rs2v.json:17-26]()\n\n---\n\n## Usage in Runners and Pipelines\n\n### Runner Input Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant CLI as \"CLI/API\"\n    participant Config as \"set_config()\"\n    participant Factory as \"init_input_info_from_args()\"\n    participant Fill as \"fill_input_info_from_defaults()\"\n    participant Pipeline as \"ShotPipeline\"\n    participant Runner as \"Runner.run_pipeline()\"\n    \n    User-\u003e\u003eCLI: args (seed, prompt, image_path, ...)\n    CLI-\u003e\u003eConfig: Filter args by ALL_INPUT_INFO_KEYS\n    Config--\u003e\u003eCLI: config dict (model_path, infer_steps, ...)\n    CLI-\u003e\u003eFactory: Create InputInfo from args\n    Factory--\u003e\u003eCLI: InputInfo with UNSET fields\n    CLI-\u003e\u003eFill: Apply default_input_info from config\n    Fill--\u003e\u003eCLI: InputInfo with defaults filled\n    CLI-\u003e\u003ePipeline: input_info\n    Pipeline-\u003e\u003eRunner: run_pipeline(input_info)\n    Runner-\u003e\u003eRunner: Use seed, prompt, shapes, etc.\n    Runner--\u003e\u003ePipeline: generated output\n```\n\n**Sources:** [lightx2v/utils/set_config.py:38-41](), [lightx2v/shot_runner/shot_base.py:74-78]()\n\n### Shape Calculation Fields\n\nRunners use InputInfo shape fields to calculate latent dimensions:\n\n```python\n# In WanAudioRunner.run_input_encoder()\nlatent_shape = input_info.latent_shape  # [B, F, H, W]\nF, H, W = latent_shape[1], latent_shape[2], latent_shape[3]\n\n# Calculate latent dimensions from target video\nvideo_height = input_info.target_shape[1]\nvideo_width = input_info.target_shape[2]\nlatent_height = video_height // vae_stride[1]\nlatent_width = video_width // vae_stride[2]\n```\n\nThe shape fields track transformations through the pipeline:\n1. **original_shape**: Raw input dimensions\n2. **resized_shape**: After preprocessing (crop/pad)\n3. **latent_shape**: In VAE latent space\n4. **target_shape**: Final output dimensions\n\n**Sources:** [lightx2v/utils/input_info.py:40-44](), [lightx2v/utils/input_info.py:101-105]()\n\n### Runtime State in Multi-Clip Generation\n\nFor tasks like S2V and RS2V that generate multiple clips, InputInfo carries state between clips:\n\n```python\n# In ShotRS2VPipeline.generate()\nfor idx in range(total_clips):\n    # Set clip-specific state\n    clip_input_info.is_first = (idx == 0)\n    clip_input_info.is_last = (idx == total_clips - 1)\n    clip_input_info.ref_state = ref_state_sequence[idx % len(ref_state_sequence)]\n    clip_input_info.seed = clip_input_info.seed + idx\n    clip_input_info.audio_clip = audio_reader.next_frame()\n    \n    # Generate clip\n    gen_clip_video, audio_clip, gen_latents = runner.run_clip_pipeline(clip_input_info)\n    \n    # Update state for next clip\n    clip_input_info.overlap_latent = gen_latents[:, -1:]\n```\n\n**Key state fields:**\n- `overlap_frame`: Previous frame(s) for temporal consistency\n- `overlap_latent`: Previous latent(s) for initialization (see [WanAudioRunner](#5.2))\n- `audio_clip`: Preprocessed audio segment\n- `ref_state`: Reference frame selection strategy\n- `is_first`/`is_last`: Boundary markers for special handling\n\n**Sources:** [lightx2v/shot_runner/rs2v_infer.py:66-92]()\n\n### ShotPipeline InputInfo Management\n\n`ShotPipeline` manages InputInfo for multi-stage generation:\n\n```python\nclass ShotPipeline:\n    def check_input_info(self, user_input_info, clip_config):\n        \"\"\"Apply defaults and normalize UNSET values.\"\"\"\n        default_input_info = clip_config.get(\"default_input_info\", None)\n        if default_input_info is not None:\n            fill_input_info_from_defaults(user_input_info, default_input_info)\n        return user_input_info.normalize_unset_to_none()\n    \n    def update_input_info(self, input_data):\n        \"\"\"Sync external inputs to all clips.\"\"\"\n        data = self._input_data_to_dict(input_data)\n        for key in [\"seed\", \"image_path\", \"audio_path\", \"prompt\", ...]:\n            if key in data and data[key] is not None:\n                setattr(self.shot_cfg, key, data[key])\n        \n        for clip_input in self.clip_inputs.values():\n            update_input_info_from_dict(clip_input, data)\n            # Reset state fields\n            clip_input.overlap_frame = None\n            clip_input.overlap_latent = None\n            clip_input.audio_clip = None\n```\n\n**Sources:** [lightx2v/shot_runner/shot_base.py:74-105]()\n\n---\n\n## InputInfo Lifecycle Diagram\n\n```mermaid\nstateDiagram-v2\n    [*] --\u003e Creation: User provides args\n    Creation --\u003e Initialization: init_input_info_from_args()\n    Initialization --\u003e UNSET_State: All fields = UNSET\n    UNSET_State --\u003e Defaults_Applied: fill_input_info_from_defaults()\n    Defaults_Applied --\u003e Normalized: normalize_unset_to_none()\n    Normalized --\u003e Runner_Receives: ShotPipeline.check_input_info()\n    Runner_Receives --\u003e Runtime_Updates: Set audio_clip, overlap_latent\n    Runtime_Updates --\u003e Inference: Runner.run_pipeline()\n    Inference --\u003e Output_Generated\n    Output_Generated --\u003e [*]\n    \n    note right of UNSET_State\n        Fields that user didn't provide\n        remain as UNSET sentinel\n    end note\n    \n    note right of Defaults_Applied\n        Config's default_input_info\n        fills UNSET fields\n    end note\n    \n    note right of Normalized\n        UNSET -\u003e None conversion\n        before passing to runner\n    end note\n    \n    note right of Runtime_Updates\n        Runner populates state fields:\n        overlap_frame, audio_clip, etc.\n    end note\n```\n\n**Sources:** [lightx2v/utils/input_info.py:348-369](), [lightx2v/shot_runner/shot_base.py:74-105]()\n\n---\n\n## Complete InputInfo Field Reference\n\n### T2VInputInfo Fields\n| Field | Type | Purpose |\n|-------|------|---------|\n| `seed` | `int` | Random seed |\n| `prompt` | `str` | Text prompt |\n| `prompt_enhanced` | `str` | Enhanced prompt |\n| `negative_prompt` | `str` | Negative guidance |\n| `save_result_path` | `str` | Output path |\n| `return_result_tensor` | `bool` | Return tensor flag |\n| `resize_mode` | `str` | Resize strategy |\n| `latent_shape` | `list` | Latent dimensions [B,F,H,W] |\n| `target_shape` | `list` | Output shape [F,H,W] |\n\n### I2VInputInfo Additional Fields\n| Field | Type | Purpose |\n|-------|------|---------|\n| `image_path` | `str` | Reference image |\n| `original_shape` | `list` | Input image shape |\n| `resized_shape` | `list` | After preprocessing |\n| `pose` | `str` | Optional pose string |\n\n### S2VInputInfo Additional Fields\n| Field | Type | Purpose |\n|-------|------|---------|\n| `audio_path` | `str` | Audio file/directory |\n| `audio_num` | `int` | Number of speakers |\n| `with_mask` | `bool` | Use spatial masks |\n| `stream_config` | `dict` | Streaming config |\n| `overlap_frame` | `torch.Tensor` | Previous frames |\n| `overlap_latent` | `torch.Tensor` | Previous latents |\n| `audio_clip` | `torch.Tensor` | Audio segment |\n\n### RS2VInputInfo Additional Fields\n| Field | Type | Purpose |\n|-------|------|---------|\n| `ref_state` | `int` | Reference frame index |\n| `is_first` | `bool` | First clip flag |\n| `is_last` | `bool` | Last clip flag |\n\n### T2IInputInfo Fields\n| Field | Type | Purpose |\n|-------|------|---------|\n| `target_shape` | `list` | Output shape [H,W] |\n| `image_shapes` | `list` | Multiple dimensions |\n| `txt_seq_lens` | `list` | [pos_len, neg_len] |\n| `aspect_ratio` | `str` | \"1:1\", \"16:9\", etc. |\n\n### I2IInputInfo Additional Fields\n| Field | Type | Purpose |\n|-------|------|---------|\n| `image_path` | `str` | Input image |\n| `processed_image_size` | `int` | Processed size |\n| `original_size` | `list` | Original dimensions |\n\n### WorldPlay InputInfo Fields\n| Field | Type | Purpose |\n|-------|------|---------|\n| `pose` | `str` | Pose string or JSON |\n| `model_type` | `str` | \"ar\" or \"bi\" |\n| `chunk_latent_frames` | `int` | Chunk size |\n| `viewmats` | `torch.Tensor` | Camera views |\n| `Ks` | `torch.Tensor` | Camera intrinsics |\n| `action` | `torch.Tensor` | Action embeddings |\n\n**Sources:** [lightx2v/utils/input_info.py:16-284]()"])</script><script>self.__next_f.push([1,"4c:T72a0,"])</script><script>self.__next_f.push([1,"# Custom Runner Development\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [configs/seko_talk/shot/rs2v/rs2v.json](configs/seko_talk/shot/rs2v/rs2v.json)\n- [configs/seko_talk/shot/stream/f2v.json](configs/seko_talk/shot/stream/f2v.json)\n- [configs/seko_talk/shot/stream/s2v.json](configs/seko_talk/shot/stream/s2v.json)\n- [lightx2v/common/modules/weight_module.py](lightx2v/common/modules/weight_module.py)\n- [lightx2v/models/schedulers/scheduler.py](lightx2v/models/schedulers/scheduler.py)\n- [lightx2v/models/schedulers/wan/audio/scheduler.py](lightx2v/models/schedulers/wan/audio/scheduler.py)\n- [lightx2v/pipeline.py](lightx2v/pipeline.py)\n- [lightx2v/server/__main__.py](lightx2v/server/__main__.py)\n- [lightx2v/server/config.py](lightx2v/server/config.py)\n- [lightx2v/server/schema.py](lightx2v/server/schema.py)\n- [lightx2v/server/services/distributed_utils.py](lightx2v/server/services/distributed_utils.py)\n- [lightx2v/server/services/generation/base.py](lightx2v/server/services/generation/base.py)\n- [lightx2v/server/services/generation/image.py](lightx2v/server/services/generation/image.py)\n- [lightx2v/server/services/inference/worker.py](lightx2v/server/services/inference/worker.py)\n- [lightx2v/shot_runner/rs2v_infer.py](lightx2v/shot_runner/rs2v_infer.py)\n- [lightx2v/shot_runner/shot_base.py](lightx2v/shot_runner/shot_base.py)\n- [lightx2v/shot_runner/stream_infer.py](lightx2v/shot_runner/stream_infer.py)\n- [lightx2v/utils/input_info.py](lightx2v/utils/input_info.py)\n- [lightx2v/utils/set_config.py](lightx2v/utils/set_config.py)\n- [scripts/server/post_i2i_with_lora.py](scripts/server/post_i2i_with_lora.py)\n- [scripts/server/start_server_i2i_with_loradir.sh](scripts/server/start_server_i2i_with_loradir.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides a comprehensive guide for developers who need to create custom runner classes in LightX2V. A **runner** is a task-specific orchestration layer that manages the complete inference pipeline for a particular model architecture or task type (e.g., text-to-video, audio-to-video, image-to-image). Custom runners are necessary when integrating new model architectures or implementing specialized inference workflows.\n\nFor information about using existing runners, see [Model Runners and Tasks](#5). For details about the broader pipeline architecture, see [Core Architecture](#4). For multi-stage generation workflows, see [ShotPipeline - Multi-Clip Generation](#5.10).\n\n---\n\n## Runner Architecture Overview\n\n### Role in the System\n\nRunners sit between the high-level `LightX2VPipeline` interface and the low-level model components (transformers, encoders, VAEs, schedulers). They are responsible for:\n\n1. **Module initialization**: Loading and configuring all model components\n2. **Input preprocessing**: Converting user inputs to model-ready tensors\n3. **Inference orchestration**: Coordinating the encoder  diffusion  decoder pipeline\n4. **Output postprocessing**: Converting latents to final media files\n\n```mermaid\ngraph TB\n    User[\"User Interface\u003cbr/\u003e(CLI, API, Gradio)\"]\n    Pipeline[\"LightX2VPipeline\u003cbr/\u003elightx2v/pipeline.py\"]\n    Registry[\"RUNNER_REGISTER\u003cbr/\u003elightx2v/utils/registry_factory.py\"]\n    BaseRunner[\"BaseRunner\u003cbr/\u003eAbstract interface\"]\n    DefaultRunner[\"DefaultRunner\u003cbr/\u003eCommon logic\"]\n    CustomRunner[\"CustomRunner\u003cbr/\u003eYour implementation\"]\n    \n    Models[\"Model Components\u003cbr/\u003e(Transformer, Encoders, VAE, Scheduler)\"]\n    \n    User --\u003e Pipeline\n    Pipeline --\u003e Registry\n    Registry --\u003e CustomRunner\n    CustomRunner --\u003e BaseRunner\n    CustomRunner --\u003e DefaultRunner\n    CustomRunner --\u003e Models\n    \n    subgraph \"Your Implementation\"\n        CustomRunner\n    end\n    \n    subgraph \"Framework Provided\"\n        BaseRunner\n        DefaultRunner\n        Registry\n    end\n```\n\n**Sources**: [lightx2v/pipeline.py:440-444](), [lightx2v/utils/registry_factory.py]()\n\n### Inheritance Hierarchy\n\nLightX2V provides base classes with increasing levels of functionality:\n\n| Class | Purpose | Location |\n|-------|---------|----------|\n| `BaseRunner` | Abstract interface defining required methods | Implied interface |\n| `DefaultRunner` | Common pipeline logic shared across most runners | Implied base class |\n| Task-specific bases | Additional logic for model families (e.g., `WanRunner`) | `lightx2v/models/runners/wan/` |\n| Concrete runners | Full implementations for specific models/tasks | `lightx2v/models/runners/*/` |\n\n```mermaid\ngraph TB\n    BaseRunner[\"BaseRunner\u003cbr/\u003eAbstract Interface\"]\n    DefaultRunner[\"DefaultRunner\u003cbr/\u003eCommon Logic\"]\n    \n    WanRunner[\"WanRunner\u003cbr/\u003eWAN family base\"]\n    QwenRunner[\"QwenImageRunner\u003cbr/\u003eQwen models\"]\n    ZImageRunner[\"ZImageRunner\u003cbr/\u003eZ-Image models\"]\n    \n    WanAudioRunner[\"WanAudioRunner\u003cbr/\u003eS2V with audio\"]\n    WanDistillRunner[\"WanDistillRunner\u003cbr/\u003e4-step distilled\"]\n    \n    BaseRunner --\u003e DefaultRunner\n    DefaultRunner --\u003e WanRunner\n    DefaultRunner --\u003e QwenRunner\n    DefaultRunner --\u003e ZImageRunner\n    \n    WanRunner --\u003e WanAudioRunner\n    WanRunner --\u003e WanDistillRunner\n    \n    style BaseRunner fill:#ddd\n    style DefaultRunner fill:#ddd\n    style WanAudioRunner fill:#fff\n    style WanDistillRunner fill:#fff\n```\n\n**Sources**: [lightx2v/pipeline.py:13-27]()\n\n---\n\n## Runner Lifecycle and Required Methods\n\nEvery runner must implement a standard lifecycle with two primary phases:\n\n### 1. Initialization Phase: `init_modules()`\n\nThe `init_modules()` method is called once during pipeline creation. It must:\n\n- Load all model weights (transformer, text encoders, image encoders, VAE, schedulers, etc.)\n- Apply optimizations (quantization, CPU offloading, LoRA)\n- Initialize schedulers and auxiliary components\n- Prepare the model for inference\n\n```mermaid\nsequenceDiagram\n    participant Pipeline as LightX2VPipeline\n    participant Registry as RUNNER_REGISTER\n    participant Runner as CustomRunner\n    participant Models as Model Components\n    \n    Pipeline-\u003e\u003eRegistry: Get runner by model_cls\n    Registry-\u003e\u003eRunner: __init__(config)\n    Pipeline-\u003e\u003eRunner: init_modules()\n    \n    Runner-\u003e\u003eModels: Load transformer\n    Runner-\u003e\u003eModels: Load text encoders\n    Runner-\u003e\u003eModels: Load image encoders\n    Runner-\u003e\u003eModels: Load VAE\n    Runner-\u003e\u003eModels: Initialize scheduler\n    Runner-\u003e\u003eModels: Apply quantization\n    Runner-\u003e\u003eModels: Apply CPU offload\n    Runner-\u003e\u003eRunner: Setup complete\n    \n    Runner--\u003e\u003ePipeline: Ready for inference\n```\n\n**Sources**: [lightx2v/pipeline.py:440-444](), [lightx2v/shot_runner/shot_base.py:106-110]()\n\n### 2. Inference Phase: `run_pipeline(input_info)`\n\nThe `run_pipeline()` method is called for each generation request. It typically follows a three-stage pattern:\n\n```mermaid\ngraph LR\n    InputInfo[\"InputInfo\u003cbr/\u003edataclass\"]\n    \n    Stage1[\"run_input_encoder()\u003cbr/\u003eEncode text/image/audio\"]\n    Stage2[\"run_dit()\u003cbr/\u003eDiffusion denoising\"]\n    Stage3[\"run_vae_decoder()\u003cbr/\u003eDecode to pixels\"]\n    \n    Output[\"Output\u003cbr/\u003eVideo/Image file\"]\n    \n    InputInfo --\u003e Stage1\n    Stage1 --\u003e Stage2\n    Stage2 --\u003e Stage3\n    Stage3 --\u003e Output\n    \n    Stage1 -.-\u003e Encoders[\"Text/Image/Audio Encoders\"]\n    Stage2 -.-\u003e Scheduler[\"Scheduler\u003cbr/\u003e(prepare, step_pre, step_post)\"]\n    Stage2 -.-\u003e Transformer[\"Transformer Model\"]\n    Stage3 -.-\u003e VAE[\"VAE Decoder\"]\n```\n\n**Sources**: [lightx2v/shot_runner/shot_base.py:132-134](), [lightx2v/utils/input_info.py:1-410]()\n\n### Required Method Signatures\n\n```python\nclass CustomRunner:\n    def __init__(self, config: dict):\n        \"\"\"Initialize runner with configuration dictionary.\"\"\"\n        self.config = config\n        # Store config, but don't load models yet\n    \n    def init_modules(self):\n        \"\"\"Load all model components and prepare for inference.\n        \n        Called once during pipeline initialization.\n        Must load:\n        - self.model (transformer/diffusion model)\n        - self.text_encoder (if applicable)\n        - self.image_encoder (if applicable)\n        - self.vae (encoder and decoder)\n        - self.scheduler (diffusion scheduler)\n        \"\"\"\n        pass\n    \n    def run_pipeline(self, input_info: InputInfo):\n        \"\"\"Execute complete inference pipeline.\n        \n        Args:\n            input_info: Task-specific dataclass with user inputs\n                (prompt, image_path, audio_path, seed, etc.)\n        \n        Returns:\n            None (saves output to input_info.save_result_path)\n            OR dict with tensors if input_info.return_result_tensor=True\n        \"\"\"\n        pass\n```\n\n**Sources**: [lightx2v/shot_runner/shot_base.py:106-110](), [lightx2v/server/services/inference/worker.py:55-56]()\n\n---\n\n## Registration Pattern\n\n### Registering Your Runner\n\nAll runners must be registered with `RUNNER_REGISTER` to be discoverable by the pipeline system. The registration maps a `model_cls` string identifier to your runner class.\n\n**Step 1**: Import and register in `lightx2v/pipeline.py`:\n\n```python\n# Add to imports section (lightx2v/pipeline.py:13-27)\nfrom lightx2v.models.runners.custom.custom_runner import CustomRunner  # noqa: F401\n\n# The import itself triggers registration via decorator\n```\n\n**Step 2**: Use the `@RUNNER_REGISTER.register()` decorator in your runner file:\n\n```python\n# In your runner file (e.g., lightx2v/models/runners/custom/custom_runner.py)\nfrom lightx2v.utils.registry_factory import RUNNER_REGISTER\n\n@RUNNER_REGISTER.register(\"custom_model\")  # model_cls identifier\nclass CustomRunner:\n    def __init__(self, config):\n        self.config = config\n        # ...\n```\n\n**Step 3**: Users can now instantiate your runner:\n\n```python\npipeline = LightX2VPipeline(\n    task=\"t2v\",\n    model_path=\"/path/to/model\",\n    model_cls=\"custom_model\"  # Matches your registered identifier\n)\n```\n\n```mermaid\ngraph LR\n    Register[\"RUNNER_REGISTER.register('custom_model')\"]\n    Registry[\"RUNNER_REGISTER\u003cbr/\u003edict mapping\"]\n    Pipeline[\"LightX2VPipeline\u003cbr/\u003emodel_cls='custom_model'\"]\n    GetRunner[\"RUNNER_REGISTER['custom_model']\"]\n    CustomRunner[\"CustomRunner\u003cbr/\u003einstance\"]\n    \n    Register --\u003e Registry\n    Pipeline --\u003e GetRunner\n    GetRunner --\u003e Registry\n    Registry --\u003e CustomRunner\n```\n\n**Sources**: [lightx2v/pipeline.py:13-27](), [lightx2v/pipeline.py:442](), [lightx2v/utils/registry_factory.py]()\n\n---\n\n## Configuration Management\n\n### Configuration Flow\n\nConfiguration data flows from multiple sources with a clear precedence order:\n\n1. **Default config**: `get_default_config()` provides base values\n2. **Model config**: `config.json` in model directory auto-loaded\n3. **User arguments**: CLI/API parameters override defaults\n4. **Runtime updates**: `set_config()` for dynamic changes\n\n```mermaid\ngraph TB\n    Args[\"User Arguments\u003cbr/\u003e(CLI, API, or LightX2VPipeline)\"]\n    SetConfig[\"set_config(args)\u003cbr/\u003elightx2v/utils/set_config.py:135-138\"]\n    DefaultConfig[\"get_default_config()\u003cbr/\u003elightx2v/utils/set_config.py:15-35\"]\n    AutoCalc[\"auto_calc_config(config)\u003cbr/\u003elightx2v/utils/set_config.py:44-132\"]\n    \n    ModelJSON[\"model_path/config.json\"]\n    QuantJSON[\"dit_quantized_ckpt/config.json\"]\n    VAEJSON[\"model_path/vae/config.json\"]\n    \n    FinalConfig[\"Final Config Dict\u003cbr/\u003ePassed to Runner\"]\n    \n    Args --\u003e SetConfig\n    SetConfig --\u003e DefaultConfig\n    DefaultConfig --\u003e AutoCalc\n    \n    AutoCalc --\u003e ModelJSON\n    AutoCalc --\u003e QuantJSON\n    AutoCalc --\u003e VAEJSON\n    \n    ModelJSON --\u003e FinalConfig\n    QuantJSON --\u003e FinalConfig\n    VAEJSON --\u003e FinalConfig\n    AutoCalc --\u003e FinalConfig\n```\n\n**Sources**: [lightx2v/utils/set_config.py:15-35](), [lightx2v/utils/set_config.py:44-132](), [lightx2v/utils/set_config.py:135-138]()\n\n### Accessing Configuration in Runners\n\nThe configuration dictionary is available as `self.config` throughout your runner:\n\n```python\nclass CustomRunner:\n    def __init__(self, config):\n        self.config = config\n    \n    def init_modules(self):\n        # Access model path\n        model_path = self.config[\"model_path\"]\n        \n        # Access inference parameters\n        infer_steps = self.config[\"infer_steps\"]\n        sample_shift = self.config[\"sample_shift\"]\n        \n        # Check optimization flags\n        if self.config.get(\"cpu_offload\", False):\n            self.enable_cpu_offload()\n        \n        if self.config.get(\"dit_quantized\", False):\n            self.load_quantized_weights()\n```\n\n### Key Configuration Fields\n\n| Field | Type | Purpose | Default |\n|-------|------|---------|---------|\n| `model_path` | str | Root directory containing model weights | Required |\n| `model_cls` | str | Runner identifier (e.g., \"wan2.1\", \"qwen_image\") | Required |\n| `task` | str | Task type (e.g., \"t2v\", \"i2v\", \"s2v\", \"t2i\") | Required |\n| `infer_steps` | int | Number of diffusion steps | 50 |\n| `target_video_length` | int | Output video frames (video tasks) | 81 |\n| `target_height` | int | Output height in pixels | 480 |\n| `target_width` | int | Output width in pixels | 832 |\n| `sample_guide_scale` | float | CFG scale (1.0 disables CFG) | 5.0 |\n| `sample_shift` | float | Timestep schedule shift parameter | 5.0 |\n| `cpu_offload` | bool | Enable CPU offloading | False |\n| `dit_quantized` | bool | Use quantized transformer | False |\n| `lora_configs` | list | LoRA configurations | None |\n| `parallel` | dict | Distributed parallelism config | False |\n\n**Sources**: [lightx2v/utils/set_config.py:15-35](), [lightx2v/pipeline.py:129-173]()\n\n### Updating Configuration at Runtime\n\nRunners can support runtime configuration updates via `set_config()`:\n\n```python\nclass CustomRunner:\n    def set_config(self, config_updates: dict):\n        \"\"\"Update configuration dynamically.\"\"\"\n        for key, value in config_updates.items():\n            if key in self.config:\n                self.config[key] = value\n        \n        # Reconfigure dependent components\n        if \"infer_steps\" in config_updates:\n            self.scheduler.infer_steps = config_updates[\"infer_steps\"]\n```\n\n**Sources**: [lightx2v/shot_runner/shot_base.py:112-115](), [lightx2v/server/services/inference/worker.py:95]()\n\n---\n\n## Input and Output Processing\n\n### InputInfo Dataclasses\n\nLightX2V uses task-specific dataclasses to pass structured input data to runners. Each task type has a dedicated `InputInfo` class:\n\n| Task | InputInfo Class | Key Fields |\n|------|----------------|------------|\n| Text-to-Video | `T2VInputInfo` | `prompt`, `seed`, `save_result_path` |\n| Image-to-Video | `I2VInputInfo` | `prompt`, `image_path`, `seed` |\n| Audio-to-Video | `S2VInputInfo` | `prompt`, `image_path`, `audio_path`, `audio_num` |\n| Referenced S2V | `RS2VInputInfo` | Extends `S2VInputInfo` with `ref_state`, `is_first`, `is_last` |\n| Text-to-Image | `T2IInputInfo` | `prompt`, `aspect_ratio`, `image_shapes` |\n| Image-to-Image | `I2IInputInfo` | `prompt`, `image_path`, `processed_image_size` |\n\n```mermaid\ngraph TB\n    User[\"User Request\"]\n    \n    InitEmpty[\"init_empty_input_info(task)\u003cbr/\u003elightx2v/utils/input_info.py:286-314\"]\n    UpdateDict[\"update_input_info_from_dict()\u003cbr/\u003elightx2v/utils/input_info.py:384-387\"]\n    \n    T2VInfo[\"T2VInputInfo\u003cbr/\u003elines 17-28\"]\n    I2VInfo[\"I2VInputInfo\u003cbr/\u003elines 31-47\"]\n    S2VInfo[\"S2VInputInfo\u003cbr/\u003elines 88-112\"]\n    T2IInfo[\"T2IInputInfo\u003cbr/\u003elines 170-182\"]\n    \n    RunPipeline[\"runner.run_pipeline(input_info)\"]\n    \n    User --\u003e InitEmpty\n    InitEmpty --\u003e T2VInfo\n    InitEmpty --\u003e I2VInfo\n    InitEmpty --\u003e S2VInfo\n    InitEmpty --\u003e T2IInfo\n    \n    T2VInfo --\u003e UpdateDict\n    I2VInfo --\u003e UpdateDict\n    S2VInfo --\u003e UpdateDict\n    T2IInfo --\u003e UpdateDict\n    \n    UpdateDict --\u003e RunPipeline\n```\n\n**Sources**: [lightx2v/utils/input_info.py:17-410](), [lightx2v/pipeline.py:433-436]()\n\n### Creating Custom InputInfo\n\nIf your task requires additional fields, create a custom dataclass:\n\n```python\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass CustomTaskInputInfo:\n    # Required fields\n    seed: int = field(default_factory=int)\n    prompt: str = field(default_factory=str)\n    save_result_path: str = field(default_factory=str)\n    \n    # Custom fields for your task\n    custom_parameter: float = field(default_factory=float)\n    control_signal_path: str = field(default_factory=str)\n    \n    # Shape information\n    target_shape: list = field(default_factory=list)\n    \n    # Flags\n    return_result_tensor: bool = field(default_factory=lambda: False)\n\n# Register with init_empty_input_info\ndef init_empty_input_info(task):\n    if task == \"custom_task\":\n        return CustomTaskInputInfo()\n    # ... existing tasks\n```\n\n**Sources**: [lightx2v/utils/input_info.py:16-410]()\n\n### Output Handling\n\nRunners have two output modes controlled by `input_info.return_result_tensor`:\n\n**Mode 1: File Output** (default, `return_result_tensor=False`)\n- Save to `input_info.save_result_path`\n- Return `None` from `run_pipeline()`\n- Used by CLI, Gradio, API server\n\n**Mode 2: Tensor Output** (`return_result_tensor=True`)\n- Return dict with tensors: `{\"video\": tensor, \"audio\": dict}`\n- Used by ComfyUI nodes for workflow integration\n- Do not save to disk\n\n```python\ndef run_pipeline(self, input_info):\n    # ... generate output ...\n    \n    if input_info.return_result_tensor:\n        # ComfyUI mode: return tensors\n        return {\n            \"video\": video_tensor,  # [B, C, T, H, W]\n            \"audio\": {\n                \"waveform\": audio_tensor,  # [B, 1, samples]\n                \"sample_rate\": 24000\n            }\n        }\n    else:\n        # CLI/API mode: save to file\n        save_video(video_tensor, input_info.save_result_path)\n        return None\n```\n\n**Sources**: [lightx2v/shot_runner/rs2v_infer.py:96-120](), [lightx2v/pipeline.py:429]()\n\n---\n\n## Example: Creating a Simple Custom Runner\n\n### Scenario: Text-to-Video with Custom Architecture\n\nLet's implement a minimal custom runner for a hypothetical text-to-video model.\n\n**File: `lightx2v/models/runners/custom/simple_t2v_runner.py`**\n\n```python\nimport torch\nfrom loguru import logger\nfrom lightx2v.utils.registry_factory import RUNNER_REGISTER\n\n@RUNNER_REGISTER.register(\"simple_t2v\")\nclass SimpleT2VRunner:\n    \"\"\"Minimal custom runner for text-to-video generation.\"\"\"\n    \n    def __init__(self, config):\n        self.config = config\n        self.model = None\n        self.text_encoder = None\n        self.vae = None\n        self.scheduler = None\n    \n    def init_modules(self):\n        \"\"\"Load all model components.\"\"\"\n        logger.info(\"Initializing SimpleT2VRunner...\")\n        \n        # 1. Load transformer/diffusion model\n        from your_model_package import CustomTransformer\n        self.model = CustomTransformer.from_pretrained(\n            self.config[\"model_path\"]\n        )\n        \n        # 2. Load text encoder\n        from transformers import T5EncoderModel\n        self.text_encoder = T5EncoderModel.from_pretrained(\n            f\"{self.config['model_path']}/text_encoder\"\n        )\n        \n        # 3. Load VAE\n        from diffusers import AutoencoderKL\n        self.vae = AutoencoderKL.from_pretrained(\n            f\"{self.config['model_path']}/vae\"\n        )\n        \n        # 4. Initialize scheduler\n        from lightx2v.models.schedulers import EulerScheduler\n        self.scheduler = EulerScheduler(self.config)\n        \n        # 5. Apply optimizations\n        if self.config.get(\"cpu_offload\", False):\n            self._setup_cpu_offload()\n        \n        logger.info(\"SimpleT2VRunner initialized successfully\")\n    \n    def run_pipeline(self, input_info):\n        \"\"\"Execute text-to-video generation.\"\"\"\n        # Step 1: Encode text prompt\n        text_embeddings = self._encode_text(input_info.prompt)\n        \n        # Step 2: Prepare latent noise\n        latent_shape = self._calculate_latent_shape(input_info)\n        self.scheduler.prepare(\n            seed=input_info.seed,\n            latent_shape=latent_shape,\n            infer_steps=self.config[\"infer_steps\"]\n        )\n        \n        # Step 3: Diffusion loop\n        for step_idx in range(self.config[\"infer_steps\"]):\n            self.scheduler.step_pre(step_idx)\n            \n            # Model forward pass\n            noise_pred = self.model(\n                self.scheduler.latents,\n                self.scheduler.timestep_input,\n                text_embeddings\n            )\n            \n            self.scheduler.noise_pred = noise_pred\n            self.scheduler.step_post()\n        \n        # Step 4: Decode latents to pixels\n        video = self._decode_latents(self.scheduler.latents)\n        \n        # Step 5: Save or return\n        if input_info.return_result_tensor:\n            return {\"video\": video}\n        else:\n            self._save_video(video, input_info.save_result_path)\n            return None\n    \n    def _encode_text(self, prompt):\n        \"\"\"Encode text prompt to embeddings.\"\"\"\n        # Tokenize and encode\n        # ... implementation ...\n        return text_embeddings\n    \n    def _calculate_latent_shape(self, input_info):\n        \"\"\"Calculate latent tensor shape from target dimensions.\"\"\"\n        vae_stride = self.config[\"vae_stride\"]  # (4, 8, 8)\n        batch_size = 1\n        channels = 16\n        frames = (self.config[\"target_video_length\"] - 1) // vae_stride[0] + 1\n        height = self.config[\"target_height\"] // vae_stride[1]\n        width = self.config[\"target_width\"] // vae_stride[2]\n        return [batch_size, channels, frames, height, width]\n    \n    def _decode_latents(self, latents):\n        \"\"\"Decode latent tensors to pixel space.\"\"\"\n        # VAE decode\n        # ... implementation ...\n        return video\n    \n    def _save_video(self, video, path):\n        \"\"\"Save video tensor to file.\"\"\"\n        # ... implementation ...\n        pass\n    \n    def _setup_cpu_offload(self):\n        \"\"\"Configure CPU offloading for memory efficiency.\"\"\"\n        # ... implementation ...\n        pass\n```\n\n**Sources**: Conceptual example based on patterns from [lightx2v/pipeline.py:440-444](), [lightx2v/shot_runner/shot_base.py:106-110]()\n\n---\n\n## Advanced Topics\n\n### Optional Hooks and Extensions\n\nAdvanced runners can implement optional methods for enhanced functionality:\n\n| Method | Purpose | When to Implement |\n|--------|---------|-------------------|\n| `set_config(dict)` | Runtime config updates | For dynamic reconfiguration (e.g., in server mode) |\n| `switch_lora(path, strength)` | Dynamic LoRA loading | For LoRA-enabled models with runtime switching |\n| `run_clip_pipeline(input_info)` | Single clip generation | For multi-clip/streaming scenarios |\n| `_update_lora(path, strength)` | Internal LoRA update | Called by `switch_lora` |\n| `_remove_lora()` | Remove LoRA weights | Clean up before loading new LoRA |\n\n```mermaid\ngraph TB\n    BaseMethod[\"Required Methods\u003cbr/\u003einit_modules()\u003cbr/\u003erun_pipeline()\"]\n    \n    OptConfig[\"set_config(dict)\u003cbr/\u003eRuntime updates\"]\n    OptLoRA[\"switch_lora(path, strength)\u003cbr/\u003eDynamic LoRA\"]\n    OptClip[\"run_clip_pipeline(input_info)\u003cbr/\u003eMulti-stage support\"]\n    \n    Internal[\"Internal Hooks\u003cbr/\u003e_update_lora()\u003cbr/\u003e_remove_lora()\"]\n    \n    BaseMethod --\u003e OptConfig\n    BaseMethod --\u003e OptLoRA\n    BaseMethod --\u003e OptClip\n    \n    OptLoRA --\u003e Internal\n    \n    style BaseMethod fill:#eee\n    style OptConfig fill:#fff\n    style OptLoRA fill:#fff\n    style OptClip fill:#fff\n```\n\n**Sources**: [lightx2v/pipeline.py:358-366](), [lightx2v/server/services/inference/worker.py:126-157](), [lightx2v/shot_runner/shot_base.py:112-115]()\n\n### Integration with Weight Management\n\nFor models using the `WeightModule` system (common in WAN models), your runner can leverage:\n\n- **Weight loading**: `module.load(weight_dict)` from safetensors\n- **LoRA registration**: `module.register_lora(weight_dict, strength)`\n- **LoRA updates**: `module.update_lora(weight_dict, strength)`\n- **LoRA removal**: `module.remove_lora()`\n- **CPU offloading**: `module.to_cpu()` / `module.to_cuda()`\n\n```python\nclass CustomRunnerWithWeightModule:\n    def init_modules(self):\n        from lightx2v.common.modules.weight_module import WeightModule\n        \n        # Create model with weight management\n        self.model = CustomModel()  # Inherits from WeightModule\n        \n        # Load base weights\n        import safetensors.torch as st\n        weight_dict = st.load_file(f\"{self.config['model_path']}/model.safetensors\")\n        self.model.load(weight_dict)\n        \n        # Optionally load LoRA\n        if self.config.get(\"lora_configs\"):\n            for lora_cfg in self.config[\"lora_configs\"]:\n                lora_dict = st.load_file(lora_cfg[\"path\"])\n                self.model.register_lora(lora_dict, lora_cfg[\"strength\"])\n    \n    def switch_lora(self, lora_path: str, strength: float):\n        \"\"\"Support dynamic LoRA switching.\"\"\"\n        if lora_path == \"\":\n            self.model.remove_lora()\n        else:\n            import safetensors.torch as st\n            lora_dict = st.load_file(lora_path)\n            self.model.update_lora(lora_dict, strength)\n```\n\n**Sources**: [lightx2v/common/modules/weight_module.py:1-212](), [lightx2v/pipeline.py:358-366]()\n\n### Multi-Stage and Streaming Pipelines\n\nFor complex workflows requiring multiple generation stages or streaming output:\n\n1. **Inherit from `ShotPipeline`** instead of creating a standalone runner\n2. Implement `run_clip_pipeline(input_info)` for single-clip generation\n3. Use `overlap_frame` and `overlap_latent` in `input_info` for temporal consistency\n4. See [ShotPipeline - Multi-Clip Generation](#5.10) for details\n\n```python\nfrom lightx2v.shot_runner.shot_base import ShotPipeline, ClipConfig\n\nclass CustomMultiStageRunner(ShotPipeline):\n    def __init__(self, clip_configs: list[ClipConfig]):\n        super().__init__(clip_configs)\n    \n    def generate(self, args):\n        \"\"\"Coordinate multiple clips.\"\"\"\n        stage1_runner = self.clip_generators[\"stage1\"]\n        stage2_runner = self.clip_generators[\"stage2\"]\n        \n        # Generate first stage\n        stage1_output = stage1_runner.run_clip_pipeline(input_info)\n        \n        # Use stage1 output as stage2 input\n        input_info.overlap_latent = stage1_output[\"latents\"]\n        stage2_output = stage2_runner.run_clip_pipeline(input_info)\n        \n        return stage2_output\n```\n\n**Sources**: [lightx2v/shot_runner/shot_base.py:63-135](), [lightx2v/shot_runner/rs2v_infer.py:25-107](), [lightx2v/shot_runner/stream_infer.py:17-93]()\n\n### Server and Distributed Deployment\n\nWhen deploying your runner in the HTTP server (`lightx2v.server`), ensure:\n\n1. **Distributed initialization**: Handle `torch.distributed` setup in `__init__` if `config[\"parallel\"]` is set\n2. **LoRA directory support**: Accept `lora_dir` parameter for dynamic LoRA loading\n3. **Graceful error handling**: Return structured error dicts on failure\n4. **Task data validation**: Validate all required fields in `input_info`\n\n```python\nclass CustomRunner:\n    def __init__(self, config):\n        self.config = config\n        \n        # Handle distributed setup\n        if config.get(\"parallel\"):\n            import torch.distributed as dist\n            self.rank = dist.get_rank()\n            self.world_size = dist.get_world_size()\n    \n    def run_pipeline(self, input_info):\n        try:\n            # Validate inputs\n            if not input_info.prompt:\n                raise ValueError(\"Prompt is required\")\n            \n            # ... normal processing ...\n            \n        except Exception as e:\n            logger.exception(f\"Generation failed: {e}\")\n            raise  # Let server handle error response\n```\n\n**Sources**: [lightx2v/server/services/inference/worker.py:16-194](), [lightx2v/server/__main__.py:6-29]()\n\n---\n\n## Complete Checklist for Custom Runners\n\nBefore deploying your custom runner, verify:\n\n### Registration and Discovery\n- [ ] Runner class decorated with `@RUNNER_REGISTER.register(\"model_cls_name\")`\n- [ ] Import added to `lightx2v/pipeline.py`\n- [ ] Unique `model_cls` identifier chosen\n\n### Required Methods\n- [ ] `__init__(self, config)` accepts config dict\n- [ ] `init_modules(self)` loads all model components\n- [ ] `run_pipeline(self, input_info)` performs inference\n- [ ] Both file and tensor output modes supported\n\n### Configuration\n- [ ] Accesses config via `self.config` dict\n- [ ] Handles required fields: `model_path`, `task`, `infer_steps`\n- [ ] Respects optimization flags: `cpu_offload`, `dit_quantized`, `lora_configs`\n\n### Input/Output\n- [ ] Accepts appropriate `InputInfo` dataclass for task\n- [ ] Returns `None` for file mode, dict for tensor mode\n- [ ] Checks `input_info.return_result_tensor` flag\n- [ ] Saves to `input_info.save_result_path` when applicable\n\n### Optional Enhancements\n- [ ] Implements `set_config(dict)` for runtime updates\n- [ ] Implements `switch_lora(path, strength)` if LoRA-enabled\n- [ ] Handles distributed setup if `config[\"parallel\"]` is set\n- [ ] Logs initialization and inference progress\n\n### Testing\n- [ ] Test with CLI: `python lightx2v/infer.py --model_cls custom_model ...`\n- [ ] Test with Python API: `LightX2VPipeline(model_cls=\"custom_model\")`\n- [ ] Test with server: `python -m lightx2v.server --model_cls custom_model`\n- [ ] Verify memory usage with `cpu_offload=True`\n- [ ] Verify quantization with `dit_quantized=True`\n\n**Sources**: Synthesis of requirements from [lightx2v/pipeline.py](), [lightx2v/shot_runner/shot_base.py](), [lightx2v/server/services/inference/worker.py]()"])</script><script>self.__next_f.push([1,"4d:T528a,"])</script><script>self.__next_f.push([1,"# Prompt Templates and Text Processing\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py](lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py)\n- [lightx2v/models/networks/qwen_image/infer/pre_infer.py](lightx2v/models/networks/qwen_image/infer/pre_infer.py)\n- [lightx2v/models/networks/qwen_image/infer/transformer_infer.py](lightx2v/models/networks/qwen_image/infer/transformer_infer.py)\n- [lightx2v/models/networks/qwen_image/model.py](lightx2v/models/networks/qwen_image/model.py)\n- [lightx2v/models/runners/qwen_image/qwen_image_runner.py](lightx2v/models/runners/qwen_image/qwen_image_runner.py)\n- [lightx2v/models/schedulers/qwen_image/scheduler.py](lightx2v/models/schedulers/qwen_image/scheduler.py)\n- [lightx2v/models/video_encoders/hf/qwen_image/vae.py](lightx2v/models/video_encoders/hf/qwen_image/vae.py)\n\n\u003c/details\u003e\n\n\n\n## Overview\n\nPrompt templates in LightX2V control how user text and conditional images are formatted for text encoder input. Different model families use different template structures, special tokens, and image conditioning strategies. This page documents the template system, focusing on the Qwen model family which uses vision-language models for image-to-image tasks.\n\nThis page covers template configuration, image tag formatting, and tokenization patterns. For text encoder implementations, see [Input Encoder Architecture](#4.5). For overall configuration management, see [Configuration System](#8.2).\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:55-254]()\n\n---\n\n## Prompt Processing Pipeline\n\nText and image inputs flow through a template formatting stage before tokenization and encoding. The pipeline structure varies by model family and task type.\n\n### Processing Flow Diagram\n\n```mermaid\ngraph TB\n    UserPrompt[\"User Text Prompt\"]\n    CondImages[\"Conditional Images\u003cbr/\u003e(for I2I tasks)\"]\n    \n    subgraph \"Template Configuration\"\n        PromptTemplate[\"prompt_template_encode\u003cbr/\u003econfig parameter\"]\n        TemplateIdx[\"prompt_template_encode_start_idx\u003cbr/\u003econfig parameter\"]\n        UseImageID[\"USE_IMAGE_ID_IN_PROMPT\u003cbr/\u003econfig parameter\"]\n        CondSize[\"CONDITION_IMAGE_SIZE\u003cbr/\u003econfig parameter\"]\n    end\n    \n    subgraph \"Qwen25_VLForConditionalGeneration_TextEncoder\"\n        PreprocessImg[\"preprocess_image()\u003cbr/\u003eResize to CONDITION_IMAGE_SIZE\"]\n        BuildPrompt[\"Build Image Tags\u003cbr/\u003e'Picture N: \u003cvision tags\u003e' or '\u003cvision tags\u003e'\"]\n        FormatTemplate[\"template.format(img_tags + prompt)\u003cbr/\u003eApply prompt_template_encode\"]\n        ProcessorCall[\"Qwen2VLProcessor()\u003cbr/\u003eApply chat template\"]\n        Tokenize[\"Tokenize\u003cbr/\u003emax_length=1024 + drop_idx\"]\n        ModelInfer[\"text_encoder()\u003cbr/\u003eQwen25_VLForConditionalGeneration\"]\n        ExtractHidden[\"_extract_masked_hidden()\u003cbr/\u003eDrop first drop_idx tokens\"]\n    end\n    \n    Output[\"prompt_embeds\u003cbr/\u003eencoder_attention_mask\"]\n    \n    UserPrompt --\u003e FormatTemplate\n    CondImages --\u003e PreprocessImg\n    \n    PromptTemplate --\u003e FormatTemplate\n    UseImageID --\u003e BuildPrompt\n    CondSize --\u003e PreprocessImg\n    \n    PreprocessImg --\u003e BuildPrompt\n    BuildPrompt --\u003e FormatTemplate\n    \n    FormatTemplate --\u003e ProcessorCall\n    ProcessorCall --\u003e Tokenize\n    Tokenize --\u003e ModelInfer\n    ModelInfer --\u003e ExtractHidden\n    \n    TemplateIdx --\u003e ExtractHidden\n    \n    ExtractHidden --\u003e Output\n```\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:55-254]()\n\n---\n\n## Template Configuration Parameters\n\nThe `Qwen25_VLForConditionalGeneration_TextEncoder` class [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:55-76]() loads template configuration from the model config dictionary.\n\n### Core Configuration Parameters\n\n| Parameter | Type | Purpose | Default |\n|-----------|------|---------|---------|\n| `prompt_template_encode` | `str` | Template string with `{}` placeholder for prompt insertion | Model-specific |\n| `prompt_template_encode_start_idx` | `int` | Number of initial tokens to drop from encoder output | Model-specific |\n| `USE_IMAGE_ID_IN_PROMPT` | `bool` | Whether to number multiple images as \"Picture 1:\", \"Picture 2:\", etc. | `True` |\n| `CONDITION_IMAGE_SIZE` | `int` | Target pixel area for conditioning images (width  height) | `384 * 384` |\n| `VAE_IMAGE_SIZE` | `int` | Target pixel area for VAE-encoded images | `1024 * 1024` |\n\n### Initialization Code\n\n```python\nself.prompt_template_encode = config[\"prompt_template_encode\"]\nself.prompt_template_encode_start_idx = config[\"prompt_template_encode_start_idx\"]\nself.CONDITION_IMAGE_SIZE = config.get(\"CONDITION_IMAGE_SIZE\", 384 * 384)\nself.USE_IMAGE_ID_IN_PROMPT = config.get(\"USE_IMAGE_ID_IN_PROMPT\", True)\n```\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:59-66]()\n\n### Model-Specific Defaults\n\nDifferent Qwen model variants use different `CONDITION_IMAGE_SIZE` values:\n\n| Model | `CONDITION_IMAGE_SIZE` | Notes |\n|-------|------------------------|-------|\n| Qwen-Image-Edit | `1024 * 1024` | Original model |\n| Qwen-Image-Edit-2509 | `384 * 384` | Newer variant with smaller conditioning |\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:61-64]()\n\n---\n\n## Image Tag Formatting\n\nFor image-to-image tasks, the system constructs special vision tokens to represent conditional images in the prompt.\n\n### Image Tag Structure\n\nThe `USE_IMAGE_ID_IN_PROMPT` flag controls whether images are numbered in the prompt:\n\n```mermaid\ngraph TB\n    ImageList[\"image_list\u003cbr/\u003e[Image1, Image2, Image3]\"]\n    UseIDFlag[\"USE_IMAGE_ID_IN_PROMPT\"]\n    \n    subgraph \"USE_IMAGE_ID_IN_PROMPT = True\"\n        NumberedTags[\"img_prompt_template =\u003cbr/\u003e'Picture {}: \u003c|vision_start|\u003e\u003c|image_pad|\u003e\u003c|vision_end|\u003e'\"]\n        BuildNumbered[\"base_img_prompt =\u003cbr/\u003e'Picture 1: \u003c|vision_start|\u003e...\u003c|vision_end|\u003e' +\u003cbr/\u003e'Picture 2: \u003c|vision_start|\u003e...\u003c|vision_end|\u003e' +\u003cbr/\u003e'Picture 3: \u003c|vision_start|\u003e...\u003c|vision_end|\u003e'\"]\n    end\n    \n    subgraph \"USE_IMAGE_ID_IN_PROMPT = False\"\n        UnNumberedTags[\"base_img_prompt =\u003cbr/\u003e'\u003c|vision_start|\u003e\u003c|image_pad|\u003e\u003c|vision_end|\u003e'\"]\n        BuildUnNumbered[\"Single tag for all images\u003cbr/\u003e(no numbering)\"]\n    end\n    \n    ImageList --\u003e UseIDFlag\n    UseIDFlag --\u003e|True| NumberedTags\n    UseIDFlag --\u003e|False| UnNumberedTags\n    \n    NumberedTags --\u003e BuildNumbered\n    UnNumberedTags --\u003e BuildUnNumbered\n```\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:172-189]()\n\n### Vision Token Components\n\n| Token | Purpose |\n|-------|---------|\n| `\u003c|vision_start|\u003e` | Marks beginning of image region |\n| `\u003c|image_pad|\u003e` | Placeholder for image features (replaced by processor) |\n| `\u003c|vision_end|\u003e` | Marks end of image region |\n\n### Numbered vs. Unnumbered Examples\n\n**Numbered Format (`USE_IMAGE_ID_IN_PROMPT=True`):**\n```\nPicture 1: \u003c|vision_start|\u003e\u003c|image_pad|\u003e\u003c|vision_end|\u003ePicture 2: \u003c|vision_start|\u003e\u003c|image_pad|\u003e\u003c|vision_end|\u003eA cat and a dog\n```\n\n**Unnumbered Format (`USE_IMAGE_ID_IN_PROMPT=False`):**\n```\n\u003c|vision_start|\u003e\u003c|image_pad|\u003e\u003c|vision_end|\u003eA cat and a dog\n```\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:172-189]()\n\n---\n\n## Image Preprocessing and Sizing\n\nConditional images are resized to two different resolutions before encoding: one for the vision-language model and one for VAE encoding.\n\n### Preprocessing Pipeline\n\n```mermaid\nsequenceDiagram\n    participant Input as \"Input Image\u003cbr/\u003e(arbitrary size)\"\n    participant Preprocess as \"preprocess_image()\"\n    participant Calculate as \"calculate_dimensions()\"\n    participant ImageProc as \"VaeImageProcessor\"\n    \n    Input-\u003e\u003ePreprocess: Image object\n    \n    Note over Preprocess: Get original size\n    Preprocess-\u003e\u003eCalculate: CONDITION_IMAGE_SIZE, aspect_ratio\n    Calculate--\u003e\u003ePreprocess: condition_width, condition_height\n    \n    Preprocess-\u003e\u003eCalculate: VAE_IMAGE_SIZE, aspect_ratio\n    Calculate--\u003e\u003ePreprocess: vae_width, vae_height\n    \n    Preprocess-\u003e\u003eImageProc: resize(image, condition_height, condition_width)\n    ImageProc--\u003e\u003ePreprocess: condition_image\n    \n    Preprocess-\u003e\u003eImageProc: preprocess(image, vae_height, vae_width)\n    ImageProc--\u003e\u003ePreprocess: vae_image (unsqueeze(2))\n    \n    Preprocess--\u003e\u003eInput: condition_image, vae_image, sizes\n```\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:128-134]()\n\n### Size Calculation Algorithm\n\nThe `calculate_dimensions()` function [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:45-52]() maintains aspect ratio while targeting a specific pixel area:\n\n```python\ndef calculate_dimensions(target_area, ratio):\n    width = math.sqrt(target_area * ratio)\n    height = width / ratio\n    \n    width = round(width / 32) * 32\n    height = round(height / 32) * 32\n    \n    return width, height\n```\n\n**Key properties:**\n- Preserves input aspect ratio\n- Rounds to multiples of 32 (required by VAE)\n- Output satisfies: `width * height  target_area`\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:45-52](), [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:128-134]()\n\n### Dual-Resolution Strategy\n\n| Resolution Type | Purpose | Typical Size | Usage |\n|----------------|---------|--------------|-------|\n| **Condition Image** | Vision-language model input | 384384 or 10241024 area | Passed to `Qwen2VLProcessor` for vision features |\n| **VAE Image** | VAE encoder input | 10241024 area | Encoded to latent space for image conditioning |\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:128-134]()\n\n---\n\n## Template Formatting Process\n\nThe formatted prompt combines image tags and user text according to task type.\n\n### T2I (Text-to-Image) Flow\n\n```mermaid\ngraph LR\n    UserPrompt[\"User Prompt:\u003cbr/\u003e'A red car'\"]\n    Template[\"prompt_template_encode:\u003cbr/\u003e'System instruction {}'\"]\n    Format[\"template.format(prompt)\"]\n    Tokenize[\"Tokenize\"]\n    Output[\"Model Input IDs\"]\n    \n    UserPrompt --\u003e Format\n    Template --\u003e Format\n    Format --\u003e Tokenize\n    Tokenize --\u003e Output\n```\n\n**Code path:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:216-226]()\n\n### I2I (Image-to-Image) Flow\n\n```mermaid\ngraph LR\n    UserPrompt[\"User Prompt:\u003cbr/\u003e'Make it blue'\"]\n    ImageTags[\"Image Tags:\u003cbr/\u003e'Picture 1: \u003cvision_start\u003e...\u003cvision_end\u003e'\"]\n    Concat[\"Concatenate:\u003cbr/\u003eimg_tags + user_prompt\"]\n    Template[\"prompt_template_encode:\u003cbr/\u003e'System instruction {}'\"]\n    Format[\"template.format(concatenated)\"]\n    Processor[\"Qwen2VLProcessor\u003cbr/\u003e(with condition_image_list)\"]\n    Output[\"Model Input IDs + pixel_values\"]\n    \n    ImageTags --\u003e Concat\n    UserPrompt --\u003e Concat\n    Concat --\u003e Format\n    Template --\u003e Format\n    Format --\u003e Processor\n    Processor --\u003e Output\n```\n\n**Code path:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:197-214]()\n\n### Implementation Code Reference\n\n**T2I Template Formatting:**\n```python\ntemplate = self.prompt_template_encode\ndrop_idx = self.prompt_template_encode_start_idx\ntxt = [template.format(e) for e in text]\n\nmodel_inputs = self.tokenizer(\n    txt, \n    max_length=self.tokenizer_max_length + drop_idx,\n    padding=True, \n    truncation=True, \n    return_tensors=\"pt\"\n).to(AI_DEVICE)\n```\n\n**I2I Template Formatting:**\n```python\ntemplate = self.prompt_template_encode\ndrop_idx = self.prompt_template_encode_start_idx\ntxt = [template.format(base_img_prompt + e) for e in text]\n\nmodel_inputs = self.processor(\n    text=txt,\n    images=condition_image_list,\n    padding=True,\n    return_tensors=\"pt\",\n).to(AI_DEVICE)\n```\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:197-226]()\n\n---\n\n## Text Tokenization and Encoding\n\nAfter template formatting, text is tokenized and encoded through the vision-language model.\n\n### Tokenization Process\n\n```mermaid\ngraph TB\n    FormattedText[\"Formatted Text\u003cbr/\u003e(with image tags)\"]\n    \n    subgraph \"Tokenization\"\n        Tokenizer[\"Qwen2Tokenizer or\u003cbr/\u003eQwen2VLProcessor\"]\n        MaxLen[\"max_length =\u003cbr/\u003etokenizer_max_length + drop_idx\"]\n        Padding[\"padding=True\"]\n        Truncation[\"truncation=True\"]\n    end\n    \n    subgraph \"Encoding\"\n        ModelCall[\"text_encoder()\u003cbr/\u003eQwen25_VLForConditionalGeneration\"]\n        HiddenStates[\"encoder_hidden_states.hidden_states[-1]\"]\n        AttentionMask[\"model_inputs.attention_mask\"]\n    end\n    \n    subgraph \"Post-Processing\"\n        ExtractMasked[\"_extract_masked_hidden()\u003cbr/\u003eExtract valid tokens\"]\n        DropTokens[\"Drop first drop_idx tokens\"]\n        Pad[\"Pad to max sequence length\"]\n    end\n    \n    Output[\"prompt_embeds\u003cbr/\u003eencoder_attention_mask\"]\n    \n    FormattedText --\u003e Tokenizer\n    Tokenizer --\u003e MaxLen\n    MaxLen --\u003e Padding\n    Padding --\u003e Truncation\n    Truncation --\u003e ModelCall\n    \n    ModelCall --\u003e HiddenStates\n    ModelCall --\u003e AttentionMask\n    \n    HiddenStates --\u003e ExtractMasked\n    AttentionMask --\u003e ExtractMasked\n    ExtractMasked --\u003e DropTokens\n    DropTokens --\u003e Pad\n    \n    Pad --\u003e Output\n```\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:197-246]()\n\n### Token Extraction Logic\n\nThe `_extract_masked_hidden()` method [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:121-126]() extracts valid tokens based on attention mask:\n\n```python\ndef _extract_masked_hidden(self, hidden_states: torch.Tensor, mask: torch.Tensor):\n    bool_mask = mask.bool()\n    valid_lengths = bool_mask.sum(dim=1)\n    selected = hidden_states[bool_mask]\n    split_result = torch.split(selected, valid_lengths.tolist(), dim=0)\n    return split_result\n```\n\n**Process:**\n1. Convert attention mask to boolean\n2. Sum to get valid lengths per sequence\n3. Select valid tokens using boolean indexing\n4. Split back into per-sequence tensors\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:121-126]()\n\n### Token Dropping with `prompt_template_encode_start_idx`\n\nThe `drop_idx` parameter removes template prefix tokens:\n\n```python\nsplit_hidden_states = self._extract_masked_hidden(hidden_states, attention_mask)\nsplit_hidden_states = [e[drop_idx:] for e in split_hidden_states]\n```\n\n**Purpose:** Template strings often include system instructions or special formatting that should not influence the semantic embedding. The `prompt_template_encode_start_idx` specifies how many initial tokens to discard.\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:231-232]()\n\n---\n\n## Layered Generation and Image Captioning\n\nThe Qwen model supports layered image generation, where multiple image layers are generated sequentially. This mode uses automatic image captioning.\n\n### Layered Mode Configuration\n\n```python\nself.is_layered = self.config.get(\"layered\", False)\nif self.is_layered:\n    self.layers = self.config.get(\"layers\", 4)\n    self.resolution = self.config.get(\"resolution\", 640)\n    self.VAE_IMAGE_SIZE = self.resolution * self.resolution\n```\n\nWhen `layered=True`, the system:\n1. Captures the first input image\n2. Generates a caption using `get_image_caption()` \n3. Uses the caption as the prompt for generation\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:68-72]()\n\n### Image Captioning Pipeline\n\n```mermaid\ngraph TB\n    InputImage[\"Input Image\"]\n    \n    subgraph \"get_image_caption()\"\n        PromptTemplate[\"image_caption_prompt_cn or\u003cbr/\u003eimage_caption_prompt_en\"]\n        ProcessorCall[\"vl_processor()\u003cbr/\u003etext + images\"]\n        ModelGen[\"text_encoder.generate()\u003cbr/\u003emax_new_tokens=512\"]\n        BatchDecode[\"vl_processor.batch_decode()\u003cbr/\u003eskip_special_tokens=True\"]\n    end\n    \n    Caption[\"Generated Caption\u003cbr/\u003e(used as prompt)\"]\n    \n    InputImage --\u003e PromptTemplate\n    PromptTemplate --\u003e ProcessorCall\n    ProcessorCall --\u003e ModelGen\n    ModelGen --\u003e BatchDecode\n    BatchDecode --\u003e Caption\n```\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:136-153]()\n\n### Caption Prompt Templates\n\nThe system provides Chinese and English caption templates:\n\n**Chinese Template (`image_caption_prompt_cn`):** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:105-109]()\n- Detailed instructions for image annotation\n- Emphasizes object attributes, relationships, environment details\n- Instructs to identify visible text with quotation marks\n\n**English Template (`image_caption_prompt_en`):** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:110-119]()\n- Similar structure to Chinese version\n- Focuses on natural descriptive language\n- Includes environmental and atmospheric details\n\nThe `use_en_prompt` config parameter [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:104]() selects between templates.\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:104-119](), [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:136-153]()\n\n---\n\n## CPU Offloading for Text Encoders\n\nThe Qwen text encoder supports CPU offloading to reduce GPU memory pressure during inference.\n\n### Offloading Configuration\n\n```python\nself.cpu_offload = config.get(\"qwen25vl_cpu_offload\", config.get(\"cpu_offload\", False))\n```\n\nTwo hierarchical config keys:\n1. `qwen25vl_cpu_offload`: Qwen-specific override\n2. `cpu_offload`: Global offloading flag\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:73]()\n\n### Quantized Model Device Mapping\n\nWhen quantization is enabled, specific device mapping strategies apply:\n\n```python\nif self.config.get(\"qwen25vl_quantized\", False):\n    if self.config[\"cpu_offload\"]:\n        self.device_map = {\n            \"lm_head\": AI_DEVICE,\n            \"model.visual\": \"cpu\",\n            \"model.language_model\": \"cpu\",\n        }\n    else:\n        self.device_map = AI_DEVICE\n```\n\n**Device placement strategy:**\n- `lm_head`: Always on GPU (final projection layer)\n- `model.visual`: CPU if offloading enabled\n- `model.language_model`: CPU if offloading enabled\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:78-88]()\n\n### Inference-Time Device Management\n\nDuring `infer()`, the encoder moves to GPU, processes input, then returns to CPU:\n\n```python\nif self.cpu_offload:\n    if not hasattr(self, \"device_map\") or self.device_map == AI_DEVICE:\n        self.text_encoder.to(AI_DEVICE)\n\n# ... perform inference ...\n\nif self.cpu_offload:\n    if not hasattr(self, \"device_map\") or self.device_map == AI_DEVICE:\n        self.text_encoder.to(torch.device(\"cpu\"))\n    torch_device_module.empty_cache()\n    gc.collect()\n```\n\nThis pattern enables using large text encoders on GPU-constrained systems by time-multiplexing GPU memory.\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:157-251]()\n\n---\n\n## Preferred Resolutions for Qwen Image Models\n\nThe Qwen Image models define a set of preferred output resolutions for optimal quality.\n\n### PREFERRED_QWENIMAGE_RESOLUTIONS\n\nA list of 17 predefined resolutions [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:24-42]() optimized for the model architecture:\n\n| Aspect Ratio | Resolutions |\n|--------------|-------------|\n| **16:9** | (672, 1568), (688, 1504), (720, 1456), (752, 1392), (800, 1328) |\n| **Square** | (1024, 1024) |\n| **9:16** | (1328, 800), (1392, 752), (1456, 720), (1504, 688), (1568, 672) |\n| **4:3** | (832, 1248), (880, 1184), (944, 1104) |\n| **3:4** | (1104, 944), (1184, 880), (1248, 832) |\n\n**Properties:**\n- All dimensions are multiples of 16\n- Total area ranges from ~1.05M to ~1.57M pixels\n- Covers common aspect ratios (16:9, 4:3, 1:1, 3:4, 9:16)\n\nThe runner uses `calculate_dimensions()` with the `resolution` config parameter to select appropriate dimensions from this set or compute custom dimensions.\n\n**Sources:** [lightx2v/models/input_encoders/hf/qwen25/qwen25_vlforconditionalgeneration.py:24-42]()\n\n---\n\n## Summary\n\nThe Prompt Enhancement System provides optional LLM-based preprocessing to transform concise user prompts into detailed video generation captions. Key characteristics:\n\n- **Model**: Qwen2.5-32B-Instruct via HuggingFace Transformers\n- **Template**: Structured 6-element cinematographic format\n- **Integration**: Configuration-driven activation in inference pipeline\n- **Overhead**: 2-10 seconds per prompt, 32-64GB VRAM\n- **Profiling**: Integrated with debugging system via `@ProfilingContext4DebugL1`\n\nThe system balances quality improvement against computational cost, making it suitable for high-quality production workflows but optional for resource-constrained or real-time scenarios.\n\n**Sources:** [lightx2v/utils/prompt_enhancer.py:1-79]()"])</script><script>self.__next_f.push([1,"4e:T4efb,"])</script><script>self.__next_f.push([1,"# Debugging and Profiling\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [app/README.md](app/README.md)\n- [app/gradio_demo.py](app/gradio_demo.py)\n- [app/run_gradio.sh](app/run_gradio.sh)\n- [app/run_gradio_win.bat](app/run_gradio_win.bat)\n- [docs/EN/source/deploy_guides/deploy_gradio.md](docs/EN/source/deploy_guides/deploy_gradio.md)\n- [docs/EN/source/deploy_guides/deploy_local_windows.md](docs/EN/source/deploy_guides/deploy_local_windows.md)\n- [docs/ZH_CN/source/deploy_guides/deploy_gradio.md](docs/ZH_CN/source/deploy_guides/deploy_gradio.md)\n- [docs/ZH_CN/source/deploy_guides/deploy_local_windows.md](docs/ZH_CN/source/deploy_guides/deploy_local_windows.md)\n- [lightx2v/common/offload/manager.py](lightx2v/common/offload/manager.py)\n- [scripts/win/run_wan_i2v.bat](scripts/win/run_wan_i2v.bat)\n- [scripts/win/run_wan_t2v.bat](scripts/win/run_wan_t2v.bat)\n\n\u003c/details\u003e\n\n\n\nThis document covers the debugging and profiling infrastructure in LightX2V, including environment-based profiling controls, context managers for performance measurement, memory cleanup utilities, and logging strategies. These tools help developers diagnose issues, measure performance bottlenecks, and optimize inference workflows.\n\nFor information about performance optimization features (quantization, offloading, caching), see [Performance Optimization](#6). For details on configuration management, see [Configuration System](#8.2).\n\n---\n\n## Overview of Debugging Infrastructure\n\nLightX2V provides a multi-level debugging and profiling system that can be controlled through environment variables and decorator-based instrumentation. The system is designed to:\n\n- Measure execution time of different inference stages\n- Track memory allocation and cleanup\n- Provide structured logging with context\n- Enable selective profiling to minimize overhead\n\nThe profiling system is opt-in and can be configured at runtime without code changes.\n\nSources: [app/gradio_demo.py:28-30](), [app/run_gradio.sh:37-38]()\n\n---\n\n## Profiling Level Control\n\n### PROFILING_DEBUG_LEVEL Environment Variable\n\nThe `PROFILING_DEBUG_LEVEL` environment variable controls the verbosity of profiling output throughout the system:\n\n| Level | Behavior | Use Case |\n|-------|----------|----------|\n| `0` or unset | No profiling output | Production deployment, normal inference |\n| `1` | Basic timing information | High-level performance monitoring |\n| `2` | Detailed stage-by-stage timing | Development, optimization work |\n| `3` | Verbose profiling with memory stats | Deep debugging, memory leak investigation |\n\n**Setting the profiling level:**\n\n```bash\n# Linux/macOS\nexport PROFILING_DEBUG_LEVEL=2\n\n# Windows\nset PROFILING_DEBUG_LEVEL=2\n```\n\nThe level is typically set in deployment scripts before starting the inference process:\n\n```python\n# Example from gradio_demo.py\nos.environ[\"PROFILING_DEBUG_LEVEL\"] = \"2\"\n```\n\nSources: [app/gradio_demo.py:28](), [app/run_gradio.sh:37](), [scripts/win/run_wan_i2v.bat:30]()\n\n### ProfilingContext Decorators\n\nThe codebase uses decorator-based profiling contexts to measure execution time of functions and code blocks. These decorators respect the `PROFILING_DEBUG_LEVEL` setting.\n\n**Primary profiling decorators:**\n\n- `ProfilingContext(label)` - Standard profiling context that outputs timing\n- `ExcludedProfilingContext(label)` - Excludes timing from parent profiling scope\n\n**Usage pattern:**\n\n```python\nfrom lightx2v.utils.profiler import ExcludedProfilingContext\n\n@ExcludedProfilingContext(\" warm_up_cpu_buffers\")\ndef warm_up_cpu_buffers(self, blocks_num):\n    # Function body\n    # Timing will be measured and logged separately\n```\n\nThe decorator automatically:\n1. Captures start time when function is entered\n2. Executes the function body\n3. Calculates elapsed time\n4. Logs timing with the provided label if profiling level is sufficient\n5. Optionally excludes time from parent context timing\n\nSources: [lightx2v/common/offload/manager.py:91-104]()\n\n---\n\n## Profiling Context Architecture\n\n```mermaid\ngraph TB\n    EnvVar[\"Environment Variable\u003cbr/\u003ePROFILING_DEBUG_LEVEL\"]\n    \n    subgraph \"Profiling Infrastructure\"\n        ProfilerModule[\"lightx2v.utils.profiler\"]\n        ProfilingContext[\"ProfilingContext\u003cbr/\u003eStandard timing context\"]\n        ExcludedContext[\"ExcludedProfilingContext\u003cbr/\u003eExcluded from parent\"]\n        ContextStack[\"Context Stack\u003cbr/\u003eNested timing tracking\"]\n    end\n    \n    subgraph \"Instrumented Components\"\n        ManagerFunc[\"WeightAsyncStreamManager\u003cbr/\u003ewarm_up_cpu_buffers()\"]\n        RunnerMethods[\"Runner Methods\u003cbr/\u003einit_modules(), run_pipeline()\"]\n        VAEDecoder[\"VAE Decoder\u003cbr/\u003edecode()\"]\n        Scheduler[\"Scheduler\u003cbr/\u003estep_pre(), step_post()\"]\n    end\n    \n    subgraph \"Output Destinations\"\n        StdOut[\"Standard Output\u003cbr/\u003eConsole logs\"]\n        LogFile[\"Log File\u003cbr/\u003einference_logs.log\"]\n        MemoryStats[\"Memory Statistics\u003cbr/\u003eCUDA memory tracking\"]\n    end\n    \n    EnvVar --\u003e ProfilerModule\n    ProfilerModule --\u003e ProfilingContext\n    ProfilerModule --\u003e ExcludedContext\n    ProfilingContext --\u003e ContextStack\n    ExcludedContext --\u003e ContextStack\n    \n    ContextStack --\u003e ManagerFunc\n    ContextStack --\u003e RunnerMethods\n    ContextStack --\u003e VAEDecoder\n    ContextStack --\u003e Scheduler\n    \n    ManagerFunc --\u003e StdOut\n    RunnerMethods --\u003e StdOut\n    VAEDecoder --\u003e StdOut\n    Scheduler --\u003e StdOut\n    \n    StdOut --\u003e LogFile\n    MemoryStats --\u003e LogFile\n```\n\n**Profiling Context Behavior**\n\nThe profiling system maintains a context stack that tracks nested function calls. When `PROFILING_DEBUG_LEVEL \u003e= 1`:\n\n1. Each profiled function creates a new context\n2. Start time is recorded\n3. Child contexts can be nested or excluded\n4. End time is recorded on function exit\n5. Elapsed time is logged with emoji indicators (, , etc.)\n6. Parent context accumulates child times (unless excluded)\n\nSources: [lightx2v/common/offload/manager.py:8,91]()\n\n---\n\n## Memory Management and Cleanup\n\n### Memory Cleanup Utilities\n\nLightX2V provides memory cleanup utilities to address CUDA memory fragmentation and Python garbage collection issues during inference.\n\n**cleanup_memory() Function**\n\nThe `cleanup_memory()` function performs comprehensive memory cleanup:\n\n```python\n# Typical usage pattern\ndef run_inference(...):\n    cleanup_memory()  # Clear before inference\n    \n    # ... inference code ...\n    \n    cleanup_memory()  # Clear after inference\n```\n\n**Memory cleanup operations:**\n\n1. **Python garbage collection** - Forces full GC cycle\n2. **CUDA cache clearing** - Calls `torch.cuda.empty_cache()`\n3. **Device synchronization** - Ensures GPU operations complete\n4. **IPC cleanup** - Releases inter-process communication resources\n\nSources: [app/gradio_demo.py:90,324]()\n\n### Memory Cleanup Configuration\n\nThe `clean_cuda_cache` configuration option controls automatic memory cleanup behavior:\n\n| Configuration | Behavior |\n|---------------|----------|\n| `clean_cuda_cache: true` | Cleanup after each inference stage |\n| `clean_cuda_cache: false` | Manual cleanup only |\n\n**Auto-configuration determines cleanup strategy based on available VRAM:**\n\n- VRAM \u003c 12GB: Enable automatic cleanup\n- VRAM \u003e= 12GB: Disable for better performance\n\nSources: [app/gradio_demo.py:107,194]()\n\n---\n\n## Logging Infrastructure\n\n### Loguru Integration\n\nLightX2V uses the `loguru` library for structured logging with automatic rotation and context preservation.\n\n**Logger configuration:**\n\n```python\nfrom loguru import logger\n\nlogger.add(\n    \"inference_logs.log\",\n    rotation=\"100 MB\",        # Rotate when file reaches 100MB\n    encoding=\"utf-8\",         # UTF-8 for international characters\n    enqueue=True,             # Thread-safe logging\n    backtrace=True,           # Include stack traces\n    diagnose=True,            # Detailed error diagnosis\n)\n```\n\n**Log levels and usage:**\n\n| Level | Method | Use Case |\n|-------|--------|----------|\n| DEBUG | `logger.debug()` | Detailed diagnostic information |\n| INFO | `logger.info()` | General informational messages |\n| WARNING | `logger.warning()` | Warning messages, non-critical issues |\n| ERROR | `logger.error()` | Error messages, execution continues |\n| CRITICAL | `logger.critical()` | Critical errors, execution may stop |\n\nSources: [app/gradio_demo.py:14,33-40]()\n\n### Logging Patterns\n\n**Configuration logging:**\n\n```python\nlogger.info(f\"Using model: {model_path}\")\nlogger.info(f\"Inference config:\\n{json.dumps(config, indent=4, ensure_ascii=False)}\")\n```\n\n**State change logging:**\n\n```python\nlogger.info(f\"Auto-determined model_cls: {model_cls} (model type: {model_type_input})\")\nlogger.info(f\"Switched LoRA to: {lora_path} with strength={lora_strength_val}\")\n```\n\n**Error and warning logging:**\n\n```python\nlogger.warning(\"Runner does not support switch_lora method\")\nlogger.error(f\"Failed to load model: {error_message}\")\n```\n\nSources: [app/gradio_demo.py:151,235-236,309,318]()\n\n### Suppressing Noisy Logs\n\nExternal library warnings can be suppressed to reduce log clutter:\n\n```python\nimport warnings\nimport logging\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub.utils\")\n\n# Reduce logging level for HTTP libraries\nlogging.getLogger(\"httpx\").setLevel(logging.ERROR)\nlogging.getLogger(\"httpcore\").setLevel(logging.ERROR)\nlogging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n```\n\nSources: [app/gradio_demo.py:22-26]()\n\n---\n\n## Debugging Strategies\n\n### Environment Variable Configuration\n\nMultiple environment variables control debugging and runtime behavior:\n\n| Variable | Purpose | Typical Value |\n|----------|---------|---------------|\n| `PROFILING_DEBUG_LEVEL` | Controls profiling verbosity | `0`, `1`, `2`, `3` |\n| `CUDA_VISIBLE_DEVICES` | Selects GPU devices | `0`, `1`, `0,1`, etc. |\n| `CUDA_LAUNCH_BLOCKING` | Synchronous CUDA execution | `1` (for debugging) |\n| `PYTORCH_CUDA_ALLOC_CONF` | CUDA memory allocator config | `expandable_segments:True` |\n| `DTYPE` | Default tensor dtype | `BF16`, `FP16`, `FP32` |\n| `TOKENIZERS_PARALLELISM` | Tokenizer parallelism | `false` (to avoid warnings) |\n\n**Example debugging configuration:**\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0\nexport CUDA_LAUNCH_BLOCKING=1        # Synchronous for better error messages\nexport PROFILING_DEBUG_LEVEL=3       # Maximum profiling detail\nexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n```\n\nSources: [app/run_gradio.sh:34-38](), [scripts/win/run_wan_i2v.bat:28-31]()\n\n### GPU Memory Debugging\n\n**Memory tracking workflow:**\n\n1. **Before inference:**\n   ```python\n   torch.cuda.reset_peak_memory_stats()\n   start_memory = torch.cuda.memory_allocated()\n   ```\n\n2. **During inference:**\n   - Monitor with `nvidia-smi` command\n   - Check logs for memory allocation warnings\n\n3. **After inference:**\n   ```python\n   peak_memory = torch.cuda.max_memory_allocated() / 1024**3  # GB\n   logger.info(f\"Peak GPU memory: {peak_memory:.2f} GB\")\n   ```\n\n**Common memory issues and solutions:**\n\n| Issue | Symptom | Solution |\n|-------|---------|----------|\n| OOM during inference | CUDA out of memory error | Enable `cpu_offload`, reduce `num_frames` |\n| Memory fragmentation | Increasing memory usage over time | Enable `clean_cuda_cache`, use `lazy_load` |\n| Memory leak | Memory not released after inference | Check for circular references, use `cleanup_memory()` |\n\nSources: [app/gradio_demo.py:29,90,324]()\n\n### Runner Initialization Debugging\n\nThe `needs_reinit` logic determines when to reinitialize the runner:\n\n```python\nneeds_reinit = (\n    lazy_load or              # Lazy loading requires fresh runner\n    unload_modules or         # Module unloading changes state\n    global_runner is None or  # First initialization\n    cur_dit_path != current_dit_path or  # DIT model changed\n    cur_use_lora != use_lora  # LoRA usage changed\n)\n```\n\n**Debugging runner state:**\n\n```python\nlogger.debug(f\"needs_reinit: {needs_reinit}\")\nlogger.debug(f\"lazy_load: {lazy_load}, unload_modules: {unload_modules}\")\nlogger.debug(f\"cur_dit_path: {cur_dit_path}, current_dit_path: {current_dit_path}\")\nlogger.debug(f\"cur_use_lora: {cur_use_lora}, use_lora: {use_lora}\")\n```\n\nSources: [app/gradio_demo.py:158,240-248]()\n\n---\n\n## Performance Profiling Workflow\n\n### Complete Profiling Pipeline\n\n```mermaid\nflowchart TB\n    Start[\"Start Inference\"]\n    \n    subgraph \"Stage 1: Initialization\"\n        SetEnv[\"Set PROFILING_DEBUG_LEVEL=2\"]\n        InitRunner[\"Initialize Runner\u003cbr/\u003e@ProfilingContext\"]\n        LoadWeights[\"Load Weights\u003cbr/\u003e@ExcludedProfilingContext\"]\n    end\n    \n    subgraph \"Stage 2: Input Processing\"\n        EncodeText[\"Encode Text\u003cbr/\u003e@ProfilingContext\"]\n        EncodeImage[\"Encode Image\u003cbr/\u003e@ProfilingContext\"]\n        VAEEncode[\"VAE Encode\u003cbr/\u003e@ProfilingContext\"]\n    end\n    \n    subgraph \"Stage 3: Diffusion Loop\"\n        DiffusionStart[\"Start Diffusion Loop\"]\n        StepPre[\"scheduler.step_pre()\u003cbr/\u003e@ProfilingContext\"]\n        ModelInfer[\"model.infer()\u003cbr/\u003e@ProfilingContext\"]\n        StepPost[\"scheduler.step_post()\u003cbr/\u003e@ProfilingContext\"]\n        DiffusionEnd[\"End Diffusion Loop\"]\n    end\n    \n    subgraph \"Stage 4: Decoding\"\n        VAEDecode[\"VAE Decode\u003cbr/\u003e@ProfilingContext\"]\n        SaveOutput[\"Save Output\"]\n    end\n    \n    subgraph \"Stage 5: Cleanup\"\n        MemoryCleanup[\"cleanup_memory()\"]\n        LogStats[\"Log Statistics\"]\n    end\n    \n    subgraph \"Profiling Output\"\n        ConsoleOutput[\"Console Output\u003cbr/\u003eTiming per stage\"]\n        LogFile[\"inference_logs.log\u003cbr/\u003eDetailed breakdown\"]\n        MemoryReport[\"Memory Report\u003cbr/\u003ePeak/Allocated\"]\n    end\n    \n    Start --\u003e SetEnv\n    SetEnv --\u003e InitRunner\n    InitRunner --\u003e LoadWeights\n    LoadWeights --\u003e EncodeText\n    \n    EncodeText --\u003e EncodeImage\n    EncodeImage --\u003e VAEEncode\n    VAEEncode --\u003e DiffusionStart\n    \n    DiffusionStart --\u003e StepPre\n    StepPre --\u003e ModelInfer\n    ModelInfer --\u003e StepPost\n    StepPost --\u003e DiffusionEnd\n    \n    DiffusionEnd --\u003e VAEDecode\n    VAEDecode --\u003e SaveOutput\n    SaveOutput --\u003e MemoryCleanup\n    MemoryCleanup --\u003e LogStats\n    \n    InitRunner -.timing.-\u003e ConsoleOutput\n    EncodeText -.timing.-\u003e ConsoleOutput\n    ModelInfer -.timing.-\u003e ConsoleOutput\n    VAEDecode -.timing.-\u003e ConsoleOutput\n    \n    ConsoleOutput --\u003e LogFile\n    MemoryCleanup --\u003e MemoryReport\n    MemoryReport --\u003e LogFile\n```\n\n**Profiling output interpretation:**\n\n```\n warm_up_cpu_buffers: 2.34s\n  Text encoding: 0.45s\n  Image encoding: 0.32s\n  VAE encoding: 0.21s\n Diffusion step 0/4: 1.23s\n Diffusion step 1/4: 1.19s\n Diffusion step 2/4: 1.21s\n Diffusion step 3/4: 1.18s\n  VAE decoding: 0.67s\n Peak GPU memory: 14.32 GB\n Total inference time: 7.80s\n```\n\nSources: [lightx2v/common/offload/manager.py:91-104](), [app/gradio_demo.py:323-326]()\n\n### Measuring Component Performance\n\n**Recommended profiling points:**\n\n1. **Model loading:**\n   - Weight loading from disk\n   - Weight transfer to GPU\n   - Model compilation (if using torch.compile)\n\n2. **Encoding stages:**\n   - Text tokenization and encoding\n   - Image preprocessing and encoding\n   - Audio segmentation and encoding (for S2V)\n\n3. **Diffusion loop:**\n   - Per-step timing\n   - Scheduler overhead\n   - Model forward pass\n   - Attention operator performance\n\n4. **Decoding stage:**\n   - VAE latent decoding\n   - Video frame generation\n   - Post-processing and saving\n\n5. **Memory operations:**\n   - CPU offloading overhead\n   - Weight streaming time\n   - Buffer swapping time\n\nSources: [lightx2v/common/offload/manager.py:91-104]()\n\n---\n\n## Debugging Configuration Issues\n\n### Common Configuration Problems\n\n| Problem | Symptom | Debug Steps |\n|---------|---------|-------------|\n| Model path not found | FileNotFoundError | Check `model_path` in config, verify file existence |\n| Weight file mismatch | Shape mismatch error | Verify model architecture matches weights |\n| Quantization error | Runtime quantization failure | Check `quant_op` is supported, verify weight format |\n| OOM with offloading | Still out of memory | Increase `offload_granularity`, enable `lazy_load` |\n| Slow first inference | Minutes to load | Check if using HDD instead of SSD, enable `lazy_load` |\n\n**Configuration debugging pattern:**\n\n```python\n# Log the complete configuration\nlogger.info(f\"Inference config:\\n{json.dumps(config, indent=4, ensure_ascii=False)}\")\n\n# Verify critical paths\nassert os.path.exists(config['model_path']), f\"Model path not found: {config['model_path']}\"\nassert os.path.exists(config['dit_path']), f\"DIT path not found: {config['dit_path']}\"\n\n# Log auto-configuration decisions\nauto_config = get_auto_config_dict(model_type, resolution, num_frames, task_type)\nlogger.info(f\"Auto-config: {auto_config}\")\n```\n\nSources: [app/gradio_demo.py:92,236]()\n\n### System Resource Monitoring\n\n**Monitoring commands (Linux):**\n\n```bash\n# GPU monitoring\nnvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits\n\n# System memory\nfree -h | grep -E \"Mem|Swap\"\n\n# Real-time monitoring\nwatch -n 1 nvidia-smi\nhtop\n```\n\n**Monitoring commands (Windows):**\n\n```cmd\nREM GPU monitoring\nnvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits\n\nREM System memory\nwmic OS get TotalVisibleMemorySize,FreePhysicalMemory /format:table\n```\n\nThese commands are integrated into the deployment scripts for automatic resource reporting.\n\nSources: [app/run_gradio.sh:147-156](), [app/run_gradio_win.bat:164-174]()\n\n---\n\n## Advanced Debugging Techniques\n\n### ThreadPoolExecutor Debugging\n\nFor lazy loading with async weight streaming:\n\n```python\n# Monitor thread pool state\nlogger.debug(f\"ThreadPoolExecutor active threads: {executor._threads}\")\nlogger.debug(f\"Pending futures: {len(prefetch_futures)}\")\n\n# Wait for prefetch completion with timing\nwait_start = time.time()\nfor f in prefetch_futures:\n    f.result()\nwait_time = time.time() - wait_start\nlogger.debug(f\"[Prefetch] block {block_idx}: wait={wait_time:.3f}s\")\n```\n\nThe manager includes cleanup logic to ensure thread pool shutdown:\n\n```python\ndef __del__(self):\n    if hasattr(self, \"executor\") and self.executor is not None:\n        for f in self.prefetch_futures:\n            if not f.done():\n                f.result()\n        self.executor.shutdown(wait=False)\n        self.executor = None\n        logger.debug(\"ThreadPoolExecutor shut down successfully.\")\n```\n\nSources: [lightx2v/common/offload/manager.py:106-141]()\n\n### Weight Streaming Profiling\n\nThe `WeightAsyncStreamManager` provides detailed profiling for CPU/GPU buffer operations:\n\n**Key profiling points:**\n\n1. **CPU buffer warm-up:** `@ExcludedProfilingContext(\" warm_up_cpu_buffers\")`\n   - Measures time to prefetch all weights into CPU memory\n   - Useful for diagnosing disk I/O bottlenecks\n\n2. **Prefetch operations:** Track async loading with futures\n   - `start_prefetch_block()` - Initiates async disk read\n   - `swap_cpu_buffers()` - Waits for completion, measures stall time\n\n3. **CUDA buffer operations:**\n   - `init_first_buffer()` - Initial GPU transfer\n   - `prefetch_weights()` - Async GPU transfer\n   - `swap_blocks()` - Buffer synchronization overhead\n\nSources: [lightx2v/common/offload/manager.py:91-141]()\n\n---\n\n## Summary\n\nLightX2V's debugging and profiling infrastructure provides:\n\n- **Environment-controlled profiling** via `PROFILING_DEBUG_LEVEL` (0-3)\n- **Decorator-based instrumentation** with `ProfilingContext` and `ExcludedProfilingContext`\n- **Memory management utilities** including `cleanup_memory()` and automatic cache clearing\n- **Structured logging** using loguru with rotation and thread safety\n- **Configuration debugging** through detailed logging and validation\n- **Performance measurement** at initialization, encoding, diffusion, and decoding stages\n- **Resource monitoring** integrated into deployment scripts\n\nThese tools enable developers to identify bottlenecks, debug configuration issues, track memory usage, and optimize inference performance across different hardware configurations.\n\nSources: [app/gradio_demo.py:1-365](), [lightx2v/common/offload/manager.py:1-141](), [app/run_gradio.sh:1-186]()"])</script><script>self.__next_f.push([1,"5:[\"$\",\"$L15\",null,{\"repoName\":\"ModelTC/lightx2v\",\"hasConfig\":false,\"canSteer\":true,\"children\":[\"$\",\"$L16\",null,{\"wiki\":{\"metadata\":{\"repo_name\":\"ModelTC/lightx2v\",\"commit_hash\":\"5573905f\",\"generated_at\":\"2026-02-14T08:53:03.969892\",\"config\":null,\"config_source\":\"none\"},\"pages\":[{\"page_plan\":{\"id\":\"1\",\"title\":\"Overview\"},\"content\":\"$17\"},{\"page_plan\":{\"id\":\"1.1\",\"title\":\"Key Features and Optimization Techniques\"},\"content\":\"$18\"},{\"page_plan\":{\"id\":\"1.2\",\"title\":\"System Architecture Overview\"},\"content\":\"$19\"},{\"page_plan\":{\"id\":\"1.3\",\"title\":\"Hardware Platform Support\"},\"content\":\"$1a\"},{\"page_plan\":{\"id\":\"1.4\",\"title\":\"Model Ecosystem and Supported Tasks\"},\"content\":\"$1b\"},{\"page_plan\":{\"id\":\"2\",\"title\":\"Getting Started\"},\"content\":\"$1c\"},{\"page_plan\":{\"id\":\"2.1\",\"title\":\"Installation and Environment Setup\"},\"content\":\"$1d\"},{\"page_plan\":{\"id\":\"2.2\",\"title\":\"Model Download and Organization\"},\"content\":\"$1e\"},{\"page_plan\":{\"id\":\"2.3\",\"title\":\"Quick Start Tutorial\"},\"content\":\"$1f\"},{\"page_plan\":{\"id\":\"3\",\"title\":\"User Interfaces\"},\"content\":\"$20\"},{\"page_plan\":{\"id\":\"3.1\",\"title\":\"Gradio Web Interface\"},\"content\":\"$21\"},{\"page_plan\":{\"id\":\"3.2\",\"title\":\"Python API (LightX2VPipeline)\"},\"content\":\"$22\"},{\"page_plan\":{\"id\":\"3.3\",\"title\":\"Command Line Interface\"},\"content\":\"$23\"},{\"page_plan\":{\"id\":\"3.4\",\"title\":\"HTTP Server and Production Deployment\"},\"content\":\"$24\"},{\"page_plan\":{\"id\":\"3.5\",\"title\":\"ComfyUI Integration\"},\"content\":\"$25\"},{\"page_plan\":{\"id\":\"4\",\"title\":\"Core Architecture\"},\"content\":\"$26\"},{\"page_plan\":{\"id\":\"4.1\",\"title\":\"Runner System and Registry Pattern\"},\"content\":\"$27\"},{\"page_plan\":{\"id\":\"4.2\",\"title\":\"Three-Stage Inference Pipeline\"},\"content\":\"$28\"},{\"page_plan\":{\"id\":\"4.3\",\"title\":\"Scheduler System and Diffusion Process\"},\"content\":\"$29\"},{\"page_plan\":{\"id\":\"4.4\",\"title\":\"Weight Management System\"},\"content\":\"$2a\"},{\"page_plan\":{\"id\":\"4.5\",\"title\":\"Input Encoder Architecture\"},\"content\":\"$2b\"},{\"page_plan\":{\"id\":\"4.6\",\"title\":\"VAE System and Latent Space\"},\"content\":\"$2c\"},{\"page_plan\":{\"id\":\"5\",\"title\":\"Model Runners and Tasks\"},\"content\":\"$2d\"},{\"page_plan\":{\"id\":\"5.1\",\"title\":\"WAN Model Family Overview\"},\"content\":\"$2e\"},{\"page_plan\":{\"id\":\"5.2\",\"title\":\"WanAudioRunner - Audio-to-Video Generation\"},\"content\":\"$2f\"},{\"page_plan\":{\"id\":\"5.3\",\"title\":\"WanRunner - Text-to-Video and Image-to-Video\"},\"content\":\"$30\"},{\"page_plan\":{\"id\":\"5.4\",\"title\":\"WanDistillRunner - 4-Step Distilled Models\"},\"content\":\"$31\"},{\"page_plan\":{\"id\":\"5.5\",\"title\":\"Wan22MoeRunner - Mixture-of-Experts\"},\"content\":\"$32\"},{\"page_plan\":{\"id\":\"5.6\",\"title\":\"QwenImageRunner - Text/Image-to-Image Generation\"},\"content\":\"$33\"},{\"page_plan\":{\"id\":\"5.7\",\"title\":\"ZImageRunner - Fast Image Generation\"},\"content\":\"$34\"},{\"page_plan\":{\"id\":\"5.8\",\"title\":\"WanCausVidRunner - Autoregressive Video\"},\"content\":\"$35\"},{\"page_plan\":{\"id\":\"5.9\",\"title\":\"LongCatImageRunner and LTX2Runner\"},\"content\":\"$36\"},{\"page_plan\":{\"id\":\"5.10\",\"title\":\"ShotPipeline - Multi-Clip Generation\"},\"content\":\"$37\"},{\"page_plan\":{\"id\":\"6\",\"title\":\"Performance Optimization\"},\"content\":\"$38\"},{\"page_plan\":{\"id\":\"6.1\",\"title\":\"Quantization System\"},\"content\":\"$39\"},{\"page_plan\":{\"id\":\"6.2\",\"title\":\"Attention Operators and Sparse Patterns\"},\"content\":\"$3a\"},{\"page_plan\":{\"id\":\"6.3\",\"title\":\"Memory Management and CPU Offloading\"},\"content\":\"$3b\"},{\"page_plan\":{\"id\":\"6.4\",\"title\":\"Feature Caching and Streaming\"},\"content\":\"$3c\"},{\"page_plan\":{\"id\":\"6.5\",\"title\":\"Distributed and Parallel Inference\"},\"content\":\"$3d\"},{\"page_plan\":{\"id\":\"6.6\",\"title\":\"LoRA Dynamic Application\"},\"content\":\"$3e\"},{\"page_plan\":{\"id\":\"6.7\",\"title\":\"JIT Compilation and Shape Caching\"},\"content\":\"$3f\"},{\"page_plan\":{\"id\":\"7\",\"title\":\"Advanced Architecture Topics\"},\"content\":\"$40\"},{\"page_plan\":{\"id\":\"7.1\",\"title\":\"Transformer Block Architecture\"},\"content\":\"$41\"},{\"page_plan\":{\"id\":\"7.2\",\"title\":\"Rotary Position Embeddings (RoPE)\"},\"content\":\"$42\"},{\"page_plan\":{\"id\":\"7.3\",\"title\":\"Audio Adapter and Cross-Attention\"},\"content\":\"$43\"},{\"page_plan\":{\"id\":\"7.4\",\"title\":\"Triton Kernel Implementation\"},\"content\":\"$44\"},{\"page_plan\":{\"id\":\"7.5\",\"title\":\"VAE Tiling and Distributed Processing\"},\"content\":\"$45\"},{\"page_plan\":{\"id\":\"7.6\",\"title\":\"Lazy Loading and Async Weight Streaming\"},\"content\":\"$46\"},{\"page_plan\":{\"id\":\"7.7\",\"title\":\"Multi-Person Audio Processing\"},\"content\":\"$47\"},{\"page_plan\":{\"id\":\"8\",\"title\":\"Developer Tools and Workflows\"},\"content\":\"$48\"},{\"page_plan\":{\"id\":\"8.1\",\"title\":\"Model Conversion and Quantization Tools\"},\"content\":\"$49\"},{\"page_plan\":{\"id\":\"8.2\",\"title\":\"Configuration System\"},\"content\":\"$4a\"},{\"page_plan\":{\"id\":\"8.3\",\"title\":\"InputInfo and Data Structures\"},\"content\":\"$4b\"},{\"page_plan\":{\"id\":\"8.4\",\"title\":\"Custom Runner Development\"},\"content\":\"$4c\"},{\"page_plan\":{\"id\":\"8.5\",\"title\":\"Prompt Templates and Text Processing\"},\"content\":\"$4d\"},{\"page_plan\":{\"id\":\"8.6\",\"title\":\"Debugging and Profiling\"},\"content\":\"$4e\"}]},\"children\":\"$L4f\"}]}]\n"])</script><script>self.__next_f.push([1,"4f:[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]\n"])</script><script>self.__next_f.push([1,"50:I[36505,[],\"IconMark\"]\n"])</script><script>self.__next_f.push([1,"6:[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"TechArticle\\\",\\\"headline\\\":\\\"Input Encoder Architecture\\\",\\\"description\\\":\\\"This document describes the input encoder architecture in LightX2V, covering how text, image, and audio inputs are processed before entering the diffusion transformer. The encoders transform raw input\\\",\\\"image\\\":\\\"https://deepwiki.com/ModelTC/lightx2v/og-image.png\\\",\\\"datePublished\\\":\\\"2026-02-14T08:53:03.969892\\\",\\\"dateModified\\\":\\\"2026-02-14T08:53:03.969892\\\",\\\"author\\\":{\\\"@type\\\":\\\"Organization\\\",\\\"name\\\":\\\"DeepWiki\\\",\\\"url\\\":\\\"https://deepwiki.com\\\"},\\\"publisher\\\":{\\\"@type\\\":\\\"Organization\\\",\\\"name\\\":\\\"DeepWiki\\\",\\\"logo\\\":{\\\"@type\\\":\\\"ImageObject\\\",\\\"url\\\":\\\"https://deepwiki.com/icon.png\\\"}},\\\"mainEntityOfPage\\\":{\\\"@type\\\":\\\"WebPage\\\",\\\"@id\\\":\\\"https://deepwiki.com/ModelTC/lightx2v/4.5-input-encoder-architecture\\\"}}\"}}],[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]\n"])</script><script>self.__next_f.push([1,"e:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Input Encoder Architecture | ModelTC/lightx2v | DeepWiki\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"This document describes the input encoder architecture in LightX2V, covering how text, image, and audio inputs are processed before entering the diffusion transformer. The encoders transform raw input\"}],[\"$\",\"meta\",\"2\",{\"name\":\"keywords\",\"content\":\"ModelTC/lightx2v,ModelTC,lightx2v,documentation,wiki,codebase,AI documentation,Devin,Input Encoder Architecture\"}],[\"$\",\"link\",\"3\",{\"rel\":\"canonical\",\"href\":\"https://deepwiki.com/ModelTC/lightx2v/4.5-input-encoder-architecture\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:title\",\"content\":\"Input Encoder Architecture | ModelTC/lightx2v | DeepWiki\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:description\",\"content\":\"This document describes the input encoder architecture in LightX2V, covering how text, image, and audio inputs are processed before entering the diffusion transformer. The encoders transform raw input\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:url\",\"content\":\"https://deepwiki.com/ModelTC/lightx2v/4.5-input-encoder-architecture\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:site_name\",\"content\":\"DeepWiki\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:image\",\"content\":\"https://deepwiki.com/ModelTC/lightx2v/og-image.png?page=4.5\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:site\",\"content\":\"@cognition\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:creator\",\"content\":\"@cognition\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:title\",\"content\":\"Input Encoder Architecture | ModelTC/lightx2v | DeepWiki\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:description\",\"content\":\"This document describes the input encoder architecture in LightX2V, covering how text, image, and audio inputs are processed before entering the diffusion transformer. The encoders transform raw input\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:image\",\"content\":\"https://deepwiki.com/ModelTC/lightx2v/og-image.png?page=4.5\"}],[\"$\",\"link\",\"16\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"48x48\"}],[\"$\",\"link\",\"17\",{\"rel\":\"icon\",\"href\":\"/icon.png?1ee4c6a68a73a205\",\"type\":\"image/png\",\"sizes\":\"48x48\"}],[\"$\",\"link\",\"18\",{\"rel\":\"apple-touch-icon\",\"href\":\"/apple-icon.png?a4f658907db0ab87\",\"type\":\"image/png\",\"sizes\":\"180x180\"}],[\"$\",\"$L50\",\"19\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"13:\"$e:metadata\"\n"])</script></body></html>