
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Pipeline Parallelism &#8212; PyTorch 2.10 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=047068a3" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/css/jit.css?v=8de1ea5d" />
    <link rel="stylesheet" type="text/css" href="_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=8998eb7a"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=940804e7"></script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'distributed.pipelining';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://docs.pytorch.org/docs/pytorch-versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '2.10';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <script src="_static/js/runllm-widget.js?v=54a6b3cb"></script>
    <link rel="canonical" href="https://docs.pytorch.org/docs/stable/distributed.pipelining.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="PyTorch Symmetric Memory" href="symmetric_memory.html" />
    <link rel="prev" title="Distributed Optimizers" href="distributed.optim.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->


<link rel="stylesheet" type="text/css" href="_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="docs">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', '2.10');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/pytorch/pytorch" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/ray/">
                  <span class="dropdown-title">RAY</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/wp-content/uploads/2025/09/pytorch_brand_guide_091925a.pdf">
                  <span class="dropdown-title">Brand Guidelines</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started/locally">Get Started</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">Home</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/get-started/locally/">
    Install PyTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="user_guide/index.html">
    User Guide
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="pytorch-api.html">
    Reference API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="notes.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://docs.pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-external" href="https://pytorch.org/get-started/locally/">
    Install PyTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="user_guide/index.html">
    User Guide
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="pytorch-api.html">
    Reference API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="notes.html">
    Developer Notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="community/index.html">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://docs.pytorch.org/tutorials/">
    Tutorials
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://docs.pytorch.org/cppdocs/">C++</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="torch.html">torch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_tensor.html">torch.is_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_storage.html">torch.is_storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_complex.html">torch.is_complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_conj.html">torch.is_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_floating_point.html">torch.is_floating_point</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_nonzero.html">torch.is_nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_default_dtype.html">torch.set_default_dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_default_dtype.html">torch.get_default_dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_default_device.html">torch.set_default_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_default_device.html">torch.get_default_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_default_tensor_type.html">torch.set_default_tensor_type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.numel.html">torch.numel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_printoptions.html">torch.set_printoptions</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_flush_denormal.html">torch.set_flush_denormal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tensor.html">torch.tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_coo_tensor.html">torch.sparse_coo_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_csr_tensor.html">torch.sparse_csr_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_csc_tensor.html">torch.sparse_csc_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_bsr_tensor.html">torch.sparse_bsr_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_bsc_tensor.html">torch.sparse_bsc_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asarray.html">torch.asarray</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.as_tensor.html">torch.as_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.as_strided.html">torch.as_strided</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.from_file.html">torch.from_file</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.from_numpy.html">torch.from_numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.from_dlpack.html">torch.from_dlpack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frombuffer.html">torch.frombuffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.zeros.html">torch.zeros</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.zeros_like.html">torch.zeros_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ones.html">torch.ones</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ones_like.html">torch.ones_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arange.html">torch.arange</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.range.html">torch.range</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linspace.html">torch.linspace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logspace.html">torch.logspace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.eye.html">torch.eye</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.empty.html">torch.empty</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.empty_like.html">torch.empty_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.empty_strided.html">torch.empty_strided</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.full.html">torch.full</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.full_like.html">torch.full_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantize_per_tensor.html">quantize_per_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantize_per_channel.html">quantize_per_channel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dequantize.html">dequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.complex.html">torch.complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.polar.html">torch.polar</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.heaviside.html">torch.heaviside</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.adjoint.html">torch.adjoint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argwhere.html">torch.argwhere</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cat.html">torch.cat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.concat.html">torch.concat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.concatenate.html">torch.concatenate</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.conj.html">torch.conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.chunk.html">torch.chunk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dsplit.html">torch.dsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.column_stack.html">torch.column_stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dstack.html">torch.dstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gather.html">torch.gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hsplit.html">torch.hsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hstack.html">torch.hstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_add.html">torch.index_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_copy.html">torch.index_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_reduce.html">torch.index_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.index_select.html">torch.index_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.masked_select.html">torch.masked_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.movedim.html">torch.movedim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.moveaxis.html">torch.moveaxis</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.narrow.html">torch.narrow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.narrow_copy.html">torch.narrow_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nonzero.html">torch.nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.permute.html">torch.permute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.reshape.html">torch.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.row_stack.html">torch.row_stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.select.html">torch.select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.scatter.html">torch.scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diagonal_scatter.html">torch.diagonal_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.select_scatter.html">torch.select_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.slice_scatter.html">torch.slice_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.scatter_add.html">torch.scatter_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.scatter_reduce.html">torch.scatter_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.segment_reduce.html">torch.segment_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.split.html">torch.split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.squeeze.html">torch.squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.stack.html">torch.stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.swapaxes.html">torch.swapaxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.swapdims.html">torch.swapdims</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.t.html">torch.t</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.take.html">torch.take</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.take_along_dim.html">torch.take_along_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tensor_split.html">torch.tensor_split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tile.html">torch.tile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.transpose.html">torch.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unbind.html">torch.unbind</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unravel_index.html">torch.unravel_index</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unsqueeze.html">torch.unsqueeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vsplit.html">torch.vsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vstack.html">torch.vstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.where.html">torch.where</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Stream.html">Stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Generator.html">Generator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.seed.html">torch.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.manual_seed.html">torch.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.initial_seed.html">torch.initial_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_rng_state.html">torch.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_rng_state.html">torch.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bernoulli.html">torch.bernoulli</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.multinomial.html">torch.multinomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.normal.html">torch.normal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.poisson.html">torch.poisson</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rand.html">torch.rand</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rand_like.html">torch.rand_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randint.html">torch.randint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randint_like.html">torch.randint_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randn.html">torch.randn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randn_like.html">torch.randn_like</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.randperm.html">torch.randperm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quasirandom.SobolEngine.html">SobolEngine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.save.html">torch.save</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.load.html">torch.load</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_num_threads.html">torch.get_num_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_num_threads.html">torch.set_num_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_num_interop_threads.html">torch.get_num_interop_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_num_interop_threads.html">torch.set_num_interop_threads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.no_grad.html">no_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.enable_grad.html">enable_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad_mode.set_grad_enabled.html">set_grad_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_grad_enabled.html">torch.is_grad_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad_mode.inference_mode.html">inference_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_inference_mode_enabled.html">torch.is_inference_mode_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.abs.html">torch.abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.absolute.html">torch.absolute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.acos.html">torch.acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arccos.html">torch.arccos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.acosh.html">torch.acosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arccosh.html">torch.arccosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.add.html">torch.add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addcdiv.html">torch.addcdiv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addcmul.html">torch.addcmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.angle.html">torch.angle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asin.html">torch.asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arcsin.html">torch.arcsin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asinh.html">torch.asinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arcsinh.html">torch.arcsinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atan.html">torch.atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctan.html">torch.arctan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atanh.html">torch.atanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctanh.html">torch.arctanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atan2.html">torch.atan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctan2.html">torch.arctan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_not.html">torch.bitwise_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_and.html">torch.bitwise_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_or.html">torch.bitwise_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_xor.html">torch.bitwise_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_left_shift.html">torch.bitwise_left_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_right_shift.html">torch.bitwise_right_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ceil.html">torch.ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clamp.html">torch.clamp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clip.html">torch.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.conj_physical.html">torch.conj_physical</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.copysign.html">torch.copysign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cos.html">torch.cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cosh.html">torch.cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.deg2rad.html">torch.deg2rad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.div.html">torch.div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.divide.html">torch.divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.digamma.html">torch.digamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erf.html">torch.erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erfc.html">torch.erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erfinv.html">torch.erfinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.exp.html">torch.exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.exp2.html">torch.exp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.expm1.html">torch.expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fake_quantize_per_channel_affine.html">torch.fake_quantize_per_channel_affine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fake_quantize_per_tensor_affine.html">torch.fake_quantize_per_tensor_affine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fix.html">torch.fix</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.float_power.html">torch.float_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.floor.html">torch.floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.floor_divide.html">torch.floor_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmod.html">torch.fmod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frac.html">torch.frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frexp.html">torch.frexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gradient.html">torch.gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.imag.html">torch.imag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ldexp.html">torch.ldexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lerp.html">torch.lerp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lgamma.html">torch.lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log.html">torch.log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log10.html">torch.log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log1p.html">torch.log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log2.html">torch.log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp.html">torch.logaddexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp2.html">torch.logaddexp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_and.html">torch.logical_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_not.html">torch.logical_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_or.html">torch.logical_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logical_xor.html">torch.logical_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logit.html">torch.logit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hypot.html">torch.hypot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.i0.html">torch.i0</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.igamma.html">torch.igamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.igammac.html">torch.igammac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mul.html">torch.mul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.multiply.html">torch.multiply</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mvlgamma.html">torch.mvlgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nan_to_num.html">torch.nan_to_num</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.neg.html">torch.neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.negative.html">torch.negative</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nextafter.html">torch.nextafter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.polygamma.html">torch.polygamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.positive.html">torch.positive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pow.html">torch.pow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantized_batch_norm.html">torch.quantized_batch_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantized_max_pool1d.html">torch.quantized_max_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantized_max_pool2d.html">torch.quantized_max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rad2deg.html">torch.rad2deg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.real.html">torch.real</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.reciprocal.html">torch.reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.remainder.html">torch.remainder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.round.html">torch.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rsqrt.html">torch.rsqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sigmoid.html">torch.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sign.html">torch.sign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sgn.html">torch.sgn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signbit.html">torch.signbit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sin.html">torch.sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sinc.html">torch.sinc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sinh.html">torch.sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.softmax.html">torch.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sqrt.html">torch.sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.square.html">torch.square</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sub.html">torch.sub</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.subtract.html">torch.subtract</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tan.html">torch.tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tanh.html">torch.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.true_divide.html">torch.true_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trunc.html">torch.trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xlogy.html">torch.xlogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argmax.html">torch.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argmin.html">torch.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.amax.html">torch.amax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.amin.html">torch.amin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.aminmax.html">torch.aminmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.all.html">torch.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.any.html">torch.any</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.max.html">torch.max</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.min.html">torch.min</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dist.html">torch.dist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logsumexp.html">torch.logsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mean.html">torch.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nanmean.html">torch.nanmean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.median.html">torch.median</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nanmedian.html">torch.nanmedian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mode.html">torch.mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.norm.html">torch.norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nansum.html">torch.nansum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.prod.html">torch.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.quantile.html">torch.quantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nanquantile.html">torch.nanquantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.std.html">torch.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.std_mean.html">torch.std_mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sum.html">torch.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unique.html">torch.unique</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unique_consecutive.html">torch.unique_consecutive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.var.html">torch.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.var_mean.html">torch.var_mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.count_nonzero.html">torch.count_nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hash_tensor.html">torch.hash_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.allclose.html">torch.allclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argsort.html">torch.argsort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.eq.html">torch.eq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.equal.html">torch.equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ge.html">torch.ge</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.greater_equal.html">torch.greater_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gt.html">torch.gt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.greater.html">torch.greater</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isclose.html">torch.isclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isfinite.html">torch.isfinite</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isin.html">torch.isin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isinf.html">torch.isinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isposinf.html">torch.isposinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isneginf.html">torch.isneginf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isnan.html">torch.isnan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isreal.html">torch.isreal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kthvalue.html">torch.kthvalue</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.le.html">torch.le</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.less_equal.html">torch.less_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lt.html">torch.lt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.less.html">torch.less</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.maximum.html">torch.maximum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.minimum.html">torch.minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmax.html">torch.fmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmin.html">torch.fmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ne.html">torch.ne</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.not_equal.html">torch.not_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sort.html">torch.sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.topk.html">torch.topk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.msort.html">torch.msort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.stft.html">torch.stft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.istft.html">torch.istft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bartlett_window.html">torch.bartlett_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.blackman_window.html">torch.blackman_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hamming_window.html">torch.hamming_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hann_window.html">torch.hann_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kaiser_window.html">torch.kaiser_window</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_1d.html">torch.atleast_1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_2d.html">torch.atleast_2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_3d.html">torch.atleast_3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bincount.html">torch.bincount</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.block_diag.html">torch.block_diag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_tensors.html">torch.broadcast_tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_to.html">torch.broadcast_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_shapes.html">torch.broadcast_shapes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bucketize.html">torch.bucketize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cartesian_prod.html">torch.cartesian_prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cdist.html">torch.cdist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clone.html">torch.clone</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.combinations.html">torch.combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.corrcoef.html">torch.corrcoef</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cov.html">torch.cov</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cross.html">torch.cross</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cummax.html">torch.cummax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cummin.html">torch.cummin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cumprod.html">torch.cumprod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cumsum.html">torch.cumsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diag.html">torch.diag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diag_embed.html">torch.diag_embed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diagflat.html">torch.diagflat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diagonal.html">torch.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.diff.html">torch.diff</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.einsum.html">torch.einsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flatten.html">torch.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flip.html">torch.flip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fliplr.html">torch.fliplr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flipud.html">torch.flipud</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kron.html">torch.kron</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rot90.html">torch.rot90</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gcd.html">torch.gcd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.histc.html">torch.histc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.histogram.html">torch.histogram</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.histogramdd.html">torch.histogramdd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.meshgrid.html">torch.meshgrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lcm.html">torch.lcm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logcumsumexp.html">torch.logcumsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ravel.html">torch.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.renorm.html">torch.renorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.repeat_interleave.html">torch.repeat_interleave</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.roll.html">torch.roll</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.searchsorted.html">torch.searchsorted</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tensordot.html">torch.tensordot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trace.html">torch.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tril.html">torch.tril</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tril_indices.html">torch.tril_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.triu.html">torch.triu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.triu_indices.html">torch.triu_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.unflatten.html">torch.unflatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vander.html">torch.vander</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.view_as_real.html">torch.view_as_real</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.view_as_complex.html">torch.view_as_complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.resolve_conj.html">torch.resolve_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.resolve_neg.html">torch.resolve_neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addbmm.html">torch.addbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addmm.html">torch.addmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addmv.html">torch.addmv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.addr.html">torch.addr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.baddbmm.html">torch.baddbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bmm.html">torch.bmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.chain_matmul.html">torch.chain_matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cholesky.html">torch.cholesky</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cholesky_inverse.html">torch.cholesky_inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cholesky_solve.html">torch.cholesky_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dot.html">torch.dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.geqrf.html">torch.geqrf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ger.html">torch.ger</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.inner.html">torch.inner</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.inverse.html">torch.inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.det.html">torch.det</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logdet.html">torch.logdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.slogdet.html">torch.slogdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lu.html">torch.lu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lu_solve.html">torch.lu_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lu_unpack.html">torch.lu_unpack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.matmul.html">torch.matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.matrix_power.html">torch.matrix_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.matrix_exp.html">torch.matrix_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mm.html">torch.mm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mv.html">torch.mv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.orgqr.html">torch.orgqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ormqr.html">torch.ormqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.outer.html">torch.outer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pinverse.html">torch.pinverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.qr.html">torch.qr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.svd.html">torch.svd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.svd_lowrank.html">torch.svd_lowrank</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pca_lowrank.html">torch.pca_lowrank</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lobpcg.html">torch.lobpcg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trapz.html">torch.trapz</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trapezoid.html">torch.trapezoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cumulative_trapezoid.html">torch.cumulative_trapezoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.triangular_solve.html">torch.triangular_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vdot.html">torch.vdot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_abs.html">torch._foreach_abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_abs_.html">torch._foreach_abs_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_acos.html">torch._foreach_acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_acos_.html">torch._foreach_acos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_asin.html">torch._foreach_asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_asin_.html">torch._foreach_asin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_atan.html">torch._foreach_atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_atan_.html">torch._foreach_atan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_ceil.html">torch._foreach_ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_ceil_.html">torch._foreach_ceil_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cos.html">torch._foreach_cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cos_.html">torch._foreach_cos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cosh.html">torch._foreach_cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_cosh_.html">torch._foreach_cosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erf.html">torch._foreach_erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erf_.html">torch._foreach_erf_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erfc.html">torch._foreach_erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_erfc_.html">torch._foreach_erfc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_exp.html">torch._foreach_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_exp_.html">torch._foreach_exp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_expm1.html">torch._foreach_expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_expm1_.html">torch._foreach_expm1_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_floor.html">torch._foreach_floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_floor_.html">torch._foreach_floor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log.html">torch._foreach_log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log_.html">torch._foreach_log_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log10.html">torch._foreach_log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log10_.html">torch._foreach_log10_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log1p.html">torch._foreach_log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log1p_.html">torch._foreach_log1p_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log2.html">torch._foreach_log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_log2_.html">torch._foreach_log2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_neg.html">torch._foreach_neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_neg_.html">torch._foreach_neg_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_tan.html">torch._foreach_tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_tan_.html">torch._foreach_tan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sin.html">torch._foreach_sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sin_.html">torch._foreach_sin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sinh.html">torch._foreach_sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sinh_.html">torch._foreach_sinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_round.html">torch._foreach_round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_round_.html">torch._foreach_round_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sqrt.html">torch._foreach_sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sqrt_.html">torch._foreach_sqrt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_lgamma.html">torch._foreach_lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_lgamma_.html">torch._foreach_lgamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_frac.html">torch._foreach_frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_frac_.html">torch._foreach_frac_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_reciprocal.html">torch._foreach_reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_reciprocal_.html">torch._foreach_reciprocal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sigmoid.html">torch._foreach_sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_sigmoid_.html">torch._foreach_sigmoid_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_trunc.html">torch._foreach_trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_trunc_.html">torch._foreach_trunc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._foreach_zero_.html">torch._foreach_zero_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiled_with_cxx11_abi.html">torch.compiled_with_cxx11_abi</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.result_type.html">torch.result_type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.can_cast.html">torch.can_cast</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.promote_types.html">torch.promote_types</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.use_deterministic_algorithms.html">torch.use_deterministic_algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.are_deterministic_algorithms_enabled.html">torch.are_deterministic_algorithms_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_deterministic_algorithms_warn_only_enabled.html">torch.is_deterministic_algorithms_warn_only_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_deterministic_debug_mode.html">torch.set_deterministic_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_deterministic_debug_mode.html">torch.get_deterministic_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_float32_matmul_precision.html">torch.set_float32_matmul_precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_float32_matmul_precision.html">torch.get_float32_matmul_precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.set_warn_always.html">torch.set_warn_always</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.get_device_module.html">torch.get_device_module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.is_warn_always_enabled.html">torch.is_warn_always_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vmap.html">torch.vmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._assert.html">torch._assert</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_float.html">torch.sym_float</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_fresh_size.html">torch.sym_fresh_size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_int.html">torch.sym_int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_max.html">torch.sym_max</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_min.html">torch.sym_min</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_not.html">torch.sym_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_ite.html">torch.sym_ite</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sym_sum.html">torch.sym_sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cond.html">torch.cond</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compile.html">torch.compile</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="torch.aliases.html">Aliases in torch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.align_tensors.html">torch.functional.align_tensors</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.atleast_1d.html">torch.functional.atleast_1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.atleast_2d.html">torch.functional.atleast_2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.atleast_3d.html">torch.functional.atleast_3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.block_diag.html">torch.functional.block_diag</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.broadcast_shapes.html">torch.functional.broadcast_shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.broadcast_tensors.html">torch.functional.broadcast_tensors</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.cartesian_prod.html">torch.functional.cartesian_prod</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.cdist.html">torch.functional.cdist</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.chain_matmul.html">torch.functional.chain_matmul</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.einsum.html">torch.functional.einsum</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.lu.html">torch.functional.lu</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.meshgrid.html">torch.functional.meshgrid</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.norm.html">torch.functional.norm</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.split.html">torch.functional.split</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.stft.html">torch.functional.stft</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.tensordot.html">torch.functional.tensordot</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.unique.html">torch.functional.unique</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.unique_consecutive.html">torch.functional.unique_consecutive</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.functional.unravel_index.html">torch.functional.unravel_index</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="nn.aliases.html">Aliases in torch.nn</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.container.Sequential.html">Sequential</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.container.ModuleList.html">ModuleList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.container.ModuleDict.html">ModuleDict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.container.ParameterList.html">ParameterList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.container.ParameterDict.html">ParameterDict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.conv.Conv1d.html">Conv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.conv.Conv2d.html">Conv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.conv.Conv3d.html">Conv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.conv.ConvTranspose1d.html">ConvTranspose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.conv.ConvTranspose2d.html">ConvTranspose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.conv.ConvTranspose3d.html">ConvTranspose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.conv.LazyConv1d.html">LazyConv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.conv.LazyConv2d.html">LazyConv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.conv.LazyConv3d.html">LazyConv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.conv.LazyConvTranspose1d.html">LazyConvTranspose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.conv.LazyConvTranspose2d.html">LazyConvTranspose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.conv.LazyConvTranspose3d.html">LazyConvTranspose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.fold.Unfold.html">Unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.fold.Fold.html">Fold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.MaxPool1d.html">MaxPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.MaxPool2d.html">MaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.MaxPool3d.html">MaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.MaxUnpool1d.html">MaxUnpool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.MaxUnpool2d.html">MaxUnpool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.MaxUnpool3d.html">MaxUnpool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.AvgPool1d.html">AvgPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.AvgPool2d.html">AvgPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.AvgPool3d.html">AvgPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.FractionalMaxPool2d.html">FractionalMaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.FractionalMaxPool3d.html">FractionalMaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.LPPool1d.html">LPPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.LPPool2d.html">LPPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.LPPool3d.html">LPPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.AdaptiveMaxPool1d.html">AdaptiveMaxPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.AdaptiveMaxPool2d.html">AdaptiveMaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.AdaptiveMaxPool3d.html">AdaptiveMaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.AdaptiveAvgPool1d.html">AdaptiveAvgPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.AdaptiveAvgPool2d.html">AdaptiveAvgPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pooling.AdaptiveAvgPool3d.html">AdaptiveAvgPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.padding.ReflectionPad1d.html">ReflectionPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.padding.ReflectionPad2d.html">ReflectionPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.padding.ReflectionPad3d.html">ReflectionPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.padding.ReplicationPad1d.html">ReplicationPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.padding.ReplicationPad2d.html">ReplicationPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.padding.ReplicationPad3d.html">ReplicationPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.padding.ZeroPad1d.html">ZeroPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.padding.ZeroPad2d.html">ZeroPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.padding.ZeroPad3d.html">ZeroPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.padding.ConstantPad1d.html">ConstantPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.padding.ConstantPad2d.html">ConstantPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.padding.ConstantPad3d.html">ConstantPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.padding.CircularPad1d.html">CircularPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.padding.CircularPad2d.html">CircularPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.padding.CircularPad3d.html">CircularPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.ELU.html">ELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.Hardshrink.html">Hardshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.Hardsigmoid.html">Hardsigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.Hardtanh.html">Hardtanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.Hardswish.html">Hardswish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.LeakyReLU.html">LeakyReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.LogSigmoid.html">LogSigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.MultiheadAttention.html">MultiheadAttention</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.PReLU.html">PReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.ReLU.html">ReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.ReLU6.html">ReLU6</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.RReLU.html">RReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.SELU.html">SELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.CELU.html">CELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.GELU.html">GELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.Sigmoid.html">Sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.SiLU.html">SiLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.Mish.html">Mish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.Softplus.html">Softplus</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.Softshrink.html">Softshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.Softsign.html">Softsign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.Tanh.html">Tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.Tanhshrink.html">Tanhshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.Threshold.html">Threshold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.GLU.html">GLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.Softmin.html">Softmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.Softmax.html">Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.Softmax2d.html">Softmax2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.activation.LogSoftmax.html">LogSoftmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss.html">AdaptiveLogSoftmaxWithLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.batchnorm.BatchNorm1d.html">BatchNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.batchnorm.BatchNorm2d.html">BatchNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.batchnorm.BatchNorm3d.html">BatchNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.batchnorm.LazyBatchNorm1d.html">LazyBatchNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.batchnorm.LazyBatchNorm2d.html">LazyBatchNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.batchnorm.LazyBatchNorm3d.html">LazyBatchNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.normalization.GroupNorm.html">GroupNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.batchnorm.SyncBatchNorm.html">SyncBatchNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.instancenorm.InstanceNorm1d.html">InstanceNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.instancenorm.InstanceNorm2d.html">InstanceNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.instancenorm.InstanceNorm3d.html">InstanceNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.instancenorm.LazyInstanceNorm1d.html">LazyInstanceNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.instancenorm.LazyInstanceNorm2d.html">LazyInstanceNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.instancenorm.LazyInstanceNorm3d.html">LazyInstanceNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.normalization.LayerNorm.html">LayerNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.normalization.LocalResponseNorm.html">LocalResponseNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.normalization.RMSNorm.html">RMSNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.rnn.RNNBase.html">RNNBase</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.rnn.RNN.html">RNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.rnn.LSTM.html">LSTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.rnn.GRU.html">GRU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.rnn.RNNCell.html">RNNCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.rnn.LSTMCell.html">LSTMCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.rnn.GRUCell.html">GRUCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.transformer.Transformer.html">Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.transformer.TransformerEncoder.html">TransformerEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.transformer.TransformerDecoder.html">TransformerDecoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.transformer.TransformerEncoderLayer.html">TransformerEncoderLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.transformer.TransformerDecoderLayer.html">TransformerDecoderLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.linear.Identity.html">Identity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.linear.Linear.html">Linear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.linear.Bilinear.html">Bilinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.linear.LazyLinear.html">LazyLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.dropout.Dropout.html">Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.dropout.Dropout1d.html">Dropout1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.dropout.Dropout2d.html">Dropout2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.dropout.Dropout3d.html">Dropout3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.dropout.AlphaDropout.html">AlphaDropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.dropout.FeatureAlphaDropout.html">FeatureAlphaDropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.sparse.Embedding.html">Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.sparse.EmbeddingBag.html">EmbeddingBag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.distance.CosineSimilarity.html">CosineSimilarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.distance.PairwiseDistance.html">PairwiseDistance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.L1Loss.html">L1Loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.MSELoss.html">MSELoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.CrossEntropyLoss.html">CrossEntropyLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.CTCLoss.html">CTCLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.NLLLoss.html">NLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.PoissonNLLLoss.html">PoissonNLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.GaussianNLLLoss.html">GaussianNLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.KLDivLoss.html">KLDivLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.BCELoss.html">BCELoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.BCEWithLogitsLoss.html">BCEWithLogitsLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.MarginRankingLoss.html">MarginRankingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.HingeEmbeddingLoss.html">HingeEmbeddingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.MultiLabelMarginLoss.html">MultiLabelMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.HuberLoss.html">HuberLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.SmoothL1Loss.html">SmoothL1Loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.SoftMarginLoss.html">SoftMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.MultiLabelSoftMarginLoss.html">MultiLabelSoftMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.CosineEmbeddingLoss.html">CosineEmbeddingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.MultiMarginLoss.html">MultiMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.TripletMarginLoss.html">TripletMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.loss.TripletMarginWithDistanceLoss.html">TripletMarginWithDistanceLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pixelshuffle.PixelShuffle.html">PixelShuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.pixelshuffle.PixelUnshuffle.html">PixelUnshuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.upsampling.Upsample.html">Upsample</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.upsampling.UpsamplingNearest2d.html">UpsamplingNearest2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.upsampling.UpsamplingBilinear2d.html">UpsamplingBilinear2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.channelshuffle.ChannelShuffle.html">ChannelShuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad.clip_grad_norm_.html">torch.nn.utils.clip_grad.clip_grad_norm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad.clip_grad_norm.html">torch.nn.utils.clip_grad.clip_grad_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad.clip_grad_value_.html">torch.nn.utils.clip_grad.clip_grad_value_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.convert_parameters.parameters_to_vector.html">torch.nn.utils.convert_parameters.parameters_to_vector</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.convert_parameters.vector_to_parameters.html">torch.nn.utils.convert_parameters.vector_to_parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fusion.fuse_conv_bn_eval.html">torch.nn.utils.fusion.fuse_conv_bn_eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fusion.fuse_conv_bn_weights.html">torch.nn.utils.fusion.fuse_conv_bn_weights</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fusion.fuse_linear_bn_eval.html">torch.nn.utils.fusion.fuse_linear_bn_eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fusion.fuse_linear_bn_weights.html">torch.nn.utils.fusion.fuse_linear_bn_weights</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.memory_format.convert_conv2d_weight_memory_format.html">torch.nn.utils.memory_format.convert_conv2d_weight_memory_format</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.memory_format.convert_conv3d_weight_memory_format.html">torch.nn.utils.memory_format.convert_conv3d_weight_memory_format</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.weight_norm.weight_norm.html">torch.nn.utils.weight_norm.weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.weight_norm.remove_weight_norm.html">torch.nn.utils.weight_norm.remove_weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.spectral_norm.spectral_norm.html">torch.nn.utils.spectral_norm.spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.spectral_norm.remove_spectral_norm.html">torch.nn.utils.spectral_norm.remove_spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.init.skip_init.html">torch.nn.utils.init.skip_init</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="nn.html">torch.nn</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.Buffer.html">Buffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.Parameter.html">Parameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.UninitializedParameter.html">UninitializedParameter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.UninitializedBuffer.html">UninitializedBuffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Module.html">Module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Sequential.html">Sequential</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ModuleList.html">ModuleList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ModuleDict.html">ModuleDict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ParameterList.html">ParameterList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ParameterDict.html">ParameterDict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_forward_pre_hook.html">torch.nn.modules.module.register_module_forward_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_forward_hook.html">torch.nn.modules.module.register_module_forward_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_backward_hook.html">torch.nn.modules.module.register_module_backward_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html">torch.nn.modules.module.register_module_full_backward_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_full_backward_hook.html">torch.nn.modules.module.register_module_full_backward_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_buffer_registration_hook.html">torch.nn.modules.module.register_module_buffer_registration_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_module_registration_hook.html">torch.nn.modules.module.register_module_module_registration_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_parameter_registration_hook.html">torch.nn.modules.module.register_module_parameter_registration_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Conv1d.html">Conv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Conv2d.html">Conv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Conv3d.html">Conv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConvTranspose1d.html">ConvTranspose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConvTranspose2d.html">ConvTranspose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConvTranspose3d.html">ConvTranspose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConv1d.html">LazyConv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConv2d.html">LazyConv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConv3d.html">LazyConv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConvTranspose1d.html">LazyConvTranspose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConvTranspose2d.html">LazyConvTranspose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyConvTranspose3d.html">LazyConvTranspose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Unfold.html">Unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Fold.html">Fold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxPool1d.html">MaxPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxPool2d.html">MaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxPool3d.html">MaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxUnpool1d.html">MaxUnpool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxUnpool2d.html">MaxUnpool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MaxUnpool3d.html">MaxUnpool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AvgPool1d.html">AvgPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AvgPool2d.html">AvgPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AvgPool3d.html">AvgPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.FractionalMaxPool2d.html">FractionalMaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.FractionalMaxPool3d.html">FractionalMaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LPPool1d.html">LPPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LPPool2d.html">LPPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LPPool3d.html">LPPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool1d.html">AdaptiveMaxPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool2d.html">AdaptiveMaxPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool3d.html">AdaptiveMaxPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool1d.html">AdaptiveAvgPool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool2d.html">AdaptiveAvgPool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool3d.html">AdaptiveAvgPool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReflectionPad1d.html">ReflectionPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReflectionPad2d.html">ReflectionPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReflectionPad3d.html">ReflectionPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReplicationPad1d.html">ReplicationPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReplicationPad2d.html">ReplicationPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReplicationPad3d.html">ReplicationPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ZeroPad1d.html">ZeroPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ZeroPad2d.html">ZeroPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ZeroPad3d.html">ZeroPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConstantPad1d.html">ConstantPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConstantPad2d.html">ConstantPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ConstantPad3d.html">ConstantPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CircularPad1d.html">CircularPad1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CircularPad2d.html">CircularPad2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CircularPad3d.html">CircularPad3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ELU.html">ELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardshrink.html">Hardshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardsigmoid.html">Hardsigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardtanh.html">Hardtanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Hardswish.html">Hardswish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LeakyReLU.html">LeakyReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LogSigmoid.html">LogSigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiheadAttention.html">MultiheadAttention</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PReLU.html">PReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReLU.html">ReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ReLU6.html">ReLU6</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RReLU.html">RReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SELU.html">SELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CELU.html">CELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GELU.html">GELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Sigmoid.html">Sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SiLU.html">SiLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Mish.html">Mish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softplus.html">Softplus</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softshrink.html">Softshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softsign.html">Softsign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Tanh.html">Tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Tanhshrink.html">Tanhshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Threshold.html">Threshold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GLU.html">GLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softmin.html">Softmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softmax.html">Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Softmax2d.html">Softmax2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LogSoftmax.html">LogSoftmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html">AdaptiveLogSoftmaxWithLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BatchNorm1d.html">BatchNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BatchNorm2d.html">BatchNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BatchNorm3d.html">BatchNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyBatchNorm1d.html">LazyBatchNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyBatchNorm2d.html">LazyBatchNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyBatchNorm3d.html">LazyBatchNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GroupNorm.html">GroupNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SyncBatchNorm.html">SyncBatchNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.InstanceNorm1d.html">InstanceNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.InstanceNorm2d.html">InstanceNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.InstanceNorm3d.html">InstanceNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm1d.html">LazyInstanceNorm1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm2d.html">LazyInstanceNorm2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm3d.html">LazyInstanceNorm3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LayerNorm.html">LayerNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LocalResponseNorm.html">LocalResponseNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RMSNorm.html">RMSNorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RNNBase.html">RNNBase</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RNN.html">RNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LSTM.html">LSTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GRU.html">GRU</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.RNNCell.html">RNNCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LSTMCell.html">LSTMCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GRUCell.html">GRUCell</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Transformer.html">Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerEncoder.html">TransformerEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerDecoder.html">TransformerDecoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerEncoderLayer.html">TransformerEncoderLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TransformerDecoderLayer.html">TransformerDecoderLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Identity.html">Identity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Linear.html">Linear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Bilinear.html">Bilinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.LazyLinear.html">LazyLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout.html">Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout1d.html">Dropout1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout2d.html">Dropout2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Dropout3d.html">Dropout3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.AlphaDropout.html">AlphaDropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.FeatureAlphaDropout.html">FeatureAlphaDropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Embedding.html">Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.EmbeddingBag.html">EmbeddingBag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CosineSimilarity.html">CosineSimilarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PairwiseDistance.html">PairwiseDistance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.L1Loss.html">L1Loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MSELoss.html">MSELoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CrossEntropyLoss.html">CrossEntropyLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CTCLoss.html">CTCLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.NLLLoss.html">NLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PoissonNLLLoss.html">PoissonNLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.GaussianNLLLoss.html">GaussianNLLLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.KLDivLoss.html">KLDivLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BCELoss.html">BCELoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.BCEWithLogitsLoss.html">BCEWithLogitsLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MarginRankingLoss.html">MarginRankingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.HingeEmbeddingLoss.html">HingeEmbeddingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiLabelMarginLoss.html">MultiLabelMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.HuberLoss.html">HuberLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SmoothL1Loss.html">SmoothL1Loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.SoftMarginLoss.html">SoftMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiLabelSoftMarginLoss.html">MultiLabelSoftMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.CosineEmbeddingLoss.html">CosineEmbeddingLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.MultiMarginLoss.html">MultiMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TripletMarginLoss.html">TripletMarginLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.TripletMarginWithDistanceLoss.html">TripletMarginWithDistanceLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PixelShuffle.html">PixelShuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.PixelUnshuffle.html">PixelUnshuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.Upsample.html">Upsample</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.UpsamplingNearest2d.html">UpsamplingNearest2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.UpsamplingBilinear2d.html">UpsamplingBilinear2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.ChannelShuffle.html">ChannelShuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.DataParallel.html">DataParallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parallel.DistributedDataParallel.html">DistributedDataParallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_norm_.html">torch.nn.utils.clip_grad_norm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_norm.html">torch.nn.utils.clip_grad_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_value_.html">torch.nn.utils.clip_grad_value_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.get_total_norm.html">torch.nn.utils.get_total_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.clip_grads_with_norm_.html">torch.nn.utils.clip_grads_with_norm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parameters_to_vector.html">torch.nn.utils.parameters_to_vector</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.vector_to_parameters.html">torch.nn.utils.vector_to_parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_conv_bn_eval.html">torch.nn.utils.fuse_conv_bn_eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_conv_bn_weights.html">torch.nn.utils.fuse_conv_bn_weights</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_linear_bn_eval.html">torch.nn.utils.fuse_linear_bn_eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.fuse_linear_bn_weights.html">torch.nn.utils.fuse_linear_bn_weights</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.convert_conv2d_weight_memory_format.html">torch.nn.utils.convert_conv2d_weight_memory_format</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.convert_conv3d_weight_memory_format.html">torch.nn.utils.convert_conv3d_weight_memory_format</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.weight_norm.html">torch.nn.utils.weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.remove_weight_norm.html">torch.nn.utils.remove_weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.spectral_norm.html">torch.nn.utils.spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.remove_spectral_norm.html">torch.nn.utils.remove_spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.skip_init.html">torch.nn.utils.skip_init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.BasePruningMethod.html">BasePruningMethod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.PruningContainer.html">PruningContainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.Identity_class.html">Identity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.RandomUnstructured.html">RandomUnstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.L1Unstructured.html">L1Unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.RandomStructured.html">RandomStructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.LnStructured.html">LnStructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.CustomFromMask.html">CustomFromMask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.identity_function.html">torch.nn.utils.prune.identity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.random_unstructured.html">torch.nn.utils.prune.random_unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.l1_unstructured.html">torch.nn.utils.prune.l1_unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.random_structured.html">torch.nn.utils.prune.random_structured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.ln_structured.html">torch.nn.utils.prune.ln_structured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.global_unstructured.html">torch.nn.utils.prune.global_unstructured</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.custom_from_mask.html">torch.nn.utils.prune.custom_from_mask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.remove.html">torch.nn.utils.prune.remove</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.prune.is_pruned.html">torch.nn.utils.prune.is_pruned</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrizations.orthogonal.html">torch.nn.utils.parametrizations.orthogonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrizations.weight_norm.html">torch.nn.utils.parametrizations.weight_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrizations.spectral_norm.html">torch.nn.utils.parametrizations.spectral_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.register_parametrization.html">torch.nn.utils.parametrize.register_parametrization</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.remove_parametrizations.html">torch.nn.utils.parametrize.remove_parametrizations</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.cached.html">torch.nn.utils.parametrize.cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.is_parametrized.html">torch.nn.utils.parametrize.is_parametrized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.transfer_parametrizations_and_params.html">torch.nn.utils.parametrize.transfer_parametrizations_and_params</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.type_before_parametrizations.html">torch.nn.utils.parametrize.type_before_parametrizations</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.parametrize.ParametrizationList.html">ParametrizationList</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.stateless.functional_call.html">torch.nn.utils.stateless.functional_call</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.PackedSequence.html">PackedSequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pack_padded_sequence.html">torch.nn.utils.rnn.pack_padded_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pad_packed_sequence.html">torch.nn.utils.rnn.pad_packed_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pad_sequence.html">torch.nn.utils.rnn.pad_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.pack_sequence.html">torch.nn.utils.rnn.pack_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.unpack_sequence.html">torch.nn.utils.rnn.unpack_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.unpad_sequence.html">torch.nn.utils.rnn.unpad_sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.utils.rnn.invert_permutation.html">torch.nn.utils.rnn.invert_permutation</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.is_lazy.html">torch.nn.parameter.is_lazy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.factory_kwargs.html">torch.nn.factory_kwargs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.flatten.Flatten.html">Flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.flatten.Unflatten.html">Unflatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.modules.lazy.LazyModuleMixin.html">LazyModuleMixin</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv1d.html">torch.nn.functional.conv1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv2d.html">torch.nn.functional.conv2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv3d.html">torch.nn.functional.conv3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose1d.html">torch.nn.functional.conv_transpose1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose2d.html">torch.nn.functional.conv_transpose2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose3d.html">torch.nn.functional.conv_transpose3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.unfold.html">torch.nn.functional.unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.fold.html">torch.nn.functional.fold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.avg_pool1d.html">torch.nn.functional.avg_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.avg_pool2d.html">torch.nn.functional.avg_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.avg_pool3d.html">torch.nn.functional.avg_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_pool1d.html">torch.nn.functional.max_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_pool2d.html">torch.nn.functional.max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_pool3d.html">torch.nn.functional.max_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_unpool1d.html">torch.nn.functional.max_unpool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_unpool2d.html">torch.nn.functional.max_unpool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.max_unpool3d.html">torch.nn.functional.max_unpool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.lp_pool1d.html">torch.nn.functional.lp_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.lp_pool2d.html">torch.nn.functional.lp_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.lp_pool3d.html">torch.nn.functional.lp_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool1d.html">torch.nn.functional.adaptive_max_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool2d.html">torch.nn.functional.adaptive_max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool3d.html">torch.nn.functional.adaptive_max_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool1d.html">torch.nn.functional.adaptive_avg_pool1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool2d.html">torch.nn.functional.adaptive_avg_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool3d.html">torch.nn.functional.adaptive_avg_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.fractional_max_pool2d.html">torch.nn.functional.fractional_max_pool2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.fractional_max_pool3d.html">torch.nn.functional.fractional_max_pool3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.scaled_dot_product_attention.html">torch.nn.functional.scaled_dot_product_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.threshold.html">torch.nn.functional.threshold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.threshold_.html">torch.nn.functional.threshold_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.relu.html">torch.nn.functional.relu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.relu_.html">torch.nn.functional.relu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardtanh.html">torch.nn.functional.hardtanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardtanh_.html">torch.nn.functional.hardtanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardswish.html">torch.nn.functional.hardswish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.relu6.html">torch.nn.functional.relu6</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.elu.html">torch.nn.functional.elu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.elu_.html">torch.nn.functional.elu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.selu.html">torch.nn.functional.selu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.celu.html">torch.nn.functional.celu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.leaky_relu.html">torch.nn.functional.leaky_relu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.leaky_relu_.html">torch.nn.functional.leaky_relu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.prelu.html">torch.nn.functional.prelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.rrelu.html">torch.nn.functional.rrelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.rrelu_.html">torch.nn.functional.rrelu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.glu.html">torch.nn.functional.glu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.gelu.html">torch.nn.functional.gelu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.logsigmoid.html">torch.nn.functional.logsigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardshrink.html">torch.nn.functional.hardshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.tanhshrink.html">torch.nn.functional.tanhshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softsign.html">torch.nn.functional.softsign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softplus.html">torch.nn.functional.softplus</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softmin.html">torch.nn.functional.softmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softmax.html">torch.nn.functional.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.softshrink.html">torch.nn.functional.softshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.gumbel_softmax.html">torch.nn.functional.gumbel_softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.log_softmax.html">torch.nn.functional.log_softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.tanh.html">torch.nn.functional.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.sigmoid.html">torch.nn.functional.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hardsigmoid.html">torch.nn.functional.hardsigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.silu.html">torch.nn.functional.silu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.mish.html">torch.nn.functional.mish</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.batch_norm.html">torch.nn.functional.batch_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.group_norm.html">torch.nn.functional.group_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.instance_norm.html">torch.nn.functional.instance_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.layer_norm.html">torch.nn.functional.layer_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.local_response_norm.html">torch.nn.functional.local_response_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.rms_norm.html">torch.nn.functional.rms_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.normalize.html">torch.nn.functional.normalize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.linear.html">torch.nn.functional.linear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.bilinear.html">torch.nn.functional.bilinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout.html">torch.nn.functional.dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.alpha_dropout.html">torch.nn.functional.alpha_dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.feature_alpha_dropout.html">torch.nn.functional.feature_alpha_dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout1d.html">torch.nn.functional.dropout1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout2d.html">torch.nn.functional.dropout2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.dropout3d.html">torch.nn.functional.dropout3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.embedding.html">torch.nn.functional.embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.embedding_bag.html">torch.nn.functional.embedding_bag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.one_hot.html">torch.nn.functional.one_hot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pairwise_distance.html">torch.nn.functional.pairwise_distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.cosine_similarity.html">torch.nn.functional.cosine_similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pdist.html">torch.nn.functional.pdist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.binary_cross_entropy.html">torch.nn.functional.binary_cross_entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.binary_cross_entropy_with_logits.html">torch.nn.functional.binary_cross_entropy_with_logits</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.poisson_nll_loss.html">torch.nn.functional.poisson_nll_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.cosine_embedding_loss.html">torch.nn.functional.cosine_embedding_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.cross_entropy.html">torch.nn.functional.cross_entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.ctc_loss.html">torch.nn.functional.ctc_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.gaussian_nll_loss.html">torch.nn.functional.gaussian_nll_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.hinge_embedding_loss.html">torch.nn.functional.hinge_embedding_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.kl_div.html">torch.nn.functional.kl_div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.l1_loss.html">torch.nn.functional.l1_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.mse_loss.html">torch.nn.functional.mse_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.margin_ranking_loss.html">torch.nn.functional.margin_ranking_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.multilabel_margin_loss.html">torch.nn.functional.multilabel_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.multilabel_soft_margin_loss.html">torch.nn.functional.multilabel_soft_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.multi_margin_loss.html">torch.nn.functional.multi_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.nll_loss.html">torch.nn.functional.nll_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.huber_loss.html">torch.nn.functional.huber_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.smooth_l1_loss.html">torch.nn.functional.smooth_l1_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.soft_margin_loss.html">torch.nn.functional.soft_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.triplet_margin_loss.html">torch.nn.functional.triplet_margin_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.triplet_margin_with_distance_loss.html">torch.nn.functional.triplet_margin_with_distance_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pixel_shuffle.html">torch.nn.functional.pixel_shuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pixel_unshuffle.html">torch.nn.functional.pixel_unshuffle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.pad.html">torch.nn.functional.pad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.interpolate.html">torch.nn.functional.interpolate</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.upsample.html">torch.nn.functional.upsample</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.upsample_nearest.html">torch.nn.functional.upsample_nearest</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.upsample_bilinear.html">torch.nn.functional.upsample_bilinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.grid_sample.html">torch.nn.functional.grid_sample</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.affine_grid.html">torch.nn.functional.affine_grid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.torch.nn.parallel.data_parallel.html">torch.nn.functional.torch.nn.parallel.data_parallel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.ScalingType.html">ScalingType</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.SwizzleType.html">SwizzleType</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.grouped_mm.html">torch.nn.functional.grouped_mm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.scaled_mm.html">torch.nn.functional.scaled_mm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.scaled_grouped_mm.html">torch.nn.functional.scaled_grouped_mm</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="tensors.html">torch.Tensor</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_tensor.html">torch.Tensor.new_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_full.html">torch.Tensor.new_full</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_empty.html">torch.Tensor.new_empty</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_ones.html">torch.Tensor.new_ones</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.new_zeros.html">torch.Tensor.new_zeros</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_cuda.html">torch.Tensor.is_cuda</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_quantized.html">torch.Tensor.is_quantized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_meta.html">torch.Tensor.is_meta</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.device.html">torch.Tensor.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.grad.html">torch.Tensor.grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ndim.html">torch.Tensor.ndim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.real.html">torch.Tensor.real</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.imag.html">torch.Tensor.imag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nbytes.html">torch.Tensor.nbytes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.itemsize.html">torch.Tensor.itemsize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.abs.html">torch.Tensor.abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.abs_.html">torch.Tensor.abs_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.absolute.html">torch.Tensor.absolute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.absolute_.html">torch.Tensor.absolute_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acos.html">torch.Tensor.acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acos_.html">torch.Tensor.acos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccos.html">torch.Tensor.arccos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccos_.html">torch.Tensor.arccos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.add.html">torch.Tensor.add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.add_.html">torch.Tensor.add_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addbmm.html">torch.Tensor.addbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addbmm_.html">torch.Tensor.addbmm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcdiv.html">torch.Tensor.addcdiv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcdiv_.html">torch.Tensor.addcdiv_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcmul.html">torch.Tensor.addcmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addcmul_.html">torch.Tensor.addcmul_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmm.html">torch.Tensor.addmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmm_.html">torch.Tensor.addmm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sspaddmm.html">torch.Tensor.sspaddmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmv.html">torch.Tensor.addmv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addmv_.html">torch.Tensor.addmv_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addr.html">torch.Tensor.addr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.addr_.html">torch.Tensor.addr_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.adjoint.html">torch.Tensor.adjoint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.allclose.html">torch.Tensor.allclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.amax.html">torch.Tensor.amax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.amin.html">torch.Tensor.amin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.aminmax.html">torch.Tensor.aminmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.angle.html">torch.Tensor.angle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.apply_.html">torch.Tensor.apply_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argmax.html">torch.Tensor.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argmin.html">torch.Tensor.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argsort.html">torch.Tensor.argsort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.argwhere.html">torch.Tensor.argwhere</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asin.html">torch.Tensor.asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asin_.html">torch.Tensor.asin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsin.html">torch.Tensor.arcsin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsin_.html">torch.Tensor.arcsin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.as_strided.html">torch.Tensor.as_strided</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan.html">torch.Tensor.atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan_.html">torch.Tensor.atan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan.html">torch.Tensor.arctan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan_.html">torch.Tensor.arctan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan2.html">torch.Tensor.atan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atan2_.html">torch.Tensor.atan2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan2.html">torch.Tensor.arctan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctan2_.html">torch.Tensor.arctan2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.all.html">torch.Tensor.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.any.html">torch.Tensor.any</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.backward.html">torch.Tensor.backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.baddbmm.html">torch.Tensor.baddbmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.baddbmm_.html">torch.Tensor.baddbmm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bernoulli.html">torch.Tensor.bernoulli</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bernoulli_.html">torch.Tensor.bernoulli_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bfloat16.html">torch.Tensor.bfloat16</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bincount.html">torch.Tensor.bincount</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_not.html">torch.Tensor.bitwise_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_not_.html">torch.Tensor.bitwise_not_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_and.html">torch.Tensor.bitwise_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_and_.html">torch.Tensor.bitwise_and_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_or.html">torch.Tensor.bitwise_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_or_.html">torch.Tensor.bitwise_or_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_xor.html">torch.Tensor.bitwise_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_xor_.html">torch.Tensor.bitwise_xor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_left_shift.html">torch.Tensor.bitwise_left_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_left_shift_.html">torch.Tensor.bitwise_left_shift_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_right_shift.html">torch.Tensor.bitwise_right_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bitwise_right_shift_.html">torch.Tensor.bitwise_right_shift_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bmm.html">torch.Tensor.bmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.bool.html">torch.Tensor.bool</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.byte.html">torch.Tensor.byte</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.broadcast_to.html">torch.Tensor.broadcast_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cauchy_.html">torch.Tensor.cauchy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ceil.html">torch.Tensor.ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ceil_.html">torch.Tensor.ceil_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.char.html">torch.Tensor.char</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cholesky.html">torch.Tensor.cholesky</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cholesky_inverse.html">torch.Tensor.cholesky_inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cholesky_solve.html">torch.Tensor.cholesky_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.chunk.html">torch.Tensor.chunk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clamp.html">torch.Tensor.clamp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clamp_.html">torch.Tensor.clamp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clip.html">torch.Tensor.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clip_.html">torch.Tensor.clip_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.clone.html">torch.Tensor.clone</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.contiguous.html">torch.Tensor.contiguous</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.copy_.html">torch.Tensor.copy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.conj.html">torch.Tensor.conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.conj_physical.html">torch.Tensor.conj_physical</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.conj_physical_.html">torch.Tensor.conj_physical_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resolve_conj.html">torch.Tensor.resolve_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resolve_neg.html">torch.Tensor.resolve_neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.copysign.html">torch.Tensor.copysign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.copysign_.html">torch.Tensor.copysign_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cos.html">torch.Tensor.cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cos_.html">torch.Tensor.cos_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cosh.html">torch.Tensor.cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cosh_.html">torch.Tensor.cosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.corrcoef.html">torch.Tensor.corrcoef</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.count_nonzero.html">torch.Tensor.count_nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cov.html">torch.Tensor.cov</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acosh.html">torch.Tensor.acosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.acosh_.html">torch.Tensor.acosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccosh.html">torch.Tensor.arccosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arccosh_.html">torch.Tensor.arccosh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cpu.html">torch.Tensor.cpu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cross.html">torch.Tensor.cross</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cuda.html">torch.Tensor.cuda</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logcumsumexp.html">torch.Tensor.logcumsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cummax.html">torch.Tensor.cummax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cummin.html">torch.Tensor.cummin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumprod.html">torch.Tensor.cumprod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumprod_.html">torch.Tensor.cumprod_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumsum.html">torch.Tensor.cumsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cumsum_.html">torch.Tensor.cumsum_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.chalf.html">torch.Tensor.chalf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cfloat.html">torch.Tensor.cfloat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.cdouble.html">torch.Tensor.cdouble</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.data_ptr.html">torch.Tensor.data_ptr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.deg2rad.html">torch.Tensor.deg2rad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dequantize.html">torch.Tensor.dequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.det.html">torch.Tensor.det</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dense_dim.html">torch.Tensor.dense_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.detach.html">torch.Tensor.detach</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.detach_.html">torch.Tensor.detach_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diag.html">torch.Tensor.diag</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diag_embed.html">torch.Tensor.diag_embed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diagflat.html">torch.Tensor.diagflat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diagonal.html">torch.Tensor.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diagonal_scatter.html">torch.Tensor.diagonal_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fill_diagonal_.html">torch.Tensor.fill_diagonal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmax.html">torch.Tensor.fmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmin.html">torch.Tensor.fmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.diff.html">torch.Tensor.diff</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.digamma.html">torch.Tensor.digamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.digamma_.html">torch.Tensor.digamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dim.html">torch.Tensor.dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dim_order.html">torch.Tensor.dim_order</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dist.html">torch.Tensor.dist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.div.html">torch.Tensor.div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.div_.html">torch.Tensor.div_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.divide.html">torch.Tensor.divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.divide_.html">torch.Tensor.divide_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dot.html">torch.Tensor.dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.double.html">torch.Tensor.double</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dsplit.html">torch.Tensor.dsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.element_size.html">torch.Tensor.element_size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.eq.html">torch.Tensor.eq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.eq_.html">torch.Tensor.eq_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.equal.html">torch.Tensor.equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erf.html">torch.Tensor.erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erf_.html">torch.Tensor.erf_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfc.html">torch.Tensor.erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfc_.html">torch.Tensor.erfc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfinv.html">torch.Tensor.erfinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.erfinv_.html">torch.Tensor.erfinv_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.exp.html">torch.Tensor.exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.exp_.html">torch.Tensor.exp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expm1.html">torch.Tensor.expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expm1_.html">torch.Tensor.expm1_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expand.html">torch.Tensor.expand</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expand_as.html">torch.Tensor.expand_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.exponential_.html">torch.Tensor.exponential_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fix.html">torch.Tensor.fix</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fix_.html">torch.Tensor.fix_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fill_.html">torch.Tensor.fill_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.flatten.html">torch.Tensor.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.flip.html">torch.Tensor.flip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fliplr.html">torch.Tensor.fliplr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.flipud.html">torch.Tensor.flipud</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.float.html">torch.Tensor.float</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.float_power.html">torch.Tensor.float_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.float_power_.html">torch.Tensor.float_power_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor.html">torch.Tensor.floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor_.html">torch.Tensor.floor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor_divide.html">torch.Tensor.floor_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.floor_divide_.html">torch.Tensor.floor_divide_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmod.html">torch.Tensor.fmod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.fmod_.html">torch.Tensor.fmod_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.frac.html">torch.Tensor.frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.frac_.html">torch.Tensor.frac_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.frexp.html">torch.Tensor.frexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gather.html">torch.Tensor.gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gcd.html">torch.Tensor.gcd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gcd_.html">torch.Tensor.gcd_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ge.html">torch.Tensor.ge</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ge_.html">torch.Tensor.ge_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater_equal.html">torch.Tensor.greater_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater_equal_.html">torch.Tensor.greater_equal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.geometric_.html">torch.Tensor.geometric_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.geqrf.html">torch.Tensor.geqrf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ger.html">torch.Tensor.ger</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.get_device.html">torch.Tensor.get_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gt.html">torch.Tensor.gt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.gt_.html">torch.Tensor.gt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater.html">torch.Tensor.greater</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.greater_.html">torch.Tensor.greater_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.half.html">torch.Tensor.half</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hardshrink.html">torch.Tensor.hardshrink</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.heaviside.html">torch.Tensor.heaviside</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.histc.html">torch.Tensor.histc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.histogram.html">torch.Tensor.histogram</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hsplit.html">torch.Tensor.hsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hypot.html">torch.Tensor.hypot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.hypot_.html">torch.Tensor.hypot_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.i0.html">torch.Tensor.i0</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.i0_.html">torch.Tensor.i0_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igamma.html">torch.Tensor.igamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igamma_.html">torch.Tensor.igamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igammac.html">torch.Tensor.igammac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.igammac_.html">torch.Tensor.igammac_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_add_.html">torch.Tensor.index_add_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_add.html">torch.Tensor.index_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_copy_.html">torch.Tensor.index_copy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_copy.html">torch.Tensor.index_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_fill_.html">torch.Tensor.index_fill_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_fill.html">torch.Tensor.index_fill</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_put_.html">torch.Tensor.index_put_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_put.html">torch.Tensor.index_put</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_reduce_.html">torch.Tensor.index_reduce_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_reduce.html">torch.Tensor.index_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.index_select.html">torch.Tensor.index_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.indices.html">torch.Tensor.indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.inner.html">torch.Tensor.inner</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.int.html">torch.Tensor.int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.int_repr.html">torch.Tensor.int_repr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.inverse.html">torch.Tensor.inverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isclose.html">torch.Tensor.isclose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isfinite.html">torch.Tensor.isfinite</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isinf.html">torch.Tensor.isinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isposinf.html">torch.Tensor.isposinf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isneginf.html">torch.Tensor.isneginf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isnan.html">torch.Tensor.isnan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_contiguous.html">torch.Tensor.is_contiguous</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_complex.html">torch.Tensor.is_complex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_conj.html">torch.Tensor.is_conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_floating_point.html">torch.Tensor.is_floating_point</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_inference.html">torch.Tensor.is_inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_leaf.html">torch.Tensor.is_leaf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_pinned.html">torch.Tensor.is_pinned</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_set_to.html">torch.Tensor.is_set_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_shared.html">torch.Tensor.is_shared</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_signed.html">torch.Tensor.is_signed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_sparse.html">torch.Tensor.is_sparse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.istft.html">torch.Tensor.istft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.isreal.html">torch.Tensor.isreal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.item.html">torch.Tensor.item</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.kthvalue.html">torch.Tensor.kthvalue</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lcm.html">torch.Tensor.lcm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lcm_.html">torch.Tensor.lcm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ldexp.html">torch.Tensor.ldexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ldexp_.html">torch.Tensor.ldexp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.le.html">torch.Tensor.le</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.le_.html">torch.Tensor.le_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less_equal.html">torch.Tensor.less_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less_equal_.html">torch.Tensor.less_equal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lerp.html">torch.Tensor.lerp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lerp_.html">torch.Tensor.lerp_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lgamma.html">torch.Tensor.lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lgamma_.html">torch.Tensor.lgamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log.html">torch.Tensor.log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log_.html">torch.Tensor.log_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logdet.html">torch.Tensor.logdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log10.html">torch.Tensor.log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log10_.html">torch.Tensor.log10_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log1p.html">torch.Tensor.log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log1p_.html">torch.Tensor.log1p_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log2.html">torch.Tensor.log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log2_.html">torch.Tensor.log2_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.log_normal_.html">torch.Tensor.log_normal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logaddexp.html">torch.Tensor.logaddexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logaddexp2.html">torch.Tensor.logaddexp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logsumexp.html">torch.Tensor.logsumexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_and.html">torch.Tensor.logical_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_and_.html">torch.Tensor.logical_and_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_not.html">torch.Tensor.logical_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_not_.html">torch.Tensor.logical_not_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_or.html">torch.Tensor.logical_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_or_.html">torch.Tensor.logical_or_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_xor.html">torch.Tensor.logical_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logical_xor_.html">torch.Tensor.logical_xor_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logit.html">torch.Tensor.logit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.logit_.html">torch.Tensor.logit_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.long.html">torch.Tensor.long</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lt.html">torch.Tensor.lt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lt_.html">torch.Tensor.lt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less.html">torch.Tensor.less</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.less_.html">torch.Tensor.less_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lu.html">torch.Tensor.lu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.lu_solve.html">torch.Tensor.lu_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.as_subclass.html">torch.Tensor.as_subclass</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.map_.html">torch.Tensor.map_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_scatter_.html">torch.Tensor.masked_scatter_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_scatter.html">torch.Tensor.masked_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_fill_.html">torch.Tensor.masked_fill_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_fill.html">torch.Tensor.masked_fill</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.masked_select.html">torch.Tensor.masked_select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.matmul.html">torch.Tensor.matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.matrix_power.html">torch.Tensor.matrix_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.matrix_exp.html">torch.Tensor.matrix_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.max.html">torch.Tensor.max</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.maximum.html">torch.Tensor.maximum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mean.html">torch.Tensor.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.module_load.html">torch.Tensor.module_load</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nanmean.html">torch.Tensor.nanmean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.median.html">torch.Tensor.median</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nanmedian.html">torch.Tensor.nanmedian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.min.html">torch.Tensor.min</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.minimum.html">torch.Tensor.minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mm.html">torch.Tensor.mm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.smm.html">torch.Tensor.smm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mode.html">torch.Tensor.mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.movedim.html">torch.Tensor.movedim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.moveaxis.html">torch.Tensor.moveaxis</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.msort.html">torch.Tensor.msort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mul.html">torch.Tensor.mul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mul_.html">torch.Tensor.mul_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.multiply.html">torch.Tensor.multiply</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.multiply_.html">torch.Tensor.multiply_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.multinomial.html">torch.Tensor.multinomial</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mv.html">torch.Tensor.mv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mvlgamma.html">torch.Tensor.mvlgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.mvlgamma_.html">torch.Tensor.mvlgamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nansum.html">torch.Tensor.nansum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.narrow.html">torch.Tensor.narrow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.narrow_copy.html">torch.Tensor.narrow_copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ndimension.html">torch.Tensor.ndimension</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nan_to_num.html">torch.Tensor.nan_to_num</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nan_to_num_.html">torch.Tensor.nan_to_num_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ne.html">torch.Tensor.ne</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ne_.html">torch.Tensor.ne_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.not_equal.html">torch.Tensor.not_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.not_equal_.html">torch.Tensor.not_equal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.neg.html">torch.Tensor.neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.neg_.html">torch.Tensor.neg_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.negative.html">torch.Tensor.negative</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.negative_.html">torch.Tensor.negative_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nelement.html">torch.Tensor.nelement</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nextafter.html">torch.Tensor.nextafter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nextafter_.html">torch.Tensor.nextafter_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nonzero.html">torch.Tensor.nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.norm.html">torch.Tensor.norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.normal_.html">torch.Tensor.normal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.numel.html">torch.Tensor.numel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.numpy.html">torch.Tensor.numpy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.orgqr.html">torch.Tensor.orgqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ormqr.html">torch.Tensor.ormqr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.outer.html">torch.Tensor.outer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.permute.html">torch.Tensor.permute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pin_memory.html">torch.Tensor.pin_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pinverse.html">torch.Tensor.pinverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.polygamma.html">torch.Tensor.polygamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.polygamma_.html">torch.Tensor.polygamma_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.positive.html">torch.Tensor.positive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pow.html">torch.Tensor.pow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.pow_.html">torch.Tensor.pow_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.prod.html">torch.Tensor.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.put_.html">torch.Tensor.put_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.qr.html">torch.Tensor.qr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.qscheme.html">torch.Tensor.qscheme</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.quantile.html">torch.Tensor.quantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.nanquantile.html">torch.Tensor.nanquantile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_scale.html">torch.Tensor.q_scale</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_zero_point.html">torch.Tensor.q_zero_point</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_scales.html">torch.Tensor.q_per_channel_scales</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_zero_points.html">torch.Tensor.q_per_channel_zero_points</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_axis.html">torch.Tensor.q_per_channel_axis</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rad2deg.html">torch.Tensor.rad2deg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.random_.html">torch.Tensor.random_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ravel.html">torch.Tensor.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reciprocal.html">torch.Tensor.reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reciprocal_.html">torch.Tensor.reciprocal_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.record_stream.html">torch.Tensor.record_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.register_hook.html">torch.Tensor.register_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.register_post_accumulate_grad_hook.html">torch.Tensor.register_post_accumulate_grad_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.remainder.html">torch.Tensor.remainder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.remainder_.html">torch.Tensor.remainder_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.renorm.html">torch.Tensor.renorm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.renorm_.html">torch.Tensor.renorm_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.repeat.html">torch.Tensor.repeat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.repeat_interleave.html">torch.Tensor.repeat_interleave</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.requires_grad.html">torch.Tensor.requires_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.requires_grad_.html">torch.Tensor.requires_grad_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reshape.html">torch.Tensor.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reshape_as.html">torch.Tensor.reshape_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resize_.html">torch.Tensor.resize_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.resize_as_.html">torch.Tensor.resize_as_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.retain_grad.html">torch.Tensor.retain_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.retains_grad.html">torch.Tensor.retains_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.roll.html">torch.Tensor.roll</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rot90.html">torch.Tensor.rot90</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.round.html">torch.Tensor.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.round_.html">torch.Tensor.round_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rsqrt.html">torch.Tensor.rsqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.rsqrt_.html">torch.Tensor.rsqrt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter.html">torch.Tensor.scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_.html">torch.Tensor.scatter_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_add_.html">torch.Tensor.scatter_add_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_add.html">torch.Tensor.scatter_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_reduce_.html">torch.Tensor.scatter_reduce_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.scatter_reduce.html">torch.Tensor.scatter_reduce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.select.html">torch.Tensor.select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.select_scatter.html">torch.Tensor.select_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.set_.html">torch.Tensor.set_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.share_memory_.html">torch.Tensor.share_memory_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.short.html">torch.Tensor.short</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sigmoid.html">torch.Tensor.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sigmoid_.html">torch.Tensor.sigmoid_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sign.html">torch.Tensor.sign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sign_.html">torch.Tensor.sign_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.signbit.html">torch.Tensor.signbit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sgn.html">torch.Tensor.sgn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sgn_.html">torch.Tensor.sgn_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sin.html">torch.Tensor.sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sin_.html">torch.Tensor.sin_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinc.html">torch.Tensor.sinc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinc_.html">torch.Tensor.sinc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinh.html">torch.Tensor.sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sinh_.html">torch.Tensor.sinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asinh.html">torch.Tensor.asinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.asinh_.html">torch.Tensor.asinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsinh.html">torch.Tensor.arcsinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arcsinh_.html">torch.Tensor.arcsinh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.shape.html">torch.Tensor.shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.size.html">torch.Tensor.size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.slogdet.html">torch.Tensor.slogdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.slice_scatter.html">torch.Tensor.slice_scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.softmax.html">torch.Tensor.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sort.html">torch.Tensor.sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.split.html">torch.Tensor.split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_mask.html">torch.Tensor.sparse_mask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_dim.html">torch.Tensor.sparse_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sqrt.html">torch.Tensor.sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sqrt_.html">torch.Tensor.sqrt_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.square.html">torch.Tensor.square</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.square_.html">torch.Tensor.square_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.squeeze.html">torch.Tensor.squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.squeeze_.html">torch.Tensor.squeeze_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.std.html">torch.Tensor.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.stft.html">torch.Tensor.stft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.storage.html">torch.Tensor.storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.untyped_storage.html">torch.Tensor.untyped_storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.storage_offset.html">torch.Tensor.storage_offset</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.storage_type.html">torch.Tensor.storage_type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.stride.html">torch.Tensor.stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sub.html">torch.Tensor.sub</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sub_.html">torch.Tensor.sub_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.subtract.html">torch.Tensor.subtract</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.subtract_.html">torch.Tensor.subtract_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sum.html">torch.Tensor.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sum_to_size.html">torch.Tensor.sum_to_size</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.svd.html">torch.Tensor.svd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.swapaxes.html">torch.Tensor.swapaxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.swapdims.html">torch.Tensor.swapdims</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.t.html">torch.Tensor.t</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.t_.html">torch.Tensor.t_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tensor_split.html">torch.Tensor.tensor_split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tile.html">torch.Tensor.tile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to.html">torch.Tensor.to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_mkldnn.html">torch.Tensor.to_mkldnn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.take.html">torch.Tensor.take</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.take_along_dim.html">torch.Tensor.take_along_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tan.html">torch.Tensor.tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tan_.html">torch.Tensor.tan_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tanh.html">torch.Tensor.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tanh_.html">torch.Tensor.tanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atanh.html">torch.Tensor.atanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.atanh_.html">torch.Tensor.atanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctanh.html">torch.Tensor.arctanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.arctanh_.html">torch.Tensor.arctanh_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tolist.html">torch.Tensor.tolist</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.topk.html">torch.Tensor.topk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_dense.html">torch.Tensor.to_dense</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse.html">torch.Tensor.to_sparse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_csr.html">torch.Tensor.to_sparse_csr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_csc.html">torch.Tensor.to_sparse_csc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsr.html">torch.Tensor.to_sparse_bsr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsc.html">torch.Tensor.to_sparse_bsc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.trace.html">torch.Tensor.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.transpose.html">torch.Tensor.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.transpose_.html">torch.Tensor.transpose_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.triangular_solve.html">torch.Tensor.triangular_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tril.html">torch.Tensor.tril</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.tril_.html">torch.Tensor.tril_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.triu.html">torch.Tensor.triu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.triu_.html">torch.Tensor.triu_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.true_divide.html">torch.Tensor.true_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.true_divide_.html">torch.Tensor.true_divide_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.trunc.html">torch.Tensor.trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.trunc_.html">torch.Tensor.trunc_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.type.html">torch.Tensor.type</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.type_as.html">torch.Tensor.type_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unbind.html">torch.Tensor.unbind</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unflatten.html">torch.Tensor.unflatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unfold.html">torch.Tensor.unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.uniform_.html">torch.Tensor.uniform_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unique.html">torch.Tensor.unique</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unique_consecutive.html">torch.Tensor.unique_consecutive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unsqueeze.html">torch.Tensor.unsqueeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unsqueeze_.html">torch.Tensor.unsqueeze_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.values.html">torch.Tensor.values</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.var.html">torch.Tensor.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.vdot.html">torch.Tensor.vdot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.view.html">torch.Tensor.view</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.view_as.html">torch.Tensor.view_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.vsplit.html">torch.Tensor.vsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.where.html">torch.Tensor.where</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.xlogy.html">torch.Tensor.xlogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.xlogy_.html">torch.Tensor.xlogy_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.xpu.html">torch.Tensor.xpu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.zero_.html">torch.Tensor.zero_</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="autograd.html">torch.autograd</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.backward.html">torch.autograd.backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad.html">torch.autograd.grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.dual_level.html">dual_level</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.make_dual.html">torch.autograd.forward_ad.make_dual</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.unpack_dual.html">torch.autograd.forward_ad.unpack_dual</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.enter_dual_level.html">torch.autograd.forward_ad.enter_dual_level</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.exit_dual_level.html">torch.autograd.forward_ad.exit_dual_level</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.forward_ad.UnpackedDualTensor.html">UnpackedDualTensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.jacobian.html">torch.autograd.functional.jacobian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.hessian.html">torch.autograd.functional.hessian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.vjp.html">torch.autograd.functional.vjp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.jvp.html">torch.autograd.functional.jvp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.vhp.html">torch.autograd.functional.vhp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.functional.hvp.html">torch.autograd.functional.hvp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.forward.html">torch.autograd.Function.forward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.backward.html">torch.autograd.Function.backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.jvp.html">torch.autograd.Function.jvp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.Function.vmap.html">torch.autograd.Function.vmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.mark_dirty.html">torch.autograd.function.FunctionCtx.mark_dirty</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html">torch.autograd.function.FunctionCtx.mark_non_differentiable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.save_for_backward.html">torch.autograd.function.FunctionCtx.save_for_backward</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html">torch.autograd.function.FunctionCtx.set_materialize_grads</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.once_differentiable.html">torch.autograd.function.once_differentiable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.BackwardCFunction.html">BackwardCFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.InplaceFunction.html">InplaceFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.function.NestedIOFunction.html">NestedIOFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.gradcheck.gradcheck.html">torch.autograd.gradcheck.gradcheck</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.gradcheck.gradgradcheck.html">torch.autograd.gradcheck.gradgradcheck</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.gradcheck.GradcheckError.html">torch.autograd.gradcheck.GradcheckError</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.export_chrome_trace.html">torch.autograd.profiler.profile.export_chrome_trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.key_averages.html">torch.autograd.profiler.profile.key_averages</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.self_cpu_time_total.html">torch.autograd.profiler.profile.self_cpu_time_total</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.profile.total_average.html">torch.autograd.profiler.profile.total_average</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.parse_nvprof_trace.html">torch.autograd.profiler.parse_nvprof_trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.EnforceUnique.html">EnforceUnique</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.KinetoStepTracker.html">KinetoStepTracker</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.record_function.html">record_function</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.Interval.html">Interval</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.Kernel.html">Kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.MemRecordsAcc.html">MemRecordsAcc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler_util.StringTable.html">StringTable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.profiler.load_nvprof.html">torch.autograd.profiler.load_nvprof</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad_mode.set_multithreading_enabled.html">set_multithreading_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.name.html">torch.autograd.graph.Node.name</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.metadata.html">torch.autograd.graph.Node.metadata</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.next_functions.html">torch.autograd.graph.Node.next_functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.register_hook.html">torch.autograd.graph.Node.register_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.Node.register_prehook.html">torch.autograd.graph.Node.register_prehook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.graph.increment_version.html">torch.autograd.graph.increment_version</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="accelerator.html">torch.accelerator</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.device_count.html">torch.accelerator.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.is_available.html">torch.accelerator.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_accelerator.html">torch.accelerator.current_accelerator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.set_device_index.html">torch.accelerator.set_device_index</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.set_device_idx.html">torch.accelerator.set_device_idx</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_device_index.html">torch.accelerator.current_device_index</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_device_idx.html">torch.accelerator.current_device_idx</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.set_stream.html">torch.accelerator.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.current_stream.html">torch.accelerator.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.synchronize.html">torch.accelerator.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.device_index.html">device_index</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.memory.empty_cache.html">torch.accelerator.memory.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.memory.get_memory_info.html">torch.accelerator.memory.get_memory_info</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.memory.max_memory_allocated.html">torch.accelerator.memory.max_memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.memory.max_memory_reserved.html">torch.accelerator.memory.max_memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.memory.memory_allocated.html">torch.accelerator.memory.memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.memory.memory_reserved.html">torch.accelerator.memory.memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.memory.memory_stats.html">torch.accelerator.memory.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.memory.reset_accumulated_memory_stats.html">torch.accelerator.memory.reset_accumulated_memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.accelerator.memory.reset_peak_memory_stats.html">torch.accelerator.memory.reset_peak_memory_stats</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cpu.html">torch.cpu</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.current_device.html">torch.cpu.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.current_stream.html">torch.cpu.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.is_available.html">torch.cpu.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.is_initialized.html">torch.cpu.is_initialized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.synchronize.html">torch.cpu.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.stream_function.html">torch.cpu.stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.set_device.html">torch.cpu.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.device_count.html">torch.cpu.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.Stream_class.html">Stream</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cuda.html">torch.cuda</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.can_device_access_peer.html">torch.cuda.can_device_access_peer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.check_error.html">torch.cuda.check_error</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_blas_handle.html">torch.cuda.current_blas_handle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_device.html">torch.cuda.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_stream.html">torch.cuda.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.cudart.html">torch.cuda.cudart</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.default_stream.html">torch.cuda.default_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device.html">device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_count.html">torch.cuda.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_memory_used.html">torch.cuda.device_memory_used</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_of.html">device_of</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_arch_list.html">torch.cuda.get_arch_list</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_capability.html">torch.cuda.get_device_capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_name.html">torch.cuda.get_device_name</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_properties.html">torch.cuda.get_device_properties</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_gencode_flags.html">torch.cuda.get_gencode_flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_stream_from_external.html">torch.cuda.get_stream_from_external</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_sync_debug_mode.html">torch.cuda.get_sync_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.init.html">torch.cuda.init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.ipc_collect.html">torch.cuda.ipc_collect</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_available.html">torch.cuda.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_bf16_supported.html">torch.cuda.is_bf16_supported</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_initialized.html">torch.cuda.is_initialized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_tf32_supported.html">torch.cuda.is_tf32_supported</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_usage.html">torch.cuda.memory_usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_device.html">torch.cuda.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_stream.html">torch.cuda.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_sync_debug_mode.html">torch.cuda.set_sync_debug_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.stream_function.html">torch.cuda.stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.synchronize.html">torch.cuda.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.utilization.html">torch.cuda.utilization</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.temperature.html">torch.cuda.temperature</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.power_draw.html">torch.cuda.power_draw</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.clock_rate.html">torch.cuda.clock_rate</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.AcceleratorError.html">torch.cuda.AcceleratorError</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.OutOfMemoryError.html">torch.cuda.OutOfMemoryError</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_rng_state.html">torch.cuda.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_rng_state_all.html">torch.cuda.get_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_rng_state.html">torch.cuda.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_rng_state_all.html">torch.cuda.set_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.manual_seed.html">torch.cuda.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.manual_seed_all.html">torch.cuda.manual_seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.seed.html">torch.cuda.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.seed_all.html">torch.cuda.seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.initial_seed.html">torch.cuda.initial_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.broadcast.html">torch.cuda.comm.broadcast</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.broadcast_coalesced.html">torch.cuda.comm.broadcast_coalesced</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.reduce_add.html">torch.cuda.comm.reduce_add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.reduce_add_coalesced.html">torch.cuda.comm.reduce_add_coalesced</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.scatter.html">torch.cuda.comm.scatter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.comm.gather.html">torch.cuda.comm.gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.Stream_class.html">Stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.ExternalStream.html">ExternalStream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_current_stream_capturing.html">torch.cuda.is_current_stream_capturing</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.graph_pool_handle.html">torch.cuda.graph_pool_handle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.CUDAGraph.html">CUDAGraph</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.graph.html">graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.make_graphed_callables.html">torch.cuda.make_graphed_callables</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.empty_cache.html">torch.cuda.memory.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.get_per_process_memory_fraction.html">torch.cuda.memory.get_per_process_memory_fraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.list_gpu_processes.html">torch.cuda.memory.list_gpu_processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.mem_get_info.html">torch.cuda.memory.mem_get_info</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.memory_stats.html">torch.cuda.memory.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.memory_stats_as_nested_dict.html">torch.cuda.memory.memory_stats_as_nested_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.reset_accumulated_memory_stats.html">torch.cuda.memory.reset_accumulated_memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.host_memory_stats.html">torch.cuda.memory.host_memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.host_memory_stats_as_nested_dict.html">torch.cuda.memory.host_memory_stats_as_nested_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.reset_accumulated_host_memory_stats.html">torch.cuda.memory.reset_accumulated_host_memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.memory_summary.html">torch.cuda.memory.memory_summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.memory_snapshot.html">torch.cuda.memory.memory_snapshot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.memory_allocated.html">torch.cuda.memory.memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.max_memory_allocated.html">torch.cuda.memory.max_memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.reset_max_memory_allocated.html">torch.cuda.memory.reset_max_memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.memory_reserved.html">torch.cuda.memory.memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.max_memory_reserved.html">torch.cuda.memory.max_memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.set_per_process_memory_fraction.html">torch.cuda.memory.set_per_process_memory_fraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.memory_cached.html">torch.cuda.memory.memory_cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.max_memory_cached.html">torch.cuda.memory.max_memory_cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.reset_max_memory_cached.html">torch.cuda.memory.reset_max_memory_cached</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.reset_peak_memory_stats.html">torch.cuda.memory.reset_peak_memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.reset_peak_host_memory_stats.html">torch.cuda.memory.reset_peak_host_memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.caching_allocator_alloc.html">torch.cuda.memory.caching_allocator_alloc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.caching_allocator_delete.html">torch.cuda.memory.caching_allocator_delete</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.get_allocator_backend.html">torch.cuda.memory.get_allocator_backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.CUDAPluggableAllocator.html">CUDAPluggableAllocator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.change_current_allocator.html">torch.cuda.memory.change_current_allocator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.MemPool.html">MemPool</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory.caching_allocator_enable.html">torch.cuda.memory.caching_allocator_enable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.mark.html">torch.cuda.nvtx.mark</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.range_push.html">torch.cuda.nvtx.range_push</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.range_pop.html">torch.cuda.nvtx.range_pop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.nvtx.range.html">torch.cuda.nvtx.range</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.jiterator._create_jit_fn.html">torch.cuda.jiterator._create_jit_fn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.jiterator._create_multi_output_jit_fn.html">torch.cuda.jiterator._create_multi_output_jit_fn</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.tunable.html">TunableOp</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda._sanitizer.html">CUDA Stream Sanitizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.gds.gds_register_buffer.html">torch.cuda.gds.gds_register_buffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.gds.gds_deregister_buffer.html">torch.cuda.gds.gds_deregister_buffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.gds.GdsFile.html">GdsFile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.green_contexts.GreenContext.html">GreenContext</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="cuda.aliases.html">Aliases in torch.cuda</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.random.get_rng_state.html">torch.cuda.random.get_rng_state</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.random.get_rng_state_all.html">torch.cuda.random.get_rng_state_all</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.random.set_rng_state.html">torch.cuda.random.set_rng_state</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.random.set_rng_state_all.html">torch.cuda.random.set_rng_state_all</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.random.manual_seed.html">torch.cuda.random.manual_seed</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.random.manual_seed_all.html">torch.cuda.random.manual_seed_all</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.random.seed.html">torch.cuda.random.seed</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.random.seed_all.html">torch.cuda.random.seed_all</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.random.initial_seed.html">torch.cuda.random.initial_seed</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.graphs.is_current_stream_capturing.html">torch.cuda.graphs.is_current_stream_capturing</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.graphs.graph_pool_handle.html">torch.cuda.graphs.graph_pool_handle</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.graphs.CUDAGraph.html">CUDAGraph</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.graphs.graph.html">graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.graphs.make_graphed_callables.html">torch.cuda.graphs.make_graphed_callables</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.streams.Stream.html">Stream</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.streams.ExternalStream.html">ExternalStream</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.cuda.streams.Event.html">Event</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">torch.cuda.memory</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="mps.html">torch.mps</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.device_count.html">torch.mps.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.synchronize.html">torch.mps.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.get_rng_state.html">torch.mps.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.set_rng_state.html">torch.mps.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.manual_seed.html">torch.mps.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.seed.html">torch.mps.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.empty_cache.html">torch.mps.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.set_per_process_memory_fraction.html">torch.mps.set_per_process_memory_fraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.current_allocated_memory.html">torch.mps.current_allocated_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.driver_allocated_memory.html">torch.mps.driver_allocated_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.recommended_max_memory.html">torch.mps.recommended_max_memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.compile_shader.html">torch.mps.compile_shader</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.start.html">torch.mps.profiler.start</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.stop.html">torch.mps.profiler.stop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.profile.html">torch.mps.profiler.profile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.is_capturing_metal.html">torch.mps.profiler.is_capturing_metal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.is_metal_capture_enabled.html">torch.mps.profiler.is_metal_capture_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.profiler.metal_capture.html">torch.mps.profiler.metal_capture</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.event.Event.html">Event</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="xpu.html">torch.xpu</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.can_device_access_peer.html">torch.xpu.can_device_access_peer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.current_device.html">torch.xpu.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.current_stream.html">torch.xpu.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.device.html">device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.device_count.html">torch.xpu.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.device_of.html">device_of</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_arch_list.html">torch.xpu.get_arch_list</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_device_capability.html">torch.xpu.get_device_capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_device_name.html">torch.xpu.get_device_name</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_device_properties.html">torch.xpu.get_device_properties</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_gencode_flags.html">torch.xpu.get_gencode_flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_stream_from_external.html">torch.xpu.get_stream_from_external</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.init.html">torch.xpu.init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.is_available.html">torch.xpu.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.is_bf16_supported.html">torch.xpu.is_bf16_supported</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.is_initialized.html">torch.xpu.is_initialized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.is_tf32_supported.html">torch.xpu.is_tf32_supported</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_device.html">torch.xpu.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_stream.html">torch.xpu.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.stream_function.html">torch.xpu.stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.synchronize.html">torch.xpu.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_rng_state.html">torch.xpu.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.get_rng_state_all.html">torch.xpu.get_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.initial_seed.html">torch.xpu.initial_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.manual_seed.html">torch.xpu.manual_seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.manual_seed_all.html">torch.xpu.manual_seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.seed.html">torch.xpu.seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.seed_all.html">torch.xpu.seed_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_rng_state.html">torch.xpu.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.set_rng_state_all.html">torch.xpu.set_rng_state_all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.Stream_class.html">Stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory.XPUPluggableAllocator.html">XPUPluggableAllocator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory.change_current_allocator.html">torch.xpu.memory.change_current_allocator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory.empty_cache.html">torch.xpu.memory.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory.get_per_process_memory_fraction.html">torch.xpu.memory.get_per_process_memory_fraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory.max_memory_allocated.html">torch.xpu.memory.max_memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory.max_memory_reserved.html">torch.xpu.memory.max_memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory.mem_get_info.html">torch.xpu.memory.mem_get_info</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory.memory_allocated.html">torch.xpu.memory.memory_allocated</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory.memory_reserved.html">torch.xpu.memory.memory_reserved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory.memory_stats.html">torch.xpu.memory.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory.memory_stats_as_nested_dict.html">torch.xpu.memory.memory_stats_as_nested_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory.reset_accumulated_memory_stats.html">torch.xpu.memory.reset_accumulated_memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory.reset_peak_memory_stats.html">torch.xpu.memory.reset_peak_memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.xpu.memory.set_per_process_memory_fraction.html">torch.xpu.memory.set_per_process_memory_fraction</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="xpu.aliases.html">Aliases in torch.xpu</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.xpu.random.get_rng_state.html">torch.xpu.random.get_rng_state</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.xpu.random.get_rng_state_all.html">torch.xpu.random.get_rng_state_all</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.xpu.random.initial_seed.html">torch.xpu.random.initial_seed</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.xpu.random.manual_seed.html">torch.xpu.random.manual_seed</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.xpu.random.manual_seed_all.html">torch.xpu.random.manual_seed_all</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.xpu.random.seed.html">torch.xpu.random.seed</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.xpu.random.seed_all.html">torch.xpu.random.seed_all</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.xpu.random.set_rng_state.html">torch.xpu.random.set_rng_state</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.xpu.random.set_rng_state_all.html">torch.xpu.random.set_rng_state_all</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.xpu.streams.Event.html">Event</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.xpu.streams.Stream.html">Stream</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="mtia.html">torch.mtia</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.StreamContext.html">StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.current_device.html">torch.mtia.current_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.current_stream.html">torch.mtia.current_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.default_stream.html">torch.mtia.default_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.device_count.html">torch.mtia.device_count</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.init.html">torch.mtia.init</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.is_available.html">torch.mtia.is_available</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.is_bf16_supported.html">torch.mtia.is_bf16_supported</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.is_initialized.html">torch.mtia.is_initialized</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.memory_stats.html">torch.mtia.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.get_device_capability.html">torch.mtia.get_device_capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.empty_cache.html">torch.mtia.empty_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.record_memory_history.html">torch.mtia.record_memory_history</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.snapshot.html">torch.mtia.snapshot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.attach_out_of_memory_observer.html">torch.mtia.attach_out_of_memory_observer</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.set_device.html">torch.mtia.set_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.set_stream.html">torch.mtia.set_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.stream_function.html">torch.mtia.stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.synchronize.html">torch.mtia.synchronize</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.device.html">device</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.set_rng_state.html">torch.mtia.set_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.get_rng_state.html">torch.mtia.get_rng_state</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.DeferredMtiaCallError.html">torch.mtia.DeferredMtiaCallError</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.Event.html">Event</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.Stream_class.html">Stream</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="mtia.memory.html">torch.mtia.memory</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.memory.memory_stats.html">torch.mtia.memory.memory_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mtia.memory.memory_allocated.html">torch.mtia.memory.memory_allocated</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.mtia_graph.html">torch.mtia.mtia_graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="meta.html">Meta device</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="user_guide/torch_compiler/export.html">torch.export</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="user_guide/torch_compiler/export/api_reference.html">torch.export API Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="user_guide/torch_compiler/export/programming_model.html">torch.export Programming Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="user_guide/torch_compiler/export/ir_spec.html">torch.export IR Specification</a></li>
<li class="toctree-l2"><a class="reference internal" href="user_guide/torch_compiler/export/pt2_archive.html">PT2 Archive Spec</a></li>
<li class="toctree-l2"><a class="reference internal" href="user_guide/torch_compiler/export/draft_export.html">Draft Export</a></li>
<li class="toctree-l2"><a class="reference internal" href="user_guide/torch_compiler/export/joint_with_descriptors.html">Joint with descriptors</a></li>
<li class="toctree-l2"><a class="reference internal" href="cond.html">Control Flow - Cond</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="generated/exportdb/index.html">ExportDB</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.escape-hatch.html">torch.escape-hatch</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.cond.html">torch.cond</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.dynamic-shape.html">torch.dynamic-shape</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.closure.html">python.closure</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.dynamic-value.html">torch.dynamic-value</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.data-structure.html">python.data-structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.assert.html">python.assert</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.control-flow.html">python.control-flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.map.html">torch.map</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.builtin.html">python.builtin</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.object-model.html">python.object-model</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/python.context-manager.html">python.context-manager</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.operator.html">torch.operator</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/exportdb/torch.mutation.html">torch.mutation</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="user_guide/torch_compiler/torch.compiler_aot_inductor.html">AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="logging.html">torch._logging</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="generated/torch._logging.set_logs.html">torch._logging.set_logs</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/torch_compiler/torch.compiler_aot_inductor_minifier.html">AOTInductor Minifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/torch_compiler/torch.compiler_aot_inductor_debugging_guide.html">AOTInductor Debugging Guide</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="user_guide/torch_compiler/torch.compiler_ir.html">IRs</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="user_guide/torch_compiler/torch.compiler_dynamic_shapes.html">Dynamic Shapes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="user_guide/torch_compiler/compile/dynamic_shapes_core_concepts.html">Dynamic Shapes Core Concepts</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="user_guide/torch_compiler/compile/dynamic_shapes_troubleshooting.html">Troubleshooting Dynamic Shapes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="user_guide/torch_compiler/compile/dynamic_shapes_debugging_tlparse_torch_logs.html">Debugging with <code class="docutils literal notranslate"><span class="pre">tlparse</span></code> and <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=dynamic</span></code></a></li>

<li class="toctree-l4"><a class="reference internal" href="user_guide/torch_compiler/compile/dynamic_shapes_troubleshooting_guardon_errors.html">Troubleshooting GuardOnDataDependentSymNode Errors</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/torch_compiler/compile/dynamic_shapes_advanced_control_options.html">Advanced Options to Control Dynamic Behavior</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="user_guide/torch_compiler/compile/dynamic_shapes_beyond_the_basics.html">Beyond the Basics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="user_guide/torch_compiler/compile/dynamic_shapes_zero_one_specialization.html">The Zero-One Specialization Problem</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/torch_compiler/compile/dynamic_shapes_backed_unbacked.html">Backed vs Unbacked Symints</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="user_guide/torch_compiler/torch.compiler_fake_tensor.html">Fake tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="user_guide/torch_compiler/torch.compiler_transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="distributed.html">torch.distributed</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="distributed._dist2.html">Experimental Object Oriented Distributed API</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.html">torch.distributed.tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="elastic/quickstart.html">Quickstart</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/train_script.html">Train script</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/run.html">torchrun (Elastic Launch)</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/agent.html">Elastic Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/multiprocessing.html">Multiprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/errors.html">Error Propagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/rendezvous.html">Rendezvous</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/timer.html">Expiration Timers</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/metrics.html">Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/events.html">Events</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/subprocess_handler.html">Subprocess Handling</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/control_plane.html">Control Plane</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/numa.html">NUMA Binding Utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/customization.html">Customization</a></li>
<li class="toctree-l2"><a class="reference internal" href="elastic/kubernetes.html">TorchElastic Kubernetes</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.fsdp.fully_shard.html">torch.distributed.fsdp.fully_shard</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="symmetric_memory.html">torch.distributed._symmetric_memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="torch.compiler_api.html">torch.compiler</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.compile.html">torch.compiler.compile</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.reset.html">torch.compiler.reset</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.allow_in_graph.html">torch.compiler.allow_in_graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.substitute_in_graph.html">torch.compiler.substitute_in_graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.assume_constant_result.html">torch.compiler.assume_constant_result</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.list_backends.html">torch.compiler.list_backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.disable.html">torch.compiler.disable</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.set_stance.html">torch.compiler.set_stance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.set_enable_guard_collectives.html">torch.compiler.set_enable_guard_collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.cudagraph_mark_step_begin.html">torch.compiler.cudagraph_mark_step_begin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.is_compiling.html">torch.compiler.is_compiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.is_dynamo_compiling.html">torch.compiler.is_dynamo_compiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.is_exporting.html">torch.compiler.is_exporting</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.skip_guard_on_inbuilt_nn_modules_unsafe.html">torch.compiler.skip_guard_on_inbuilt_nn_modules_unsafe</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.skip_guard_on_all_nn_modules_unsafe.html">torch.compiler.skip_guard_on_all_nn_modules_unsafe</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.keep_tensor_guards_unsafe.html">torch.compiler.keep_tensor_guards_unsafe</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.skip_guard_on_globals_unsafe.html">torch.compiler.skip_guard_on_globals_unsafe</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.skip_all_guards_unsafe.html">torch.compiler.skip_all_guards_unsafe</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.compiler.nested_compile_region.html">torch.compiler.nested_compile_region</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="fft.html">torch.fft</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.fft.html">torch.fft.fft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ifft.html">torch.fft.ifft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.fft2.html">torch.fft.fft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ifft2.html">torch.fft.ifft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.fftn.html">torch.fft.fftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ifftn.html">torch.fft.ifftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.rfft.html">torch.fft.rfft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.irfft.html">torch.fft.irfft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.rfft2.html">torch.fft.rfft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.irfft2.html">torch.fft.irfft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.rfftn.html">torch.fft.rfftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.irfftn.html">torch.fft.irfftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.hfft.html">torch.fft.hfft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ihfft.html">torch.fft.ihfft</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.hfft2.html">torch.fft.hfft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ihfft2.html">torch.fft.ihfft2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.hfftn.html">torch.fft.hfftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ihfftn.html">torch.fft.ihfftn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.fftfreq.html">torch.fft.fftfreq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.rfftfreq.html">torch.fft.rfftfreq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.fftshift.html">torch.fft.fftshift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fft.ifftshift.html">torch.fft.ifftshift</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="func.html">torch.func</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="func.whirlwind_tour.html">torch.func Whirlwind Tour</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="func.api.html">torch.func API Reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.vmap.html">torch.func.vmap</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.grad.html">torch.func.grad</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.grad_and_value.html">torch.func.grad_and_value</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.vjp.html">torch.func.vjp</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.jvp.html">torch.func.jvp</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.linearize.html">torch.func.linearize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.jacrev.html">torch.func.jacrev</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.jacfwd.html">torch.func.jacfwd</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.hessian.html">torch.func.hessian</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.functionalize.html">torch.func.functionalize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.functional_call.html">torch.func.functional_call</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.stack_module_state.html">torch.func.stack_module_state</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.replace_all_batch_norm_modules_.html">torch.func.replace_all_batch_norm_modules_</a></li>
<li class="toctree-l3"><a class="reference internal" href="func.batch_norm.html">Patching Batch Norm</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.func.debug_unwrap.html">torch.func.debug_unwrap</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="func.ux_limitations.html">UX Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="func.migrating.html">Migrating from functorch to torch.func</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="fx.experimental.html">torch.fx.experimental</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.sym_node.is_channels_last_contiguous_2d.html">torch.fx.experimental.sym_node.is_channels_last_contiguous_2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.sym_node.is_channels_last_contiguous_3d.html">torch.fx.experimental.sym_node.is_channels_last_contiguous_3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.sym_node.is_channels_last_strides_2d.html">torch.fx.experimental.sym_node.is_channels_last_strides_2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.sym_node.is_channels_last_strides_3d.html">torch.fx.experimental.sym_node.is_channels_last_strides_3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.sym_node.is_contiguous.html">torch.fx.experimental.sym_node.is_contiguous</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.sym_node.is_non_overlapping_and_dense_indicator.html">torch.fx.experimental.sym_node.is_non_overlapping_and_dense_indicator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.sym_node.method_to_operator.html">torch.fx.experimental.sym_node.method_to_operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.sym_node.sympy_is_channels_last_contiguous_2d.html">torch.fx.experimental.sym_node.sympy_is_channels_last_contiguous_2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.sym_node.sympy_is_channels_last_contiguous_3d.html">torch.fx.experimental.sym_node.sympy_is_channels_last_contiguous_3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.sym_node.sympy_is_channels_last_strides_2d.html">torch.fx.experimental.sym_node.sympy_is_channels_last_strides_2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.sym_node.sympy_is_channels_last_strides_3d.html">torch.fx.experimental.sym_node.sympy_is_channels_last_strides_3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.sym_node.sympy_is_channels_last_strides_generic.html">torch.fx.experimental.sym_node.sympy_is_channels_last_strides_generic</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.sym_node.sympy_is_contiguous.html">torch.fx.experimental.sym_node.sympy_is_contiguous</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.sym_node.sympy_is_contiguous_generic.html">torch.fx.experimental.sym_node.sympy_is_contiguous_generic</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.sym_node.to_node.html">torch.fx.experimental.sym_node.to_node</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html">ShapeEnv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.DimDynamic.html">DimDynamic</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint.html">StrictMinMaxConstraint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.RelaxedUnspecConstraint.html">RelaxedUnspecConstraint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.EqualityConstraint.html">EqualityConstraint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.SymbolicContext.html">SymbolicContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.StatelessSymbolicContext.html">StatelessSymbolicContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.StatefulSymbolicContext.html">StatefulSymbolicContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.SubclassSymbolicContext.html">SubclassSymbolicContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html">DimConstraints</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnvSettings.html">ShapeEnvSettings</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.ConvertIntKey.html">ConvertIntKey</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.CallMethodKey.html">CallMethodKey</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.html">PropagateUnbackedSymInts</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.DivideByKey.html">DivideByKey</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.InnerTensorKey.html">InnerTensorKey</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.Specialization.html">Specialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.hint_int.html">torch.fx.experimental.symbolic_shapes.hint_int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.is_concrete_int.html">torch.fx.experimental.symbolic_shapes.is_concrete_int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.is_concrete_bool.html">torch.fx.experimental.symbolic_shapes.is_concrete_bool</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.is_concrete_float.html">torch.fx.experimental.symbolic_shapes.is_concrete_float</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.has_free_symbols.html">torch.fx.experimental.symbolic_shapes.has_free_symbols</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.has_free_unbacked_symbols.html">torch.fx.experimental.symbolic_shapes.has_free_unbacked_symbols</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.guard_or_true.html">torch.fx.experimental.symbolic_shapes.guard_or_true</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.guard_or_false.html">torch.fx.experimental.symbolic_shapes.guard_or_false</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.guard_size_oblivious.html">torch.fx.experimental.symbolic_shapes.guard_size_oblivious</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.sym_and.html">torch.fx.experimental.symbolic_shapes.sym_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.sym_eq.html">torch.fx.experimental.symbolic_shapes.sym_eq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.sym_or.html">torch.fx.experimental.symbolic_shapes.sym_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.constrain_range.html">torch.fx.experimental.symbolic_shapes.constrain_range</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.constrain_unify.html">torch.fx.experimental.symbolic_shapes.constrain_unify</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.canonicalize_bool_expr.html">torch.fx.experimental.symbolic_shapes.canonicalize_bool_expr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.statically_known_true.html">torch.fx.experimental.symbolic_shapes.statically_known_true</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.statically_known_false.html">torch.fx.experimental.symbolic_shapes.statically_known_false</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.has_static_value.html">torch.fx.experimental.symbolic_shapes.has_static_value</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.lru_cache.html">torch.fx.experimental.symbolic_shapes.lru_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.check_consistent.html">torch.fx.experimental.symbolic_shapes.check_consistent</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.compute_unbacked_bindings.html">torch.fx.experimental.symbolic_shapes.compute_unbacked_bindings</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.rebind_unbacked.html">torch.fx.experimental.symbolic_shapes.rebind_unbacked</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.resolve_unbacked_bindings.html">torch.fx.experimental.symbolic_shapes.resolve_unbacked_bindings</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.is_accessor_node.html">torch.fx.experimental.symbolic_shapes.is_accessor_node</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.cast_symbool_to_symint_guardless.html">torch.fx.experimental.symbolic_shapes.cast_symbool_to_symint_guardless</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.create_contiguous.html">torch.fx.experimental.symbolic_shapes.create_contiguous</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.error.html">torch.fx.experimental.symbolic_shapes.error</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.eval_guards.html">torch.fx.experimental.symbolic_shapes.eval_guards</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.eval_is_non_overlapping_and_dense.html">torch.fx.experimental.symbolic_shapes.eval_is_non_overlapping_and_dense</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.find_symbol_binding_fx_nodes.html">torch.fx.experimental.symbolic_shapes.find_symbol_binding_fx_nodes</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.free_symbols.html">torch.fx.experimental.symbolic_shapes.free_symbols</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.free_unbacked_symbols.html">torch.fx.experimental.symbolic_shapes.free_unbacked_symbols</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.fx_placeholder_targets.html">torch.fx.experimental.symbolic_shapes.fx_placeholder_targets</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.fx_placeholder_vals.html">torch.fx.experimental.symbolic_shapes.fx_placeholder_vals</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.guard_bool.html">torch.fx.experimental.symbolic_shapes.guard_bool</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.guard_float.html">torch.fx.experimental.symbolic_shapes.guard_float</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.guard_int.html">torch.fx.experimental.symbolic_shapes.guard_int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.guard_scalar.html">torch.fx.experimental.symbolic_shapes.guard_scalar</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.has_hint.html">torch.fx.experimental.symbolic_shapes.has_hint</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.has_symbolic_sizes_strides.html">torch.fx.experimental.symbolic_shapes.has_symbolic_sizes_strides</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.is_nested_int.html">torch.fx.experimental.symbolic_shapes.is_nested_int</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.is_symbol_binding_fx_node.html">torch.fx.experimental.symbolic_shapes.is_symbol_binding_fx_node</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.is_symbolic.html">torch.fx.experimental.symbolic_shapes.is_symbolic</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.expect_true.html">torch.fx.experimental.symbolic_shapes.expect_true</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.symbolic_shapes.log_lru_cache_stats.html">torch.fx.experimental.symbolic_shapes.log_lru_cache_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.make_fx.html">torch.fx.experimental.proxy_tensor.make_fx</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.handle_sym_dispatch.html">torch.fx.experimental.proxy_tensor.handle_sym_dispatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.get_proxy_mode.html">torch.fx.experimental.proxy_tensor.get_proxy_mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.maybe_enable_thunkify.html">torch.fx.experimental.proxy_tensor.maybe_enable_thunkify</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.maybe_disable_thunkify.html">torch.fx.experimental.proxy_tensor.maybe_disable_thunkify</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.thunkify.html">torch.fx.experimental.proxy_tensor.thunkify</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.track_tensor.html">torch.fx.experimental.proxy_tensor.track_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.track_tensor_tree.html">torch.fx.experimental.proxy_tensor.track_tensor_tree</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.decompose.html">torch.fx.experimental.proxy_tensor.decompose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.disable_autocast_cache.html">torch.fx.experimental.proxy_tensor.disable_autocast_cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.disable_proxy_modes_tracing.html">torch.fx.experimental.proxy_tensor.disable_proxy_modes_tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.extract_val.html">torch.fx.experimental.proxy_tensor.extract_val</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.fake_signature.html">torch.fx.experimental.proxy_tensor.fake_signature</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.fetch_object_proxy.html">torch.fx.experimental.proxy_tensor.fetch_object_proxy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.fetch_sym_proxy.html">torch.fx.experimental.proxy_tensor.fetch_sym_proxy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.has_proxy_slot.html">torch.fx.experimental.proxy_tensor.has_proxy_slot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.is_sym_node.html">torch.fx.experimental.proxy_tensor.is_sym_node</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.maybe_handle_decomp.html">torch.fx.experimental.proxy_tensor.maybe_handle_decomp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.proxy_call.html">torch.fx.experimental.proxy_tensor.proxy_call</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.set_meta.html">torch.fx.experimental.proxy_tensor.set_meta</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.set_original_aten_op.html">torch.fx.experimental.proxy_tensor.set_original_aten_op</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.set_proxy_slot.html">torch.fx.experimental.proxy_tensor.set_proxy_slot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.proxy_tensor.snapshot_fake.html">torch.fx.experimental.proxy_tensor.snapshot_fake</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.unification.unification_tools.assoc.html">torch.fx.experimental.unification.unification_tools.assoc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.unification.unification_tools.assoc_in.html">torch.fx.experimental.unification.unification_tools.assoc_in</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.unification.unification_tools.dissoc.html">torch.fx.experimental.unification.unification_tools.dissoc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.unification.unification_tools.first.html">torch.fx.experimental.unification.unification_tools.first</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.unification.unification_tools.keyfilter.html">torch.fx.experimental.unification.unification_tools.keyfilter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.unification.unification_tools.keymap.html">torch.fx.experimental.unification.unification_tools.keymap</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.unification.unification_tools.merge.html">torch.fx.experimental.unification.unification_tools.merge</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.unification.unification_tools.merge_with.html">torch.fx.experimental.unification.unification_tools.merge_with</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.unification.unification_tools.update_in.html">torch.fx.experimental.unification.unification_tools.update_in</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.unification.unification_tools.valfilter.html">torch.fx.experimental.unification.unification_tools.valfilter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.unification.unification_tools.valmap.html">torch.fx.experimental.unification.unification_tools.valmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.unification.unification_tools.itemfilter.html">torch.fx.experimental.unification.unification_tools.itemfilter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.unification.unification_tools.itemmap.html">torch.fx.experimental.unification.unification_tools.itemmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.migrate_gradual_types.transform_to_z3.transform_algebraic_expression.html">torch.fx.experimental.migrate_gradual_types.transform_to_z3.transform_algebraic_expression</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.migrate_gradual_types.transform_to_z3.transform_all_constraints.html">torch.fx.experimental.migrate_gradual_types.transform_to_z3.transform_all_constraints</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.migrate_gradual_types.transform_to_z3.transform_all_constraints_trace_time.html">torch.fx.experimental.migrate_gradual_types.transform_to_z3.transform_all_constraints_trace_time</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.migrate_gradual_types.transform_to_z3.transform_dimension.html">torch.fx.experimental.migrate_gradual_types.transform_to_z3.transform_dimension</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.migrate_gradual_types.transform_to_z3.transform_to_z3.html">torch.fx.experimental.migrate_gradual_types.transform_to_z3.transform_to_z3</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fx.experimental.migrate_gradual_types.transform_to_z3.transform_var.html">torch.fx.experimental.migrate_gradual_types.transform_to_z3.transform_var</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="jit.html">torch.jit</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="jit_language_reference.html">TorchScript Language Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_language_reference_v2.html">TorchScript Language Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_python_reference.html">Python Language Reference Coverage</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_unsupported.html">TorchScript Unsupported PyTorch Constructs</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_builtin_functions.html">torch.jit.supported_ops</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.script.html">torch.jit.script</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.trace.html">torch.jit.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.script_if_tracing.html">torch.jit.script_if_tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.trace_module.html">torch.jit.trace_module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.fork.html">torch.jit.fork</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.wait.html">torch.jit.wait</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.ScriptModule.html">ScriptModule</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.ScriptFunction.html">ScriptFunction</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.freeze.html">torch.jit.freeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.optimize_for_inference.html">torch.jit.optimize_for_inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.enable_onednn_fusion.html">torch.jit.enable_onednn_fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.onednn_fusion_enabled.html">torch.jit.onednn_fusion_enabled</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.set_fusion_strategy.html">torch.jit.set_fusion_strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.strict_fusion.html">strict_fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.save.html">torch.jit.save</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.load.html">torch.jit.load</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.ignore.html">torch.jit.ignore</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.unused.html">torch.jit.unused</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.interface.html">torch.jit.interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.isinstance.html">torch.jit.isinstance</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.Attribute.html">Attribute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.jit.annotate.html">torch.jit.annotate</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="linalg.html">torch.linalg</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.norm.html">torch.linalg.norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.vector_norm.html">torch.linalg.vector_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.matrix_norm.html">torch.linalg.matrix_norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.diagonal.html">torch.linalg.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.det.html">torch.linalg.det</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.slogdet.html">torch.linalg.slogdet</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.cond.html">torch.linalg.cond</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.matrix_rank.html">torch.linalg.matrix_rank</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.cholesky.html">torch.linalg.cholesky</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.qr.html">torch.linalg.qr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.lu.html">torch.linalg.lu</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.lu_factor.html">torch.linalg.lu_factor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.eig.html">torch.linalg.eig</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.eigvals.html">torch.linalg.eigvals</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.eigh.html">torch.linalg.eigh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.eigvalsh.html">torch.linalg.eigvalsh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.svd.html">torch.linalg.svd</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.svdvals.html">torch.linalg.svdvals</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.solve.html">torch.linalg.solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.solve_triangular.html">torch.linalg.solve_triangular</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.lu_solve.html">torch.linalg.lu_solve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.lstsq.html">torch.linalg.lstsq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.inv.html">torch.linalg.inv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.pinv.html">torch.linalg.pinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.matrix_exp.html">torch.linalg.matrix_exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.matrix_power.html">torch.linalg.matrix_power</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.cross.html">torch.linalg.cross</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.matmul.html">torch.linalg.matmul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.vecdot.html">torch.linalg.vecdot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.multi_dot.html">torch.linalg.multi_dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.householder_product.html">torch.linalg.householder_product</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.tensorinv.html">torch.linalg.tensorinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.tensorsolve.html">torch.linalg.tensorsolve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.vander.html">torch.linalg.vander</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.cholesky_ex.html">torch.linalg.cholesky_ex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.inv_ex.html">torch.linalg.inv_ex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.solve_ex.html">torch.linalg.solve_ex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.lu_factor_ex.html">torch.linalg.lu_factor_ex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.ldl_factor.html">torch.linalg.ldl_factor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.ldl_factor_ex.html">torch.linalg.ldl_factor_ex</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.linalg.ldl_solve.html">torch.linalg.ldl_solve</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="signal.html">torch.signal</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.bartlett.html">torch.signal.windows.bartlett</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.blackman.html">torch.signal.windows.blackman</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.cosine.html">torch.signal.windows.cosine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.exponential.html">torch.signal.windows.exponential</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.gaussian.html">torch.signal.windows.gaussian</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.general_cosine.html">torch.signal.windows.general_cosine</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.general_hamming.html">torch.signal.windows.general_hamming</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.hamming.html">torch.signal.windows.hamming</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.hann.html">torch.signal.windows.hann</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.kaiser.html">torch.signal.windows.kaiser</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signal.windows.nuttall.html">torch.signal.windows.nuttall</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="nativert.html">torch.nativert</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="nn.attention.html">torch.nn.attention</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.attention.sdpa_kernel.html">torch.nn.attention.sdpa_kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.attention.SDPBackend.html">SDPBackend</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.attention.register_flash_attention_impl.html">torch.nn.attention.register_flash_attention_impl</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.attention.activate_flash_attention_impl.html">torch.nn.attention.activate_flash_attention_impl</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.attention.list_flash_attention_impls.html">torch.nn.attention.list_flash_attention_impls</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.attention.current_flash_attention_impl.html">torch.nn.attention.current_flash_attention_impl</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.attention.flex_attention.html">torch.nn.attention.flex_attention</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="nn.attention.bias.html">torch.nn.attention.bias</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.nn.attention.bias.CausalBias.html">torch.nn.attention.bias.CausalBias</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.nn.attention.bias.causal_lower_right.html">torch.nn.attention.bias.causal_lower_right</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.nn.attention.bias.causal_upper_left.html">torch.nn.attention.bias.causal_upper_left</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.nn.attention.bias.CausalVariant.html">CausalVariant</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="nn.attention.experimental.html">torch.nn.attention.experimental</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.attention.varlen.html">torch.nn.attention.varlen</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="onnx.html">torch.onnx</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="onnx_export.html">torch.export-based ONNX Exporter</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_ops.html">torch.onnx.ops</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_verification.html">torch.onnx.verification</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx_testing.html">torch.onnx.testing</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="optim.html">torch.optim</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.add_param_group.html">torch.optim.Optimizer.add_param_group</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.load_state_dict.html">torch.optim.Optimizer.load_state_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_load_state_dict_pre_hook.html">torch.optim.Optimizer.register_load_state_dict_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_load_state_dict_post_hook.html">torch.optim.Optimizer.register_load_state_dict_post_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.state_dict.html">torch.optim.Optimizer.state_dict</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_state_dict_pre_hook.html">torch.optim.Optimizer.register_state_dict_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_state_dict_post_hook.html">torch.optim.Optimizer.register_state_dict_post_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.step.html">torch.optim.Optimizer.step</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_step_pre_hook.html">torch.optim.Optimizer.register_step_pre_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.register_step_post_hook.html">torch.optim.Optimizer.register_step_post_hook</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Optimizer.zero_grad.html">torch.optim.Optimizer.zero_grad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adadelta.html">Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adafactor.html">Adafactor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adagrad.html">Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adam.html">Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.AdamW.html">AdamW</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.SparseAdam.html">SparseAdam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Adamax.html">Adamax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.ASGD.html">ASGD</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.LBFGS.html">LBFGS</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Muon.html">Muon</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.NAdam.html">NAdam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.RAdam.html">RAdam</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.RMSprop.html">RMSprop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.Rprop.html">Rprop</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.SGD.html">SGD</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.LRScheduler.html">LRScheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.LambdaLR.html">LambdaLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.MultiplicativeLR.html">MultiplicativeLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.StepLR.html">StepLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.MultiStepLR.html">MultiStepLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ConstantLR.html">ConstantLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.LinearLR.html">LinearLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ExponentialLR.html">ExponentialLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.PolynomialLR.html">PolynomialLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.CosineAnnealingLR.html">CosineAnnealingLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ChainedScheduler.html">ChainedScheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.SequentialLR.html">SequentialLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html">ReduceLROnPlateau</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.CyclicLR.html">CyclicLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.OneCycleLR.html">OneCycleLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html">CosineAnnealingWarmRestarts</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.swa_utils.AveragedModel.html">AveragedModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.optim.swa_utils.SWALR.html">SWALR</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="optim.aliases.html">Aliases in torch.optim</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.adadelta.Adadelta_class.html">Adadelta</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.adadelta.adadelta_function.html">torch.optim.adadelta.adadelta</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.adagrad.Adagrad_class.html">Adagrad</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.adagrad.adagrad_function.html">torch.optim.adagrad.adagrad</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.adam.Adam_class.html">Adam</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.adam.adam_function.html">torch.optim.adam.adam</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.adamax.Adamax_class.html">Adamax</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.adamax.adamax_function.html">torch.optim.adamax.adamax</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.adamw.AdamW_class.html">AdamW</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.adamw.adamw_function.html">torch.optim.adamw.adamw</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.asgd.ASGD_class.html">ASGD</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.asgd.asgd_function.html">torch.optim.asgd.asgd</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.lbfgs.LBFGS.html">LBFGS</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.nadam.NAdam_class.html">NAdam</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.nadam.nadam_function.html">torch.optim.nadam.nadam</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.radam.RAdam_class.html">RAdam</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.radam.radam_function.html">torch.optim.radam.radam</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.rmsprop.RMSprop_class.html">RMSprop</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.rmsprop.rmsprop_function.html">torch.optim.rmsprop.rmsprop</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.rprop.Rprop_class.html">Rprop</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.rprop.rprop_function.html">torch.optim.rprop.rprop</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.sgd.SGD_class.html">SGD</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.sgd.sgd_function.html">torch.optim.sgd.sgd</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.optim.sparse_adam.SparseAdam.html">SparseAdam</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="quantization.html">Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="quantization-support.html">Quantization API Reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize.html">quantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_dynamic.html">quantize_dynamic</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_qat.html">quantize_qat</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.prepare.html">prepare</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.prepare_qat.html">prepare_qat</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.convert.html">convert</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fuse_modules.fuse_modules.html">fuse_modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.QuantStub.html">QuantStub</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.DeQuantStub.html">DeQuantStub</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.QuantWrapper.html">QuantWrapper</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.add_quant_dequant.html">add_quant_dequant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.swap_module.html">swap_module</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.propagate_qconfig_.html">propagate_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.default_eval_fn.html">default_eval_fn</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.utils.activation_is_dynamically_quantized.html">activation_is_dynamically_quantized</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.utils.activation_is_int32_quantized.html">activation_is_int32_quantized</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.utils.activation_is_int8_quantized.html">activation_is_int8_quantized</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.utils.activation_is_statically_quantized.html">activation_is_statically_quantized</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.utils.determine_qparams.html">determine_qparams</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.utils.check_min_max_valid.html">check_min_max_valid</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.utils.calculate_qmin_qmax.html">calculate_qmin_qmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.utils.validate_qmin_qmax.html">validate_qmin_qmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_fx.html">prepare_fx</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_qat_fx.html">prepare_qat_fx</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.convert_fx.html">convert_fx</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.fuse_fx.html">fuse_fx</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html">QConfigMapping</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping.html">get_default_qconfig_mapping</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping.html">get_default_qat_qconfig_mapping</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.BackendConfig.html">BackendConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html">BackendPatternConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeConfig.html">DTypeConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeWithConstraints.html">DTypeWithConstraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.ObservationType.html">ObservationType</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.utils.entry_to_pretty_str.html">entry_to_pretty_str</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.utils.pattern_to_human_readable.html">pattern_to_human_readable</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.backend_config.utils.remove_boolean_dispatch_from_name.html">remove_boolean_dispatch_from_name</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.FuseCustomConfig.html">FuseCustomConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html">PrepareCustomConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html">ConvertCustomConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry.html">StandaloneModuleConfigEntry</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.utils.all_node_args_except_first.html">all_node_args_except_first</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.utils.all_node_args_have_no_tensors.html">all_node_args_have_no_tensors</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.utils.collect_producer_nodes.html">collect_producer_nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.utils.create_getattr_from_value.html">create_getattr_from_value</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.utils.create_node_from_old_node_preserve_meta.html">create_node_from_old_node_preserve_meta</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.utils.graph_module_from_producer_nodes.html">graph_module_from_producer_nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.utils.maybe_get_next_module.html">maybe_get_next_module</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.utils.node_arg_is_bias.html">node_arg_is_bias</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.utils.node_arg_is_weight.html">node_arg_is_weight</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fx.utils.return_arg_list.html">return_arg_list</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.pt2e.export_utils.model_is_exported.html">model_is_exported</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.pt2e.lowering.lower_pt2e_quantized_to_x86.html">lower_pt2e_quantized_to_x86</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.generate_numeric_debug_handle.html">generate_numeric_debug_handle</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.CUSTOM_KEY.html">CUSTOM_KEY</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY.html">NUMERIC_DEBUG_HANDLE_KEY</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.prepare_for_propagation_comparison.html">prepare_for_propagation_comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.extract_results_from_loggers.html">extract_results_from_loggers</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.compare_results.html">compare_results</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.quantize_per_tensor.html">quantize_per_tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.quantize_per_channel.html">quantize_per_channel</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.dequantize.html">dequantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.view.html">torch.Tensor.view</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.as_strided.html">torch.Tensor.as_strided</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.expand.html">torch.Tensor.expand</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.flatten.html">torch.Tensor.flatten</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.select.html">torch.Tensor.select</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.ne.html">torch.Tensor.ne</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.eq.html">torch.Tensor.eq</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.ge.html">torch.Tensor.ge</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.le.html">torch.Tensor.le</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.gt.html">torch.Tensor.gt</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.lt.html">torch.Tensor.lt</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.copy_.html">torch.Tensor.copy_</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.clone.html">torch.Tensor.clone</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.dequantize.html">torch.Tensor.dequantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.equal.html">torch.Tensor.equal</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.int_repr.html">torch.Tensor.int_repr</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.max.html">torch.Tensor.max</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.mean.html">torch.Tensor.mean</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.min.html">torch.Tensor.min</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.q_scale.html">torch.Tensor.q_scale</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.q_zero_point.html">torch.Tensor.q_zero_point</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_scales.html">torch.Tensor.q_per_channel_scales</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_zero_points.html">torch.Tensor.q_per_channel_zero_points</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.q_per_channel_axis.html">torch.Tensor.q_per_channel_axis</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.resize_.html">torch.Tensor.resize_</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.sort.html">torch.Tensor.sort</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.Tensor.topk.html">torch.Tensor.topk</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.ObserverBase.html">ObserverBase</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.MinMaxObserver.html">MinMaxObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.MovingAverageMinMaxObserver.html">MovingAverageMinMaxObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html">PerChannelMinMaxObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver.html">MovingAveragePerChannelMinMaxObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.HistogramObserver.html">HistogramObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.PlaceholderObserver.html">PlaceholderObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.RecordingObserver.html">RecordingObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.NoopObserver.html">NoopObserver</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.get_observer_state_dict.html">get_observer_state_dict</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.load_observer_state_dict.html">load_observer_state_dict</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_observer.html">default_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_placeholder_observer.html">default_placeholder_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_debug_observer.html">default_debug_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_weight_observer.html">default_weight_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_histogram_observer.html">default_histogram_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_per_channel_weight_observer.html">default_per_channel_weight_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_dynamic_quant_observer.html">default_dynamic_quant_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.default_float_qparams_observer.html">default_float_qparams_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.AffineQuantizedObserverBase.html">AffineQuantizedObserverBase</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.Granularity.html">Granularity</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.MappingType.html">MappingType</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.PerAxis.html">PerAxis</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.PerBlock.html">PerBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.PerGroup.html">PerGroup</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.PerRow.html">PerRow</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.PerTensor.html">PerTensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.PerToken.html">PerToken</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.TorchAODType.html">TorchAODType</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.ZeroPointDomain.html">ZeroPointDomain</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.observer.get_block_size.html">get_block_size</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FakeQuantizeBase.html">FakeQuantizeBase</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FakeQuantize.html">FakeQuantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.html">FixedQParamsFakeQuantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize.html">FusedMovingAvgObsFakeQuantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fake_quant.html">default_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_weight_fake_quant.html">default_weight_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant.html">default_per_channel_weight_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_histogram_fake_quant.html">default_histogram_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_act_fake_quant.html">default_fused_act_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant.html">default_fused_wt_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant.html">default_fused_per_channel_wt_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.disable_fake_quant.html">disable_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.enable_fake_quant.html">enable_fake_quant</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.disable_observer.html">disable_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.enable_observer.html">enable_observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.QConfig.html">QConfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qconfig.html">default_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_debug_qconfig.html">default_debug_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_per_channel_qconfig.html">default_per_channel_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_dynamic_qconfig.html">default_dynamic_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float16_dynamic_qconfig.html">float16_dynamic_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float16_static_qconfig.html">float16_static_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.per_channel_dynamic_qconfig.html">per_channel_dynamic_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig.html">float_qparams_weight_only_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qat_qconfig.html">default_qat_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_weight_only_qconfig.html">default_weight_only_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_activation_only_qconfig.html">default_activation_only_qconfig</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qat_qconfig_v2.html">default_qat_qconfig_v2</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU1d.html">ConvReLU1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU2d.html">ConvReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU3d.html">ConvReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn1d.html">ConvBn1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn2d.html">ConvBn2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn3d.html">ConvBn3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU1d.html">ConvBnReLU1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU2d.html">ConvBnReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU3d.html">ConvBnReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.BNReLU2d.html">BNReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.BNReLU3d.html">BNReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn1d.html">ConvBn1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU1d.html">ConvBnReLU1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn2d.html">ConvBn2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU2d.html">ConvBnReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvReLU2d.html">ConvReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn3d.html">ConvBn3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU3d.html">ConvBnReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvReLU3d.html">ConvReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.update_bn_stats.html">update_bn_stats</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.freeze_bn_stats.html">freeze_bn_stats</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.BNReLU2d.html">BNReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.BNReLU3d.html">BNReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU1d.html">ConvReLU1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU2d.html">ConvReLU2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU3d.html">ConvReLU3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.qat.Conv2d.html">Conv2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.qat.Conv3d.html">Conv3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.qat.Linear.html">Linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.qat.dynamic.Linear.html">Linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.ReLU6.html">ReLU6</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Hardswish.html">Hardswish</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.ELU.html">ELU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.LeakyReLU.html">LeakyReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Sigmoid.html">Sigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.BatchNorm2d.html">BatchNorm2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.BatchNorm3d.html">BatchNorm3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv1d.html">Conv1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv2d.html">Conv2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv3d.html">Conv3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose1d.html">ConvTranspose1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose2d.html">ConvTranspose2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose3d.html">ConvTranspose3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Embedding.html">Embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.EmbeddingBag.html">EmbeddingBag</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.FloatFunctional.html">FloatFunctional</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.FXFloatFunctional.html">FXFloatFunctional</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.QFunctional.html">QFunctional</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.Linear.html">Linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.LayerNorm.html">LayerNorm</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.GroupNorm.html">GroupNorm</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm1d.html">InstanceNorm1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm2d.html">InstanceNorm2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm3d.html">InstanceNorm3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.avg_pool2d.html">avg_pool2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.avg_pool3d.html">avg_pool3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.adaptive_avg_pool2d.html">adaptive_avg_pool2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.adaptive_avg_pool3d.html">adaptive_avg_pool3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv1d.html">conv1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv2d.html">conv2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv3d.html">conv3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.interpolate.html">interpolate</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.linear.html">linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.max_pool1d.html">max_pool1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.max_pool2d.html">max_pool2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.celu.html">celu</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.leaky_relu.html">leaky_relu</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardtanh.html">hardtanh</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardswish.html">hardswish</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.threshold.html">threshold</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.elu.html">elu</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardsigmoid.html">hardsigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.clamp.html">clamp</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample.html">upsample</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample_bilinear.html">upsample_bilinear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample_nearest.html">upsample_nearest</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantizable.LSTM.html">LSTM</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantizable.MultiheadAttention.html">MultiheadAttention</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.Linear.html">Linear</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.LSTM.html">LSTM</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.GRU.html">GRU</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.RNNCell.html">RNNCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.LSTMCell.html">LSTMCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.GRUCell.html">GRUCell</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="quantization-support.aliases.html">Aliases in torch.ao</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvReLU1d.html">ConvReLU1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d.html">ConvReLU2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvReLU3d.html">ConvReLU3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU1d.html">ConvBnReLU1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU2d.html">ConvBnReLU2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvBnReLU3d.html">ConvBnReLU3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.modules.linear_fused.LinearBn1d.html">LinearBn1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.modules.linear_relu.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU1d.html">ConvReLU1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d.html">ConvReLU2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU3d.html">ConvReLU3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.modules.bn_relu.BNReLU2d.html">BNReLU2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.modules.bn_relu.BNReLU3d.html">BNReLU3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.modules.conv_add.ConvAdd2d.html">ConvAdd2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.modules.conv_add.ConvAddReLU2d.html">ConvAddReLU2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.modules.linear_relu.LinearLeakyReLU.html">LinearLeakyReLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.modules.linear_relu.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.modules.linear_relu.LinearTanh.html">LinearTanh</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu.LinearReLU.html">LinearReLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.modules.fused.ConvAdd2d.html">ConvAdd2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.modules.fused.ConvAddReLU2d.html">ConvAddReLU2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.modules.fused.LinearBn1d.html">LinearBn1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.modules.fused.LinearLeakyReLU.html">LinearLeakyReLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.intrinsic.modules.fused.LinearTanh.html">LinearTanh</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.qat.modules.conv.Conv1d.html">Conv1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.qat.modules.conv.Conv2d.html">Conv2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.qat.modules.conv.Conv3d.html">Conv3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.qat.modules.embedding_ops.Embedding.html">Embedding</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.qat.modules.embedding_ops.EmbeddingBag.html">EmbeddingBag</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.qat.modules.linear.Linear.html">Linear</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantizable.modules.activation.MultiheadAttention.html">MultiheadAttention</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantizable.modules.rnn.LSTM.html">LSTM</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantizable.modules.rnn.LSTMCell.html">LSTMCell</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.modules.conv.Conv1d.html">Conv1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.modules.conv.Conv2d.html">Conv2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.modules.conv.Conv3d.html">Conv3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.modules.conv.ConvTranspose1d.html">ConvTranspose1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.modules.conv.ConvTranspose2d.html">ConvTranspose2d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.modules.conv.ConvTranspose3d.html">ConvTranspose3d</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.modules.linear.Linear.html">Linear</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.modules.rnn.GRU.html">GRU</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.modules.rnn.GRUCell.html">GRUCell</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.modules.rnn.LSTM.html">LSTM</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.modules.rnn.LSTMCell.html">LSTMCell</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.modules.rnn.PackedParameter.html">PackedParameter</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.modules.rnn.RNNBase.html">RNNBase</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.modules.rnn.RNNCell.html">RNNCell</a></li>
<li class="toctree-l4"><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.modules.rnn.RNNCellBase.html">RNNCellBase</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="rpc/rref.html">Remote Reference Protocol</a></li>
<li class="toctree-l2"><a class="reference internal" href="rpc/distributed_autograd.html">Distributed Autograd Design</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="masked.html">torch.masked</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.abs.html">torch.abs</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.absolute.html">torch.absolute</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.acos.html">torch.acos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arccos.html">torch.arccos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.acosh.html">torch.acosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arccosh.html">torch.arccosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.angle.html">torch.angle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asin.html">torch.asin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arcsin.html">torch.arcsin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.asinh.html">torch.asinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arcsinh.html">torch.arcsinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atan.html">torch.atan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctan.html">torch.arctan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atanh.html">torch.atanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctanh.html">torch.arctanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_not.html">torch.bitwise_not</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ceil.html">torch.ceil</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clamp.html">torch.clamp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.clip.html">torch.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.conj_physical.html">torch.conj_physical</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cos.html">torch.cos</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cosh.html">torch.cosh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.deg2rad.html">torch.deg2rad</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.digamma.html">torch.digamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erf.html">torch.erf</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erfc.html">torch.erfc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.erfinv.html">torch.erfinv</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.exp.html">torch.exp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.exp2.html">torch.exp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.expm1.html">torch.expm1</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fix.html">torch.fix</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.floor.html">torch.floor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.frac.html">torch.frac</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lgamma.html">torch.lgamma</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log.html">torch.log</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log10.html">torch.log10</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log1p.html">torch.log1p</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.log2.html">torch.log2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logit.html">torch.logit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.i0.html">torch.i0</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isnan.html">torch.isnan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nan_to_num.html">torch.nan_to_num</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.neg.html">torch.neg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.negative.html">torch.negative</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.positive.html">torch.positive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.pow.html">torch.pow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rad2deg.html">torch.rad2deg</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.reciprocal.html">torch.reciprocal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.round.html">torch.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.rsqrt.html">torch.rsqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sigmoid.html">torch.sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sign.html">torch.sign</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sgn.html">torch.sgn</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signbit.html">torch.signbit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sin.html">torch.sin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sinc.html">torch.sinc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sinh.html">torch.sinh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sqrt.html">torch.sqrt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.square.html">torch.square</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tan.html">torch.tan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.tanh.html">torch.tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.trunc.html">torch.trunc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.angle.html">torch.angle</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.positive.html">torch.positive</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.signbit.html">torch.signbit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.isnan.html">torch.isnan</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.add.html">torch.add</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atan2.html">torch.atan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.arctan2.html">torch.arctan2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_and.html">torch.bitwise_and</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_or.html">torch.bitwise_or</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_xor.html">torch.bitwise_xor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_left_shift.html">torch.bitwise_left_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.bitwise_right_shift.html">torch.bitwise_right_shift</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.div.html">torch.div</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.divide.html">torch.divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.floor_divide.html">torch.floor_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmod.html">torch.fmod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp.html">torch.logaddexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp2.html">torch.logaddexp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mul.html">torch.mul</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.multiply.html">torch.multiply</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nextafter.html">torch.nextafter</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.remainder.html">torch.remainder</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sub.html">torch.sub</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.subtract.html">torch.subtract</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.true_divide.html">torch.true_divide</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.eq.html">torch.eq</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ne.html">torch.ne</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.le.html">torch.le</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ge.html">torch.ge</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.greater.html">torch.greater</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.greater_equal.html">torch.greater_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.gt.html">torch.gt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.less_equal.html">torch.less_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.lt.html">torch.lt</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.less.html">torch.less</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.maximum.html">torch.maximum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.minimum.html">torch.minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmax.html">torch.fmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmin.html">torch.fmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.not_equal.html">torch.not_equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp.html">torch.logaddexp</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.logaddexp2.html">torch.logaddexp2</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.equal.html">torch.equal</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmin.html">torch.fmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.minimum.html">torch.minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.fmax.html">torch.fmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sum.html">torch.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.mean.html">torch.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.amin.html">torch.amin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.amax.html">torch.amax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argmin.html">torch.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.argmax.html">torch.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.prod.html">torch.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.all.html">torch.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.norm.html">torch.norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.var.html">torch.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.std.html">torch.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.atleast_1d.html">torch.atleast_1d</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_tensors.html">torch.broadcast_tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.broadcast_to.html">torch.broadcast_to</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.cat.html">torch.cat</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.chunk.html">torch.chunk</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.column_stack.html">torch.column_stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.dsplit.html">torch.dsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.flatten.html">torch.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hsplit.html">torch.hsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hstack.html">torch.hstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.kron.html">torch.kron</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.meshgrid.html">torch.meshgrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.narrow.html">torch.narrow</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.functional.unfold.html">torch.nn.functional.unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.ravel.html">torch.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.select.html">torch.select</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.split.html">torch.split</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.stack.html">torch.stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.t.html">torch.t</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.transpose.html">torch.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vsplit.html">torch.vsplit</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.vstack.html">torch.vstack</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expand.html">torch.Tensor.expand</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.expand_as.html">torch.Tensor.expand_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reshape.html">torch.Tensor.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.reshape_as.html">torch.Tensor.reshape_as</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.unfold.html">torch.Tensor.unfold</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.view.html">torch.Tensor.view</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="size.html">torch.Size</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="sparse.html">torch.sparse</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_sparse.html">torch.Tensor.is_sparse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_sparse_csr.html">torch.Tensor.is_sparse_csr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.dense_dim.html">torch.Tensor.dense_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_dim.html">torch.Tensor.sparse_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_mask.html">torch.Tensor.sparse_mask</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse.html">torch.Tensor.to_sparse</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_coo.html">torch.Tensor.to_sparse_coo</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_csr.html">torch.Tensor.to_sparse_csr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_csc.html">torch.Tensor.to_sparse_csc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsr.html">torch.Tensor.to_sparse_bsr</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsc.html">torch.Tensor.to_sparse_bsc</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.to_dense.html">torch.Tensor.to_dense</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.values.html">torch.Tensor.values</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.coalesce.html">torch.Tensor.coalesce</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_resize_.html">torch.Tensor.sparse_resize_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.sparse_resize_and_clear_.html">torch.Tensor.sparse_resize_and_clear_</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.is_coalesced.html">torch.Tensor.is_coalesced</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.indices.html">torch.Tensor.indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.crow_indices.html">torch.Tensor.crow_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.col_indices.html">torch.Tensor.col_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.row_indices.html">torch.Tensor.row_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.Tensor.ccol_indices.html">torch.Tensor.ccol_indices</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_coo_tensor.html">torch.sparse_coo_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_csr_tensor.html">torch.sparse_csr_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_csc_tensor.html">torch.sparse_csc_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_bsr_tensor.html">torch.sparse_bsr_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_bsc_tensor.html">torch.sparse_bsc_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse_compressed_tensor.html">torch.sparse_compressed_tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.sum.html">torch.sparse.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.addmm.html">torch.sparse.addmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.sampled_addmm.html">torch.sparse.sampled_addmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.mm.html">torch.sparse.mm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sspaddmm.html">torch.sspaddmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.hspmm.html">torch.hspmm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.smm.html">torch.smm</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.softmax.html">torch.sparse.softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.spsolve.html">torch.sparse.spsolve</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.log_softmax.html">torch.sparse.log_softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.spdiags.html">torch.sparse.spdiags</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.check_sparse_tensor_invariants.html">check_sparse_tensor_invariants</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.sparse.as_sparse_gradcheck.html">torch.sparse.as_sparse_gradcheck</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="utils.html">torch.utils</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.rename_privateuse1_backend.html">torch.utils.rename_privateuse1_backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.generate_methods_for_privateuse1_backend.html">torch.utils.generate_methods_for_privateuse1_backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.get_cpp_backtrace.html">torch.utils.get_cpp_backtrace</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.set_module.html">torch.utils.set_module</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.swap_tensors.html">torch.utils.swap_tensors</a></li>
</ul>
</details></li>



<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="future_mod.html">torch.__future__</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="logging.html">torch._logging</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/torch._logging.set_logs.html">torch._logging.set_logs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="torch_environment_variables.html">Torch Environment Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="threading_environment_variables.html">Threading Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda_environment_variables.html">CUDA Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="mps_environment_variables.html">MPS Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="debugging_environment_variables.html">Debugging Environment Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="miscellaneous_environment_variables.html">Miscellaneous Environment Variables</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="logging.html">torch._logging</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="generated/torch._logging.set_logs.html">torch._logging.set_logs</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="torch_nccl_environment_variables.html">PYTORCH ProcessGroupNCCL Environment Variables</a></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="pytorch-api.html" class="nav-link">Reference API</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Pipeline Parallelism</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5"></span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="pytorch-api.html">
        <meta itemprop="name" content="Reference API">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Pipeline Parallelism">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="pipeline-parallelism">
<h1>Pipeline Parallelism<a class="headerlink" href="#pipeline-parallelism" title="Link to this heading">#</a></h1>
<p class="date-info-last-verified" style="color: #6c6c6d; font-size: small;">Created On: Jun 16, 2025 | Last Updated On: Aug 13, 2025</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.distributed.pipelining</span></code> is currently in alpha state and under
development. API changes may be possible. It was migrated from the <a class="reference external" href="https://github.com/pytorch/PiPPy">PiPPy</a> project.</p>
</div>
<section id="why-pipeline-parallel">
<h2>Why Pipeline Parallel?<a class="headerlink" href="#why-pipeline-parallel" title="Link to this heading">#</a></h2>
<p>Pipeline Parallelism is one of the <strong>primitive</strong> parallelism for deep learning.
It allows the <strong>execution</strong> of a model to be partitioned such that multiple
<strong>micro-batches</strong> can execute different parts of the model code concurrently.
Pipeline parallelism can be an effective technique for:</p>
<ul class="simple">
<li><p>large-scale training</p></li>
<li><p>bandwidth-limited clusters</p></li>
<li><p>large model inference</p></li>
</ul>
<p>The above scenarios share a commonality that the computation per device cannot
hide the communication of conventional parallelism, for example, the weight
all-gather of FSDP.</p>
</section>
<section id="what-is-torch-distributed-pipelining">
<h2>What is <code class="docutils literal notranslate"><span class="pre">torch.distributed.pipelining</span></code>?<a class="headerlink" href="#what-is-torch-distributed-pipelining" title="Link to this heading">#</a></h2>
<p>While promising for scaling, pipelining is often difficult to implement because
it needs to <strong>partition the execution</strong> of a model in addition to model weights.
The partitioning of execution often requires intrusive code changes to your
model. Another aspect of complexity comes from <strong>scheduling micro-batches in a
distributed environment</strong>, with <strong>data flow dependency</strong> considered.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">pipelining</span></code> package provides a toolkit that does said things
<strong>automatically</strong> which allows easy implementation of pipeline parallelism
on <strong>general</strong> models.</p>
<p>It consists of two parts: a
<strong>splitting frontend</strong> and a <strong>distributed runtime</strong>.
The splitting frontend takes your model code as-is, splits it up into model
partitions, and captures the data-flow relationship. The distributed runtime
executes the pipeline stages on different devices in parallel, handling things
like micro-batch splitting, scheduling, communication, and gradient propagation,
etc.</p>
<p>Overall, the <code class="docutils literal notranslate"><span class="pre">pipelining</span></code> package provides the following features:</p>
<ul class="simple">
<li><p>Splitting of model code based on simple specification.</p></li>
<li><p>Rich support for pipeline schedules, including GPipe, 1F1B,
Interleaved 1F1B and Looped BFS, and providing the infrastructure for writing
customized schedules.</p></li>
<li><p>First-class support for cross-host pipeline parallelism, as this is where PP
is typically used (over slower interconnects).</p></li>
<li><p>Composability with other PyTorch parallel techniques such as data parallel
(DDP, FSDP) or tensor parallel. The <a class="reference external" href="https://github.com/pytorch/torchtitan">TorchTitan</a> project demonstrates a 3D parallel
application on the Llama model.</p></li>
</ul>
</section>
<section id="step-1-build-pipelinestage">
<h2>Step 1: build <code class="docutils literal notranslate"><span class="pre">PipelineStage</span></code><a class="headerlink" href="#step-1-build-pipelinestage" title="Link to this heading">#</a></h2>
<p>Before we can use a <code class="docutils literal notranslate"><span class="pre">PipelineSchedule</span></code>, we need to create <code class="docutils literal notranslate"><span class="pre">PipelineStage</span></code>
objects that wrap the part of the model running in that stage. The
<code class="docutils literal notranslate"><span class="pre">PipelineStage</span></code> is responsible for allocating communication buffers and
creating send/recv ops to communicate with its peers. It manages intermediate
buffers e.g. for the outputs of forward that have not been consumed yet, and it
provides a utility for running the backwards for the stage model.</p>
<p>A <code class="docutils literal notranslate"><span class="pre">PipelineStage</span></code> needs to know the input and output shapes for the stage
model, so that it can correctly allocate communication buffers. The shapes must
be static, e.g. at runtime the shapes can not change from step to step. A class
<code class="docutils literal notranslate"><span class="pre">PipeliningShapeError</span></code> will be raised if runtime shapes do not match the
expected shapes. When composing with other paralleisms or applying mixed
precision, these techniques must be taken into account so the <code class="docutils literal notranslate"><span class="pre">PipelineStage</span></code>
knows the correct shape (and dtype) for the output of the stage module at
runtime.</p>
<p>Users may construct a <code class="docutils literal notranslate"><span class="pre">PipelineStage</span></code> instance directly, by passing in an
<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> representing the portion of the model that should run on the
stage. This may require changes to the original model code. See the example
in <a class="reference internal" href="#option-1-manual"><span class="std std-ref">Option 1: splitting a model manually</span></a>.</p>
<p>Alternatively, the splitting frontend can use graph partitioning to split your
model into a series of <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> automatically. This technique requires the
model is traceable with <code class="docutils literal notranslate"><span class="pre">torch.Export</span></code>. Composability of the resulting
<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> with other parallelism techniques is experimental, and may require
some workarounds. Usage of this frontend may be more appealing if the user
cannot easily change the model code. See <a class="reference internal" href="#option-2-tracer"><span class="std std-ref">Option 2: splitting a model automatically</span></a> for more
information.</p>
</section>
<section id="step-2-use-pipelineschedule-for-execution">
<h2>Step 2: use <code class="docutils literal notranslate"><span class="pre">PipelineSchedule</span></code> for execution<a class="headerlink" href="#step-2-use-pipelineschedule-for-execution" title="Link to this heading">#</a></h2>
<p>We can now attach the <code class="docutils literal notranslate"><span class="pre">PipelineStage</span></code> to a pipeline schedule, and run the
schedule with input data. Here is a GPipe example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.distributed.pipelining</span> <span class="kn">import</span> <span class="n">ScheduleGPipe</span>

<span class="c1"># Create a schedule</span>
<span class="n">schedule</span> <span class="o">=</span> <span class="n">ScheduleGPipe</span><span class="p">(</span><span class="n">stage</span><span class="p">,</span> <span class="n">n_microbatches</span><span class="p">)</span>

<span class="c1"># Input data (whole batch)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Run the pipeline with input `x`</span>
<span class="c1"># `x` will be divided into microbatches automatically</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">schedule</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">schedule</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>Note that the above code needs to be launched for each worker, thus we use a
launcher service to launch multiple processes:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>torchrun<span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="m">2</span><span class="w"> </span>example.py
</pre></div>
</div>
</section>
<section id="options-for-splitting-a-model">
<h2>Options for Splitting a Model<a class="headerlink" href="#options-for-splitting-a-model" title="Link to this heading">#</a></h2>
<section id="option-1-splitting-a-model-manually">
<span id="option-1-manual"></span><h3>Option 1: splitting a model manually<a class="headerlink" href="#option-1-splitting-a-model-manually" title="Link to this heading">#</a></h3>
<p>To directly construct a <code class="docutils literal notranslate"><span class="pre">PipelineStage</span></code>, the user is responsible for providing
a single <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> instance that owns the relevant <code class="docutils literal notranslate"><span class="pre">nn.Parameters</span></code> and
<code class="docutils literal notranslate"><span class="pre">nn.Buffers</span></code>, and defines a <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method that executes the operations
relevant for that stage. For example, a condensed version of the Transformer
class defined in Torchtitan shows a pattern of building an easily partitionable
model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_args</span><span class="p">:</span> <span class="n">ModelArgs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tok_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

        <span class="c1"># Using a ModuleDict lets us delete layers without affecting names,</span>
        <span class="c1"># ensuring checkpoints will correctly save and load.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">layer_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_args</span><span class="o">.</span><span class="n">n_layers</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">layer_id</span><span class="p">)]</span> <span class="o">=</span> <span class="n">TransformerBlock</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="c1"># Handling layers being &#39;None&#39; at runtime enables easy pipeline splitting</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tok_embeddings</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tok_embeddings</span> <span class="k">else</span> <span class="n">tokens</span>

        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">freqs_cis</span><span class="p">)</span>

        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="k">else</span> <span class="n">h</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="k">else</span> <span class="n">h</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>A model defined in this manner can be easily configured per stage by first
initializing the whole model (using meta-device to avoid OOM errors), deleting
undesired layers for that stage, and then creating a PipelineStage that wraps
the model. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;meta&quot;</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">num_stages</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;This is a simple 2-stage example&quot;</span>

    <span class="c1"># we construct the entire model, then delete the parts we do not need for this stage</span>
    <span class="c1"># in practice, this can be done using a helper function that automatically divides up layers across stages.</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">stage_index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># prepare the first stage model</span>
        <span class="k">del</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">]</span>
        <span class="n">model</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">model</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">elif</span> <span class="n">stage_index</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># prepare the second stage model</span>
        <span class="n">model</span><span class="o">.</span><span class="n">tok_embeddings</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">del</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="s2">&quot;0&quot;</span><span class="p">]</span>

    <span class="kn">from</span> <span class="nn">torch.distributed.pipelining</span> <span class="kn">import</span> <span class="n">PipelineStage</span>
    <span class="n">stage</span> <span class="o">=</span> <span class="n">PipelineStage</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">stage_index</span><span class="p">,</span>
        <span class="n">num_stages</span><span class="p">,</span>
        <span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>When composing with other Data or Model parallelism techniques, <code class="docutils literal notranslate"><span class="pre">output_args</span></code>
may also be required, if the output shape/dtype of the model chunk will be
affected.</p>
</section>
<section id="option-2-splitting-a-model-automatically">
<span id="option-2-tracer"></span><h3>Option 2: splitting a model automatically<a class="headerlink" href="#option-2-splitting-a-model-automatically" title="Link to this heading">#</a></h3>
<p>If you have a full model and do not want to spend time on modifying it into a
sequence of model partitions, the <code class="docutils literal notranslate"><span class="pre">pipeline</span></code> API is here to help.
Here is a brief example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="n">Layer</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm</span> <span class="o">=</span> <span class="n">LMHead</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>If we print the model, we can see multiple hierarchies, which makes it hard to split by hand:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Model</span><span class="p">(</span>
  <span class="p">(</span><span class="n">emb</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
  <span class="p">(</span><span class="n">layers</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span> <span class="mi">2</span> <span class="n">x</span> <span class="n">Layer</span><span class="p">(</span>
      <span class="p">(</span><span class="n">lin</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">lm</span><span class="p">):</span> <span class="n">LMHead</span><span class="p">(</span>
    <span class="p">(</span><span class="n">proj</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Let us see how the <code class="docutils literal notranslate"><span class="pre">pipeline</span></code> API works:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.distributed.pipelining</span> <span class="kn">import</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">SplitPoint</span>

<span class="c1"># An example micro-batch input</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
    <span class="n">module</span><span class="o">=</span><span class="n">mod</span><span class="p">,</span>
    <span class="n">mb_args</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,),</span>
    <span class="n">split_spec</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;layers.1&quot;</span><span class="p">:</span> <span class="n">SplitPoint</span><span class="o">.</span><span class="n">BEGINNING</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">pipeline</span></code> API splits your model given a <code class="docutils literal notranslate"><span class="pre">split_spec</span></code>, where
<code class="docutils literal notranslate"><span class="pre">SplitPoint.BEGINNING</span></code> stands for adding a split point
<em>before</em> execution of certain submodule in the <code class="docutils literal notranslate"><span class="pre">forward</span></code> function, and
similarly, <code class="docutils literal notranslate"><span class="pre">SplitPoint.END</span></code> for split point <em>after</em> such.</p>
<p>If we <code class="docutils literal notranslate"><span class="pre">print(pipe)</span></code>, we can see:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">GraphModule</span><span class="p">(</span>
  <span class="p">(</span><span class="n">submod_0</span><span class="p">):</span> <span class="n">GraphModule</span><span class="p">(</span>
    <span class="p">(</span><span class="n">emb</span><span class="p">):</span> <span class="n">InterpreterModule</span><span class="p">()</span>
    <span class="p">(</span><span class="n">layers</span><span class="p">):</span> <span class="n">Module</span><span class="p">(</span>
      <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">InterpreterModule</span><span class="p">(</span>
        <span class="p">(</span><span class="n">lin</span><span class="p">):</span> <span class="n">InterpreterModule</span><span class="p">()</span>
      <span class="p">)</span>
    <span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">submod_1</span><span class="p">):</span> <span class="n">GraphModule</span><span class="p">(</span>
    <span class="p">(</span><span class="n">layers</span><span class="p">):</span> <span class="n">Module</span><span class="p">(</span>
      <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">InterpreterModule</span><span class="p">(</span>
        <span class="p">(</span><span class="n">lin</span><span class="p">):</span> <span class="n">InterpreterModule</span><span class="p">()</span>
      <span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="n">lm</span><span class="p">):</span> <span class="n">InterpreterModule</span><span class="p">(</span>
      <span class="p">(</span><span class="n">proj</span><span class="p">):</span> <span class="n">InterpreterModule</span><span class="p">()</span>
    <span class="p">)</span>
  <span class="p">)</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">submod_0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">submod_0</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>  <span class="n">x</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">submod_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">submod_1</span><span class="p">(</span><span class="n">submod_0</span><span class="p">);</span>  <span class="n">submod_0</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">submod_1</span><span class="p">,)</span>
</pre></div>
</div>
<p>The model partitions are represented by submodules (<code class="docutils literal notranslate"><span class="pre">submod_0</span></code>,
<code class="docutils literal notranslate"><span class="pre">submod_1</span></code>), each of which is reconstructed with original model operations, weights
and hierarchies. In addition, a root-level <code class="docutils literal notranslate"><span class="pre">forward</span></code> function is
reconstructed to capture the data flow between those partitions. Such data flow
will be replayed by the pipeline runtime later, in a distributed fashion.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Pipe</span></code> object provides a method for retrieving the model partitions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">stage_mod</span> <span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">pipe</span><span class="o">.</span><span class="n">get_stage_module</span><span class="p">(</span><span class="n">stage_idx</span><span class="p">)</span>
</pre></div>
</div>
<p>The returned <code class="docutils literal notranslate"><span class="pre">stage_mod</span></code> is a <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>, with which you can create an
optimizer, save or load checkpoints, or apply other parallelisms.</p>
<p><code class="docutils literal notranslate"><span class="pre">Pipe</span></code> also allows you to create a distributed stage runtime on a device given
a <code class="docutils literal notranslate"><span class="pre">ProcessGroup</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">stage</span> <span class="o">=</span> <span class="n">pipe</span><span class="o">.</span><span class="n">build_stage</span><span class="p">(</span><span class="n">stage_idx</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span>
</pre></div>
</div>
<p>Alternatively, if you would like to build the stage runtime later after some
modification to the <code class="docutils literal notranslate"><span class="pre">stage_mod</span></code>, you can use a functional version of the
<code class="docutils literal notranslate"><span class="pre">build_stage</span></code> API. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.distributed.pipelining</span> <span class="kn">import</span> <span class="n">build_stage</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span>

<span class="n">dp_mod</span> <span class="o">=</span> <span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">stage_mod</span><span class="p">)</span>
<span class="n">info</span> <span class="o">=</span> <span class="n">pipe</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
<span class="n">stage</span> <span class="o">=</span> <span class="n">build_stage</span><span class="p">(</span><span class="n">dp_mod</span><span class="p">,</span> <span class="n">stage_idx</span><span class="p">,</span> <span class="n">info</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">pipeline</span></code> frontend uses a tracer (<code class="docutils literal notranslate"><span class="pre">torch.export</span></code>) to capture your
model into a single graph. If your model is not full-graphable, you can use
our manual frontend below.</p>
</div>
</section>
</section>
<section id="hugging-face-examples">
<h2>Hugging Face Examples<a class="headerlink" href="#hugging-face-examples" title="Link to this heading">#</a></h2>
<p>In the <a class="reference external" href="https://github.com/pytorch/PiPPy">PiPPy</a> repo where this package was
original created, we kept examples based on unmodified Hugging Face models.
See the <a class="reference external" href="https://github.com/pytorch/PiPPy/tree/main/examples/huggingface">examples/huggingface</a> directory.</p>
<p>Examples include:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/PiPPy/tree/main/examples/huggingface/pippy_gpt2.py">GPT2</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/PiPPy/tree/main/examples/llama">Llama</a></p></li>
</ul>
</section>
<section id="technical-deep-dive">
<h2>Technical Deep Dive<a class="headerlink" href="#technical-deep-dive" title="Link to this heading">#</a></h2>
<section id="how-does-the-pipeline-api-split-a-model">
<h3>How does the <code class="docutils literal notranslate"><span class="pre">pipeline</span></code> API split a model?<a class="headerlink" href="#how-does-the-pipeline-api-split-a-model" title="Link to this heading">#</a></h3>
<p>First, the <code class="docutils literal notranslate"><span class="pre">pipeline</span></code> API turns our model into a directed acyclic graph (DAG)
by tracing the model. It traces the model using <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>  a PyTorch 2
full-graph capturing tool.</p>
<p>Then, it groups together the <strong>operations and parameters</strong> needed by a stage
into a reconstructed submodule: <code class="docutils literal notranslate"><span class="pre">submod_0</span></code>, <code class="docutils literal notranslate"><span class="pre">submod_1</span></code>, </p>
<p>Different from conventional submodule access methods like <code class="docutils literal notranslate"><span class="pre">Module.children()</span></code>,
the <code class="docutils literal notranslate"><span class="pre">pipeline</span></code> API does not only cut the module structure of your model, but
also the <strong>forward</strong> function of your model.</p>
<p>This is necessary because model structure like <code class="docutils literal notranslate"><span class="pre">Module.children()</span></code> merely
captures information during <code class="docutils literal notranslate"><span class="pre">Module.__init__()</span></code>, and does not capture any
information about <code class="docutils literal notranslate"><span class="pre">Module.forward()</span></code>. Said differently, <code class="docutils literal notranslate"><span class="pre">Module.children()</span></code>
lacks information about the following aspects key to pipelininig:</p>
<ul class="simple">
<li><p>Execution order of child modules in <code class="docutils literal notranslate"><span class="pre">forward</span></code></p></li>
<li><p>Activation flows between child modules</p></li>
<li><p>Whether there are any functional operators between child modules (for example,
<code class="docutils literal notranslate"><span class="pre">relu</span></code> or <code class="docutils literal notranslate"><span class="pre">add</span></code> operations will not be captured by <code class="docutils literal notranslate"><span class="pre">Module.children()</span></code>).</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">pipeline</span></code> API, on the contrary, makes sure that the <code class="docutils literal notranslate"><span class="pre">forward</span></code> behavior
is truly preserved. It also captures the activation flow between the partitions,
helping the distributed runtime to make correct send/receive calls without human
intervention.</p>
<p>Another flexibility of the <code class="docutils literal notranslate"><span class="pre">pipeline</span></code> API is that split points can be at
arbitrary levels within your model hierarchy. In the split partitions, the original model
hierarchy related to that partition will be reconstructed at no cost to you.
At a result, fully-qualified names (FQNs) pointing to a submodule or parameter
would be still valid, and services that relies on FQNs (such as FSDP, TP or
checkpointing) can still run with your partitioned modules with almost zero code
change.</p>
</section>
</section>
<section id="implementing-your-own-schedule">
<h2>Implementing Your Own Schedule<a class="headerlink" href="#implementing-your-own-schedule" title="Link to this heading">#</a></h2>
<p>You can implement your own pipeline schedule by extending one of the following two class:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">PipelineScheduleSingle</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PipelineScheduleMulti</span></code></p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">PipelineScheduleSingle</span></code> is for schedules that assigns <em>only one</em> stage per rank.
<code class="docutils literal notranslate"><span class="pre">PipelineScheduleMulti</span></code> is for schedules that assigns multiple stages per rank.</p>
<p>For example, <code class="docutils literal notranslate"><span class="pre">ScheduleGPipe</span></code> and <code class="docutils literal notranslate"><span class="pre">Schedule1F1B</span></code> are subclasses of <code class="docutils literal notranslate"><span class="pre">PipelineScheduleSingle</span></code>.
Whereas, <code class="docutils literal notranslate"><span class="pre">ScheduleInterleaved1F1B</span></code>, <code class="docutils literal notranslate"><span class="pre">ScheduleLoopedBFS</span></code>, <code class="docutils literal notranslate"><span class="pre">ScheduleInterleavedZeroBubble</span></code>, and <code class="docutils literal notranslate"><span class="pre">ScheduleZBVZeroBubble</span></code>
are subclasses of <code class="docutils literal notranslate"><span class="pre">PipelineScheduleMulti</span></code>.</p>
</section>
<section id="logging">
<h2>Logging<a class="headerlink" href="#logging" title="Link to this heading">#</a></h2>
<p>You can turn on additional logging using the <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS</span></code> environment variable from <a class="reference external" href="https://pytorch.org/docs/main/logging.html#module-torch._logging">torch._logging</a>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=+pp</span></code> will display <code class="docutils literal notranslate"><span class="pre">logging.DEBUG</span></code> messages and all levels above it.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=pp</span></code> will display <code class="docutils literal notranslate"><span class="pre">logging.INFO</span></code> messages and above.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=-pp</span></code> will display <code class="docutils literal notranslate"><span class="pre">logging.WARNING</span></code> messages and above.</p></li>
</ul>
</section>
<section id="module-torch.distributed.pipelining">
<span id="api-reference"></span><h2>API Reference<a class="headerlink" href="#module-torch.distributed.pipelining" title="Link to this heading">#</a></h2>
<section id="model-split-apis">
<h3>Model Split APIs<a class="headerlink" href="#model-split-apis" title="Link to this heading">#</a></h3>
<p>The following set of APIs transform your model into a pipeline representation.</p>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.pipelining.SplitPoint">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.</span></span><span class="sig-name descname"><span class="pre">SplitPoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/_IR.py#L1153"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.SplitPoint" title="Link to this definition">#</a></dt>
<dd><p>Enum representing the points at which a split can occur in the execution of a submodule.
:ivar BEGINNING: Represents adding a split point <em>before</em> the execution of a certain submodule in the <cite>forward</cite> function.
:ivar END: Represents adding a split point <em>after</em> the execution of a certain submodule in the <cite>forward</cite> function.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.pipelining.pipeline">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.</span></span><span class="sig-name descname"><span class="pre">pipeline</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mb_args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mb_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_policy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/_IR.py#L1208"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.pipeline" title="Link to this definition">#</a></dt>
<dd><p>Split a module based on a specification.</p>
<p>See <cite>Pipe</cite> for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.modules.module.Module"><em>Module</em></a>)  The module to be split.</p></li>
<li><p><strong>mb_args</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><em>Any</em></a><em>, </em><em>...</em><em>]</em>)  Example positional inputs, in micro-batch form.</p></li>
<li><p><strong>mb_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><em>Any</em></a><em>] </em><em>| </em><em>None</em>)  Example keyword inputs, in micro-batch form. (default: <cite>None</cite>)</p></li>
<li><p><strong>split_spec</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><a class="reference internal" href="#torch.distributed.pipelining.SplitPoint" title="torch.distributed.pipelining._IR.SplitPoint"><em>SplitPoint</em></a><em>] </em><em>| </em><em>None</em>)  A dictionary using submodule names as split marker. (default: <cite>None</cite>)</p></li>
<li><p><strong>split_policy</strong> (<a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable" title="(in Python v3.14)"><em>Callable</em></a><em>[</em><em>[</em><a class="reference internal" href="fx.html#torch.fx.GraphModule" title="torch.fx.graph_module.GraphModule"><em>GraphModule</em></a><em>]</em><em>, </em><a class="reference internal" href="fx.html#torch.fx.GraphModule" title="torch.fx.graph_module.GraphModule"><em>GraphModule</em></a><em>] </em><em>| </em><em>None</em>)  The policy to use for splitting the module. (default: <cite>None</cite>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>A pipeline representation of class <cite>Pipe</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.pipelining.Pipe">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.</span></span><span class="sig-name descname"><span class="pre">Pipe</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">split_gm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_stages</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_loss_and_backward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_spec</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/_IR.py#L536"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.Pipe" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.pipelining.pipe_split">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.</span></span><span class="sig-name descname"><span class="pre">pipe_split</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/_IR.py#L338"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.pipe_split" title="Link to this definition">#</a></dt>
<dd><p>pipe_split is a special operator that is used to mark the boundary between
stages in a module. It is used to split the module into stages. It is a
no-op if your annotated module is run eagerly.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mm_param</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">pipe_split</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>The above example will be split into two stages.</p>
</dd></dl>

</section>
<section id="module-torch.distributed.pipelining.microbatch">
<span id="microbatch-utilities"></span><h3>Microbatch Utilities<a class="headerlink" href="#module-torch.distributed.pipelining.microbatch" title="Link to this heading">#</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.pipelining.microbatch.TensorChunkSpec">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.microbatch.</span></span><span class="sig-name descname"><span class="pre">TensorChunkSpec</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">split_dim</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/microbatch.py#L59"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.microbatch.TensorChunkSpec" title="Link to this definition">#</a></dt>
<dd><p>Class used to specify chunking of inputs</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.pipelining.microbatch.split_args_kwargs_into_chunks">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.microbatch.</span></span><span class="sig-name descname"><span class="pre">split_args_kwargs_into_chunks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chunks</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/microbatch.py#L308"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.microbatch.split_args_kwargs_into_chunks" title="Link to this definition">#</a></dt>
<dd><p>Given a sequence of args and kwargs, split them into a number of chunks
according to  their respective chunking specs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><em>Any</em></a><em>, </em><em>...</em><em>]</em>)  Tuple of args</p></li>
<li><p><strong>kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><em>Any</em></a><em>] </em><em>| </em><em>None</em>)  Dict of kwargs</p></li>
<li><p><strong>chunks</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a>)  Number of chunks to split the args and kwargs into</p></li>
<li><p><strong>args_chunk_spec</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><em>tuple</em></a><em>[</em><a class="reference internal" href="#torch.distributed.pipelining.microbatch.TensorChunkSpec" title="torch.distributed.pipelining.microbatch.TensorChunkSpec"><em>TensorChunkSpec</em></a><em>, </em><em>...</em><em>] </em><em>| </em><em>None</em>)  chunking specs for args, in same shape as args</p></li>
<li><p><strong>kwargs_chunk_spec</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><a class="reference internal" href="#torch.distributed.pipelining.microbatch.TensorChunkSpec" title="torch.distributed.pipelining.microbatch.TensorChunkSpec"><em>TensorChunkSpec</em></a><em>] </em><em>| </em><em>None</em>)  chunking specs for kwargs, in same shape as kwargs</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>List of sharded args
kwargs_split: List of sharded kwargs</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>args_split</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.pipelining.microbatch.merge_chunks">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.microbatch.</span></span><span class="sig-name descname"><span class="pre">merge_chunks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">chunks</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chunk_spec</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/microbatch.py#L423"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.microbatch.merge_chunks" title="Link to this definition">#</a></dt>
<dd><p>Given a list of chunks, merge them into a single value according to
the chunk spec.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>chunks</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><em>Any</em></a><em>]</em>)  list of chunks</p></li>
<li><p><strong>chunk_spec</strong>  Chunking spec for the chunks</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Merged value</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>value</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-torch.distributed.pipelining.stage">
<span id="pipeline-stages"></span><h3>Pipeline Stages<a class="headerlink" href="#module-torch.distributed.pipelining.stage" title="Link to this heading">#</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.pipelining.stage.PipelineStage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.stage.</span></span><span class="sig-name descname"><span class="pre">PipelineStage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">submodule</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage_index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_stages</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dw_builder</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/stage.py#L1321"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.stage.PipelineStage" title="Link to this definition">#</a></dt>
<dd><p>A class representing a pipeline stage in a pipeline parallelism setup.</p>
<p>PipelineStage assumes sequential partitioning of the model, i.e. the model is split into chunks where outputs from
one chunk feed into inputs of the next chunk, with no skip connections.</p>
<p>PipelineStage performs runtime shape/dtype inference automatically by propagating the outputs from stage0 to
stage1 and so forth, in linear order.  To bypass shape inference, pass the <cite>input_args</cite> and <cite>output_args</cite> to each
PipelineStage instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>submodule</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>)  The PyTorch module wrapped by this stage.</p></li>
<li><p><strong>stage_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a>)  The ID of this stage.</p></li>
<li><p><strong>num_stages</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a>)  The total number of stages.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><em>torch.device</em></a>)  The device where this stage is located.</p></li>
<li><p><strong>input_args</strong> (<em>Union</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a><em>, </em><em>Tuple</em><em>[</em><em>torch.tensor</em><em>]</em><em>]</em><em>, </em><em>optional</em>)  The input arguments for the submodule.</p></li>
<li><p><strong>output_args</strong> (<em>Union</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a><em>, </em><em>Tuple</em><em>[</em><em>torch.tensor</em><em>]</em><em>]</em><em>, </em><em>optional</em>)  The output arguments for the submodule.</p></li>
<li><p><strong>group</strong> (<em>dist.ProcessGroup</em><em>, </em><em>optional</em>)  The process group for distributed training. If None, default group.</p></li>
<li><p><strong>dw_builder</strong> (<em>Optional</em><em>[</em><em>Callable</em><em>[</em><em>[</em><em>]</em><em>, </em><em>Callable</em><em>[</em><em>...</em><em>, </em><em>None</em><em>]</em><em>]</em>)  If provided, dw_builder will build a new dw_runner function
that will the W action (input weights) for F, I, W (Fwd, Input, Weight) zero bubble schedules.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.pipelining.stage.build_stage">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.stage.</span></span><span class="sig-name descname"><span class="pre">build_stage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage_module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage_index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pipe_info</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/stage.py#L1291"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.stage.build_stage" title="Link to this definition">#</a></dt>
<dd><p>Create a pipeline stage given a stage_module to be wrapped by this stage
and pipeline information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>stage_module</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>torch.nn.Module</em></a>)  the module to be wrapped by this stage</p></li>
<li><p><strong>stage_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a>)  the index of this stage in the pipeline</p></li>
<li><p><strong>pipe_info</strong> (<em>PipeInfo</em>)  information about the pipeline, can be retrieved by <cite>pipe.info()</cite></p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><em>torch.device</em></a>)  the device to be used by this stage</p></li>
<li><p><strong>group</strong> (<em>Optional</em><em>[</em><em>dist.ProcessGroup</em><em>]</em>)  the process group to be used by this stage</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a pipeline stage that can run with <cite>PipelineSchedules</cite>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>_PipelineStage</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-torch.distributed.pipelining.schedules">
<span id="pipeline-schedules"></span><h3>Pipeline Schedules<a class="headerlink" href="#module-torch.distributed.pipelining.schedules" title="Link to this heading">#</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.pipelining.schedules.ScheduleGPipe">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.schedules.</span></span><span class="sig-name descname"><span class="pre">ScheduleGPipe</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_microbatches</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_merge_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/schedules.py#L727"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.schedules.ScheduleGPipe" title="Link to this definition">#</a></dt>
<dd><p>The GPipe schedule.
Will go through all the microbatches in a fill-drain manner.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.pipelining.schedules.Schedule1F1B">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.schedules.</span></span><span class="sig-name descname"><span class="pre">Schedule1F1B</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_microbatches</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_merge_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/schedules.py#L846"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.schedules.Schedule1F1B" title="Link to this definition">#</a></dt>
<dd><p>The 1F1B schedule.
Will perform one forward and one backward on the microbatches in steady state.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.pipelining.schedules.ScheduleInterleaved1F1B">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.schedules.</span></span><span class="sig-name descname"><span class="pre">ScheduleInterleaved1F1B</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stages</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_microbatches</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_merge_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backward_requires_autograd</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/schedules.py#L2493"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.schedules.ScheduleInterleaved1F1B" title="Link to this definition">#</a></dt>
<dd><p>The Interleaved 1F1B schedule.
See <a class="reference external" href="https://arxiv.org/pdf/2104.04473">https://arxiv.org/pdf/2104.04473</a> for details.
Will perform one forward and one backward on the microbatches in steady
state and supports multiple stages per rank. When microbatches are ready for
multiple local stages, Interleaved 1F1B prioritizes the earlier microbatch
(also called depth first).</p>
<p>This schedule is mostly similar to the original paper.
It differs by being relaxing the requirement of num_microbatch % pp_size == 0.
Using the flex_pp schedule, we will have num_rounds = max(1, n_microbatches // pp_group_size) and
it works as long as n_microbatches % num_rounds is 0. As a few examples, support</p>
<ol class="arabic simple">
<li><p>pp_group_size = 4, n_microbatches = 10. We will have num_rounds = 2 and n_microbatches % 2 is 0.</p></li>
<li><p>pp_group_size = 4, n_microbatches = 3. We will have num_rounds = 1 and n_microbatches % 1 is 0.</p></li>
</ol>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.pipelining.schedules.ScheduleLoopedBFS">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.schedules.</span></span><span class="sig-name descname"><span class="pre">ScheduleLoopedBFS</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stages</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_microbatches</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_merge_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backward_requires_autograd</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/schedules.py#L2287"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.schedules.ScheduleLoopedBFS" title="Link to this definition">#</a></dt>
<dd><p>Breadth-First Pipeline Parallelism.
See <a class="reference external" href="https://arxiv.org/abs/2211.05953">https://arxiv.org/abs/2211.05953</a> for details.
Similar to Interleaved 1F1B, Looped BFS supports multiple stages per rank.
What is different is that when microbatches are ready for multiple local
stages, Loops BFS will prioritizes the earlier stage, running all available
microbatches at once.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.pipelining.schedules.ScheduleInterleavedZeroBubble">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.schedules.</span></span><span class="sig-name descname"><span class="pre">ScheduleInterleavedZeroBubble</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stages</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_microbatches</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_merge_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backward_requires_autograd</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/schedules.py#L2614"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.schedules.ScheduleInterleavedZeroBubble" title="Link to this definition">#</a></dt>
<dd><p>The Interleaved Zero Bubble schedule.
See <a class="reference external" href="https://arxiv.org/pdf/2401.10241">https://arxiv.org/pdf/2401.10241</a> for details.
Will perform one forward and one backward on inputs for the microbatches in steady
state and supports multiple stages per rank. Uses the backward for weights to fill in
the pipeline bubble.</p>
<p>In particular this is implementing the ZB1P schedule in the paper.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.pipelining.schedules.ScheduleZBVZeroBubble">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.schedules.</span></span><span class="sig-name descname"><span class="pre">ScheduleZBVZeroBubble</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stages</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_microbatches</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_merge_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backward_requires_autograd</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/schedules.py#L2808"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.schedules.ScheduleZBVZeroBubble" title="Link to this definition">#</a></dt>
<dd><p>The Zero Bubble schedule (ZBV variant).
See <a class="reference external" href="https://arxiv.org/pdf/2401.10241">https://arxiv.org/pdf/2401.10241</a> Section 6 for details.</p>
<p>This schedules requires exactly two stages per rank.</p>
<p>This schedule will perform one forward and one backward on inputs for the microbatches in steady
state and supports multiple stages per rank. Uses backward with respect to weights to fill in
the pipeline bubble.</p>
<p>This ZB-V schedule would have the zero bubble property only if time forward == time backward input == time backward weights.
In practice, this is not likely true for real models so alternatively
a greedy scheduler could be implemented for unequal/unbalanced time.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.pipelining.schedules.ScheduleDualPipeV">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.schedules.</span></span><span class="sig-name descname"><span class="pre">ScheduleDualPipeV</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stages</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_microbatches</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_merge_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backward_requires_autograd</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/schedules.py#L2994"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.schedules.ScheduleDualPipeV" title="Link to this definition">#</a></dt>
<dd><p>The DualPipeV schedule. A more efficient schedule variant based on the
DualPipe schedule introduced by DeepSeek in <a class="reference external" href="https://arxiv.org/pdf/2412.19437">https://arxiv.org/pdf/2412.19437</a></p>
<p>Based on the open sourced code from <a class="github reference external" href="https://github.com/deepseek-ai/DualPipe">deepseek-ai/DualPipe</a></p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.pipelining.schedules.PipelineScheduleSingle">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.schedules.</span></span><span class="sig-name descname"><span class="pre">PipelineScheduleSingle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_microbatches</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_merge_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/schedules.py#L543"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.schedules.PipelineScheduleSingle" title="Link to this definition">#</a></dt>
<dd><p>Base class for single-stage schedules.
Implements the <cite>step</cite> method.
Derived classes should implement <cite>_step_microbatches</cite>.</p>
<p>Gradients are scaled by num_microbatches depending on the <cite>scale_grads</cite> argument, defaulting to True.  This setting
should match the configuration of your loss_fn, which may either average losses (scale_grads=True)
or sum losses (scale_grads=False).</p>
<dl class="field-list simple">
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.pipelining.schedules.PipelineScheduleSingle.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">losses</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_outputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/schedules.py#L605"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.schedules.PipelineScheduleSingle.step" title="Link to this definition">#</a></dt>
<dd><p>Run one iteration of the pipeline schedule with <em>whole-batch</em> input.
Will chunk the input into microbatches automatically, and go through the
microbatches according to the schedule implementation.</p>
<p>args: positional arguments to the model (as in non-pipeline case).
kwargs: keyword arguments to the model (as in non-pipeline case).
target: target for the loss function.
losses: a list to store the losses for each microbatch.
return_outputs: whether to return the outputs from the last stage.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.pipelining.schedules.PipelineScheduleMulti">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.pipelining.schedules.</span></span><span class="sig-name descname"><span class="pre">PipelineScheduleMulti</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stages</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_microbatches</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_chunk_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_merge_spec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_full_backward</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backward_requires_autograd</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/schedules.py#L1462"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.schedules.PipelineScheduleMulti" title="Link to this definition">#</a></dt>
<dd><p>Base class for multi-stage schedules.
Implements the <cite>step</cite> method.</p>
<p>Gradients are scaled by num_microbatches depending on the <cite>scale_grads</cite> argument, defaulting to True.  This setting
should match the configuration of your loss_fn, which may either average losses (scale_grads=True)
or sum losses (scale_grads=False).</p>
<dl class="field-list simple">
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.pipelining.schedules.PipelineScheduleMulti.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">losses</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_outputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.10.0/torch/distributed/pipelining/schedules.py#L1593"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.pipelining.schedules.PipelineScheduleMulti.step" title="Link to this definition">#</a></dt>
<dd><p>Run one iteration of the pipeline schedule with <em>whole-batch</em> input.
Will chunk the input into microbatches automatically, and go through the
microbatches according to the schedule implementation.</p>
<p>args: positional arguments to the model (as in non-pipeline case).
kwargs: keyword arguments to the model (as in non-pipeline case).
target: target for the loss function.
losses: a list to store the losses for each microbatch.
return_outputs: whether to return the outputs from the last stage.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5"></span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="distributed.optim.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Distributed Optimizers</p>
      </div>
    </a>
    <a class="right-next"
       href="symmetric_memory.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">PyTorch Symmetric Memory</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="distributed.optim.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Distributed Optimizers</p>
      </div>
    </a>
    <a class="right-next"
       href="symmetric_memory.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">PyTorch Symmetric Memory</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-pipeline-parallel">Why Pipeline Parallel?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-torch-distributed-pipelining">What is <code class="docutils literal notranslate"><span class="pre">torch.distributed.pipelining</span></code>?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-build-pipelinestage">Step 1: build <code class="docutils literal notranslate"><span class="pre">PipelineStage</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-use-pipelineschedule-for-execution">Step 2: use <code class="docutils literal notranslate"><span class="pre">PipelineSchedule</span></code> for execution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#options-for-splitting-a-model">Options for Splitting a Model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#option-1-splitting-a-model-manually">Option 1: splitting a model manually</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#option-2-splitting-a-model-automatically">Option 2: splitting a model automatically</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face-examples">Hugging Face Examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#technical-deep-dive">Technical Deep Dive</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-the-pipeline-api-split-a-model">How does the <code class="docutils literal notranslate"><span class="pre">pipeline</span></code> API split a model?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-your-own-schedule">Implementing Your Own Schedule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logging">Logging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.distributed.pipelining">API Reference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-split-apis">Model Split APIs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.SplitPoint"><code class="docutils literal notranslate"><span class="pre">SplitPoint</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.pipeline"><code class="docutils literal notranslate"><span class="pre">pipeline()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.Pipe"><code class="docutils literal notranslate"><span class="pre">Pipe</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.pipe_split"><code class="docutils literal notranslate"><span class="pre">pipe_split()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.distributed.pipelining.microbatch">Microbatch Utilities</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.microbatch.TensorChunkSpec"><code class="docutils literal notranslate"><span class="pre">TensorChunkSpec</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.microbatch.split_args_kwargs_into_chunks"><code class="docutils literal notranslate"><span class="pre">split_args_kwargs_into_chunks()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.microbatch.merge_chunks"><code class="docutils literal notranslate"><span class="pre">merge_chunks()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.distributed.pipelining.stage">Pipeline Stages</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.stage.PipelineStage"><code class="docutils literal notranslate"><span class="pre">PipelineStage</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.stage.build_stage"><code class="docutils literal notranslate"><span class="pre">build_stage()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#module-torch.distributed.pipelining.schedules">Pipeline Schedules</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.schedules.ScheduleGPipe"><code class="docutils literal notranslate"><span class="pre">ScheduleGPipe</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.schedules.Schedule1F1B"><code class="docutils literal notranslate"><span class="pre">Schedule1F1B</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.schedules.ScheduleInterleaved1F1B"><code class="docutils literal notranslate"><span class="pre">ScheduleInterleaved1F1B</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.schedules.ScheduleLoopedBFS"><code class="docutils literal notranslate"><span class="pre">ScheduleLoopedBFS</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.schedules.ScheduleInterleavedZeroBubble"><code class="docutils literal notranslate"><span class="pre">ScheduleInterleavedZeroBubble</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.schedules.ScheduleZBVZeroBubble"><code class="docutils literal notranslate"><span class="pre">ScheduleZBVZeroBubble</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.schedules.ScheduleDualPipeV"><code class="docutils literal notranslate"><span class="pre">ScheduleDualPipeV</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.schedules.PipelineScheduleSingle"><code class="docutils literal notranslate"><span class="pre">PipelineScheduleSingle</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.schedules.PipelineScheduleSingle.step"><code class="docutils literal notranslate"><span class="pre">PipelineScheduleSingle.step()</span></code></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.schedules.PipelineScheduleMulti"><code class="docutils literal notranslate"><span class="pre">PipelineScheduleMulti</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#torch.distributed.pipelining.schedules.PipelineScheduleMulti.step"><code class="docutils literal notranslate"><span class="pre">PipelineScheduleMulti.step()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>
    
       <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/pytorch/pytorch/edit/main/docs/source/distributed.pipelining.md">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="_sources/distributed.pipelining.md.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright  The Linux Foundation. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
       Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Pipeline Parallelism",
       "headline": "Pipeline Parallelism",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/distributed.pipelining.html",
       "articleBody": "Pipeline Parallelism# Created On: Jun 16, 2025 | Last Updated On: Aug 13, 2025 Note torch.distributed.pipelining is currently in alpha state and under development. API changes may be possible. It was migrated from the PiPPy project. Why Pipeline Parallel?# Pipeline Parallelism is one of the primitive parallelism for deep learning. It allows the execution of a model to be partitioned such that multiple micro-batches can execute different parts of the model code concurrently. Pipeline parallelism can be an effective technique for: large-scale training bandwidth-limited clusters large model inference The above scenarios share a commonality that the computation per device cannot hide the communication of conventional parallelism, for example, the weight all-gather of FSDP. What is torch.distributed.pipelining?# While promising for scaling, pipelining is often difficult to implement because it needs to partition the execution of a model in addition to model weights. The partitioning of execution often requires intrusive code changes to your model. Another aspect of complexity comes from scheduling micro-batches in a distributed environment, with data flow dependency considered. The pipelining package provides a toolkit that does said things automatically which allows easy implementation of pipeline parallelism on general models. It consists of two parts: a splitting frontend and a distributed runtime. The splitting frontend takes your model code as-is, splits it up into \u201cmodel partitions\u201d, and captures the data-flow relationship. The distributed runtime executes the pipeline stages on different devices in parallel, handling things like micro-batch splitting, scheduling, communication, and gradient propagation, etc. Overall, the pipelining package provides the following features: Splitting of model code based on simple specification. Rich support for pipeline schedules, including GPipe, 1F1B, Interleaved 1F1B and Looped BFS, and providing the infrastructure for writing customized schedules. First-class support for cross-host pipeline parallelism, as this is where PP is typically used (over slower interconnects). Composability with other PyTorch parallel techniques such as data parallel (DDP, FSDP) or tensor parallel. The TorchTitan project demonstrates a \u201c3D parallel\u201d application on the Llama model. Step 1: build PipelineStage# Before we can use a PipelineSchedule, we need to create PipelineStage objects that wrap the part of the model running in that stage. The PipelineStage is responsible for allocating communication buffers and creating send/recv ops to communicate with its peers. It manages intermediate buffers e.g. for the outputs of forward that have not been consumed yet, and it provides a utility for running the backwards for the stage model. A PipelineStage needs to know the input and output shapes for the stage model, so that it can correctly allocate communication buffers. The shapes must be static, e.g. at runtime the shapes can not change from step to step. A class PipeliningShapeError will be raised if runtime shapes do not match the expected shapes. When composing with other paralleisms or applying mixed precision, these techniques must be taken into account so the PipelineStage knows the correct shape (and dtype) for the output of the stage module at runtime. Users may construct a PipelineStage instance directly, by passing in an nn.Module representing the portion of the model that should run on the stage. This may require changes to the original model code. See the example in Option 1: splitting a model manually. Alternatively, the splitting frontend can use graph partitioning to split your model into a series of nn.Module automatically. This technique requires the model is traceable with torch.Export. Composability of the resulting nn.Module with other parallelism techniques is experimental, and may require some workarounds. Usage of this frontend may be more appealing if the user cannot easily change the model code. See Option 2: splitting a model automatically for more information. Step 2: use PipelineSchedule for execution# We can now attach the PipelineStage to a pipeline schedule, and run the schedule with input data. Here is a GPipe example: from torch.distributed.pipelining import ScheduleGPipe # Create a schedule schedule = ScheduleGPipe(stage, n_microbatches) # Input data (whole batch) x = torch.randn(batch_size, in_dim, device=device) # Run the pipeline with input `x` # `x` will be divided into microbatches automatically if rank == 0: schedule.step(x) else: output = schedule.step() Note that the above code needs to be launched for each worker, thus we use a launcher service to launch multiple processes: torchrun --nproc_per_node=2 example.py Options for Splitting a Model# Option 1: splitting a model manually# To directly construct a PipelineStage, the user is responsible for providing a single nn.Module instance that owns the relevant nn.Parameters and nn.Buffers, and defines a forward() method that executes the operations relevant for that stage. For example, a condensed version of the Transformer class defined in Torchtitan shows a pattern of building an easily partitionable model. class Transformer(nn.Module): def __init__(self, model_args: ModelArgs): super().__init__() self.tok_embeddings = nn.Embedding(...) # Using a ModuleDict lets us delete layers without affecting names, # ensuring checkpoints will correctly save and load. self.layers = torch.nn.ModuleDict() for layer_id in range(model_args.n_layers): self.layers[str(layer_id)] = TransformerBlock(...) self.output = nn.Linear(...) def forward(self, tokens: torch.Tensor): # Handling layers being \u0027None\u0027 at runtime enables easy pipeline splitting h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens for layer in self.layers.values(): h = layer(h, self.freqs_cis) h = self.norm(h) if self.norm else h output = self.output(h).float() if self.output else h return output A model defined in this manner can be easily configured per stage by first initializing the whole model (using meta-device to avoid OOM errors), deleting undesired layers for that stage, and then creating a PipelineStage that wraps the model. For example: with torch.device(\"meta\"): assert num_stages == 2, \"This is a simple 2-stage example\" # we construct the entire model, then delete the parts we do not need for this stage # in practice, this can be done using a helper function that automatically divides up layers across stages. model = Transformer() if stage_index == 0: # prepare the first stage model del model.layers[\"1\"] model.norm = None model.output = None elif stage_index == 1: # prepare the second stage model model.tok_embeddings = None del model.layers[\"0\"] from torch.distributed.pipelining import PipelineStage stage = PipelineStage( model, stage_index, num_stages, device, ) When composing with other Data or Model parallelism techniques, output_args may also be required, if the output shape/dtype of the model chunk will be affected. Option 2: splitting a model automatically# If you have a full model and do not want to spend time on modifying it into a sequence of \u201cmodel partitions\u201d, the pipeline API is here to help. Here is a brief example: class Model(torch.nn.Module): def __init__(self) -\u003e None: super().__init__() self.emb = torch.nn.Embedding(10, 3) self.layers = torch.nn.ModuleList( Layer() for _ in range(2) ) self.lm = LMHead() def forward(self, x: torch.Tensor) -\u003e torch.Tensor: x = self.emb(x) for layer in self.layers: x = layer(x) x = self.lm(x) return x If we print the model, we can see multiple hierarchies, which makes it hard to split by hand: Model( (emb): Embedding(10, 3) (layers): ModuleList( (0-1): 2 x Layer( (lin): Linear(in_features=3, out_features=3, bias=True) ) ) (lm): LMHead( (proj): Linear(in_features=3, out_features=3, bias=True) ) ) Let us see how the pipeline API works: from torch.distributed.pipelining import pipeline, SplitPoint # An example micro-batch input x = torch.LongTensor([1, 2, 4, 5]) pipe = pipeline( module=mod, mb_args=(x,), split_spec={ \"layers.1\": SplitPoint.BEGINNING, } ) The pipeline API splits your model given a split_spec, where SplitPoint.BEGINNING stands for adding a split point before execution of certain submodule in the forward function, and similarly, SplitPoint.END for split point after such. If we print(pipe), we can see: GraphModule( (submod_0): GraphModule( (emb): InterpreterModule() (layers): Module( (0): InterpreterModule( (lin): InterpreterModule() ) ) ) (submod_1): GraphModule( (layers): Module( (1): InterpreterModule( (lin): InterpreterModule() ) ) (lm): InterpreterModule( (proj): InterpreterModule() ) ) ) def forward(self, x): submod_0 = self.submod_0(x); x = None submod_1 = self.submod_1(submod_0); submod_0 = None return (submod_1,) The \u201cmodel partitions\u201d are represented by submodules (submod_0, submod_1), each of which is reconstructed with original model operations, weights and hierarchies. In addition, a \u201croot-level\u201d forward function is reconstructed to capture the data flow between those partitions. Such data flow will be replayed by the pipeline runtime later, in a distributed fashion. The Pipe object provides a method for retrieving the \u201cmodel partitions\u201d: stage_mod : nn.Module = pipe.get_stage_module(stage_idx) The returned stage_mod is a nn.Module, with which you can create an optimizer, save or load checkpoints, or apply other parallelisms. Pipe also allows you to create a distributed stage runtime on a device given a ProcessGroup: stage = pipe.build_stage(stage_idx, device, group) Alternatively, if you would like to build the stage runtime later after some modification to the stage_mod, you can use a functional version of the build_stage API. For example: from torch.distributed.pipelining import build_stage from torch.nn.parallel import DistributedDataParallel dp_mod = DistributedDataParallel(stage_mod) info = pipe.info() stage = build_stage(dp_mod, stage_idx, info, device, group) Note The pipeline frontend uses a tracer (torch.export) to capture your model into a single graph. If your model is not full-graph\u2019able, you can use our manual frontend below. Hugging Face Examples# In the PiPPy repo where this package was original created, we kept examples based on unmodified Hugging Face models. See the examples/huggingface directory. Examples include: GPT2 Llama Technical Deep Dive# How does the pipeline API split a model?# First, the pipeline API turns our model into a directed acyclic graph (DAG) by tracing the model. It traces the model using torch.export \u2013 a PyTorch 2 full-graph capturing tool. Then, it groups together the operations and parameters needed by a stage into a reconstructed submodule: submod_0, submod_1, \u2026 Different from conventional submodule access methods like Module.children(), the pipeline API does not only cut the module structure of your model, but also the forward function of your model. This is necessary because model structure like Module.children() merely captures information during Module.__init__(), and does not capture any information about Module.forward(). Said differently, Module.children() lacks information about the following aspects key to pipelininig: Execution order of child modules in forward Activation flows between child modules Whether there are any functional operators between child modules (for example, relu or add operations will not be captured by Module.children()). The pipeline API, on the contrary, makes sure that the forward behavior is truly preserved. It also captures the activation flow between the partitions, helping the distributed runtime to make correct send/receive calls without human intervention. Another flexibility of the pipeline API is that split points can be at arbitrary levels within your model hierarchy. In the split partitions, the original model hierarchy related to that partition will be reconstructed at no cost to you. At a result, fully-qualified names (FQNs) pointing to a submodule or parameter would be still valid, and services that relies on FQNs (such as FSDP, TP or checkpointing) can still run with your partitioned modules with almost zero code change. Implementing Your Own Schedule# You can implement your own pipeline schedule by extending one of the following two class: PipelineScheduleSingle PipelineScheduleMulti PipelineScheduleSingle is for schedules that assigns only one stage per rank. PipelineScheduleMulti is for schedules that assigns multiple stages per rank. For example, ScheduleGPipe and Schedule1F1B are subclasses of PipelineScheduleSingle. Whereas, ScheduleInterleaved1F1B, ScheduleLoopedBFS, ScheduleInterleavedZeroBubble, and ScheduleZBVZeroBubble are subclasses of PipelineScheduleMulti. Logging# You can turn on additional logging using the TORCH_LOGS environment variable from torch._logging: TORCH_LOGS=+pp will display logging.DEBUG messages and all levels above it. TORCH_LOGS=pp will display logging.INFO messages and above. TORCH_LOGS=-pp will display logging.WARNING messages and above. API Reference# Model Split APIs# The following set of APIs transform your model into a pipeline representation. class torch.distributed.pipelining.SplitPoint(value)[source]# Enum representing the points at which a split can occur in the execution of a submodule. :ivar BEGINNING: Represents adding a split point before the execution of a certain submodule in the forward function. :ivar END: Represents adding a split point after the execution of a certain submodule in the forward function. torch.distributed.pipelining.pipeline(module, mb_args, mb_kwargs=None, split_spec=None, split_policy=None)[source]# Split a module based on a specification. See Pipe for more details. Parameters: module (Module) \u2013 The module to be split. mb_args (tuple[Any, ...]) \u2013 Example positional inputs, in micro-batch form. mb_kwargs (dict[str, Any] | None) \u2013 Example keyword inputs, in micro-batch form. (default: None) split_spec (dict[str, SplitPoint] | None) \u2013 A dictionary using submodule names as split marker. (default: None) split_policy (Callable[[GraphModule], GraphModule] | None) \u2013 The policy to use for splitting the module. (default: None) Return type: A pipeline representation of class Pipe. class torch.distributed.pipelining.Pipe(split_gm, num_stages, has_loss_and_backward, loss_spec)[source]# torch.distributed.pipelining.pipe_split()[source]# pipe_split is a special operator that is used to mark the boundary between stages in a module. It is used to split the module into stages. It is a no-op if your annotated module is run eagerly. Example \u003e\u003e\u003e def forward(self, x): \u003e\u003e\u003e x = torch.mm(x, self.mm_param) \u003e\u003e\u003e x = torch.relu(x) \u003e\u003e\u003e pipe_split() \u003e\u003e\u003e x = self.lin(x) \u003e\u003e\u003e return x The above example will be split into two stages. Microbatch Utilities# class torch.distributed.pipelining.microbatch.TensorChunkSpec(split_dim)[source]# Class used to specify chunking of inputs torch.distributed.pipelining.microbatch.split_args_kwargs_into_chunks(args, kwargs, chunks, args_chunk_spec=None, kwargs_chunk_spec=None)[source]# Given a sequence of args and kwargs, split them into a number of chunks according to their respective chunking specs. Parameters: args (tuple[Any, ...]) \u2013 Tuple of args kwargs (dict[str, Any] | None) \u2013 Dict of kwargs chunks (int) \u2013 Number of chunks to split the args and kwargs into args_chunk_spec (tuple[TensorChunkSpec, ...] | None) \u2013 chunking specs for args, in same shape as args kwargs_chunk_spec (dict[str, TensorChunkSpec] | None) \u2013 chunking specs for kwargs, in same shape as kwargs Returns: List of sharded args kwargs_split: List of sharded kwargs Return type: args_split torch.distributed.pipelining.microbatch.merge_chunks(chunks, chunk_spec)[source]# Given a list of chunks, merge them into a single value according to the chunk spec. Parameters: chunks (list[Any]) \u2013 list of chunks chunk_spec \u2013 Chunking spec for the chunks Returns: Merged value Return type: value Pipeline Stages# class torch.distributed.pipelining.stage.PipelineStage(submodule, stage_index, num_stages, device, input_args=None, output_args=None, group=None, dw_builder=None)[source]# A class representing a pipeline stage in a pipeline parallelism setup. PipelineStage assumes sequential partitioning of the model, i.e. the model is split into chunks where outputs from one chunk feed into inputs of the next chunk, with no skip connections. PipelineStage performs runtime shape/dtype inference automatically by propagating the outputs from stage0 to stage1 and so forth, in linear order. To bypass shape inference, pass the input_args and output_args to each PipelineStage instance. Parameters: submodule (nn.Module) \u2013 The PyTorch module wrapped by this stage. stage_index (int) \u2013 The ID of this stage. num_stages (int) \u2013 The total number of stages. device (torch.device) \u2013 The device where this stage is located. input_args (Union[torch.Tensor, Tuple[torch.tensor]], optional) \u2013 The input arguments for the submodule. output_args (Union[torch.Tensor, Tuple[torch.tensor]], optional) \u2013 The output arguments for the submodule. group (dist.ProcessGroup, optional) \u2013 The process group for distributed training. If None, default group. dw_builder (Optional[Callable[[], Callable[..., None]]) \u2013 If provided, dw_builder will build a new dw_runner function that will the W action (input weights) for F, I, W (Fwd, Input, Weight) zero bubble schedules. torch.distributed.pipelining.stage.build_stage(stage_module, stage_index, pipe_info, device, group=None)[source]# Create a pipeline stage given a stage_module to be wrapped by this stage and pipeline information. Parameters: stage_module (torch.nn.Module) \u2013 the module to be wrapped by this stage stage_index (int) \u2013 the index of this stage in the pipeline pipe_info (PipeInfo) \u2013 information about the pipeline, can be retrieved by pipe.info() device (torch.device) \u2013 the device to be used by this stage group (Optional[dist.ProcessGroup]) \u2013 the process group to be used by this stage Returns: a pipeline stage that can run with PipelineSchedules. Return type: _PipelineStage Pipeline Schedules# class torch.distributed.pipelining.schedules.ScheduleGPipe(stage, n_microbatches, loss_fn=None, args_chunk_spec=None, kwargs_chunk_spec=None, output_merge_spec=None, scale_grads=True)[source]# The GPipe schedule. Will go through all the microbatches in a fill-drain manner. class torch.distributed.pipelining.schedules.Schedule1F1B(stage, n_microbatches, loss_fn=None, args_chunk_spec=None, kwargs_chunk_spec=None, output_merge_spec=None, scale_grads=True)[source]# The 1F1B schedule. Will perform one forward and one backward on the microbatches in steady state. class torch.distributed.pipelining.schedules.ScheduleInterleaved1F1B(stages, n_microbatches, loss_fn=None, args_chunk_spec=None, kwargs_chunk_spec=None, output_merge_spec=None, scale_grads=True, backward_requires_autograd=True)[source]# The Interleaved 1F1B schedule. See https://arxiv.org/pdf/2104.04473 for details. Will perform one forward and one backward on the microbatches in steady state and supports multiple stages per rank. When microbatches are ready for multiple local stages, Interleaved 1F1B prioritizes the earlier microbatch (also called \u201cdepth first\u201d). This schedule is mostly similar to the original paper. It differs by being relaxing the requirement of num_microbatch % pp_size == 0. Using the flex_pp schedule, we will have num_rounds = max(1, n_microbatches // pp_group_size) and it works as long as n_microbatches % num_rounds is 0. As a few examples, support pp_group_size = 4, n_microbatches = 10. We will have num_rounds = 2 and n_microbatches % 2 is 0. pp_group_size = 4, n_microbatches = 3. We will have num_rounds = 1 and n_microbatches % 1 is 0. class torch.distributed.pipelining.schedules.ScheduleLoopedBFS(stages, n_microbatches, loss_fn=None, output_merge_spec=None, scale_grads=True, backward_requires_autograd=True)[source]# Breadth-First Pipeline Parallelism. See https://arxiv.org/abs/2211.05953 for details. Similar to Interleaved 1F1B, Looped BFS supports multiple stages per rank. What is different is that when microbatches are ready for multiple local stages, Loops BFS will prioritizes the earlier stage, running all available microbatches at once. class torch.distributed.pipelining.schedules.ScheduleInterleavedZeroBubble(stages, n_microbatches, loss_fn=None, args_chunk_spec=None, kwargs_chunk_spec=None, output_merge_spec=None, scale_grads=True, backward_requires_autograd=True)[source]# The Interleaved Zero Bubble schedule. See https://arxiv.org/pdf/2401.10241 for details. Will perform one forward and one backward on inputs for the microbatches in steady state and supports multiple stages per rank. Uses the backward for weights to fill in the pipeline bubble. In particular this is implementing the ZB1P schedule in the paper. class torch.distributed.pipelining.schedules.ScheduleZBVZeroBubble(stages, n_microbatches, loss_fn=None, args_chunk_spec=None, kwargs_chunk_spec=None, output_merge_spec=None, scale_grads=True, backward_requires_autograd=True)[source]# The Zero Bubble schedule (ZBV variant). See https://arxiv.org/pdf/2401.10241 Section 6 for details. This schedules requires exactly two stages per rank. This schedule will perform one forward and one backward on inputs for the microbatches in steady state and supports multiple stages per rank. Uses backward with respect to weights to fill in the pipeline bubble. This ZB-V schedule would have the \u201czero bubble\u201d property only if time forward == time backward input == time backward weights. In practice, this is not likely true for real models so alternatively a greedy scheduler could be implemented for unequal/unbalanced time. class torch.distributed.pipelining.schedules.ScheduleDualPipeV(stages, n_microbatches, loss_fn=None, args_chunk_spec=None, kwargs_chunk_spec=None, output_merge_spec=None, scale_grads=True, backward_requires_autograd=True)[source]# The DualPipeV schedule. A more efficient schedule variant based on the DualPipe schedule introduced by DeepSeek in https://arxiv.org/pdf/2412.19437 Based on the open sourced code from deepseek-ai/DualPipe class torch.distributed.pipelining.schedules.PipelineScheduleSingle(stage, n_microbatches, loss_fn=None, args_chunk_spec=None, kwargs_chunk_spec=None, output_merge_spec=None, scale_grads=True)[source]# Base class for single-stage schedules. Implements the step method. Derived classes should implement _step_microbatches. Gradients are scaled by num_microbatches depending on the scale_grads argument, defaulting to True. This setting should match the configuration of your loss_fn, which may either average losses (scale_grads=True) or sum losses (scale_grads=False). step(*args, target=None, losses=None, return_outputs=True, **kwargs)[source]# Run one iteration of the pipeline schedule with whole-batch input. Will chunk the input into microbatches automatically, and go through the microbatches according to the schedule implementation. args: positional arguments to the model (as in non-pipeline case). kwargs: keyword arguments to the model (as in non-pipeline case). target: target for the loss function. losses: a list to store the losses for each microbatch. return_outputs: whether to return the outputs from the last stage. class torch.distributed.pipelining.schedules.PipelineScheduleMulti(stages, n_microbatches, loss_fn=None, args_chunk_spec=None, kwargs_chunk_spec=None, output_merge_spec=None, use_full_backward=None, scale_grads=True, backward_requires_autograd=True)[source]# Base class for multi-stage schedules. Implements the step method. Gradients are scaled by num_microbatches depending on the scale_grads argument, defaulting to True. This setting should match the configuration of your loss_fn, which may either average losses (scale_grads=True) or sum losses (scale_grads=False). step(*args, target=None, losses=None, return_outputs=True, **kwargs)[source]# Run one iteration of the pipeline schedule with whole-batch input. Will chunk the input into microbatches automatically, and go through the microbatches according to the schedule implementation. args: positional arguments to the model (as in non-pipeline case). kwargs: keyword arguments to the model (as in non-pipeline case). target: target for the loss function. losses: a list to store the losses for each microbatch. return_outputs: whether to return the outputs from the last stage.",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/distributed.pipelining.html"
       },
       "datePublished": "Jun 16, 2025T00:00:00Z",
       "dateModified": "Aug 13, 2025T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>